FASE 6 – v1_baseline (Modelo sin Recuperación)
---------------------------------------------

El objetivo de esta fase fue implementar y evaluar un sistema base de preguntas y respuestas sobre documentos académicos, 
sin utilizar ningún mecanismo de recuperación de información. Esta versión, denominada v1_baseline, sirve como línea base 
para comparar el impacto de los sistemas RAG que se implementarán en fases posteriores.

En v1_baseline, el modelo de lenguaje recibe únicamente la pregunta del usuario, sin acceso al contenido de los documentos 
PDF ni a fragmentos extraídos previamente. Las respuestas se generan exclusivamente a partir del conocimiento interno del 
modelo, sin ningún tipo de evidencia documental.

==================================================
1. Implementación del sistema baseline
==================================================
Se creó el módulo:
    src/v1_baseline/

Dentro de este módulo se implementó un sistema que:
    - recibe una pregunta en lenguaje natural,
    - construye un prompt mínimo sin contexto externo,
    - genera una respuesta directamente desde el modelo de lenguaje.

El sistema utiliza el modelo de lenguaje Google/Flan-T5 y Qwen/Qwen2.5-1.5B-Instruct como modelo base para la generación 
de respuestas.

El prompt está compuesto por:
    - una instrucción general al modelo, que define el rol de asistente académico,
    - la pregunta del usuario seguida de un marcador de respuesta.

No se incluyen reglas explícitas de verificación, citación de fuentes ni mecanismos de abstención ante preguntas no respondibles.
No se utiliza el índice FAISS, ni embeddings, ni fragmentos de texto. 
El sistema no tiene conocimiento explícito de los documentos del dataset.
Para reducir la variabilidad en la generación, se fija una semilla aleatoria para reducir variabilidad a nivel del script,
aunque el modelo de lenguaje no es completamente determinista

==================================================
2. Evaluación sobre el dataset de preguntas
==================================================
Ejecución:
    python -m src.v1_baseline.run_baseline_eval

El sistema v1_baseline fue evaluado utilizando el conjunto de preguntas asociadas a los documentos del dataset, incluyendo:
    - preguntas factuales (respondibles desde el documento),
    - preguntas imposibles (no respondibles desde el documento).

Para cada pregunta, el sistema genera una respuesta sin acceso al documento correspondiente. Las respuestas producidas se 
almacenan en un archivo JSON que incluye, para cada entrada:
    - identificador de la pregunta,
    - identificador del documento asociado,
    - texto de la pregunta,
    - tipo de pregunta,
    - respuesta generada por el modelo.

Estos resultados se utilizan posteriormente para el análisis comparativo con versiones más avanzadas del sistema.

==================================================
3. Resultados observados
==================================================
Los resultados muestran que el modelo:
    - genera una respuesta para todas las preguntas evaluadas,
    - responde tanto a preguntas factuales como a preguntas imposibles,
    - produce respuestas plausibles pero no verificadas,
    - no distingue entre preguntas respondibles y no respondibles,
    - puede incurrir en alucinaciones al afirmar información inexistente.

En particular, ante preguntas imposibles, el modelo no se abstiene, sino que genera respuestas incorrectas o especulativas, 
presentadas con un alto grado de confianza.

==================================================
4. Conclusiones de la fase
==================================================
Esta fase evidencia claramente las limitaciones de un modelo de lenguaje sin acceso a información documental ni mecanismos 
de control. Si bien el sistema es capaz de producir respuestas lingüísticamente coherentes, no puede garantizar su corrección 
ni su fundamentación en los documentos fuente.

Así mismo demuestra empíricamente que, en ausencia de recuperación documental y verificación, los modelos de lenguaje tienden 
a generar respuestas no fundamentadas incluso ante preguntas no respondibles, lo que motiva el uso de RAG y mecanismos de 
abstención.

Los resultados justifican la necesidad de incorporar mecanismos de recuperación de contexto (RAG), citación de evidencia y 
estrategias de abstención, los cuales se abordarán en las siguientes fases del proyecto.

El sistema v1_baseline se utiliza como referencia cuantitativa y cualitativa para comparar el desempeño de las versiones 
v2_rag_basic y v3_rag_advanced.