[
  {
    "question_id": "q1",
    "doc_id": "arxiv_251210922_sparseswaps",
    "question": "What is SparseSwaps?",
    "type": "fact",
    "answer": "SparseSwaps is a partial sparse sparsity optimization algorithm. It is a warm-start agnostic, nearly hyperparameter-free, and scalable. It consistently reduces perplexity and improves zero-shot accuracy across modern GPT architectures."
  },
  {
    "question_id": "q2",
    "doc_id": "arxiv_251210922_sparseswaps",
    "question": "What problem does SparseSwaps solve?",
    "type": "fact",
    "answer": "SparseSwaps is a partial optimization problem for sparsity sparsity. It is a warm-start agnostic, nearly hyperparameter-free, and scalable. It consistently reduces per-layer pruning error and improves perplexity and zero-shot accuracy across modern GPT architectures."
  },
  {
    "question_id": "q3",
    "doc_id": "arxiv_251210922_sparseswaps",
    "question": "Does the paper mention quantum computing?",
    "type": "impossible",
    "answer": "The paper discusses the development of a language model called LIMA (Less Is More for Alignment). The paper discusses the development of a language model called LIMA (Less Is More for Alignment). The paper discusses the development of a language model called LIMA (Less Is More for Alignment). The paper discusses the development of a language model called LIMA (Less Is More for Alignment). The paper discusses the development of a language model called LIMA (Less Is More for Alignment). The paper discusses the development of a language model called LIMA (Less Is More for Alignment). The paper discusses the development of a language model called LIMA (Less Is More for Alignment). The paper discusses the development of a language model called LIMA (Less Is More for Alignment). The paper discusses the development of a language model called LIMA (Less Is More for Alignment). The paper discusses the development of a language model called LIMA (Less Is More"
  }
]