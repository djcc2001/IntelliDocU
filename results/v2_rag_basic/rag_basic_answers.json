[
  {
    "question_id": "q1",
    "doc_id": "arxiv_251210922_sparseswaps",
    "question": "What is SparseSwaps?",
    "type": "fact",
    "answer": "You are an assistant that answers questions using ONLY the provided context.\nIf the answer is not contained in the context, say you do not know.\n\nContext:\n[FRAGMENT 1] (doc:arxiv_251210922_sparseswaps, page:5, frag:2):\nwhen selecting ( p, u ). 2. 2 the sparseswaps algorithm building upon the preceding observations, we present our complete algorithm. the method takes as input a weight matrix w ∈rdout×din, the gram matrix g = [UNK] ( accumulated during calibration ), and a warmstart pruning mask m init ∈ { 0, 1 } dout×din that already satisfies the desired sparsity constraints, e. g., obtained from wanda ( sun et al., 2024 ) or ria ( zhang et al., 2024 ). the algorithm enforces any sparsity pattern that operates per - row, including per - row sparsity ( fixed number of zeros per row, cf. sun et al. ( 2023 ) ) and structured n : m sparsity patterns ( e. g., 2 : 4 or 4 : 8, mishra et al. ( 2021 ) ). all swap operations maintain the sparsity constraints throughout optimization ; for n : m sparsity, swaps are restricted to occur only within the same n : m blocks, while for perrow sparsity, the total number of pruned weights per row remains constant. even though each swap only changes two mask entries, the cumulative effect of multiple swaps can dramatically reduce reconstruction error compared to the initial solution. we explain the main phases of the algorithm : preparation : we initialize with the warmstart mask m init. the gram matrix g is precomputed once per layer by accumulating g = p b x :, [UNK] :, b during the calibration forward pass. row processing ( lines 2 - 5 ) : for each row i, we extract weights w and current mask m, define pruned and unpruned index sets p and u, and compute the initial correlation vector c = g · ( ( 1 − m ) [UNK] ). 5\n\n[FRAGMENT 2] (doc:arxiv_251210922_sparseswaps, page:8, frag:2):\n72 % + sparseswaps 2 : 4 49. 90 % 64. 60 % 52. 30 % 51. 46 % 60. 31 % 3. 2 efficiency and hyperparameter ablations resource requirements. sparseswaps is more resource - intensive than dsnot and, as a drop - in refinement, requires at least the resources of the chosen warm - start method. beyond that, sparseswaps needs memory to store the gram matrix g ∈rdin×din ( once per layer ) and the correlation vector c ∈rdin ( per row ), and compute to perform the 1 - swaps ; see the preceding section for the theoretical complexity. while we have argued in the introduction that the additional compute can be justified when amortized over many llm inference requests, we note that the overhead grows only linearly with the number of 1 - swap iterations tmax. table 1 shows that few iterations already yield substantial gains in both perplexity and local error reduction, especially at higher sparsity. table 3 reports wall - clock times for pruning llama - 3. 1 - 8b to 60 % sparsity on a single h100 gpu. the tmax = 0 baseline includes calibration data sampling, wanda pruning, gram matrix computation, and evaluation ; each additional iteration of sparseswaps adds a relatively small overhead. for comparison, wanda and sparsegpt take approximately 4 and 10 minutes, respectively. we note that our implementation can be further optimized and that the algorithm is fully parallelizable across rows. effect of the number of reconstruction samples. figure 2 in the appendix shows the perplexity versus the number of reconstruction samples for 50 % and 60 % unstructured sparsity when using wanda as well as sparseswaps with a wanda warmstart. we observe that the perplexity decreases 8\n\n[FRAGMENT 3] (doc:arxiv_251210922_sparseswaps, page:15, frag:0):\na appendix a. 1 further results table 4 : perplexity ( ↓, lower is better ) comparison on wikitext. we report sparseswaps refinement with magnitude warmstart for 50 % and 60 % sparsity. best values are highlighted in bold. we omit standard deviations for legibility. perplexity ↓ llama - 3. 1 gemma - 2 deepseek method sparsity 8b 9b 7b magnitude 50 % 68. 89 31. 87 25. 05 + sparseswaps 50 % 52. 26 19. 11 16. 23 magnitude 60 % 3486. 26 184. 52 330. 07 + sparseswaps 60 % 264. 92 60. 04 80. 24 0 100 200 300 400 500 number of samples 10. 2 10. 4 10. 6 perplexity llama - 3. 1 - 8b ( 50 % sparsity ) wanda sparseswaps ( a ) 50 % unstructured sparsity 0 100 200 300 400 500 number of samples 19 20 21 22 23 perplexity llama - 3. 1 - 8b ( 60 % sparsity ) wanda sparseswaps ( b ) 60 % unstructured sparsity figure 2 : perplexity versus the number of reconstruction samples for unstructured sparsity using wanda warmstart. 15\n\n[FRAGMENT 4] (doc:arxiv_251210922_sparseswaps, page:9, frag:0):\ntable 3 : wall - clock time for applying sparseswaps to llama - 3. 1 - 8b at 60 % sparsity on a single h100 gpu. tmax 0 1 2 5 10 25 wall - clock time 8m15s 10m17s 12m7s 17m20s 26m13s 52m29s drastically when using more samples, which leads to sparseswaps slightly outperforming wanda for 50 % sparsity, despite its advantage typically being larger at higher sparsity. we emphasize that the number of reconstruction samples does not affect sparseswaps ’ s swap evaluation efficiency : the gram matrix g = [UNK] fixed size din × din regardless of b. 4 conclusion we revisited the mask selection problem for post - training pruning and showed that it can be made substantially more tractable, even at llm scale. we observed that row decoupling via equal perrow sparsity yields independent subproblems, and that individual 1 - swaps can be evaluated in o ( 1 ) time using the gram matrix g = [UNK]. this enables tractable optimization of the true rowwise quadratic loss on gpus. the resulting method, sparseswaps, is warm - start agnostic, nearly hyperparameter - free, and scalable. it consistently reduces per - layer pruning error and improves perplexity and zero - shot accuracy across modern gpt architectures. our work is not without limitations. while per - row sparsity is not necessarily detrimental for llms, our approach is restricted to that setting and only partially adapts to truly unstructured sparsity ; in its current form, the algorithm can handle unstructured sparsity but cannot reallocate sparsity levels across rows. furthermore, runtime and memory remain non - trivial for large architectures. acknowledgments this research was partially supported by the dfg cluster of excellence math + ( exc - 2046 / 1, project id 390685689 ) funded by the deutsche forschungsgemeinschaft ( dfg ) as well as by the german federal ministry of research, technology and space ( fund number 16is23025b ). 9\n\n[FRAGMENT 5] (doc:arxiv_251210922_sparseswaps, page:8, frag:1):\n. 82 10. 57 12. 03 13. 85 + sparseswaps 2 : 4 20. 90 16. 33 10. 50 11. 80 13. 28 accuracy ↑ llama - 3. 1 gemma - 2 yi - 1. 5 deepseek qwen2. 5 method sparsity 8b 9b 9b 7b 7b wanda 60 % 48. 18 % 63. 39 % 53. 59 % 50. 74 % 59. 26 % + dsnot 60 % 48. 18 % 63. 49 % 53. 79 % 50. 75 % 59. 26 % + sparseswaps 60 % 50. 78 % 63. 84 % 54. 84 % 51. 02 % 60. 15 % ria 60 % 49. 56 % 64. 37 % 52. 81 % 50. 92 % 59. 84 % + dsnot 60 % 49. 56 % 64. 43 % 52. 96 % 50. 83 % 59. 81 % + sparseswaps 60 % 51. 02 % 64. 32 % 54. 45 % 51. 47 % 61. 22 % wanda 2 : 4 46. 80 % 63. 73 % 52. 58 % 51. 02 % 59. 52 % + dsnot 2 : 4 47. 01 % 63. 66 % 52. 16 % 50. 78 % 59. 09 % + sparseswaps 2 : 4 48. 83 % 64. 70 % 52. 43 % 50. 36 % 59. 92 % ria 2 : 4 47. 87 % 63. 87 % 52. 68 % 51. 22 % 58. 66 % + dsnot 2 : 4 47. 13 % 64. 17 % 51. 36 % 49. 86 % 59. 72 % + sparseswaps 2 : 4 49. 90 % 64. 60 % 52. 30 % 51. 46 % 60. 31 % 3. 2 efficiency and hyperparameter ablations resource requirements. sparseswaps is more resource - intensive than dsnot and, as a drop - in refinement, requires at least the resources of the chosen warm - start method. beyond that, sparseswaps needs memory to store the gram matrix g ∈rdin×din ( once per layer ) and the correlation vector c ∈rdin ( per row ), and compute to perform the 1 - swaps ; see the preceding section for the theoretical complexity. while we have argued in the introduction that the additional compute can be\n\n\n\nQuestion:\nWhat is SparseSwaps?\n\nAnswer:"
  },
  {
    "question_id": "q2",
    "doc_id": "arxiv_251210922_sparseswaps",
    "question": "What problem does SparseSwaps solve?",
    "type": "fact",
    "answer": "You are an assistant that answers questions using ONLY the provided context.\nIf the answer is not contained in the context, say you do not know.\n\nContext:\n[FRAGMENT 1] (doc:arxiv_251210922_sparseswaps, page:5, frag:2):\nwhen selecting ( p, u ). 2. 2 the sparseswaps algorithm building upon the preceding observations, we present our complete algorithm. the method takes as input a weight matrix w ∈rdout×din, the gram matrix g = [UNK] ( accumulated during calibration ), and a warmstart pruning mask m init ∈ { 0, 1 } dout×din that already satisfies the desired sparsity constraints, e. g., obtained from wanda ( sun et al., 2024 ) or ria ( zhang et al., 2024 ). the algorithm enforces any sparsity pattern that operates per - row, including per - row sparsity ( fixed number of zeros per row, cf. sun et al. ( 2023 ) ) and structured n : m sparsity patterns ( e. g., 2 : 4 or 4 : 8, mishra et al. ( 2021 ) ). all swap operations maintain the sparsity constraints throughout optimization ; for n : m sparsity, swaps are restricted to occur only within the same n : m blocks, while for perrow sparsity, the total number of pruned weights per row remains constant. even though each swap only changes two mask entries, the cumulative effect of multiple swaps can dramatically reduce reconstruction error compared to the initial solution. we explain the main phases of the algorithm : preparation : we initialize with the warmstart mask m init. the gram matrix g is precomputed once per layer by accumulating g = p b x :, [UNK] :, b during the calibration forward pass. row processing ( lines 2 - 5 ) : for each row i, we extract weights w and current mask m, define pruned and unpruned index sets p and u, and compute the initial correlation vector c = g · ( ( 1 − m ) [UNK] ). 5\n\n[FRAGMENT 2] (doc:arxiv_251210922_sparseswaps, page:8, frag:2):\n72 % + sparseswaps 2 : 4 49. 90 % 64. 60 % 52. 30 % 51. 46 % 60. 31 % 3. 2 efficiency and hyperparameter ablations resource requirements. sparseswaps is more resource - intensive than dsnot and, as a drop - in refinement, requires at least the resources of the chosen warm - start method. beyond that, sparseswaps needs memory to store the gram matrix g ∈rdin×din ( once per layer ) and the correlation vector c ∈rdin ( per row ), and compute to perform the 1 - swaps ; see the preceding section for the theoretical complexity. while we have argued in the introduction that the additional compute can be justified when amortized over many llm inference requests, we note that the overhead grows only linearly with the number of 1 - swap iterations tmax. table 1 shows that few iterations already yield substantial gains in both perplexity and local error reduction, especially at higher sparsity. table 3 reports wall - clock times for pruning llama - 3. 1 - 8b to 60 % sparsity on a single h100 gpu. the tmax = 0 baseline includes calibration data sampling, wanda pruning, gram matrix computation, and evaluation ; each additional iteration of sparseswaps adds a relatively small overhead. for comparison, wanda and sparsegpt take approximately 4 and 10 minutes, respectively. we note that our implementation can be further optimized and that the algorithm is fully parallelizable across rows. effect of the number of reconstruction samples. figure 2 in the appendix shows the perplexity versus the number of reconstruction samples for 50 % and 60 % unstructured sparsity when using wanda as well as sparseswaps with a wanda warmstart. we observe that the perplexity decreases 8\n\n[FRAGMENT 3] (doc:arxiv_251210922_sparseswaps, page:9, frag:0):\ntable 3 : wall - clock time for applying sparseswaps to llama - 3. 1 - 8b at 60 % sparsity on a single h100 gpu. tmax 0 1 2 5 10 25 wall - clock time 8m15s 10m17s 12m7s 17m20s 26m13s 52m29s drastically when using more samples, which leads to sparseswaps slightly outperforming wanda for 50 % sparsity, despite its advantage typically being larger at higher sparsity. we emphasize that the number of reconstruction samples does not affect sparseswaps ’ s swap evaluation efficiency : the gram matrix g = [UNK] fixed size din × din regardless of b. 4 conclusion we revisited the mask selection problem for post - training pruning and showed that it can be made substantially more tractable, even at llm scale. we observed that row decoupling via equal perrow sparsity yields independent subproblems, and that individual 1 - swaps can be evaluated in o ( 1 ) time using the gram matrix g = [UNK]. this enables tractable optimization of the true rowwise quadratic loss on gpus. the resulting method, sparseswaps, is warm - start agnostic, nearly hyperparameter - free, and scalable. it consistently reduces per - layer pruning error and improves perplexity and zero - shot accuracy across modern gpt architectures. our work is not without limitations. while per - row sparsity is not necessarily detrimental for llms, our approach is restricted to that setting and only partially adapts to truly unstructured sparsity ; in its current form, the algorithm can handle unstructured sparsity but cannot reallocate sparsity levels across rows. furthermore, runtime and memory remain non - trivial for large architectures. acknowledgments this research was partially supported by the dfg cluster of excellence math + ( exc - 2046 / 1, project id 390685689 ) funded by the deutsche forschungsgemeinschaft ( dfg ) as well as by the german federal ministry of research, technology and space ( fund number 16is23025b ). 9\n\n[FRAGMENT 4] (doc:arxiv_251210922_sparseswaps, page:7, frag:1):\n% unstructured sparsity and 100 1 - swap iterations. sparseswaps consistently improves state - of - the - art methods. table 2 summarizes the main results and reports perplexity ( upper half, lower is better ) and zero - shot accuracy ( lower half, higher is better ) for warmstart masks ( wanda, ria ) as well as their refinements using dsnot and sparseswaps. for both 60 % unstructured and 2 : 4 semi - structured sparsity, sparseswaps ( with 100 1 - swap iterations ) consistently reduces perplexity and improves zero - shot accuracy over wanda and ria warm start masks. while dsnot similarly yields improvements, it falls short of sparseswaps. note that we left the pruning criterion of dsnot, which partially uses the wanda saliency, unchanged, even when using ria warmstart. for unstructured ria, we report results when enforcing a perrow sparsity constraint ; while ria yields good ( and slightly better ) results when enforcing truely unstructured sparsity, we decided to include the results for the per - row setting as this allows direct refinement of the mask with sparseswaps and dsnot. sparseswaps successfully optimizes the per - layer pruning loss. figure 1 shows the per - layer reductions in local pruning error relative to a wanda warmstart, grouping layers by their corresponding transformer block of llama - 3. 1 - 8b. we observe drastic improvements of close to 70 % compared to wanda, demonstrating that sparseswaps is able to successfully optimize the local loss. the attn. o proj seems to consistently benefit the most across blocks, with reductions of the objective in equation 1 ranging between 40 % - 60 %. large local error reductions do not always imply reduced perplexity. from table 2 we observe substantial perplexity gains, especially when sparsity more strongly degrades model quality ( cf. table 4 in the appendix, which shows more drastic improvements when using magnitude pruning, which more strongly degrades model quality ). in contrast, when quality is less affected ( e. g., at 50 % sparsity where wanda performs well ), sparseswaps yields limited perplexity gains despite significant local error reductions : table 1 reports perplexity\n\n[FRAGMENT 5] (doc:arxiv_251210922_sparseswaps, page:15, frag:0):\na appendix a. 1 further results table 4 : perplexity ( ↓, lower is better ) comparison on wikitext. we report sparseswaps refinement with magnitude warmstart for 50 % and 60 % sparsity. best values are highlighted in bold. we omit standard deviations for legibility. perplexity ↓ llama - 3. 1 gemma - 2 deepseek method sparsity 8b 9b 7b magnitude 50 % 68. 89 31. 87 25. 05 + sparseswaps 50 % 52. 26 19. 11 16. 23 magnitude 60 % 3486. 26 184. 52 330. 07 + sparseswaps 60 % 264. 92 60. 04 80. 24 0 100 200 300 400 500 number of samples 10. 2 10. 4 10. 6 perplexity llama - 3. 1 - 8b ( 50 % sparsity ) wanda sparseswaps ( a ) 50 % unstructured sparsity 0 100 200 300 400 500 number of samples 19 20 21 22 23 perplexity llama - 3. 1 - 8b ( 60 % sparsity ) wanda sparseswaps ( b ) 60 % unstructured sparsity figure 2 : perplexity versus the number of reconstruction samples for unstructured sparsity using wanda warmstart. 15\n\n\n\nQuestion:\nWhat problem does SparseSwaps solve?\n\nAnswer:"
  },
  {
    "question_id": "q3",
    "doc_id": "arxiv_251210922_sparseswaps",
    "question": "Does the paper mention quantum computing?",
    "type": "impossible",
    "answer": "You are an assistant that answers questions using ONLY the provided context.\nIf the answer is not contained in the context, say you do not know.\n\nContext:\n[FRAGMENT 1] (doc:arxiv_251210931_asynchronous_reasoning, page:9, frag:1):\nthinking, 2025. accessed : 2025 - 04 - 07. [ 20 ] anthropic. claude 3. 7 sonnet and claude code, 2024. accessed : 2025. 04. 02. [ 21 ] deepseek - ai, daya guo, dejian yang, haowei zhang, junxiao song, ruoyu zhang, runxin xu, qihao zhu, shirong ma, peiyi wang, and xiao bi et al. deepseek - r1 : incentivizing reasoning capability in llms via reinforcement learning, 2025. [ 22 ] qwen team. qwq - 32b : embracing the power of reinforcement learning, march 2025. [ 23 ] kimi team, yifan bai, yiping bao, guanduo chen, jiahao chen, ningxin chen, ruijue chen, yanru chen, yuankun chen, yutian chen, zhuofu chen, jialei cui, hao ding, mengnan dong, angang du, chenzhuang du, dikang du, yulun du, yu fan, yichen feng, kelin fu, bofei gao, hongcheng gao, peizhong gao, tong gao, xinran gu, longyu guan, haiqing guo, jianhang guo, hao hu, xiaoru hao, tianhong he, weiran he, wenyang he, chao hong, yangyang hu, zhenxing hu, weixiao huang, zhiqi huang, zihao huang, tao jiang, zhejun jiang, xinyi jin, yongsheng kang, guokun lai, cheng li, fang li, haoyang li, ming li, wentao li, yanhao li, yiwei li, zhaowei li, zheming li, hongzhan lin, xiaohan lin, zongyu lin, chengyin liu, chenyu liu, hongzhang liu, jingyuan liu, junqi liu, liang liu, shaowei liu, t. y. liu, tianwei liu, weizhou liu, yangyang liu, yibo liu, yiping liu, yue liu, zhengying liu, enzhe lu, lijun lu, shengling ma, xinyu ma, yingwei ma, shaoguang mao, jie mei, xin men, yibo mia\n\n[FRAGMENT 2] (doc:arxiv_251210931_asynchronous_reasoning, page:10, frag:0):\n[ 28 ] sarah bro trasmundi and juan toro. mind wandering in reading : an embodied approach. frontiers in human neuroscience, 17, 2023. [ 29 ] daisuke akiba. ctrl + alt + inner speech : a verbal – cognitive scaffold ( vcs ) model of pathways to computational thinking. journal of intelligence, 13 ( 12 ), 2025. [ 30 ] kevin p. madore and anthony d. wagner. multicosts of multitasking. cerebrum : the dana forum on brain science, 2019 : cer – 04 – 19, 2019. [ 31 ] maurizio corbetta, gaurav patel, and gordon l shulman. the reorienting system of the human brain : from environment to theory of mind. neuron, 58 ( 3 ) : 306 – 324, 2008. [ 32 ] openai. gpt - 4o system card. online technical report, 2024. voice - mode multimodal model supporting audio, text, and vision. available at https : / / openai. com / index / hello - gpt - 4o. [ 33 ] paul k. rubenstein, chulayuth asawaroengchai, duc dung nguyen, ankur bapna, zalan borsos, felix de chaumont quitry, peter chen, dalia el badawy, wei han, eugene kharitonov, hannah muckenhirn, dirk padfield, james qin, danny rozenberg, tara sainath, johan schalkwyk, matt sharifi, michelle tadmor ramanovich, marco tagliasacchi, alexandru tudor, mihajlo velimirovi´c, damien vincent, jiahui yu, yongqiang wang, vicky zayats, neil zeghidour, yu zhang, zhishuai zhang, lukas zilka, and christian frank. audiopalm : a large language model that can speak and listen. arxiv preprint arxiv : 2306. 12925, 2023. [ 34 ] dong zhang, shimin li, xin zhang, jun zhan, pengyu wang, yaqian zhou, and xipeng qiu. speechgpt : empowering large language models with intrinsic cross - modal conversational abilities. in findings of the association for computational linguistics : emnlp 2023, pages 15757\n\n[FRAGMENT 3] (doc:arxiv_251210931_asynchronous_reasoning, page:9, frag:3):\nxu, xinran xu, yangchuan xu, ziyao xu, junjie yan, yuzi yan, xiaofei yang, ying yang, zhen yang, zhilin yang, zonghan yang, haotian yao, xingcheng yao, wenjie ye, zhuorui ye, bohong yin, longhui yu, enming yuan, hongbang yuan, mengjie yuan, haobing zhan, dehao zhang, hao zhang, wanlu zhang, xiaobin zhang, yangkun zhang, yizhi zhang, yongting zhang, yu zhang, yutao zhang, yutong zhang, zheng zhang, haotian zhao, yikai zhao, huabin zheng, shaojie zheng, jianren zhou, xinyu zhou, zaida zhou, zhen zhu, weiyu zhuang, and xinxing zu. kimi k2 : open agentic intelligence, 2025. [ 24 ] arc prize foundation. openai ’ s new o3 system scores breakthrough on arc - agi - pub, 2024. accessed : 2025. 03. 28. [ 25 ] humanity ’ s last exam contributors. humanity ’ s last exam : a benchmark for frontier ai capabilities. arxiv preprint arxiv : 2501. 14249, 2025. [ 26 ] nicole landi, stephen frost, w menc, rebecca sandak, and kenneth pugh. neurobiological bases of reading comprehension : insights from neuroimaging studies of word - level and text - level processing in skilled and impaired readers. reading & writing quarterly : overcoming learning difficulties, 29 : 145 – 167, 04 2013. [ 27 ] bingjiang lyu, william d. marslen - wilson, yuxing fang, and lorraine k. tyler. finding structure during incremental speech comprehension. elife, 12 : e89311, 2023. 9\n\n[FRAGMENT 4] (doc:arxiv_251210931_asynchronous_reasoning, page:5, frag:0):\np p + t p + t + w 0 query writer thinker block writer view positions positions prompt block writer block thinker view prompt block p + w p + w + t p 0 query thinker writer block thinker block implementation query thinker query thinker query thinker query thinker t t + w t + w + p offset : 0 thinker block writer block prompt block figure 3 : concurrent thinking and writing implemented as batched inference. the newly added tokens attend to cache blocks with additional query rotations. the checkered areas represent tokens that are not visible in the current view. h brackets i denote concatenation, iq is the query position, ip k, it k, iw k are cache block positions from the writer ’ s point of view ( see figure 3 ) and kp, t, w are the corresponding key vectors. then, we can equivalently compute attention as : a : = h ρ ( q, iq−ip k ) kp, ρ ( q, iq−it k ) kt, ρ ( q, iq−iw k ) kw i. this reformulation allows us to compute kp, t, w once, store it in kv cache and only rotate attention queries for the currently processed tokens during each forward pass. technical considerations. to summarize, our implementation consists of the custom kv cache and an attention kernel that uses the query rotation trick above. in practice, we use more than 3 kv blocks : in addition to the prompt, thinking and response tokens, we also have short linker tokens that fit between thinking writing blocks. these linkers are implemented as separate kv blocks that are visible only in one of the views ( thinker or writer ). if a block is not visible on the current view, we give it a large position index so it is ignored by the causal masked lm attention kernel. this implementation can efficiently parallelize thinking and writing the response for small batch sizes. however, when applied to large batches, it can be optimized further by only processing the non - masked query - key pairs that actually contribute to the attention output. in future work, we plan to explore implementing more general kernels for asyncreasoning based on vllm ’ s paged attention [ 43 ]. 4. experiments in this section, we conduct an initial evaluation of asyncreasoning and analyze its components. we run our evaluations on qwen\n\n[FRAGMENT 5] (doc:arxiv_251210931_asynchronous_reasoning, page:14, frag:1):\nyi - chang chen, sattar vakili, and da shan shiu. group think : multiple concurrent reasoning agents collaborating at token level granularity, 2025. [ 102 ] tong zheng, hongming zhang, wenhao yu, xiaoyang wang, runpeng dai, rui liu, huiwen bao, chengsong huang, heng huang, and dong yu. parallel - r1 : towards parallel thinking via reinforcement learning, 2025. [ 103 ] in gim, seung seob lee, and lin zhong. asynchronous llm function calling, 2024. [ 104 ] sehoon kim, suhong moon, ryan tabrizi, nicholas lee, michael w mahoney, kurt keutzer, and amir gholami. an llm compiler for parallel function calling. in forty - first international conference on machine learning, 2024. [ 105 ] junlong tong, yingqi fan, anhao zhao, yunpu ma, and xiaoyu shen. streamingthinker : large language models can think while reading, 2025. [ 106 ] shoutao guo, shaolei zhang, zhengrui ma, and yang feng. large language models are read / write policymakers for simultaneous generation, 2025. [ 107 ] donghang wu, haoyang zhang, jun chen, xiangyu, zhang, hexin liu, eng siong chng, fei tian, xuerui yang, xiangyu zhang, daxin jiang, and gang yu. mind - paced speaking : a dual - brain approach to realtime reasoning in spoken language models, 2025. [ 108 ] anthony liang, jonathan berant, adam fisch, abhimanyu goyal, kalpesh krishna, and jacob eisenstein. plantain : plan - answer interleaved reasoning, 2025. [ 109 ] a vaswani. attention is all you need. advances in neural information processing systems, 2017. [ 110 ] juho lee, yoonho lee, jungtaek kim, adam kosiorek, seungjin choi, and yee whye teh. set transformer : a framework for attention - based permutation - invariant neural networks. in proceedings of the 36th international conference on machine learning, pages 3744 – 3753, 2019. [ 111 ] peter shaw, jakob uszkoreit, and ashish vaswani. self - attention with relative\n\n\n\nQuestion:\nDoes the paper mention quantum computing?\n\nAnswer:"
  }
]