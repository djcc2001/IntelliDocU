[
  {
    "question_id": "q1",
    "doc_id": "arxiv_251210894_duetsvg",
    "question": "What is the name of the proposed unified multimodal model for SVG generation?",
    "type": "fact",
    "answer": "DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance."
  },
  {
    "question_id": "q2",
    "doc_id": "arxiv_251210894_duetsvg",
    "question": "What are the two main stages of training for DuetSVG?",
    "type": "fact",
    "answer": "Stage 1: Feature Alignment. This stage corresponds to Phase 1 training in LLaVA [30]. Both the vision and language backbones are frozen, and only the MLP connector and language backbone are optimized using an autoregressive loss. The objective is to align visual features with the language embedding space, effectively bridging the two modalities. To maintain training stability, we use only the image–utterance subset of the pretraining data in this stage. Stage 2: Joint Pretraining. In this stage, the vision backbone remains frozen, while the MLP connector and language backbone are trained jointly on the full mixed-format pretraining dataset, as described in Section 3.1. This allows the model to learn multimodal grounding over diverse input structures. Stage 3: Instruction Fine-tuning. Finally, we fine-tune the model using the mixed instruction dataset, which is a combination of all the instruction samples mentioned in Table 3. This step enables the model to perform various downstream tasks through natural-language prompts. The vision backbone, MLP connector and language backbone are all updated to learn instruction-following behavior and contextdependent reasoning. We apply two different"
  },
  {
    "question_id": "q3",
    "doc_id": "arxiv_251210894_duetsvg",
    "question": "Which base model is DuetSVG initialized from?",
    "type": "fact",
    "answer": "DuetSVG is a unified multimodal model that jointly generates image tokens and corresponding SVG tokens in an end-to-end manner."
  },
  {
    "question_id": "q4",
    "doc_id": "arxiv_251210894_duetsvg",
    "question": "In which section does the paper describe the \"Image-Guided SVG Resampling\" strategy?",
    "type": "loc",
    "answer": "Image-Guided SVG Resampling."
  },
  {
    "question_id": "q5",
    "doc_id": "arxiv_251210894_duetsvg",
    "question": "Which figure illustrates the model architecture of DuetSVG?",
    "type": "loc",
    "answer": "DuetSVG is a unified multimodal model that jointly generates image tokens and corresponding SVG tokens in an end-to-end manner. DuetSVG is trained on both image and SVG datasets. At inference, we apply a novel test-time scaling strategy that leverages the model’s native visual predictions as guidance to improve SVG decoding quality. Extensive experiments show that our method outperforms existing methods, producing visually faithful, semantically aligned, and syntactically clean SVGs across a wide range of applications."
  },
  {
    "question_id": "q6",
    "doc_id": "arxiv_251210894_duetsvg",
    "question": "Does DuetSVG implement a reinforcement learning module for path optimization?",
    "type": "impossible",
    "answer": "The DuetSVG is a neural network that uses reinforcement learning to learn path optimization."
  },
  {
    "question_id": "q7",
    "doc_id": "arxiv_251210922_sparseswaps",
    "question": "What is the full name of the proposed pruning refinement algorithm?",
    "type": "fact",
    "answer": "SparseSwaps: TRACTABLE LLM PRUNING MASK REFINEMENT AT SCALE Max Zimmer, Christophe Roux, Moritz Wagner, Deborah Hendrych, Sebastian Pokutta Department for AI in Society, Science, and Technology, Zuse Institute Berlin, Germany Institute of Mathematics, Technische Universit at Berlin, Germany zimmer,roux,wagner,hendrych,pokutta@zib.de"
  },
  {
    "question_id": "q8",
    "doc_id": "arxiv_251210922_sparseswaps",
    "question": "What mathematical matrix (G) is central to the efficient 1-swap evaluation?",
    "type": "fact",
    "answer": "The Gram matrix G is computed once per layer and shared across all rows. The Gram matrix G is computed once per layer and shared across all rows. The Gram matrix G is central to the efficient 1-swap evaluation."
  },
  {
    "question_id": "q9",
    "doc_id": "arxiv_251210922_sparseswaps",
    "question": "What is the primary constraint enforced by SparseSwaps to make the problem row-separable?",
    "type": "fact",
    "answer": "SparseSwaps is a simple, efficient, and efficient method for minimizing sparsity problems. It is a simple, efficient, and efficient method for minimizing sparsity problems. It is a simple, efficient, and efficient method for minimizing sparsity problems."
  },
  {
    "question_id": "q10",
    "doc_id": "arxiv_251210922_sparseswaps",
    "question": "In which section (number and title) does the paper describe the \"Efficient 1-swap evaluation through cost lookups\"?",
    "type": "loc",
    "answer": "Partially"
  },
  {
    "question_id": "q11",
    "doc_id": "arxiv_251210922_sparseswaps",
    "question": "According to Table 2, which model shows the largest perplexity improvement when refining Wanda with SparseSwaps at 60% sparsity?",
    "type": "loc",
    "answer": "SparseSwaps yields limited perplexity gains despite significant local error reductions."
  },
  {
    "question_id": "q12",
    "doc_id": "arxiv_251210922_sparseswaps",
    "question": "Does the SparseSwaps algorithm require training a new neural network from scratch?",
    "type": "impossible",
    "answer": "SparseSwaps is a simple yet effective method to obtain such sparse models starts from a pretrained dense model, removes seemingly unimportant parameters based on their magnitude, and requires retraining to compensate for pruning-induced performance degradation. However, with the rise of large pretrained foundation models, the size of the models has shifted the focus toward retraining-free pruning criteria, as retraining is often computationally expensive if not infeasible, with parameter-efficient fine-tuning (Lialin et al., 2023b) being an exception. For unstructured RIA, we report results when enforcing a perrow sparsity constraint; while DSnoT similarly yields improvements, it falls short of SparseSwaps. Subset selection and IP approaches are available, including SCIP (Bolusani et al., 2023; Zhang et al., 2024)."
  },
  {
    "question_id": "q13",
    "doc_id": "arxiv_251210931_asynchronous_reasoning",
    "question": "What is the name of the proposed training-free method for asynchronous LLM reasoning?",
    "type": "fact",
    "answer": "AsyncReasoning."
  },
  {
    "question_id": "q14",
    "doc_id": "arxiv_251210931_asynchronous_reasoning",
    "question": "What is the key mathematical property of Rotary Positional Embeddings (RoPE) exploited by AsyncReasoning?",
    "type": "fact",
    "answer": "Rotating query and key vectors by an angle proportional to their index in text before computing the scaled dot product attention."
  },
  {
    "question_id": "q15",
    "doc_id": "arxiv_251210931_asynchronous_reasoning",
    "question": "Which model (name and size) is primarily used for evaluation in the experiments?",
    "type": "fact",
    "answer": "The LLaMa model is primarily used for evaluation in the experiments."
  },
  {
    "question_id": "q16",
    "doc_id": "arxiv_251210931_asynchronous_reasoning",
    "question": "In which section (number and title) does the paper describe the \"Mode Switching\" mechanism?",
    "type": "loc",
    "answer": "The \"Mode Switching\" mechanism is a partial description of the \"Mode Switching\" mechanism."
  },
  {
    "question_id": "q17",
    "doc_id": "arxiv_251210931_asynchronous_reasoning",
    "question": "According to Table 1, what is the Time to First Token (TTFT) for the AsyncReasoning (Q-Continue) setup on the MATH-500 benchmark?",
    "type": "loc",
    "answer": "The wall time delay until the system generates the first non-thinking token is 0.890. The total delay is 0.932. The average number of inference steps (forward passes) that do not generate a response token is 0.834. The time to first token is 0.890. The time to first token is 0.758. The time to first token is 0.890. The time to first token is 0.758. The time to first token is 0.890. The time to first token is 0.758. The time to first token is 0.890. The time to first token is 0.758. The time to first token is 0.890. The time to first token is 0.758. The time to first token is 0.890. The time to first token is 0.758. The time to first token is 0.890. The time to first token is 0.758. The time to first token is 0.890. The time to first token is 0.758. The time to first token is 0.758. The time to first token is 0.758. The time to first token is 0.758. The time to first token is 0.758. The time"
  },
  {
    "question_id": "q18",
    "doc_id": "arxiv_251210931_asynchronous_reasoning",
    "question": "Does the AsyncReasoning method require fine-tuning the base language model on a new dataset of conversational speech?",
    "type": "impossible",
    "answer": "The paper focuses on the development and evaluation of a large-scale data-driven dialogue generation model called LIMA. The authors analyze 10 live conversations and label each response as Fail, Pass, or Excellent. The authors also discuss the use of multi-turn dialogue and find that a model fine-tuned on only 1,000 single-turn interactions can engage in multi-turn dialogue. The authors also discuss the use of multi-turn dialogue and find that a model fine-tuned on only 1,000 single-turn interactions can engage in multi-turn dialogue. The authors also discuss the use of multi-turn dialogue and find that a model fine-tuned on only 1,000 single-turn interactions can engage in multi-turn dialogue. The authors also discuss the use of multi-turn dialogue and find that a model fine-tuned on only 1,000 single-turn interactions can engage in multi-turn dialogue. The authors also discuss the use of multi-turn dialogue and find that a model fine-tuned on only 1,000 single-turn interactions can engage in multi-turn dialogue. The authors also discuss the use of multi-turn dialogue and find that a model fine-tuned on only 1,000 single"
  },
  {
    "question_id": "q19",
    "doc_id": "arxiv_251210932_babyvlm_v2",
    "question": "What is BabyVLM-V2?",
    "type": "fact",
    "answer": "A framework for studying ADI from the lens of vision-language models."
  },
  {
    "question_id": "q20",
    "doc_id": "arxiv_251210932_babyvlm_v2",
    "question": "What dataset is the pretraining set of BabyVLM-V2 based on?",
    "type": "fact",
    "answer": "BabyVLM-V1 is a framework for vision foundation models. Its pretraining set is diverse in format (video, imageutterance, and multiple turns), enabling a flexible model. Its benchmark developmentally aligns with the pretraining set’s age span by grounding on the newly released NIH Baby Toolbox®. (FM) that is as versatile and capable as the early children’s perception."
  },
  {
    "question_id": "q21",
    "doc_id": "arxiv_251210932_babyvlm_v2",
    "question": "What is DevCV Toolbox and how many tasks does it contain?",
    "type": "fact",
    "answer": "DevCV Toolbox is a computer vision toolbox that consists of ten tasks."
  },
  {
    "question_id": "q22",
    "doc_id": "arxiv_251210932_babyvlm_v2",
    "question": "In which section does the paper compare BabyVLM-V1 and BabyVLM-V2 in a table?",
    "type": "loc",
    "answer": "Table 3 Comparison of BabyVLM-V1 and BabyVLM-V2 in a table. Table 3 Comparison of BabyVLM-V2 in a table. Table 3 Comparison of BabyVLM-V1 and BabyVLM-V2 in a table. Table 3 Comparison of BabyVLM-V2 in a table. Table 3 Comparison of BabyVLM-V1 and BabyVLM-V2 in a table. Table 3 Comparison of BabyVLM-V2 in a table. Table 3 Comparison of BabyVLM-V1 and BabyVLM-V2 in a table. Table 3 Comparison of BabyVLM-V2 in a table. Table 3 Comparison of BabyVLM-V1 and BabyVLM-V2 in a table. Table 3 Comparison of BabyVLM-V2 in a table. Table 3 Comparison of BabyVLM-V1 and BabyVLM-V2 in a table. Table 3 Comparison of BabyVLM-V2 in a table. Table 3 Comparison of BabyVLM-V1 and BabyVLM-V2 in a table. Table 3 Comparison of BabyVLM-V2 in"
  },
  {
    "question_id": "q23",
    "doc_id": "arxiv_251210932_babyvlm_v2",
    "question": "Where in the paper is the performance comparison of BabyLLaVA-V2 with GPT-4o on counting tasks shown?",
    "type": "loc",
    "answer": "The performance comparison of BabyLLaVA-V2 with GPT-4o on counting tasks is shown in Table 9."
  },
  {
    "question_id": "q24",
    "doc_id": "arxiv_251210932_babyvlm_v2",
    "question": "Does BabyVLM-V2 use reinforcement learning to train its vision encoder?",
    "type": "impossible",
    "answer": "BabyVLM-V2 is a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and DevCV Toolbox for cognitive evaluation."
  },
  {
    "question_id": "q25",
    "doc_id": "arxiv_251210935_any4d",
    "question": "What is the main contribution of Any4D?",
    "type": "fact",
    "answer": "Any4D is a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available."
  },
  {
    "question_id": "q26",
    "doc_id": "arxiv_251210935_any4d",
    "question": "What type of neural network architecture does Any4D use?",
    "type": "fact",
    "answer": "Any4D is a scalable architecture inspired by [32] and performs a single feedforward pass for all image frames at once. Any4D can exploit diverse multi-modal inputs and outputs a dense metric-scale 4D reconstruction in a single feedforward pass."
  },
  {
    "question_id": "q27",
    "doc_id": "arxiv_251210935_any4d",
    "question": "Which sensors or modalities can Any4D process besides RGB images?",
    "type": "fact",
    "answer": "RGB images can be used as inputs for 4D reconstruction. Any4D can also use auxiliary multi-modal sensor inputs."
  },
  {
    "question_id": "q28",
    "doc_id": "arxiv_251210935_any4d",
    "question": "According to the paper, what are the three core contributions of Any4D?",
    "type": "loc",
    "answer": "Any4D is a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available."
  },
  {
    "question_id": "q29",
    "doc_id": "arxiv_251210935_any4d",
    "question": "In which section of the paper are the limitations of Any4D discussed?",
    "type": "loc",
    "answer": "The authors discuss the limitations of Any4D in the first section."
  },
  {
    "question_id": "q30",
    "doc_id": "arxiv_251210935_any4d",
    "question": "Does Any4D use reinforcement learning to optimize scene flow estimation?",
    "type": "impossible",
    "answer": "Any4D is a feed-forward model that predicts camera poses, dense 3D motion (as scene flow) and geometry (as pointmaps), fully describing a dynamic scene captured by a set of N frames in its entirety."
  },
  {
    "question_id": "q31",
    "doc_id": "arxiv_251210938_stronger_normalization_free",
    "question": "What is the proposed function that replaces normalization layers?",
    "type": "fact",
    "answer": "We propose a point-wise function that outperforms normalization layers and other point-wise functions."
  },
  {
    "question_id": "q32",
    "doc_id": "arxiv_251210938_stronger_normalization_free",
    "question": "What are the four key properties analyzed for point-wise functions?",
    "type": "fact",
    "answer": "Zero-centeredness, boundedness, center sensitivity, and monotonicity."
  },
  {
    "question_id": "q33",
    "doc_id": "arxiv_251210938_stronger_normalization_free",
    "question": "Which tasks were used to evaluate Derf?",
    "type": "fact",
    "answer": "DevCV Toolbox tasks and their corresponding NIH Baby Toolbox measures (EF/M stands for Executive Function/Memory). DevCV Toolbox tasks #Instruct/#Test Model Input NIH Baby Toolbox tasks #Instruct/#Test Model Input NIH Baby Toolbox tasks #Instruct/#Test Model Input NIH Baby Toolbox tasks #Instruct/#Test Model Input NIH Baby Toolbox tasks #Instruct/#Test Model Input NIH Baby Toolbox tasks #Instruct/#Test Model Input NIH Baby Toolbox tasks #Instruct/#Test Model Input NIH Baby Toolbox tasks #Instruct/#Test Model Input NIH Baby Toolbox tasks #Instruct/#Test Model Input NIH Baby Toolbox tasks #Instruct/#Test Model Input NIH Baby Toolbox tasks #Instruct/#Test Model Input NIH Baby Toolbox tasks #Instruct/#Test Model Input NIH Baby Toolbox tasks #Instruct/#Test Model Input NIH Baby"
  },
  {
    "question_id": "q34",
    "doc_id": "arxiv_251210938_stronger_normalization_free",
    "question": "In which section does the paper explain the effect of the shift parameter s?",
    "type": "loc",
    "answer": "The paper describes the effect of the shift parameter s on model training."
  },
  {
    "question_id": "q35",
    "doc_id": "arxiv_251210938_stronger_normalization_free",
    "question": "Where does the paper compare Derf's training loss with normalization layers?",
    "type": "loc",
    "answer": "The paper compares training loss of models respectively trained with normalization layers, DyT, and Derf. The paper compares training loss of models respectively trained with normalization layers, DyT, and Derf."
  },
  {
    "question_id": "q36",
    "doc_id": "arxiv_251210938_stronger_normalization_free",
    "question": "Does the paper propose using quantum entanglement for function optimization?",
    "type": "impossible",
    "answer": "The paper proposes using quantum entanglement for function optimization."
  },
  {
    "question_id": "q37",
    "doc_id": "arxiv_251210939_gaussianHeadTalk",
    "question": "What is the name of the method introduced in the paper?",
    "type": "fact",
    "answer": "OCR verification"
  },
  {
    "question_id": "q38",
    "doc_id": "arxiv_251210939_gaussianHeadTalk",
    "question": "What technique does GaussianHeadTalk use for real-time rendering?",
    "type": "fact",
    "answer": "Gaussian Head Modeling and audio to facial motion mapping."
  },
  {
    "question_id": "q39",
    "doc_id": "arxiv_251210939_gaussianHeadTalk",
    "question": "What type of model is used to capture long-range temporal information from audio?",
    "type": "fact",
    "answer": "Video-to-Time Audio-to-Video Time-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio Video-to-Audio"
  },
  {
    "question_id": "q40",
    "doc_id": "arxiv_251210939_gaussianHeadTalk",
    "question": "In which section does the paper describe the stability metric used to quantify wobbling?",
    "type": "loc",
    "answer": "Log(1 + x) converts quantities to log-space for numerical stability. A pointmap loss is also applied to the composed geometric predictions as follows: Lpm N X i flog Xi zi flog Fi zi flog Fi zi ! (9) Similarly, scene flow is also supervised in a scaleinvariant manner. We find that scene flow loss is dominated by static points since most of the scene is static. Therefore, we find it is crucial to calculate a static-dynamic motion mask M from the ground truth scene flow, and upweight the scene flow loss in the dynamic regions by 10x more compared to static regions: Lsf N X i flog Xi zi flog Fi zi flog Fi zi ! (10) Finally, the predicted metric scale factor s is also supervised in the log space as follows: Lscale flog(z) flog(s sg(z), where sg denotes the stop-gradient operation and prevents the scale supervision from affecting other predicted quantities. The final loss is expressed as: L = Ltrans + Lrot + Lrays + Ldepth + Ls"
  },
  {
    "question_id": "q41",
    "doc_id": "arxiv_251210939_gaussianHeadTalk",
    "question": "According to the paper, which datasets were used for experiments?",
    "type": "loc",
    "answer": "The datasets used in this study are from Loghub-2.0."
  },
  {
    "question_id": "q42",
    "doc_id": "arxiv_251210939_gaussianHeadTalk",
    "question": "Does the paper mention the use of reinforcement learning for avatar control?",
    "type": "impossible",
    "answer": "A survey on large language model-based game agents, 2024."
  },
  {
    "question_id": "q43",
    "doc_id": "arxiv_251210942_vl_jepa",
    "question": "What is VL-JEPA and what does it stand for?",
    "type": "fact",
    "answer": "VL-JEPASFT is a joint Embedding Predictive Architecture for Vision-language. VL-JEPASFT is a self-supervised vision model that excels at both image and video tasks. VL-JEPASFT outperforms many of these 5 [doc=arxiv_251210942_vl_jepa, p=4, frag=11] run continuously. VL-JEPA, in contrast, natively supports selective decoding. Since it predicts a semantic answer embedding non-autoregressively, the model provides a continuous semantic stream of SYthat can be monitored in real time. This stream can be stabilized with simple smoothing (e.g., average pooling) and decoded only when a significant semantic shift is detected, such as when the local window variance exceeds a threshold. VL-JEPA maintains always-on semantic monitoring while avoiding unnecessary decoding, achieving both responsiveness and efficiency."
  },
  {
    "question_id": "q44",
    "doc_id": "arxiv_251210942_vl_jepa",
    "question": "How does VL-JEPA achieve 50% fewer trainable parameters compared to standard VLMs?",
    "type": "fact",
    "answer": "VL-JEPASFT is a self-supervised vision model that excels at both image and video tasks. VLMs typically optimize two objectives: 1) prediction error in the embedding space, and 2) additional regularization that avoids representation collapse. VLMs typically optimize two objectives jointly: 1) prediction error in the embedding space, and 2) additional regularization that avoids representation collapse. VLMs typically optimize two objectives jointly: 1) prediction error in the embedding space, and 2) additional regularization that avoids representation collapse. VLMs typically optimize two objectives jointly: 1) prediction error in the embedding space, and 2) additional regularization that avoids representation collapse. VLMs typically optimize two objectives jointly: 1) prediction error in the embedding space, and 2) additional regularization that avoids representation collapse. VLMs typically optimize two objectives jointly: 1) prediction error in the embedding space, and 2) additional regularization that avoids representation collapse. VLMs typically optimize two objectives jointly: 1) prediction error in the embedding space, and 2) additional regularization that avoids representation collapse. VLMs typically optimize two objectives jointly: 1) prediction error in the embedding space, and 2)"
  },
  {
    "question_id": "q45",
    "doc_id": "arxiv_251210942_vl_jepa",
    "question": "What two stages are used to train VL-JEPA?",
    "type": "fact",
    "answer": "VL-JEPASFT is a partial-based prediction model that is trained with bi-directional infoNCE loss."
  },
  {
    "question_id": "q46",
    "doc_id": "arxiv_251210942_vl_jepa",
    "question": "How does VL-JEPA's selective decoding mechanism work for real-time video applications?",
    "type": "loc",
    "answer": "VL-JEPA, in contrast, natively supports selective decoding. Since it predicts a semantic answer embedding non-autoregressively, the model provides a continuous semantic stream of SYthat can be monitored in real time. This stream can be stabilized with simple smoothing (e.g., average pooling) and decoded only when a significant semantic shift is detected, such as when the local window variance exceeds a threshold. In this way, VL-JEPA maintains always-on semantic monitoring while avoiding unnecessary decoding, achieving both responsiveness and efficiency."
  },
  {
    "question_id": "q47",
    "doc_id": "arxiv_251210942_vl_jepa",
    "question": "What are the main differences between VL-JEPA's embedding-space prediction approach and traditional token-space prediction in VLMs?",
    "type": "loc",
    "answer": "VL-JEPA uses a joint Embedding Predictive Architecture (JEPA) to predict the target embedding SY, instead of reconstructing the raw target Yin token space as in traditional VLMs. VL-JEPA is a vision-language model built on a Joint Embedding Predictive Architecture (JEPA) that uses a 0.5B predictor to map the target embedding SY to an embedding SY. VL-JEPA is a vision-language model built on a Joint Embedding Predictive Architecture (JEPA) that uses a 0.5B predictor to map the target embedding SY to an embedding SY. VL-JEPA is a vision-language model built on a Joint Embedding Predictive Architecture (JEPA) that uses a 0.5B predictor to map the target embedding SY to an embedding SY. VL-JEPA is a vision-language model built on a Joint Embedding Predictive Architecture (JEPA) that uses a 0.5B predictor to map"
  },
  {
    "question_id": "q48",
    "doc_id": "arxiv_251210942_vl_jepa",
    "question": "Does VL-JEPA use reinforcement learning for optimizing its vision encoder parameters?",
    "type": "impossible",
    "answer": "VL-JEPA is a self-supervised vision model that performs conditional latent prediction over vision and text, and preserves efficiency while enabling flexible, multitask architecture."
  },
  {
    "question_id": "q49",
    "doc_id": "arxiv_251210953_bidirectional_normalizing_flow",
    "question": "What is the main contribution of Bidirectional Normalizing Flow (BiFlow)?",
    "type": "fact",
    "answer": "Bidirectional Normalizing Flow (BiFlow) is a framework that removes the need for an exact analytic inverse."
  },
  {
    "question_id": "q50",
    "doc_id": "arxiv_251210953_bidirectional_normalizing_flow",
    "question": "Which architecture does BiFlow use for the reverse model?",
    "type": "fact",
    "answer": "Bidirectional Normalizing Flow (BiFlow) is a framework that removes the need for an exact analytic inverse."
  },
  {
    "question_id": "q51",
    "doc_id": "arxiv_251210953_bidirectional_normalizing_flow",
    "question": "What dataset and resolution were used in the experiments?",
    "type": "fact",
    "answer": "The experiment was conducted on a partial dataset and resolution of 50, 100, 200 DPI."
  },
  {
    "question_id": "q52",
    "doc_id": "arxiv_251210953_bidirectional_normalizing_flow",
    "question": "In which section does the paper describe the three strategies for learning the reverse model?",
    "type": "loc",
    "answer": "The authors describe the three strategies for learning the reverse model in Part 1 of the paper."
  },
  {
    "question_id": "q53",
    "doc_id": "arxiv_251210953_bidirectional_normalizing_flow",
    "question": "According to Table 3, how much faster is BiFlow compared to iTARFlow on TPU?",
    "type": "loc",
    "answer": "BiFlow achieves one to two orders of magnitude faster sampling on TPU, GPU, and CPU, while attaining superior generation quality."
  },
  {
    "question_id": "q54",
    "doc_id": "arxiv_251210953_bidirectional_normalizing_flow",
    "question": "Does the paper propose using a quantum neural network for training?",
    "type": "impossible",
    "answer": "The paper proposes using a quantum neural network for training."
  },
  {
    "question_id": "q55",
    "doc_id": "arxiv_251210954_group_diffusion",
    "question": "What is Group Diffusion?",
    "type": "fact",
    "answer": "Group Diffusion is a partial partial denoising method for generating images."
  },
  {
    "question_id": "q56",
    "doc_id": "arxiv_251210954_group_diffusion",
    "question": "What is the main contribution of GroupDiff?",
    "type": "fact",
    "answer": "GroupDiff provides a substantially improved generation performance across various design choices, achieving a much better FID score than the vanilla model. Below, we provide a detailed analysis of the impact of each component. Group model. The leading diffusion systems usually benefit from Classifier-Free Guidance [15], which takes the joint effect [doc=arxiv_251210954_group_diffusion, p=20, frag=49] GroupDiff-4* Class 1.76 283.5 0.81 0.61 + GroupDiff-4* CLIP-L 1.40 290.7 0.79 0.64 Table 7. System-level performance of pixel diffusion models evaluated on ImageNet 256256. : continue training from pre-trained checkpoint for an additional 100 epochs. setting. Table 6 shows CLIP-L yields the optimality performance while the simplest GroupDiff-4obtains a considerable improvement (14.5%) over the baseline, highlighting the effectiveness of cross-sample attention."
  },
  {
    "question_id": "q57",
    "doc_id": "arxiv_251210954_group_diffusion",
    "question": "What is the scaling effect observed in GroupDiff?",
    "type": "fact",
    "answer": "GroupDiff-l is a computationally lightweight model that provides a substantially improved generation quality across various design choices."
  },
  {
    "question_id": "q58",
    "doc_id": "arxiv_251210954_group_diffusion",
    "question": "Where in the paper is the definition of cross-attention score given?",
    "type": "loc",
    "answer": "The definition of cross-attention score is given in the Appendix."
  },
  {
    "question_id": "q59",
    "doc_id": "arxiv_251210954_group_diffusion",
    "question": "In which section are the limitations of GroupDiff discussed?",
    "type": "loc",
    "answer": "The main properties of GroupDiff are: GroupDiff-l is computationally lightweight compared to GroupDiff-f and close to baseline systems."
  },
  {
    "question_id": "q60",
    "doc_id": "arxiv_251210954_group_diffusion",
    "question": "Does the paper mention the application of GroupDiff to quantum image generation?",
    "type": "impossible",
    "answer": "The paper focuses on the development of a new generation model for pixel diffusion."
  },
  {
    "question_id": "q61",
    "doc_id": "arxiv_251209841_ChronusOmni.pdf",
    "question": "What are the six subtasks of audiovisual temporal grounding introduced in ChronusOmni?",
    "type": "fact",
    "answer": "Video-to-Time, Time-to-Video, Audio-to-Time, Time-to-Audio, Videoto-Audio, and Audio-to-Video."
  },
  {
    "question_id": "q62",
    "doc_id": "arxiv_251209841_ChronusOmni.pdf",
    "question": "Which large language model does ChronusOmni use as its backbone?",
    "type": "fact",
    "answer": "ChronusOmni uses 64 frames as its backbone."
  },
  {
    "question_id": "q63",
    "doc_id": "arxiv_251209841_ChronusOmni.pdf",
    "question": "What is the name of the dataset introduced in the paper to support audiovisual temporal grounding?",
    "type": "fact",
    "answer": "ChronusAV is a temporally-accurate, modality-complete, and cross-modal-aligned dataset to support the training and evaluation on the audiovisual temporal grounding task."
  },
  {
    "question_id": "q64",
    "doc_id": "arxiv_251209841_ChronusOmni.pdf",
    "question": "What is the average duration of videos in the ChronusAV dataset in seconds?",
    "type": "loc",
    "answer": "The average duration of videos in the ChronusAV dataset is 226 seconds."
  },
  {
    "question_id": "q65",
    "doc_id": "arxiv_251209841_ChronusOmni.pdf",
    "question": "In the T2V subtask, what evaluation metric is used to measure caption quality besides BLEU-4 and CIDEr?",
    "type": "loc",
    "answer": "SFT is crucial for caption quality. T2V CIDEr rises from 0.69 (w/o SFT) to 5.07, and T2A from 6.91 to 34.30. GRPO provides the largest single gains in video-audio alignment: removing it reduces all V2A and A2V metrics by over 3 times. With all components enabled, ChronusOmni achieves the best results across all six directions and all metrics, confirming that all three components are complementary and jointly necessary for strong audiovisual multimodal temporal grounding."
  },
  {
    "question_id": "q66",
    "doc_id": "arxiv_251209841_ChronusOmni.pdf",
    "question": "According to the paper, what is the exact number of video frames sampled in the V2A subtask during training in Section 6.3?",
    "type": "impossible",
    "answer": "The model trained on 64 frames."
  },
  {
    "question_id": "q67",
    "doc_id": "arxiv_251209897_SCOPE.pdf",
    "question": "What is the name of the environment used in the paper to evaluate SCOPE?",
    "type": "fact",
    "answer": "SCOPE is a hierarchical planning framework that uses LLM-generated subgoals only once at initialization, derived from suboptimal demonstration trajectories."
  },
  {
    "question_id": "q68",
    "doc_id": "arxiv_251209897_SCOPE.pdf",
    "question": "How many parameters does the SCOPE model have according to Table 3?",
    "type": "fact",
    "answer": "The SCOPE model has a total of 7 parameters."
  },
  {
    "question_id": "q69",
    "doc_id": "arxiv_251209897_SCOPE.pdf",
    "question": "What does the manager agent propose in the hierarchical planning framework?",
    "type": "fact",
    "answer": "A high-level plan based on the current environment state and the ultimate goal."
  },
  {
    "question_id": "q70",
    "doc_id": "arxiv_251209897_SCOPE.pdf",
    "question": "In Section 5.1, what is the rate at which random actions are injected into the generated trajectories to mimic human-like exploration?",
    "type": "loc",
    "answer": "The rate at which random actions are injected into the generated trajectories to mimic human-like exploration is a partial rate."
  },
  {
    "question_id": "q71",
    "doc_id": "arxiv_251209897_SCOPE.pdf",
    "question": "According to Table 4, which backend model achieves a success rate of 0.58 with 24B parameters?",
    "type": "loc",
    "answer": "GPT-4o (OpenAI et al., 2024) 0.58 1.8T No Mistral Small 3 (Mistral AI, 2025) 0.58 24B Yes SCOPE (ours) 0.56 11.04M GPT-3.5 (Brown et al., 2024) 0.52 175B No GPT-4o mini (OpenAI et al., 2024a) 0.43 8B No DeepSeek-R1-Distill-Qwen-32B (DeepSeek-AI et al., 2025) 0.13 32B Yes Claude-3 Haiku (Anthropic AI, 2024) 0.00 20B No"
  },
  {
    "question_id": "q72",
    "doc_id": "arxiv_251209897_SCOPE.pdf",
    "question": "In which year was the TextCraft environment first published, and who were its original creators?",
    "type": "impossible",
    "answer": "The text-based RL framework was first published in 2024. It was inspired by Minecraft."
  },
  {
    "question_id": "q73",
    "doc_id": "arxiv_251210262_VLM_NCD.pdf",
    "question": "What is the name of the proposed method in this paper for Novel Class Discovery?",
    "type": "fact",
    "answer": "LLM-NCD is a multimodal framework that breaks this bottleneck by fusing visualtextual semantics and prototype-guided clustering."
  },
  {
    "question_id": "q74",
    "doc_id": "arxiv_251210262_VLM_NCD.pdf",
    "question": "Which pre-trained model is used as the backbone for multimodal feature extraction?",
    "type": "fact",
    "answer": "Vision backbone, MLP connector and language backbone are trained jointly on the full mixed-format pretraining dataset. This allows the model to learn multimodal grounding over diverse input structures. Stage 2: Joint Pretraining. In this stage, the vision backbone remains frozen, while the MLP connector and language backbone are trained jointly on the full mixed-format pretraining dataset, as described in Section 3.1. This allows the model to learn multimodal grounding over diverse input structures. Stage 3: Instruction Fine-tuning. Finally, we fine-tune the model using the mixed instruction dataset, which is a combination of all the instruction samples mentioned in Table 3. This step enables the model to perform various downstream tasks through natural-language prompts. The vision backbone training completes in less than one hour, while the vision backbone completes in 4 days. Next, training the MLP connector requires approximately five hours. Joint pretraining on the mixed-format dataset takes roughly 34 hours to converge. Finally, instruction tuning takes 60 hours."
  },
  {
    "question_id": "q75",
    "doc_id": "arxiv_251210262_VLM_NCD.pdf",
    "question": "What is the evaluation metric used to measure clustering performance in the experiments?",
    "type": "fact",
    "answer": "Clustering accuracy (ACC) is the clustering accuracy measure used to measure the performance of the model."
  },
  {
    "question_id": "q76",
    "doc_id": "arxiv_251210262_VLM_NCD.pdf",
    "question": "According to Table 1, how many classes are in the unlabelled set (|y_u|) for CIFAR-100?",
    "type": "loc",
    "answer": "The number of classes in the unlabelled set is a partial number."
  },
  {
    "question_id": "q77",
    "doc_id": "arxiv_251210262_VLM_NCD.pdf",
    "question": "In Table 2, what is the accuracy achieved by the proposed method on the 'New' subset of ImageNet-100?",
    "type": "loc",
    "answer": "The method achieves the best performance on most classes, often improving significantly over previous methods."
  },
  {
    "question_id": "q78",
    "doc_id": "arxiv_251210262_VLM_NCD.pdf",
    "question": "What is the exact weight decay value used during the fine-tuning of the last block of the visual transformer in Section 4.1?",
    "type": "impossible",
    "answer": "The weight decay value used during the fine-tuning of the last block of the visual transformer in Section 4.1 is 0.05."
  },
  {
    "question_id": "q79",
    "doc_id": "arxiv_251210326_StainNet.pdf",
    "question": "What is the name of the SSL method used to pre-train StainNet?",
    "type": "fact",
    "answer": "The method is based on the MoCov3-based SSL approach."
  },
  {
    "question_id": "q80",
    "doc_id": "arxiv_251210326_StainNet.pdf",
    "question": "How many patch images are used for pre-training StainNet?",
    "type": "fact",
    "answer": "1.4 million patch images are used for pre-training StainNet."
  },
  {
    "question_id": "q81",
    "doc_id": "arxiv_251210326_StainNet.pdf",
    "question": "What is the main staining method that most existing pathology foundation models are pre-trained on?",
    "type": "fact",
    "answer": "HematoxylinEosin (H&E) is the most commonly used histological staining method, providing basic structural information by staining the cell nuclei and cytoplasm. However, H&E staining does not fully reveal certain cell features or tissue details, especially in the detection of immune responses or protein expression. As a result, several special staining methods have been developed to address these limitations (Gridley, 1957). Common special staining methods, such as IHC, Masson’s trichrome, and acid phosphatase staining, can highlight specific proteins, cell subtypes, or extracellular matrix components. For example, IHC utilizes antibodies to bind to target proteins, making it a valuable tool for cancer diagnosis and disease subtyping (Lin et al., 2024b). To address this issue, we propose StainNet, a specialized foundation model for special stains based on the vision transformer (ViT) architecture. StainNet adopts a selfdistillation SSL approach and is trained on over 1.4 million patch images cropping from 20,2"
  },
  {
    "question_id": "q82",
    "doc_id": "arxiv_251210326_StainNet.pdf",
    "question": "In which section of the paper is the pre-training loss curve of StainNet discussed?",
    "type": "loc",
    "answer": "The pre-training loss curve of StainNet is discussed in Part 1 of the paper."
  },
  {
    "question_id": "q83",
    "doc_id": "arxiv_251210326_StainNet.pdf",
    "question": "What database is used as the source of special staining WSIs for pre-training StainNet?",
    "type": "loc",
    "answer": "HISTAI database."
  },
  {
    "question_id": "q84",
    "doc_id": "arxiv_251210326_StainNet.pdf",
    "question": "What specific hardware configuration was used to fine-tune the downstream tasks?",
    "type": "impossible",
    "answer": "NVIDIA A10 GPU."
  },
  {
    "question_id": "q85",
    "doc_id": "arxiv_251210362_Visual_Funnel.pdf",
    "question": "What is the name of the two-step approach proposed to resolve Contextual Blindness?",
    "type": "fact",
    "answer": "Visual Funnel."
  },
  {
    "question_id": "q86",
    "doc_id": "arxiv_251210362_Visual_Funnel.pdf",
    "question": "Which model architecture does InstructBLIP use to compress visual information?",
    "type": "fact",
    "answer": "Q-Former is a partial model architecture that uses learnable query tokens to compress visual information."
  },
  {
    "question_id": "q87",
    "doc_id": "arxiv_251210362_Visual_Funnel.pdf",
    "question": "What term is used to describe the failure that occurs when single-crop methods isolate fine details from their broader context?",
    "type": "fact",
    "answer": "Contextual Blindness."
  },
  {
    "question_id": "q88",
    "doc_id": "arxiv_251210362_Visual_Funnel.pdf",
    "question": "In which section of the paper is the entropy-guided scale determination mechanism explained?",
    "type": "loc",
    "answer": "The main paper describes the entropy-guided scale determination mechanism."
  },
  {
    "question_id": "q89",
    "doc_id": "arxiv_251210362_Visual_Funnel.pdf",
    "question": "Which dataset is used in the ablation study to evaluate the impact of portfolio size?",
    "type": "loc",
    "answer": "Private dataset."
  },
  {
    "question_id": "q90",
    "doc_id": "arxiv_251210362_Visual_Funnel.pdf",
    "question": "What is the name of the first author of the ViCrop paper referenced in this work?",
    "type": "impossible",
    "answer": "Vivek Natarajan."
  },
  {
    "question_id": "q91",
    "doc_id": "arxiv_251210365_GPG.pdf",
    "question": "What is the full name of the theorem proposed for Transformer-based policies?",
    "type": "fact",
    "answer": "Generalized Policy Gradient Theorem IT1 IT2 IT|input| OT1 OT2 s1 s2 = [s1, a1] ... ... The Transformer Blocks OT1 a1 OT2 ... a2 OT3 a3 s3 = [s2, a2]"
  },
  {
    "question_id": "q92",
    "doc_id": "arxiv_251210365_GPG.pdf",
    "question": "Which algorithm does GRPO extend by using grouped trajectories for advantage computation?",
    "type": "fact",
    "answer": "GRPO uses a group of G output trajectories to compute the advantage Aold(s, a) as shown in Figure 1. This is especially useful for scenarios (e.g., math and coding) where the partial trajectory is not verifiable, but the whole trajectories can be evaluated with verified rewards."
  },
  {
    "question_id": "q93",
    "doc_id": "arxiv_251210365_GPG.pdf",
    "question": "What are the two special cases of the GPG framework mentioned in the paper?",
    "type": "fact",
    "answer": "Token-level Policy Gradient: When K = 1 (i.e., each macro-action corresponds to a single output token, MAi OTi), our GPG reduces to: J() = E |output| X i=1 log (OTi|MSi) Q(MSi, OTi) which is precisely the standard Policy Gradient Theorem, i.e., Equation (3). This establishes the standard policy gradient as a special case of our generalized framework. Token-level Policy Gradient: When K = 1 (i.e., each macro-action corresponds to a single output token, MAi OTi), our GPG reduces to: J() = E |output| X i=1 log (OTi|MSi) Q(MSi, OTi) which is precisely the standard Policy Gradient Theorem, i.e., Equation (3). This establishes the standard policy gradient as a special case of our generalized framework. Token-level Policy Gradient: When K = 1 (i.e., each macro-"
  },
  {
    "question_id": "q94",
    "doc_id": "arxiv_251210365_GPG.pdf",
    "question": "In which section of the paper is the 'Macro-action Segmentation' step explained?",
    "type": "loc",
    "answer": "Section 4 [doc=arxiv_251210365_GPG, p=4, frag=7] K Y T =1 (MAT | MST ) (11) where T represents the macro timestep. Section 4 Derivation of the GPG Theorem Given the macro states MSi and macro actions MAi as defined above, we establish the following Generalized Policy Gradient (GPG) Theorem for Transformer-based policies: J() =E K X T =1 [ log (MAT | MST ) T ] (12) A principal advantage of the GPG Theorem lies in its capacity to accommodate macro-action segments with arbitrary length. This flexible formulation yields significant practical benefits: notably, it naturally supports trajectory segmentation using special tokens (e.g., [SEP], [CLS] or [TOOL]). We elaborate on these applications and implementation considerations in Section 4."
  },
  {
    "question_id": "q95",
    "doc_id": "arxiv_251210365_GPG.pdf",
    "question": "Which model is used as the judge in the LLM-as-Judge evaluation protocol?",
    "type": "loc",
    "answer": "The LLM-as-a-Judge evaluation protocol uses a partial judge."
  },
  {
    "question_id": "q96",
    "doc_id": "arxiv_251210365_GPG.pdf",
    "question": "What is the name of the first author of the GRPO paper referenced in this work?",
    "type": "impossible",
    "answer": "Vicuna-7B"
  },
  {
    "question_id": "q97",
    "doc_id": "arxiv_251210534_Achieving_Olympia_Level.pdf",
    "question": "What is the name of the symbolic engine used as a tool by InternGeometry?",
    "type": "fact",
    "answer": "metry-DDAR is a symbolic engine for InternGeometry."
  },
  {
    "question_id": "q98",
    "doc_id": "arxiv_251210534_Achieving_Olympia_Level.pdf",
    "question": "How many geometry problems from IMO 2000–2024 does InternGeometry solve?",
    "type": "fact",
    "answer": "We use IMO 50 (Chervonyi et al., 2025) as the test set, which includes all geometry problems from IMO 2000 to IMO 2024. We additionally evaluate InternGeometry on the geometry problem from IMO 2025 in Table 2. We use AlphaGeometry 2 (Chervonyi et al., 2025) and SeedGeometry (Chervonyi et al., 2025) as our baselines, both of which are state-of-the-art geometry proving methods based on expert models. The performance of these baselines is taken directly from the results reported in their respective papers."
  },
  {
    "question_id": "q99",
    "doc_id": "arxiv_251210534_Achieving_Olympia_Level.pdf",
    "question": "What metric is used as an indicator of task complexity in the CBRL framework?",
    "type": "fact",
    "answer": "DDAR proof step count is used as an indicator of task complexity in the CBRL framework."
  },
  {
    "question_id": "q100",
    "doc_id": "arxiv_251210534_Achieving_Olympia_Level.pdf",
    "question": "In which appendix are the improvements to InternGeometry-DDAR described?",
    "type": "loc",
    "answer": "Appendix B IMPROVEMENTS IN INTERNGEOMETRY-DDAR InternGeometry-DDAR builds upon open-sourced symbolic engines (i.e., Newclid (Sicca et al., 2024) and AlphaGeometry (Chervonyi et al., 2025) and mainly consist of two components: a deductive database and algebraic reasoning. The former expands the current set of premises toward the proof goal based on geometry rules, while the latter performs angle, length, and ratio chasing using Gaussian elimination. We introduce three main improvements: dynamic diagram adjustment, the incorporation of syntax and rules for handling double points, and the addition of new predicates and rules. First, open-sourced symbolic engines can only create points one by one based on existing construction definitions, with each point constrained by at most two construction definitions. However, in IMO geometry problem, it is often necessary to make global adjustments to previously constructed points so they satisfy more specific requirements (e.g., a line defined as two existing points may also need to be tangent to a existing"
  },
  {
    "question_id": "q101",
    "doc_id": "arxiv_251210534_Achieving_Olympia_Level.pdf",
    "question": "Which algorithm is described in Appendix D for generating geometry questions?",
    "type": "loc",
    "answer": "InternGeometry is a medalist-level LLM agent for geometry. It is a medalist-level LLM agent for geometry. It is a powerful symbolic tool for InternGeometry."
  },
  {
    "question_id": "q102",
    "doc_id": "arxiv_251210534_Achieving_Olympia_Level.pdf",
    "question": "What is the name of the first author of the AlphaGeometry 2 paper referenced in this work?",
    "type": "impossible",
    "answer": "Chervonyi et al., 2025."
  },
  {
    "question_id": "q103",
    "doc_id": "arxiv_251210554_Grounding_Tokens.pdf",
    "question": "What are the two core types of tokens introduced in GETok?",
    "type": "fact",
    "answer": "Grid tokens are the two core types of tokens introduced in GETok. Grid tokens are the two core types of tokens introduced in GETok. Grid tokens are the two core types of tokens introduced in GETok."
  },
  {
    "question_id": "q104",
    "doc_id": "arxiv_251210554_Grounding_Tokens.pdf",
    "question": "Which base model is used for GETok in the experiments?",
    "type": "fact",
    "answer": "InternThinker-32B is used as the backbone model for the method."
  },
  {
    "question_id": "q105",
    "doc_id": "arxiv_251210554_Grounding_Tokens.pdf",
    "question": "What is the name of the greedy algorithm used to convert masks into grid tokens?",
    "type": "fact",
    "answer": "edy Mask-to-Token Conversion We have developed a greedy algorithm to facilitate the transformation from masks to grid tokens."
  },
  {
    "question_id": "q106",
    "doc_id": "arxiv_251210554_Grounding_Tokens.pdf",
    "question": "In the Self-Improving RL framework, which token is used to enable iterative refinement and self-correction?",
    "type": "loc",
    "answer": "The TSM OCR prediction is used to enable iterative refinement and self-correction. The TSM OCR prediction is used to enable iterative refinement and self-correction."
  },
  {
    "question_id": "q107",
    "doc_id": "arxiv_251210554_Grounding_Tokens.pdf",
    "question": "According to the experiments, what grid size achieves a good trade-off between spatial precision and vocabulary overhead?",
    "type": "loc",
    "answer": "Grid resolution is a key parameter for GETok, governing the trade-off between spatial precision and vocabulary expansion. Grid resolution is a crucial parameter for GETok, governing the trade-off between spatial precision and vocabulary expansion."
  },
  {
    "question_id": "q108",
    "doc_id": "arxiv_251210554_Grounding_Tokens.pdf",
    "question": "What is the exact training loss value reported for GETok-SFT on the ReasonSeg validation set?",
    "type": "impossible",
    "answer": "The training loss for GETok-SFT on the ReasonSeg validation set is a partial loss."
  },
  {
    "question_id": "q109",
    "doc_id": "arxiv_251210808_Graph_Laplacian_Transformer.pdf",
    "question": "What are the names of the two main modules proposed in the GLAT framework?",
    "type": "fact",
    "answer": "GSM8k-Symbolic (OCR-correct subset) and MMLU [16] AI2-ARC [9] GSM-Sym [27] Text Mix Img Text Mix Img Text Mix Img Deepseek-T 29.5 26.6 25.3 34.2 28.7 27.5 15.5 0.8 0.7 Deepseek-S 54.9 54.9 51.2 69.1 68.7 70.0 67.3 54.7 60.9 GPT-4o-mini 84.4 77.2 77.0 93.6 89.3 89.9 91.4 86.7 89.0 GPT-5-mini 89.3 86.5 87.8 92.6 91.7 93.3 Phi-3.5 61.1 41.5 32.5 73.3 54.3 37.9 66.9 42.9 48.3 Phi-4 47.0 44.2 31.4 59.9 48.5 34.5 55.8 54.5 Qwen-2.5-7B 72.6 71.8 72.3 86.1 87.0 88.3 89.3 89.9 93.6 89.9 91.4 86.7 89.0 GPT-5-mini 89.3 86.5 87"
  },
  {
    "question_id": "q110",
    "doc_id": "arxiv_251210808_Graph_Laplacian_Transformer.pdf",
    "question": "Which pretrained model is used for local feature extraction in the Iterative Refinement Module (IRM)?",
    "type": "fact",
    "answer": "ResNet50 is used for local feature extraction in the Iterative Refinement Module (IRM) and context-aware scoring using a frozen FM."
  },
  {
    "question_id": "q111",
    "doc_id": "arxiv_251210808_Graph_Laplacian_Transformer.pdf",
    "question": "How many publicly available datasets were used to evaluate the proposed model?",
    "type": "fact",
    "answer": "Three publicly available datasets were used to evaluate the proposed model."
  },
  {
    "question_id": "q112",
    "doc_id": "arxiv_251210808_Graph_Laplacian_Transformer.pdf",
    "question": "In the Graph Laplacian Transformer, what does the learnable filter L_θ optimize during training?",
    "type": "loc",
    "answer": "The Graph Laplacian Transformer optimizes partiality of the training task by minimizing the influence of neighboring patches."
  },
  {
    "question_id": "q113",
    "doc_id": "arxiv_251210808_Graph_Laplacian_Transformer.pdf",
    "question": "According to the ablation study, which attention mechanism performed best in terms of spatial consistency for prostate cancer grading?",
    "type": "loc",
    "answer": "GLA integrates graph Laplacian constraints to maintain spatial coherence across histologically similar regions while refining feature embeddings."
  },
  {
    "question_id": "q114",
    "doc_id": "arxiv_251210808_Graph_Laplacian_Transformer.pdf",
    "question": "What is the exact FLOPs value reported for the full proposed model (Exp. 1) in Table 2?",
    "type": "impossible",
    "answer": "The FLOPs value for the full proposed model is a partial value."
  },
  {
    "question_id": "q115",
    "doc_id": "arxiv_251210847_Large_Language_Model.pdf",
    "question": "What is the total number of records in the superconductor database compiled using LLM extraction?",
    "type": "fact",
    "answer": "78,203 records, covering 19,058 unique compositions, are compiled using LLM extraction."
  },
  {
    "question_id": "q116",
    "doc_id": "arxiv_251210847_Large_Language_Model.pdf",
    "question": "Which fine-tuning method was used to adapt the LLMs for superconductivity tasks?",
    "type": "fact",
    "answer": "We used a XGBoost ensemble and a fully connected feedforward neural network ensemble to train the LLMs for three tasks: (i) classifying superconductors vs. nonsuperconductors, (ii) predicting the superconducting transition temperature directly from composition or structure-informed inputs, and (iii) inverse design of candidate compositions conditioned on target Tc."
  },
  {
    "question_id": "q117",
    "doc_id": "arxiv_251210847_Large_Language_Model.pdf",
    "question": "Which open-source LLM achieved the highest accuracy in the superconductivity classification task?",
    "type": "fact",
    "answer": "Large language models (LLMs) offer new opportunities for automated data extraction and property prediction across materials science, yet their use in superconductivity research remains limited. Here we construct a large experimental database of 78,203 records, covering 19,058 unique chemical compositions, extracted from scientific literature using LLM driven workflow. Each entry includes chemical composition, critical temperature, measurement pressure, structural descriptors, and critical fields. We fine-tune several open-source LLMs for three tasks: (i) classifying superconductors vs. nonsuperconductors, (ii) predicting the superconducting transition temperature directly from composition or structure-informed inputs, and (iii) inverse design of candidate compositions conditioned on target Tc. The fine-tuned LLMs achieve performance comparable to traditional feature-based models—and in some cases exceed them—while substantially outperforming their base versions and capturing meaningful chemical and structural trends. The inverse-design model generates chemically plausible compositions, including 28% novel candidates not seen in training. Finally, applying the trained predictors to GNoME database identifies unreported"
  },
  {
    "question_id": "q118",
    "doc_id": "arxiv_251210847_Large_Language_Model.pdf",
    "question": "Why did the inclusion of full CIFs in the regression task lead to decreased performance compared to composition-only models?",
    "type": "loc",
    "answer": "CIFs significantly increase sequence length and token complexity, since each file encodes lattice parameters, atomic coordinates, Wyckoff positions, and site occupations. This reduction can be attributed to two factors: (i) CIF inputs produce substantially longer and more complex token sequences, making optimization more challenging, especially under parameter-efficient fine-tuning; and (ii) the CIF-matched subset is considerably smaller and biased toward low-Tc compounds, resulting in reduced data diversity and fewer high-temperature examples for the model to learn from."
  },
  {
    "question_id": "q119",
    "doc_id": "arxiv_251210847_Large_Language_Model.pdf",
    "question": "How many novel compositions did the inverse design model generate that were not present in the training set?",
    "type": "loc",
    "answer": "Across all queries, the model produced 4,290 compositions not present in the training set, of which 2,890 were unique. This indicates that the model possesses meaningful generative capability and is not merely memorizing its training distribution. Among the novel compositions, 2,768 were generated with both a chemical formula and an associated space group (Fig. 6c). This indicates that the model possesses [doc=arxiv_251210847_Large_Language_Model, p=10, frag=27] lower predictive accuracy."
  },
  {
    "question_id": "q120",
    "doc_id": "arxiv_251210847_Large_Language_Model.pdf",
    "question": "What is the exact mean absolute error (MAE) reported for the Qwen3-14B model trained on composition + crystal system + space group inputs?",
    "type": "impossible",
    "answer": "The MAE for the Qwen3-14B model is a partial error."
  },
  {
    "question_id": "q121",
    "doc_id": "arxiv_230712057_external_reasoning_v2",
    "question": "What is the main goal of the External Reasoning framework proposed in the paper?",
    "type": "fact",
    "answer": "A tiered policy for External Reasoning is established."
  },
  {
    "question_id": "q122",
    "doc_id": "arxiv_230712057_external_reasoning_v2",
    "question": "What are the three levels of assistance defined in the policy-oriented multi-LLM system?",
    "type": "fact",
    "answer": "The policy-oriented multi-LLM system is a multi-level system that enables open-world multi-task agents."
  },
  {
    "question_id": "q123",
    "doc_id": "arxiv_230712057_external_reasoning_v2",
    "question": "Which embedding model is used by default to encode documents and queries in the retrieval component?",
    "type": "fact",
    "answer": "Text-embedding-ada is the default model for generating embeddings. This choice is rooted in its ability to offer a balance between performance and computational efficiency for a broad range of queries. However, when the system is engaged in the 'extreme level assistance' mode - a setting that is triggered when faced with particularly challenging queries we opt to maximize all system components to their highest capacity. In this mode, we employ text-embedding-ada owing to its superior capability to create high-quality embeddings. Additionally, the retrieval mechanism, which uses CLIP’s cross-modal joint embeddings for text retrieval, further enhances performance. Overall, in NCD settings, semantic information plays a crucial role in improving model representation and performance, and retrieval-based augmentation mechanisms further improve clustering accuracy compared to using visual features alone. Descriptive retrieval text is essential to our method. In typical datasets, text descriptions may vary in their association with images. Ideally, we aim to encode salient objects and meaningful details within images to enhance representation learning for object recognition tasks. For contrastive models, the learned"
  },
  {
    "question_id": "q124",
    "doc_id": "arxiv_230712057_external_reasoning_v2",
    "question": "In which section does the paper describe the policy-oriented multi-LLM assistance mechanism and escalation strategy?",
    "type": "loc",
    "answer": "The paper describes the policy-oriented multi-LLM assistance mechanism and escalation strategy."
  },
  {
    "question_id": "q125",
    "doc_id": "arxiv_230712057_external_reasoning_v2",
    "question": "Where does the paper explain how summarization improves retrieval performance for implicit (Type 2) questions?",
    "type": "loc",
    "answer": "Summarization improves retrieval performance for implicit (Type 2) questions."
  },
  {
    "question_id": "q126",
    "doc_id": "arxiv_230712057_external_reasoning_v2",
    "question": "What was the total monetary cost in USD of training and evaluating the full External Reasoning system?",
    "type": "impossible",
    "answer": "The total cost of training and evaluating the full External Reasoning system is estimated at USD 2.6 billion."
  },
  {
    "question_id": "q127",
    "doc_id": "arxiv_251024031_llmloganalyzer",
    "question": "What is the name of the clustering-based log analysis chatbot proposed in the paper?",
    "type": "fact",
    "answer": "LLMLogAnalyzer is a clustering-based log analysis chatbot that leverages Large Language Models and Machine Learning algorithms to enhance log analysis accuracy, efficiency, and effectiveness."
  },
  {
    "question_id": "q128",
    "doc_id": "arxiv_251024031_llmloganalyzer",
    "question": "Which log parsing algorithm is used by LLMLogAnalyzer to convert raw logs into structured data?",
    "type": "fact",
    "answer": "LLMLogAnalyzer uses LLM to identify the log type from the raw log file. This log type is then used as a parameter for the Drain algorithm. The Drain algorithm is then used to parse raw logs into a structured format with associated events."
  },
  {
    "question_id": "q129",
    "doc_id": "arxiv_251024031_llmloganalyzer",
    "question": "What are the four main stages of the LLMLogAnalyzer framework?",
    "type": "fact",
    "answer": "Partial: Identify the log type from the raw log file. Partial: Identify the log type from among 16 supported candidates, including Windows, Mac, Linux, and various application and system logs from Loghub. Partial: Identify the log type from among 16 supported candidates, including Windows, Mac, Linux, and various application and system logs from Loghub. Partial: Identify the log type from among 16 supported candidates, including Windows, Mac, Linux, and various application and system logs from Loghub. Partial: Identify the log type from among 16 supported candidates, including Windows, Mac, Linux, and various application and system logs from Loghub. Partial: Identify the log type from among 16 supported candidates, including Windows, Mac, Linux, and various application and system logs from Loghub. Partial: Identify the log type from among 16 supported candidates, including Windows, Mac, Linux, and various application and system logs from Loghub. Partial: Identify the log type from among 16 supported candidates, including Windows, Mac, Linux, and various application and system logs from Loghub. Partial: Identify the log type from among 16 supported candidates, including Windows,"
  },
  {
    "question_id": "q130",
    "doc_id": "arxiv_251024031_llmloganalyzer",
    "question": "In which section does the paper describe the routing mechanism that classifies queries into all, partial, and general tiers?",
    "type": "loc",
    "answer": "The Router, a critical component of LLMLogAnalyzer, performs query analysis, categorizing queries into three distinct categories: all events, partial, and general."
  },
  {
    "question_id": "q131",
    "doc_id": "arxiv_251024031_llmloganalyzer",
    "question": "Where does the paper explain the use of ROUGE-1 and cosine similarity as evaluation metrics?",
    "type": "loc",
    "answer": "A review of the literature on the use of ROUGE-1 and cosine similarity as evaluation metrics."
  },
  {
    "question_id": "q132",
    "doc_id": "arxiv_251024031_llmloganalyzer",
    "question": "What was the exact training time in hours required to train the LLMLogAnalyzer model from scratch?",
    "type": "impossible",
    "answer": "The LLMLogAnalyzer model is a partial training model that is trained from scratch."
  },
  {
    "question_id": "q133",
    "doc_id": "arxiv_251207230_strings",
    "question": "What is the main objective of the STRinGS framework proposed in the paper?",
    "type": "fact",
    "answer": "We propose STRinGS, a text-aware, selective refinement framework to address this issue for 3D reconstruction."
  },
  {
    "question_id": "q134",
    "doc_id": "arxiv_251207230_strings",
    "question": "What metric is introduced by the authors to quantitatively evaluate text readability in reconstructed 3D scenes?",
    "type": "fact",
    "answer": "Selective Text Refinement in Gaussian Splatting (STRinGS) is a novel framework for explicit text refinement in 3D reconstructions. It enables accurate and readable text in rendered novel views without compromising image quality. It also proposes OCR Character Error Rate (OCRCER) as a text readability measure to compare rendered and ground-truth images using a standard Optical Character Recongizer [5]. It achieves an average of 23.0% relative improvement in OCR-CER over standard 3DGS [14] at 30K iterations and 63.6% relative improvement in OCRCER at 7K training iterations."
  },
  {
    "question_id": "q135",
    "doc_id": "arxiv_251207230_strings",
    "question": "What dataset do the authors introduce to benchmark text reconstruction performance in text-rich 3D scenes?",
    "type": "fact",
    "answer": "STRinGS2 [doc=arxiv_251207230_strings, p=6, frag=16] 3DGS [14] Mip-Splatting [32] 3DGS-MCMC [15] EDC-AbsGS [6] STRinGS (Ours) Ground Truth"
  },
  {
    "question_id": "q136",
    "doc_id": "arxiv_251207230_strings",
    "question": "In which section does the paper describe the two-phase optimization strategy for selective text refinement?",
    "type": "loc",
    "answer": "Phase 2: Full Scene Refinement"
  },
  {
    "question_id": "q137",
    "doc_id": "arxiv_251207230_strings",
    "question": "Where does the paper explain how OCR-CER is computed using OCR outputs from rendered and ground-truth images?",
    "type": "loc",
    "answer": "The authors describe how OCR-based Character Error Rate is computed using OCR outputs from rendered and ground-truth images. For each evaluation image, we compute the Character Error Rate (CER): the normalized Levenshtein distance between recognized and ground-truth text, using a recall-based approach that penalizes missing and mismatched ground-truth characters. OCR-CER reflects how well the reconstructed image retains readable and accurate textual information. The CER scores are aggregated across all evaluation views within each scene. Additional details are provided in Appendix C. 5.2. Comparison with Existing Works Baselines. We compare against vanilla 3DGS [14] and other recent methods. While there are no existing methods targeting text reconstruction, Mip-Splatting [32], 3DGSMCMC [15], AbsGS[31], and EDC-AbsGS [6] 1 serve as strong baselines as they refine the overall scene."
  },
  {
    "question_id": "q138",
    "doc_id": "arxiv_251207230_strings",
    "question": "Does the STRinGS framework incorporate a reinforcement learning component to optimize Gaussian parameters?",
    "type": "impossible",
    "answer": "STRinGS is a hybrid strategy that enables targeted and region-aware optimization, ensuring sharp and readable text while preserving overall scene quality."
  },
  {
    "question_id": "q139",
    "doc_id": " arxiv_251208922_unit_tair",
    "question": "What is the main objective of the UniT framework?",
    "type": "fact",
    "answer": "Visual Funnel (VF) is a training-free methodology that holistically addresses both Localization and Integration in a holistic manner."
  },
  {
    "question_id": "q140",
    "doc_id": " arxiv_251208922_unit_tair",
    "question": "Which three main components are integrated in the UniT framework?",
    "type": "fact",
    "answer": "UniT is a framework for a wide range of applications, including computer vision, computer architecture, and computer science."
  },
  {
    "question_id": "q141",
    "doc_id": " arxiv_251208922_unit_tair",
    "question": "Which Vision-Language Model is selected as the VLM component in UniT?",
    "type": "fact",
    "answer": "VL-JEPA is a model with the joint embedding predictive architecture (JEPA) for vision-language tasks."
  },
  {
    "question_id": "q142",
    "doc_id": " arxiv_251208922_unit_tair",
    "question": "In which section does the paper describe the complementary roles of the VLM and TSM?",
    "type": "loc",
    "answer": "Tab. 5 and Fig. 4"
  },
  {
    "question_id": "q143",
    "doc_id": " arxiv_251208922_unit_tair",
    "question": "Which figure illustrates the overall UniT framework architecture?",
    "type": "loc",
    "answer": "Figure 1b shows the overall UniT framework architecture."
  },
  {
    "question_id": "q144",
    "doc_id": " arxiv_251208922_unit_tair",
    "question": "Does UniT employ reinforcement learning to optimize the diffusion denoising process?",
    "type": "impossible",
    "answer": "We use a method called GroupDiff to train the diffusion model for both conditional and unconditional denoising."
  },
  {
    "question_id": "q145",
    "doc_id": "arxiv_251208923_cross_modal_inconsistency_mllms",
    "question": "What is the main issue investigated in this paper regarding multimodal large language models?",
    "type": "fact",
    "answer": "Multimodal large language models are a common feature of the world."
  },
  {
    "question_id": "q146",
    "doc_id": "arxiv_251208923_cross_modal_inconsistency_mllms",
    "question": "What types of multimodal tasks are primarily used to evaluate cross-modal inconsistency in the study?",
    "type": "fact",
    "answer": "Multimodal large language models (MLLMs) are trained to represent vision and language in a shared space. But does this joint representation enable consistent reasoning across modalities? We introduce REST and REST+ as benchmarks to measure cross-modal inconsistency. Unlike previous benchmarks, we include a new set of tasks (SOEBENCH) that is guaranteed to not be seen during pre-training and control for OCR complexity. Next, we evaluate 15 frontier MLLMs and find substantial inconsistencies across modalities (at least 10% inconsistency), even when controlling for OCR. This leaves a notable gap of solvable questions that current models fail to capture. Finally, we analyse the internal representations of matching samples (i.e., samples with the same information in different modalities) and find that they show higher cosine similarity than non-matching pairs, and this similarity magnitude correlates with our consistency score on our benchmark. We will release all code and data upon publication."
  },
  {
    "question_id": "q147",
    "doc_id": "arxiv_251208923_cross_modal_inconsistency_mllms",
    "question": "What evaluation strategy is proposed to systematically measure cross-modal inconsistency?",
    "type": "fact",
    "answer": "We introduce REST and REST+ as benchmarks to measure cross-modal inconsistency. We measure answer consistency across modalities with the Render-Equivalence Rate (RER) and the Cross-Modality Failure Rate (CFR) and evaluate 15 MLLMs (on OCR correct, sorted on REST) and find that the degree of cross-modal inconsistency varies substantially across models even when controlling for OCR. We also evaluate inconsistency in questions solvable only through specific modalities, i.e., those answerable only through specific modalities, where text, image, mixed, and C(q, m) indicate correctness (1 correct, 0 incorrect) for question q and modality m. We include only the questions where text is perfectly recognised. We only evaluate letters and digits to focus on semantic recognition rather than punctuation."
  },
  {
    "question_id": "q148",
    "doc_id": "arxiv_251208923_cross_modal_inconsistency_mllms",
    "question": "In which section does the paper define the formal concept of cross-modal inconsistency?",
    "type": "loc",
    "answer": "Multimodal large language models (MLLMs) are trained to represent vision and language in a shared space. But does this joint representation enable consistent reasoning across modalities? We introduce REST and REST+ as benchmarks to measure cross-modal inconsistency. Unlike previous benchmarks, we include a new set of tasks (SOEBENCH) that is guaranteed to not be seen during pre-training and control for OCR complexity. Next, we evaluate 15 frontier MLLMs and find substantial inconsistencies across modalities (at least 10% inconsistency), even when controlling for OCR. This leaves a notable gap of solvable questions that current models fail to capture. Finally, we analyse the internal representations of matching samples (i.e., samples with the same information in different modalities) and find that they show higher cosine similarity than non-matching pairs, and this similarity magnitude correlates with our consistency score on our benchmark. We will release all code and data upon publication."
  },
  {
    "question_id": "q149",
    "doc_id": "arxiv_251208923_cross_modal_inconsistency_mllms",
    "question": "Where does the paper discuss empirical results comparing different multimodal large language models?",
    "type": "loc",
    "answer": "National Science Review, 11(12): nwae403, 2024. 1 [44] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. CVPR, 2024. 1, 3, 8 [47] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, and Graham Neubig. Mmmu-pro: A more robust multi-discipline multimodal understanding and reasoning benchmark for expert agi. CVPR, 2024. 1, 3, 8 [47] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shengbang Tong, Yuxuan Sun"
  },
  {
    "question_id": "q150",
    "doc_id": "arxiv_251208923_cross_modal_inconsistency_mllms",
    "question": "Does the paper report the exact energy consumption in kilowatt-hours for training the evaluated models?",
    "type": "impossible",
    "answer": "The paper does not report the exact energy consumption for training the evaluated models."
  },
  {
    "question_id": "q151",
    "doc_id": "arxiv_251209350_textguider",
    "question": "What problem in text-to-image generation does TextGuider specifically aim to address?",
    "type": "fact",
    "answer": "Text omission is a problem in text-to-image generation that is largely unresolved."
  },
  {
    "question_id": "q152",
    "doc_id": "arxiv_251209350_textguider",
    "question": "What two loss functions are introduced in TextGuider to improve attention alignment during text rendering?",
    "type": "fact",
    "answer": "We propose a training-free latent guidance strategy that leverages our novel attention-based loss functions to mitigate text omission."
  },
  {
    "question_id": "q153",
    "doc_id": "arxiv_251209350_textguider",
    "question": "Which evaluation metrics are used to assess text rendering quality in the experiments?",
    "type": "fact",
    "answer": "Sentence Accuracy (Sen. Acc.) and Normalized Edit Distance (NED) are evaluation metrics used to assess text rendering quality in the experiments."
  },
  {
    "question_id": "q154",
    "doc_id": "arxiv_251209350_textguider",
    "question": "In which section of the paper are the split loss and wrap loss formally defined and explained?",
    "type": "loc",
    "answer": "The paper describes the split loss and wrap loss in the context of the Discrete Diffusion Models."
  },
  {
    "question_id": "q155",
    "doc_id": "arxiv_251209350_textguider",
    "question": "Which figure illustrates the difference in attention maps between successful and failed text rendering cases?",
    "type": "loc",
    "answer": "The attention maps of the opening quotation mark and the textual content tokens are spatially well-separated. The attention maps for each textual content token are spatially well-separated. The attention maps for each textual content token are spatially well-separated. The attention maps for each textual content token are spatially well-separated. The attention maps for each textual content token are spatially well-separated. The attention maps for each textual content token are spatially well-separated. The attention maps for each textual content token are spatially well-separated. The attention maps for each textual content token are spatially well-separated. The attention maps for each textual content token are spatially well-separated. The attention maps for each textual content token are spatially well-separated. The attention maps for each textual content token are spatially well-separated. The attention maps for each textual content token are spatially well-separated. The attention maps for each textual content token are spatially well-separated. The attention"
  },
  {
    "question_id": "q156",
    "doc_id": "arxiv_251209350_textguider",
    "question": "Does the paper report experiments evaluating TextGuider on real-world printed documents scanned with OCR hardware?",
    "type": "impossible",
    "answer": "TextGuider is a text-aware, selective refinement framework that addresses text omission through training-free latent guidance. It guides the attention map of the textual contents to be aligned with the appropriate text regions."
  },
  {
    "question_id": "q157",
    "doc_id": "arxiv_251209874_pdf_formula_benchmark",
    "question": "What is the main motivation for introducing a new benchmark for mathematical formula extraction from PDFs?",
    "type": "fact",
    "answer": "We introduce a novel benchmarking framework centered on synthetically generated PDFs with precise LaTeX ground truth, enabling systematic control over layout, formulas, and content characteristics."
  },
  {
    "question_id": "q158",
    "doc_id": "arxiv_251209874_pdf_formula_benchmark",
    "question": "What synthetic data source is used to generate formulas for the benchmark PDFs, and how are trivial formulas filtered out?",
    "type": "fact",
    "answer": "The formula dataset component extracts and processes mathematical formulas from Wikipedia to create the wikipedia-latex-formulas-319k collection. The benchmark dataset component generates synthetic PDFs with precise ground truth by randomly combining sampled formulas from this dataset with text segments and inline formulas using randomly composed LaTeX templates. The evaluation pipeline component matches parsed text against ground truth using two-stage LLM-based matching and evaluates all formula pairs. This filtering step, which reduced the dataset by approximately half, yielded the final wikipedia-latex-formulas-319k dataset4, which we contribute as part of this work and which serves as the data pool from which formulas are randomly sampled for benchmark PDF generation."
  },
  {
    "question_id": "q159",
    "doc_id": "arxiv_251209874_pdf_formula_benchmark",
    "question": "Which evaluation approach shows the highest correlation with human judgment for formula correctness and semantic equivalence?",
    "type": "fact",
    "answer": "LLM-based evaluation achieves substantially higher correlation with human judgment for formula correctness and semantic equivalence than CDM or text-based metrics."
  },
  {
    "question_id": "q160",
    "doc_id": "arxiv_251209874_pdf_formula_benchmark",
    "question": "In which section of the paper is the robust two-stage LLM-based formula matching pipeline described?",
    "type": "loc",
    "answer": "To address the matching challenges outlined above, we developed a robust two-stage approach that combines LLM-based semantic matching with deterministic fuzzy matching validation, following the principle of turning unstable models into stable systems."
  },
  {
    "question_id": "q161",
    "doc_id": "arxiv_251209874_pdf_formula_benchmark",
    "question": "Which figure presents the correlation comparison between automated metrics and human evaluation scores?",
    "type": "loc",
    "answer": "Figure 2 shows the correlation comparison between automated metrics and human evaluation scores."
  },
  {
    "question_id": "q162",
    "doc_id": "arxiv_251209874_pdf_formula_benchmark",
    "question": "Does the paper report an evaluation of the benchmark on scanned historical documents from the 19th century?",
    "type": "impossible",
    "answer": "The paper reports an evaluation of the benchmark on scanned historical documents from the 19th century."
  },
  {
    "question_id": "q163",
    "doc_id": "arxiv_251210877_guided_transfer_learning",
    "question": "What is the primary goal of Guided Transfer Learning (GTL) for discrete diffusion models?",
    "type": "fact",
    "answer": "This method enables sampling from a target distribution without modifying the pretrained denoiser. This enables sampling from a target distribution without modifying the pretrained denoiser. This significantly reduces the training cost compared to conventional finetuning approaches."
  },
  {
    "question_id": "q164",
    "doc_id": "arxiv_251210877_guided_transfer_learning",
    "question": "How does Guided Transfer Learning (GTL) address computational challenges in discrete diffusion models with large vocabularies and long sequences?",
    "type": "fact",
    "answer": "We introduce a guided transfer learning framework for discrete diffusion models. This enables sampling from a target distribution without modifying the pretrained denoiser. This significantly reduces the training cost compared to conventional finetuning approaches. We further introduce a guided sampling algorithm that reduces the number of guidance calls per denoising step by focusing computation on plannerselected positions and top candidate tokens. This design improves efficiency."
  },
  {
    "question_id": "q165",
    "doc_id": "arxiv_251210877_guided_transfer_learning",
    "question": "Which task was used to evaluate GTL in the paper, and how did GTL perform compared to vanilla and fine-tuned models?",
    "type": "fact",
    "answer": "A task to extract multi-page tables."
  },
  {
    "question_id": "q166",
    "doc_id": "arxiv_251210877_guided_transfer_learning",
    "question": "In which section of the paper is the computational cost comparison between different methods discussed?",
    "type": "loc",
    "answer": "Params/Cost Inference Params/Cost Params/Cost Inference Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/Cost Params/C"
  },
  {
    "question_id": "q167",
    "doc_id": "arxiv_251210877_guided_transfer_learning",
    "question": "Which figure shows the performance comparison of GTL with vanilla and fine-tuned models across different target-domain training data fractions?",
    "type": "loc",
    "answer": "GTL outperforms vanilla and fine-tuned diffusion across all data-scarcity regimes while training only 7% as many parameters (Fig. 1)."
  },
  {
    "question_id": "q168",
    "doc_id": "arxiv_251210877_guided_transfer_learning",
    "question": "Does the paper provide experiments evaluating GTL's performance on real-world image datasets?",
    "type": "impossible",
    "answer": "We provide a theoretical analysis and formal theorem of the proposed guided transfer learning method, demonstrating that ratio-based guidance correctly recovers the target reverse transition."
  },
  {
    "question_id": "q169",
    "doc_id": "arxiv_251210888_pubtables_v2",
    "question": "What is the main contribution of PubTables-1M v2 compared to the original PubTables-1M dataset?",
    "type": "fact",
    "answer": "PubTables-1M v2 is the first large-scale dataset for evaluating models on fully end-to-end document-level table extraction and the first dataset containing multi-page tables. Each table is annotated with its structure, content, and the bounding boxes for each section of the table on each page it spans. Each table is annotated with its bounding box (or boxes) on every page it spans, as well as its structure and text content. Each table is annotated with its bounding box (or boxes) on every page it spans, as well as its structure and text content. Each table is annotated with its bounding box (or boxes) on every page it spans, as well as its structure and text content. Each table is annotated with its bounding box (or boxes) on every page it spans, as well as its structure and text content. Each table is annotated with its bounding box (or boxes) on every page it spans, as well as its structure and text content. Each table is annotated with its bounding box (or boxes) on every page it spans, as well as its structure and"
  },
  {
    "question_id": "q170",
    "doc_id": "arxiv_251210888_pubtables_v2",
    "question": "From which types of documents are the tables in PubTables-1M v2 primarily sourced?",
    "type": "fact",
    "answer": "PubTables-1M v2 is the first large-scale dataset for evaluating models on fully end-to-end document-level table extraction and the first dataset containing multi-page tables. Each table is annotated with its structure, content, and the bounding boxes for each section of the table on each page it spans. PubTables-1M v2 is the first large-scale dataset for evaluating models on fully end-to-end document-level table extraction and the first dataset containing multi-page tables. Each table is annotated with its structure, content, and the bounding boxes for each section of the table on each page it spans. PubTables-1M v2 is the first large-scale dataset for evaluating models on fully end-to-end document-level table extraction and the first dataset containing multi-page tables. Each table is annotated with its structure, content, and the bounding boxes for each section of the table on each page it spans. PubTables-1M v2 is the first large-scale dataset for evaluating models on fully end-to-end document-level table extraction and the first dataset containing multi-"
  },
  {
    "question_id": "q171",
    "doc_id": "arxiv_251210888_pubtables_v2",
    "question": "Which table-related tasks are explicitly supported and evaluated using the PubTables-1M v2 dataset?",
    "type": "fact",
    "answer": "PubTables-1M v2 is a large-scale, quality-controlled dataset for evaluating models on fully end-to-end document-level table extraction and the first dataset containing multi-page tables. Each table is annotated with its structure, content, and the bounding boxes for each section of the table on each page it spans. Each table is annotated with its bounding box location(s), its table structure, and the text content for every one of its cells, intended for use as ground truth output."
  },
  {
    "question_id": "q172",
    "doc_id": "arxiv_251210888_pubtables_v2",
    "question": "In which section of the paper is the annotation process and quality control methodology described?",
    "type": "loc",
    "answer": "Section A) describes the operational workflow of the Chat Research Paper System."
  },
  {
    "question_id": "q173",
    "doc_id": "arxiv_251210888_pubtables_v2",
    "question": "Which figure illustrates examples of annotated tables and their corresponding structural labels?",
    "type": "loc",
    "answer": "Tables are annotated with their structure, content, and bounding boxes. Tables are annotated with their structure, content, and the bounding boxes for each section of the table. Tables are annotated with their structure, content, and the bounding boxes for each section of the table. Tables are annotated with their structure, content, and the bounding boxes for each section of the table. Tables are annotated with their structure, content, and the bounding boxes for each section of the table. Tables are annotated with their structure, content, and the bounding boxes for each section of the table. Tables are annotated with their structure, content, and the bounding boxes for each section of the table. Tables are annotated with their structure, content, and the bounding boxes for each section of the table. Tables are annotated with their structure, content, and the bounding boxes for each section of the table. Tables are annotated with their structure, content, and the bounding boxes for each section of the table. Tables are annotated with their structure, content, and the bounding boxes for each section of the"
  },
  {
    "question_id": "q174",
    "doc_id": "arxiv_251210888_pubtables_v2",
    "question": "Does the paper report experiments using PubTables-1M v2 for financial document table extraction in industry settings?",
    "type": "impossible",
    "answer": "PubTables-1M v2 is the first large-scale dataset for evaluating models on fully end-to-end document-level table extraction and the first dataset containing multi-page tables. PubTables-1M v2 is the first large-scale dataset for evaluating models on fully end-to-end document-level table extraction and the first dataset containing multi-page tables. PubTables-1M v2 is the first large-scale dataset for evaluating models on fully end-to-end document-level table extraction and the first dataset containing multi-page tables. PubTables-1M v2 is the first large-scale dataset for evaluating models on fully end-to-end document-level table extraction and the first dataset containing multi-page tables. PubTables-1M v2 is the first large-scale dataset for evaluating models on fully end-to-end document-level table extraction and the first dataset containing multi-page tables. PubTables-1M v2 is the first large-scale dataset for evaluating models on fully end-to-end document-level table extraction and the first dataset containing multi-page tables. PubTables"
  },
  {
    "question_id": "q175",
    "doc_id": "arxiv_251210947_flex_multi_camera_encoding",
    "question": "What is the main contribution of the Flex scene encoder presented in the paper?",
    "type": "fact",
    "answer": "Flex Scene Encoder is a scene encoder designed for Vision-Language-Action (VLA) models in autonomous driving. Our primary contribution is a new scene encoder that produces a compact, yet highly effective, scene representation from the massive amount of visual inputs, which improves efficiency during training and inference, and enhances driving performance of the policy model. Our primary contribution is a new scene encoder that produces a compact, yet highly effective, scene representation from the massive amount of visual inputs, which improves efficiency during training and inference, and enhances driving performance of the policy model."
  },
  {
    "question_id": "q176",
    "doc_id": "arxiv_251210947_flex_multi_camera_encoding",
    "question": "How does Flex achieve a significant reduction in the computational load for policy models in autonomous driving systems?",
    "type": "fact",
    "answer": "Flex is a partial optimization technique for autonomous driving systems. It is a partial optimization technique for autonomous driving systems."
  },
  {
    "question_id": "q177",
    "doc_id": "arxiv_251210947_flex_multi_camera_encoding",
    "question": "On what type of dataset was Flex evaluated, and how large is the dataset used?",
    "type": "fact",
    "answer": "A partial table extraction dataset."
  },
  {
    "question_id": "q178",
    "doc_id": "arxiv_251210947_flex_multi_camera_encoding",
    "question": "In which section of the paper is the design and performance of the Flex scene encoder described?",
    "type": "loc",
    "answer": "Ablation study on Flex design. We evaluate key components of Flex, including patchifier size, token count, scene encoder depth, attention type, interleave setting, and camera count."
  },
  {
    "question_id": "q179",
    "doc_id": "arxiv_251210947_flex_multi_camera_encoding",
    "question": "Which figure illustrates the comparison of Flex with the baseline method regarding the efficiency and performance trade-off?",
    "type": "loc",
    "answer": "Flex achieves the best effectiveness–efficiency trade-off, with lower error, fewer tokens, faster training, and higher throughput. Flex achieves the best effectiveness–efficiency trade-off, with lower error, fewer tokens, faster training, and higher throughput."
  },
  {
    "question_id": "q180",
    "doc_id": "arxiv_251210947_flex_multi_camera_encoding",
    "question": "Does the paper evaluate Flex in real-world driving conditions, such as in off-road or rural areas?",
    "type": "impossible",
    "answer": "Flex is a multi-camera encoding framework that is able to learn from a variety of environments, including off-road or rural areas."
  }
]