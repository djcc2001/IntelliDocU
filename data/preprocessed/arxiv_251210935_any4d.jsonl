{"pdf_id": "arxiv_251210935_any4d", "page": 1, "text": "Any4D: Unified Feed-Forward Metric 4D Reconstruction\nany-4d.github.io\nJay Karhade\nNikhil Keetha\nYuchen Zhang\nTanisha Gupta\nAkash Sharma\nSebastian Scherer\nDeva Ramanan\nCarnegie Mellon University\nFigure 1. Any4D is a flexible feed-forward model capable of producing dense metric 4D reconstructions using N frames as input.\nAny4D is up to 15× faster and 3× better than prior state-of-the-art, where performance can be further boosted by using diverse sensors as\ninput. Note that Any4D produces dense 3D tracking vectors, but here we visualize the sparse 3D motion tracks for simplicity.\nAbstract\nWe present Any4D, a scalable multi-view transformer\nfor metric-scale, dense feed-forward 4D reconstruction.\nAny4D directly generates per-pixel motion and geometry\npredictions for N frames, in contrast to prior work that typ-\nically focuses on either 2-view dense scene flow or sparse\n3D point tracking. Moreover, unlike other recent methods\nfor 4D reconstruction from monocular RGB videos, Any-\n4D can process additional modalities and sensors such as\nRGB-D frames, IMU-based egomotion, and Radar Doppler\nmeasurements, when available.\nOne of the key innova-\ntions that allows for such a flexible framework is a mod-\nular representation of a 4D scene; specifically, per-view\n4D predictions are encoded using a variety of egocentric\nfactors (depthmaps and camera intrinsics) represented in\nlocal camera coordinates, and allocentric factors (camera\nextrinsics and scene flow) represented in global world co-\nordinates. We achieve superior performance across diverse\nsetups - both in terms of accuracy (2 −3× lower error)\nand compute efficiency (15× faster) - opening avenues for\nmultiple downstream applications.\n1. Introduction\nReconstructing the 4D (3D + t) world from sensor obser-\nvations is a long-standing goal of computer vision. Such\na technology can unlock a wide range of downstream\ntasks. In generative AI, 4D reconstruction can improve dy-\nnamic video synthesis [8, 41, 72, 84], video understanding\n[24, 96], and the creation of interactive dynamic assets such\nas VR avatars. In robotics, 4D scene reconstruction can sig-\nnificantly improve predictive control (MPC) for an agent\n1\narXiv:2512.10935v1  [cs.CV]  11 Dec 2025", "clean_text": "Any4D: Unified Feed-Forward Metric 4D Reconstruction any-4d.github.io Jay Karhade Nikhil Keetha Yuchen Zhang Tanisha Gupta Akash Sharma Sebastian Scherer Deva Ramanan Carnegie Mellon University Figure 1. Any4D is a flexible feed-forward model capable of producing dense metric 4D reconstructions using N frames as input. Any4D is up to 15× faster and 3× better than prior state-of-the-art, where performance can be further boosted by using diverse sensors as input. Note that Any4D produces dense 3D tracking vectors, but here we visualize the sparse 3D motion tracks for simplicity. Abstract We present Any4D, a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available. One of the key innovations that allows for such a flexible framework is a modular representation of a 4D scene; specifically, per-view 4D predictions are encoded using a variety of egocentric factors (depthmaps and camera intrinsics) represented in local camera coordinates, and allocentric factors (camera extrinsics and scene flow) represented in global world coordinates. We achieve superior performance across diverse setups - both in terms of accuracy (2 −3× lower error) and compute efficiency (15× faster) - opening avenues for multiple downstream applications. 1. Introduction Reconstructing the 4D (3D + t) world from sensor observations is a long-standing goal of computer vision. Such a technology can unlock a wide range of downstream tasks. In generative AI, 4D reconstruction can improve dynamic video synthesis [8, 41, 72, 84], video understanding [24, 96], and the creation of interactive dynamic assets such as VR avatars. In robotics, 4D scene reconstruction can significantly improve predictive control (MPC) for an agent 1 arXiv:2512.10935v1 [cs.CV] 11 Dec 2025"}
{"pdf_id": "arxiv_251210935_any4d", "page": 2, "text": "navigating and manipulating in a physical world [44, 52].\nAlthough there has been significant recent progress on\n4D reconstruction [16, 27, 37, 41, 60, 78, 92], dynamic\nreconstruction of in the wild videos remains challenging\nfor many reasons.\nFirst, 4D reconstruction is severely\nunder-constrained, requiring simplifying assumptions such\nas rigid motion, smoothness priors, or a mostly-static world\nassumption.\nSecond, there is a lack of large-scale 4D\ndatasets. Unlike million-scale video [10] and 3D datasets\n[2, 3], reliable high-quality 4D reconstruction datasets are\nstill limited to a few thousand scenes, primarily obtained via\nsimulation [18, 95]. Third, because 4D reconstruction and\ntracking is such a challenging problem, progress has been\nlargely achieved by treating dynamic attribute prediction as\nindependent sub tasks (i.e., 3D tracking, video-consistent\ndepth estimation, scene flow estimation, camera pose esti-\nmation in dynamic scenes). This focus on sub tasks has led\nto fragmented datasets and benchmarks that lack consistent\n4D definitions and annotations. This is unsatisfying because\nall sub tasks observe the same underlying 4D world!\nTo create a universal system that can reliably work on in\nthe wild videos, we seek to address the following desider-\nata: a) efficiency: much prior work often makes use of iter-\native optimization-based methods as a post-processing step\nthat maybe too slow for real-time deployment. b) multi-\nmodality: Many robotic platforms use additional sensors\nbeyond cameras, but most prior work fails to exploit such\ndiverse configurations. c) metric scale outputs: while ex-\nisting 4D reconstruction methods produce outputs in a nor-\nmalized coordinate frame, physical agents undeniably oper-\nate in the metric-scale physical world.\nTaking a step in this direction, Any4D is a unified and\nscalable model with the following 3 core contributions:\n• Dense Metric-Scale 4D Reconstruction: Any4D pre-\ndicts the dense geometry and motion of the scene in met-\nric coordinates, unlike existing methods that can recon-\nstruct only up-to-scale or sparse tracks. We propose a fac-\ntored 4D representation consisting of per-view allocentric\nfactors (for scene flow and poses) and egocentric factors\n(for intrinsics and depth). This factored 4D representation\nallows us to train on diverse datasets with partial annota-\ntions, including metric-scale 3D reconstruction datasets\nwithout motion annotations, and non-metric datasets with\nmotion annotations.\n• Flexible Multi-Modal Inputs: When available, Any4D\ncan further improve its 4D reconstruction by exploiting\nadditional input modalities like depth from RGBD sen-\nsors, camera poses from IMUs or doppler velocity from\nRADARs compared to image-only 4D reconstruction.\n• Efficient Inference: Any4D infers both geometry and\nmotion from N video frames in a single feed-forward\npass, bypassing existing work that only predict motion\nfor 2 frame inputs or require computationally-expensive\noptimization, making Any4D up to 15× faster than the\nnext best performing method.\n2. Related Work\nReconstruction of Dynamic Scenes:\nReconstruction\nand camera pose estimation for static scenes has a rich his-\ntory.\nIt has been studied as Simultaneous Location and\nMapping (SLAM) [13, 15, 31, 33, 50, 65] when visual ob-\nservations occur in a temporal sequence, and as structure-\nfrom-motion (SFM) [1, 62, 64, 71] otherwise. Since tra-\nditional optimization-based reconstruction is at odds with\ndynamics reconstruction, many approaches relied on ad-\nhoc semantic and motion masks to discard dynamic re-\ngions of a scene [6, 21, 36, 57]. Subsequently, advances\nin data-driven monocular depth [14, 58, 59, 88] and opti-\ncal flow [67, 94] estimation have not only enabled data-\ndriven static reconstruction methods [68], but have also\nsparked research [34, 37, 40, 45, 63, 84] in dynamic scene\nreconstruction. Although methods such as MegaSaM [40]\nare promising, they rely on per-scene optimization, making\nthem ill-suited for real-time use. More recently, following\nthe success of end-to-end methods[26, 80], methods such\nas MonST3R [92] handle dynamic scenes by making inde-\npendent per-frame predictions. However, they still require\npost-hoc optimization to establish explicit correspondences.\nTo alleviate this, [32, 77, 83] also show the potential of feed-\nforward multi-view inference from a set of images. Follow-\ning this line of work, Any4D is a feed-forward model that\npredicts camera poses, dense 3D motion (as scene flow) and\ngeometry (as pointmaps), fully describing a dynamic scene\ncaptured by a set of N frames in its entirety.\nScene Flow:\nScene flow was introduced in [75] as the\nproblem of recovering the 3D motion vector field for ev-\nery point on every surface observed in a scene. Any op-\ntical flow then is the perspective projection of scene flow\nonto the camera plane. Subsequently, it has been studied\nthrough a wide range of approaches, ranging from vari-\national methods [5, 25, 55] to learning-based supervised\nmethods [42, 81] and self-supervised methods [49, 56, 85].\nDespite these advances, solutions to scene flow estimation\nhave largely been tailored to specific downstream use cases,\nexploiting access to privileged information. In autonomous\nvehicles (AVs), scene flow approaches [11, 74] typically ac-\ncess sensor pose through inertial and proprioceptive sen-\nsors. Similarly, RAFT-3D [69] assumes access to depth.\nRecently, [41] proposed to build upon [77] for scene flow\nand view synthesis. However, in the spectrum of dynamism\nin a scene, we observe that all the above scene flow methods\nare limited to simplistic scenes like [7, 47, 48] with minimal\ndynamic motion. Our model is instead capable of directly\npredicting scene flow in the allocentric coordinate frame .\n2", "clean_text": "navigating and manipulating in a physical world [44, 52]. Although there has been significant recent progress on 4D reconstruction [16, 27, 37, 41, 60, 78, 92], dynamic reconstruction of in the wild videos remains challenging for many reasons. First, 4D reconstruction is severely under-constrained, requiring simplifying assumptions such as rigid motion, smoothness priors, or a mostly-static world assumption. Second, there is a lack of large-scale 4D datasets. Unlike million-scale video [10] and 3D datasets [2, 3], reliable high-quality 4D reconstruction datasets are still limited to a few thousand scenes, primarily obtained via simulation [18, 95]. Third, because 4D reconstruction and tracking is such a challenging problem, progress has been largely achieved by treating dynamic attribute prediction as independent sub tasks (i.e., 3D tracking, video-consistent depth estimation, scene flow estimation, camera pose estimation in dynamic scenes). This focus on sub tasks has led to fragmented datasets and benchmarks that lack consistent 4D definitions and annotations. This is unsatisfying because all sub tasks observe the same underlying 4D world! To create a universal system that can reliably work on in the wild videos, we seek to address the following desiderata: a) efficiency: much prior work often makes use of iterative optimization-based methods as a post-processing step that maybe too slow for real-time deployment. b) multimodality: Many robotic platforms use additional sensors beyond cameras, but most prior work fails to exploit such diverse configurations. c) metric scale outputs: while existing 4D reconstruction methods produce outputs in a normalized coordinate frame, physical agents undeniably operate in the metric-scale physical world. Taking a step in this direction, Any4D is a unified and scalable model with the following 3 core contributions: • Dense Metric-Scale 4D Reconstruction: Any4D predicts the dense geometry and motion of the scene in metric coordinates, unlike existing methods that can reconstruct only up-to-scale or sparse tracks. We propose a factored 4D representation consisting of per-view allocentric factors (for scene flow and poses) and egocentric factors (for intrinsics and depth). This factored 4D representation allows us to train on diverse datasets with partial annotations, including metric-scale 3D reconstruction datasets without motion annotations, and non-metric datasets with motion annotations. • Flexible Multi-Modal Inputs: When available, Any4D can further improve its 4D reconstruction by exploiting additional input modalities like depth from RGBD sensors, camera poses from IMUs or doppler velocity from RADARs compared to image-only 4D reconstruction. • Efficient Inference: Any4D infers both geometry and motion from N video frames in a single feed-forward pass, bypassing existing work that only predict motion for 2 frame inputs or require computationally-expensive optimization, making Any4D up to 15× faster than the next best performing method. 2. Related Work Reconstruction of Dynamic Scenes: Reconstruction and camera pose estimation for static scenes has a rich history. It has been studied as Simultaneous Location and Mapping (SLAM) [13, 15, 31, 33, 50, 65] when visual observations occur in a temporal sequence, and as structurefrom-motion (SFM) [1, 62, 64, 71] otherwise. Since traditional optimization-based reconstruction is at odds with dynamics reconstruction, many approaches relied on adhoc semantic and motion masks to discard dynamic regions of a scene [6, 21, 36, 57]. Subsequently, advances in data-driven monocular depth [14, 58, 59, 88] and optical flow [67, 94] estimation have not only enabled datadriven static reconstruction methods [68], but have also sparked research [34, 37, 40, 45, 63, 84] in dynamic scene reconstruction. Although methods such as MegaSaM [40] are promising, they rely on per-scene optimization, making them ill-suited for real-time use. More recently, following the success of end-to-end methods[26, 80], methods such as MonST3R [92] handle dynamic scenes by making independent per-frame predictions. However, they still require post-hoc optimization to establish explicit correspondences. To alleviate this, [32, 77, 83] also show the potential of feedforward multi-view inference from a set of images. Following this line of work, Any4D is a feed-forward model that predicts camera poses, dense 3D motion (as scene flow) and geometry (as pointmaps), fully describing a dynamic scene captured by a set of N frames in its entirety. Scene Flow: Scene flow was introduced in [75] as the problem of recovering the 3D motion vector field for every point on every surface observed in a scene. Any optical flow then is the perspective projection of scene flow onto the camera plane. Subsequently, it has been studied through a wide range of approaches, ranging from variational methods [5, 25, 55] to learning-based supervised methods [42, 81] and self-supervised methods [49, 56, 85]. Despite these advances, solutions to scene flow estimation have largely been tailored to specific downstream use cases, exploiting access to privileged information. In autonomous vehicles (AVs), scene flow approaches [11, 74] typically access sensor pose through inertial and proprioceptive sensors. Similarly, RAFT-3D [69] assumes access to depth. Recently, [41] proposed to build upon [77] for scene flow and view synthesis. However, in the spectrum of dynamism in a scene, we observe that all the above scene flow methods are limited to simplistic scenes like [7, 47, 48] with minimal dynamic motion. Our model is instead capable of directly predicting scene flow in the allocentric coordinate frame . 2"}
{"pdf_id": "arxiv_251210935_any4d", "page": 3, "text": "Existing 4D Reconstruction Models\nAny4D\nImages-only inference\nFlexible Multi-Modal inference\nSparse 3D motion prediction\nDense 3D motion prediction\nMetric Scale\nNon-metric\nFigure 2. Any4D’s unified capabilities overcome major limitations of existing 4D reconstruction models.\n3D Tracking:\nWhile scene flow has been defined for\nshort-range motion typically for a pair of image frames,\npoint tracking [61] is the task of tracking a pixel trajec-\ntory over a long time horizon.\nFollowing methods such\nas [20, 29, 30] that show the success of 2D point-tracking,\nTAPVID-3D [35] introduced a benchmark to address the\nproblem of 3D point tracking. Subsequently, [51, 86, 91]\nproposed methods for obtaining 3D point tracks and im-\nproving this benchmark.\nHowever, [51, 86] focus on\nego-centric 3D point-tracking, unlike Any4D which re-\ngresses allocentric 3D point-tracks. [87] recently proposed\na method for allocentric 3D point tracking, by jointly opti-\nmizing camera motion, 2D and 3D point tracks. However,\nit is important to note that these methods can only track\nsparse points and require knowledge of poses and depth, ei-\nther from ground truth or from running off the shelf models,\nlimiting real-time deployment. In contrast, Any4D natively\nsupports dense 3D point tracking and can take flexible in-\nputs, allowing adoption on a range of platforms.\nConcurrent and Recent Work:\nWe acknowledge con-\ncurrent works [16, 19, 27, 41, 43, 66, 93] that focus on pre-\ndicting geometry and motion, with [27, 41] being limited to\nextremely small camera and scene motion. Any4D differs\nfrom all concurrent methods in 3 ways (see Fig. 2). First,\nall concurrent methods require multiple feedforward passes\nto infer the motion, whereas Any4D adopts a scalable archi-\ntecture inspired by [32] and performs a single feedforward\npass for all image frames at once. Second, these methods\nonly accept image inputs, while Any4D which can exploit\ndiverse multi-modal inputs. Third, unlike the concurrent\nworks, we are the only method to produce metric scale 4D\nreconstructions. We believe that the open-source release of\nAny4D will set a strong foundation for the community.\n3. Any4D\nAny4D is a transformer that takes flexible multi-modal in-\nputs and outputs a dense metric-scale 4D reconstruction in\na single feed-forward pass. In addition to a set RGB images\nI ≜{Ii}N\ni=1, Any4D can use auxiliary multi-modal sensor\ninputs which we denote as O ≜(Oi)N\ni=1. Then, our model\ncan be represented as a function that maps these inputs to a\nfactored output representation as follows:\n(˜s, { ˜Ri, ˜Di, ˜Ti, ˜Fi}N\ni=1) = Any4D\n\u0000I, O\n\u0001\n,\n(1)\nwhere the optional inputs O can contain information such as\ndepth maps, camera intrinsics, camera poses from external\nsystems or IMU and Doppler velocity from RADAR.\nModel predictions are denoted with ∼in order to differ-\nentiate them from ground-truth targets or auxiliary inputs.\nPredictions include a metric scaling factor ˜s ∈R for the en-\ntire scene, egocentric quantities predicted in the local cam-\nera coordinate frame, namely\n• ray directions for each view, i.e., ˜Ri ∈R3×H×W\n• scale-normalized depth along the rays for each view, i.e.,\n˜Di ∈R1×H×W .\nand allocentric quantities predicted in a consistent world\ncoordinate frame, namely\n• scale-normalized forward scene flow from the first view\nto all other views, i.e., ˜Fi ∈R3×H×W .\n• camera pose of each view in the coordinate system of the\nfirst view, i.e., ˜Ti ≜[pi, qi] ∈R7 represented using a\nscale-normalized translation vector and quaternion.\nNow, given these output factors from Any4D, one can re-\ncover the predicted metric-scale geometry ˜Gi in the form\nof pointmaps [80] by composing the individual quantities\nas\n˜Gi = ˜s · ˜Ti · ˜Ri · ˜Di\n∈R3×H×W .\n(2)\nSimilarly, allocentric scene flow ˜\nMi and pointmaps after\nmotion ˜\nG′i can be recovered as\n˜\nMi = ˜s · ˜Fi\n∈R3×H×W\n(3)\n˜\nG′i = ˜Gi + ˜\nMi\n∈R3×H×W\n(4)\nWe show in Sec. 4, that this parameterization of motion and\ngeometry is optimal for model performance compared to\nother parameterizations.\n3", "clean_text": "Existing 4D Reconstruction Models Any4D Images-only inference Flexible Multi-Modal inference Sparse 3D motion prediction Dense 3D motion prediction Metric Scale Non-metric Figure 2. Any4D’s unified capabilities overcome major limitations of existing 4D reconstruction models. 3D Tracking: While scene flow has been defined for short-range motion typically for a pair of image frames, point tracking [61] is the task of tracking a pixel trajectory over a long time horizon. Following methods such as [20, 29, 30] that show the success of 2D point-tracking, TAPVID-3D [35] introduced a benchmark to address the problem of 3D point tracking. Subsequently, [51, 86, 91] proposed methods for obtaining 3D point tracks and improving this benchmark. However, [51, 86] focus on ego-centric 3D point-tracking, unlike Any4D which regresses allocentric 3D point-tracks. [87] recently proposed a method for allocentric 3D point tracking, by jointly optimizing camera motion, 2D and 3D point tracks. However, it is important to note that these methods can only track sparse points and require knowledge of poses and depth, either from ground truth or from running off the shelf models, limiting real-time deployment. In contrast, Any4D natively supports dense 3D point tracking and can take flexible inputs, allowing adoption on a range of platforms. Concurrent and Recent Work: We acknowledge concurrent works [16, 19, 27, 41, 43, 66, 93] that focus on predicting geometry and motion, with [27, 41] being limited to extremely small camera and scene motion. Any4D differs from all concurrent methods in 3 ways (see Fig. 2). First, all concurrent methods require multiple feedforward passes to infer the motion, whereas Any4D adopts a scalable architecture inspired by [32] and performs a single feedforward pass for all image frames at once. Second, these methods only accept image inputs, while Any4D which can exploit diverse multi-modal inputs. Third, unlike the concurrent works, we are the only method to produce metric scale 4D reconstructions. We believe that the open-source release of Any4D will set a strong foundation for the community. 3. Any4D Any4D is a transformer that takes flexible multi-modal inputs and outputs a dense metric-scale 4D reconstruction in a single feed-forward pass. In addition to a set RGB images I ≜{Ii}N i=1, Any4D can use auxiliary multi-modal sensor inputs which we denote as O ≜(Oi)N i=1. Then, our model can be represented as a function that maps these inputs to a factored output representation as follows: (˜s, { ˜Ri, ˜Di, ˜Ti, ˜Fi}N i=1) = Any4D \u0000I, O \u0001 , (1) where the optional inputs O can contain information such as depth maps, camera intrinsics, camera poses from external systems or IMU and Doppler velocity from RADAR. Model predictions are denoted with ∼in order to differentiate them from ground-truth targets or auxiliary inputs. Predictions include a metric scaling factor ˜s ∈R for the entire scene, egocentric quantities predicted in the local camera coordinate frame, namely • ray directions for each view, i.e., ˜Ri ∈R3×H×W • scale-normalized depth along the rays for each view, i.e., ˜Di ∈R1×H×W . and allocentric quantities predicted in a consistent world coordinate frame, namely • scale-normalized forward scene flow from the first view to all other views, i.e., ˜Fi ∈R3×H×W . • camera pose of each view in the coordinate system of the first view, i.e., ˜Ti ≜[pi, qi] ∈R7 represented using a scale-normalized translation vector and quaternion. Now, given these output factors from Any4D, one can recover the predicted metric-scale geometry ˜Gi in the form of pointmaps [80] by composing the individual quantities as ˜Gi = ˜s · ˜Ti · ˜Ri · ˜Di ∈R3×H×W . (2) Similarly, allocentric scene flow ˜ Mi and pointmaps after motion ˜ G′i can be recovered as ˜ Mi = ˜s · ˜Fi ∈R3×H×W (3) ˜ G′i = ˜Gi + ˜ Mi ∈R3×H×W (4) We show in Sec. 4, that this parameterization of motion and geometry is optimal for model performance compared to other parameterizations. 3"}
{"pdf_id": "arxiv_251210935_any4d", "page": 4, "text": "Doppler\nDPT (Scene ﬂow)\nScene Flow\nPredicted Depth\nCamera Poses\nRay Directions\nPose head\nScale MLP\nAlternating-Attention Transformer\nShared weights\nScale token\nReference view token\nView 1 Patch tokens\nView-N Patch tokens\nView Encoders\nRGB encoder\n(DINOv2) \nRay\ndirections \nRay\ndepth \nPose \nTranslation \nPose\nRotation\nDoppler \nEncoder\nView Encoders\nRGB encoder\n(DINOv2) \nRay\ndirections \nRay\ndepth \nPose \nTranslation \nPose\nRotation\nDoppler \nEncoder\nView N\nPose\nDepth\nRays\nRGB\nView 1\nOptional inputs\nOptional inputs\nPose\nDepth\nDoppler\nRays\nRGB\nDPT (Geometry)\nMetric Scale\n Factor\nFigure 3. Any4D predicts a factorized dense metric 4D reconstruction represented as a global metric scale, per-view egocentric factors\n(depth maps and ray directions) and per-view allocentric factors (forward scene flow and camera poses) as explained in Sec. 3. Any4D\nis a N-view transformer, consisting of modality-specific encoders, followed by an alternating-attention transformer to produce contextual\npatch embeddings. The output tokens from the transformer are then decoded using individual decoders specific to each factor.\n3.1. Architecture\nAny4D largely follows a multi-view transformer architec-\nture, similar to [32] (see Fig. 3). Conceptually, it can be\nseparated into three sections: a) modality specific input en-\ncoders, b) a multi-view transformer backbone that attends\nto the tokens from all views, and c) output representation\nheads which decode the tokens into the factorized output\nvariables for each view.\nMulti-Modal Input Encoders:\nRGB inputs I and aux-\niliary multi-modal sensor inputs O are mapped to view-\nspecific patch tokens through multi-modal view encoders\nwith shared weights for input views which map to a\nR1024×H/14×W/14 feature space.\nWe follow the design\nchoices in [32] for RGB, depth, camera poses and intrinsics\nencoders, and additionally, add a CNN encoder to encode\ndoppler velocity. We summarize these below:\n• RGB Images: DINOv2 [53] for encoding images, to ex-\ntract the layer-normalized patch-level features from the fi-\nnal layer of DINOv2 ViT-Large, FI ∈R1024×H/14×W/14.\n• Depth Images: A shallow CNN encoder is used to en-\ncode depth images, where we normalize the input depth\nbefore passing it to the depth encoder. The normalization\nfactor is computed independently for each local view.\n• Doppler Velocity: Doppler velocity is also encoded us-\ning a CNN-based encoder. However, here the normaliza-\ntion factor for encoding the doppler velocity is computed\nfrom the first-view pointmap and shared globally.\n• Camera Intrinsics: Camera intrinsics are encoded as\nrays, and also use a CNN that maps the 3-channel ray-\ndirections into the same 1024-dimensional latent space.\n• Camera Poses: Two 4-layer MLP encoders are used for\ncamera rotation and translation that map normalized input\nposes to latent vectors, frot ∈R1024 and ftrans ∈R1024.\nThe normalization factor for pose translation is computed\nglobally across all views, and a positional encoding is\nused to indicate the reference view pref ∈R1024.\n• Metric Scale Token: For metric-scale data, the depth\nscale and pose scale obtained from normalizing depth and\npose are first transformed to log-scale and then encoded\nusing a 4-layer MLP, yielding two R1024 latent features.\nAll multimodal encodings thus obtained are aggregated\nvia summation into a per-view embedding Fview\n∈\nR1024×H/14×W/14, which are flattened into tokens, along\nwith an added learnable token to learn the metric-scale.\nTransformer Backbone:\nWe use an alternating-attention\ntransformer [77] across the views, consisting of 12 blocks of\n12 multi-head attention and MLPs. Each transformer block\nprocesses tokens with a latent dimension of 768 and con-\ntains MLPs with a ratio of 4, similar to the ViT-Base archi-\n4", "clean_text": "Doppler DPT (Scene ﬂow) Scene Flow Predicted Depth Camera Poses Ray Directions Pose head Scale MLP Alternating-Attention Transformer Shared weights Scale token Reference view token View 1 Patch tokens View-N Patch tokens View Encoders RGB encoder (DINOv2) Ray directions Ray depth Pose Translation Pose Rotation Doppler Encoder View Encoders RGB encoder (DINOv2) Ray directions Ray depth Pose Translation Pose Rotation Doppler Encoder View N Pose Depth Rays RGB View 1 Optional inputs Optional inputs Pose Depth Doppler Rays RGB DPT (Geometry) Metric Scale Factor Figure 3. Any4D predicts a factorized dense metric 4D reconstruction represented as a global metric scale, per-view egocentric factors (depth maps and ray directions) and per-view allocentric factors (forward scene flow and camera poses) as explained in Sec. 3. Any4D is a N-view transformer, consisting of modality-specific encoders, followed by an alternating-attention transformer to produce contextual patch embeddings. The output tokens from the transformer are then decoded using individual decoders specific to each factor. 3.1. Architecture Any4D largely follows a multi-view transformer architecture, similar to [32] (see Fig. 3). Conceptually, it can be separated into three sections: a) modality specific input encoders, b) a multi-view transformer backbone that attends to the tokens from all views, and c) output representation heads which decode the tokens into the factorized output variables for each view. Multi-Modal Input Encoders: RGB inputs I and auxiliary multi-modal sensor inputs O are mapped to viewspecific patch tokens through multi-modal view encoders with shared weights for input views which map to a R1024×H/14×W/14 feature space. We follow the design choices in [32] for RGB, depth, camera poses and intrinsics encoders, and additionally, add a CNN encoder to encode doppler velocity. We summarize these below: • RGB Images: DINOv2 [53] for encoding images, to extract the layer-normalized patch-level features from the final layer of DINOv2 ViT-Large, FI ∈R1024×H/14×W/14. • Depth Images: A shallow CNN encoder is used to encode depth images, where we normalize the input depth before passing it to the depth encoder. The normalization factor is computed independently for each local view. • Doppler Velocity: Doppler velocity is also encoded using a CNN-based encoder. However, here the normalization factor for encoding the doppler velocity is computed from the first-view pointmap and shared globally. • Camera Intrinsics: Camera intrinsics are encoded as rays, and also use a CNN that maps the 3-channel raydirections into the same 1024-dimensional latent space. • Camera Poses: Two 4-layer MLP encoders are used for camera rotation and translation that map normalized input poses to latent vectors, frot ∈R1024 and ftrans ∈R1024. The normalization factor for pose translation is computed globally across all views, and a positional encoding is used to indicate the reference view pref ∈R1024. • Metric Scale Token: For metric-scale data, the depth scale and pose scale obtained from normalizing depth and pose are first transformed to log-scale and then encoded using a 4-layer MLP, yielding two R1024 latent features. All multimodal encodings thus obtained are aggregated via summation into a per-view embedding Fview ∈ R1024×H/14×W/14, which are flattened into tokens, along with an added learnable token to learn the metric-scale. Transformer Backbone: We use an alternating-attention transformer [77] across the views, consisting of 12 blocks of 12 multi-head attention and MLPs. Each transformer block processes tokens with a latent dimension of 768 and contains MLPs with a ratio of 4, similar to the ViT-Base archi4"}
{"pdf_id": "arxiv_251210935_any4d", "page": 5, "text": "Any4D\nSpatialTrackerV2\nSt4RTrack\nFigure 4. Any4D provides dense and precise motion estimation, where on the other hand, state-of-the-art baselines either produce\nreliable but sparse motion (SpatialTrackerV2 [87]) or dense per-pixel motion that is not accurate (St4RTrack [16]). For SpatialTrack-\nerV2, we are only able to uniformly query a maximum of 2500 points with a H100 GPU using 80 gigabytes of GPU memory. Note that we\ndon’t use any pre-computed segmentation mask but purely threshold our scene flow output to get a binary motion mask. St4RTrack cannot\nproduce good binary motion masks due to incorrect scene flow predictions on object boundaries and the background.\ntecture. Furthermore, consistent with [32] we choose to not\nuse 2-D rotary positional encoding (RoPE) for the inputs,\nand also employ Flash Attention [12] for efficiency.\nOutput Representation Heads:\nWe decode the multi-\nview tokens from the transformer backbone into a factored\noutput representation as follows:\n• Geometry DPT Head: We use a dense prediction trans-\nformer (DPT) [58] head to predict per-view ray directions\n˜Ri, up-to-scale ray depths ˜Di, and confidence masks.\n• Motion DPT Head: A second DPT head is tasked to pre-\ndict per-view forward allocentric scene flow ˜Fi . The\nscene flow represents motion of points in the reference\nview-0 to all other views.\n• Pose Decoder: The pose decoder is an average-pooling-\nbased CNN decoder that predicts per-view, up-to-scale\ntranslations and quaternions ˜Ti ≜[pi, qi].\n• Metric Scale Decoder: We use a lightweight MLP de-\ncoder to predict the log scale metric scaling factor, which\nis subsequently exponentiated.\n3.2. Training Details\nDatasets:\nDespite recent efforts [27], there is a lack\nof large-scale datasets that contain dynamic scene mo-\ntion annotations.\nIn fact, reliable, high-quality scene\nflow annotations are sparse and only available from sim-\nulation engines [18, 28].\nWe address this challenge\nin this work by a) finetuning large-scale pretrained ge-\nometry models and b) training with partial supervi-\nsion. Owing to our factored representation, we are able\nto train on a mixture of both geometry-only and dy-\nnamic datasets, where they can be synthetic or real-\nworld with varying sparsity of labels: BlendedMVS [89],\nMegaDepth [39], ScanNet++ [90], VKITTI2 [7], Paral-\nlelDomain4D [73], Waymo-DriveTrack [4], SAIL-VOS3D\n[23] PointOdyssey [95], Dynamic Replica [28] and Kubric\n[18] data generated by CoTracker3[29] and GCD[73]. De-\ntailed information of all datasets used for training is avail-\nable in the appendix.\nTraining with Multi-Modal Conditioning:\nWe prepro-\ncess the datasets and generate multi-modal inputs offline for\nfaster training. Geometric inputs consisting of poses, depths\nand intrinsics are directly taken from the dataset annota-\ntions. To simulate doppler velocity, we take the radial com-\nponent of egocentric scene flow between data pairs. During\ntraining, multi-modal conditioning is applied with a proba-\nbility of 0.7, i.e., 70% of training iterations include multi-\nmodal inputs alongside images. Additionally, we ensure\nthat individual modalities (depth, rays, poses, and doppler)\nare independently removed with a probability of 0.5 to pro-\nmote effective learning in flexible input configurations. Fi-\n5", "clean_text": "Any4D SpatialTrackerV2 St4RTrack Figure 4. Any4D provides dense and precise motion estimation, where on the other hand, state-of-the-art baselines either produce reliable but sparse motion (SpatialTrackerV2 [87]) or dense per-pixel motion that is not accurate (St4RTrack [16]). For SpatialTrackerV2, we are only able to uniformly query a maximum of 2500 points with a H100 GPU using 80 gigabytes of GPU memory. Note that we don’t use any pre-computed segmentation mask but purely threshold our scene flow output to get a binary motion mask. St4RTrack cannot produce good binary motion masks due to incorrect scene flow predictions on object boundaries and the background. tecture. Furthermore, consistent with [32] we choose to not use 2-D rotary positional encoding (RoPE) for the inputs, and also employ Flash Attention [12] for efficiency. Output Representation Heads: We decode the multiview tokens from the transformer backbone into a factored output representation as follows: • Geometry DPT Head: We use a dense prediction transformer (DPT) [58] head to predict per-view ray directions ˜Ri, up-to-scale ray depths ˜Di, and confidence masks. • Motion DPT Head: A second DPT head is tasked to predict per-view forward allocentric scene flow ˜Fi . The scene flow represents motion of points in the reference view-0 to all other views. • Pose Decoder: The pose decoder is an average-poolingbased CNN decoder that predicts per-view, up-to-scale translations and quaternions ˜Ti ≜[pi, qi]. • Metric Scale Decoder: We use a lightweight MLP decoder to predict the log scale metric scaling factor, which is subsequently exponentiated. 3.2. Training Details Datasets: Despite recent efforts [27], there is a lack of large-scale datasets that contain dynamic scene motion annotations. In fact, reliable, high-quality scene flow annotations are sparse and only available from simulation engines [18, 28]. We address this challenge in this work by a) finetuning large-scale pretrained geometry models and b) training with partial supervision. Owing to our factored representation, we are able to train on a mixture of both geometry-only and dynamic datasets, where they can be synthetic or realworld with varying sparsity of labels: BlendedMVS [89], MegaDepth [39], ScanNet++ [90], VKITTI2 [7], ParallelDomain4D [73], Waymo-DriveTrack [4], SAIL-VOS3D [23] PointOdyssey [95], Dynamic Replica [28] and Kubric [18] data generated by CoTracker3[29] and GCD[73]. Detailed information of all datasets used for training is available in the appendix. Training with Multi-Modal Conditioning: We preprocess the datasets and generate multi-modal inputs offline for faster training. Geometric inputs consisting of poses, depths and intrinsics are directly taken from the dataset annotations. To simulate doppler velocity, we take the radial component of egocentric scene flow between data pairs. During training, multi-modal conditioning is applied with a probability of 0.7, i.e., 70% of training iterations include multimodal inputs alongside images. Additionally, we ensure that individual modalities (depth, rays, poses, and doppler) are independently removed with a probability of 0.5 to promote effective learning in flexible input configurations. Fi5"}
{"pdf_id": "arxiv_251210935_any4d", "page": 6, "text": "nally, we initialize our network with MapAnything weights\n[32]. For each training batch, we sample up to 4 views from\nthe datasets and train on 1 H100 node for 100 epochs.\nLosses:\nAny4D is trained using a combination of geo-\nmetric and motion losses based on the type of annotation\navailable. Ray directions representing the camera intrinsics\nand quaternions are scale-agnostic, and therefore can be su-\npervised via simple regression losses:\nLrays ≜\nN\nX\ni=1\n∥Ri −˜Ri∥\n(5)\nLrotation ≜\nN\nX\ni=1\nmin(∥qi −˜qi∥, ∥−qi + ˜qi∥).\n(6)\nOn the other hand, geometric quantities such as camera\ntranslations ti, ray depths Di and scene flow Fi are pre-\ndicted in a scale-normalized coordinate frame. Following\nprior work [32, 38, 80], we use the ground-truth validity\nmasks Vi and pointmaps Xi and compute the ground-truth\nscale as the average euclidean distance of valid points with\nrespect to the world origin (given by the first view camera\nframe): z = ∥{Xi[Vi]}N\ni ∥/ PN\ni Vi.\nTo compute scale-\ninvariant losses, we also compute a scale factor derived\nfrom our predictions ˜z = ∥{ ˜Xi[Vi]}N\ni ∥/ PN\ni Vi:\nLtrans ≜\nN\nX\ni\n\r\r\r\r\nti\nzi\n−\n˜ti\n˜zi\n\r\r\r\r ,\n(7)\nLdepth ≜\nN\nX\ni\n\r\r\r\r\rflog\n\u0012Di\nzi\n\u0013\n−flog\n ˜Di\n˜zi\n!\r\r\r\r\r\n(8)\nwhere flog(x) ≜(x/∥x∥) log(1 + ∥x∥) converts quantities\nto log-space for numerical stability. A pointmap loss is also\napplied to the composed geometric predictions as follows:\nLpm ≜\nN\nX\ni\n\r\r\r\r\rflog\n\u0012Xi\nzi\n\u0013\n−flog\n ˜Xi\n˜zi\n!\r\r\r\r\r\n(9)\nSimilarly, scene flow is also supervised in a scale-\ninvariant manner. We find that scene flow loss is dominated\nby static points since most of the scene is static. Therefore,\nwe find it is crucial to calculate a static-dynamic motion\nmask M from the ground truth scene flow, and upweight the\nscene flow loss in the dynamic regions by 10x more com-\npared to static regions:\nLsf ≜\nN\nX\ni\nM ·\n\r\r\r\r\rflog\n\u0012Fi\nzi\n\u0013\n−flog\n ˜Fi\n˜zi\n!\r\r\r\r\r\n(10)\nFinally, the predicted metric scale factor ˜s is also super-\nvised in the log space as follows: Lscale ≜∥flog(z) −\nflog(˜s · sg(˜z)∥, where sg denotes the stop-gradient opera-\ntion and prevents the scale supervision from affecting other\npredicted quantities. The final loss is expressed as:\nL = Ltrans + Lrot + Lrays + Ldepth + Lsf + Lmask\n(11)\n4. Results & Analysis\nWe evaluate Any4D on diverse benchmarking setups specif-\nically designed for allocentric 4D reconstruction, and com-\npare against state-of-the-art (SOTA) methods.\n3D Tracking:\nThere is a lack of standard and unified\nbenchmarks for evaluating 4D reconstruction in the existing\nliterature. To create allocentric 3D tracking benchmarks, we\nfollow [16] and repurpose existing 3D tracking benchmark,\nparticularly TAPVID-3D [35]. However, TAPVID-3D have\ntheir own limitations: the Aria Digital Twin (ADT) se-\nquences are largely static, Parallel-Studio sequences contain\nfixed-camera viewpoints, while the DriveTrack sequences\nare extremely sparse.\nHence, we choose to drop ADT,\nand keep Parallel-Studio and Drive-Track benchmarking se-\nquences. We also add unseen held-out sequences from Dy-\nnamic Replica [28] and a zero-shot dataset LSFOdyssey\n[76], both of which contain camera motion along with 3D\ntracking labels. The final benchmark contains ∼170 se-\nquences across 4 datasets of up to 64 frames in length. We\nevaluate Any4D against SOTA 3D trackers SpatialTrack-\nerV2 [87] and St4RTrack [16]. We also compose 3D re-\nconstruction models [32, 38, 77, 92] with 2D tracks from\nCoTracker3 [29] for comparison.\nWe use standard benchmarking protocols [16, 35, 69, 87]\nto evaluate the quality of our 4D reconstruction. Follow-\ning [16], we first perform median-scaling to align to met-\nric space. We report average percent of points within delta\nfor 3D points after motion (APD) and inlier percentage τ\nfor scene flow. We also report End Point Error (EPE) for\n3D points after motion (dynamic points) and 3D scene flow\nvectors. APD and τ are defined as:\nAPD =\nX\ni,t\n1 ·\n\u0010\r\r\rPi,t −˜Pi,t\n\r\r\r < δ3D\n\u0011\n(12)\nτ =\nX\ni,t\n1 ·\n\u0010\r\r\rFi,t −˜Fi,t\n\r\r\r < 0.1m\n\u0011\n(13)\nwhere ˜Pi represents the predicted 3D point after motion\nand ˜Fi is the corresponding scene flow vector at time t.\nFor APD, we use thresholds δ3D ∈{0.1, 0.3, 0.5, 1.0} m.\nAs evident in Tab. 1, Any4D shows state-of-the-art perfor-\nmance across all datasets. Furthermore, it is 15× faster than\nthe closest performing method, SpatialTrackerV2. This is\nfurther reinforced qualitatively in Fig. 4.\nDense Scene Flow:\nWe construct allocentric scene flow\nbenchmarks by repurposing 2 egocentric scene flow bench-\nmarking datasets:\nVKITTI-2 [7] and Kubric-4D [72].\nWhile scene flow in VKITTI-2 is limited to small consec-\nutive frame motion, we can simulate scene flow across 60\nframes and 16 camera viewpoints from Kubric4D (GCD).\nHence we create 2 variants for Kubric4D (GCD): (a) scene\nflow from static camera movement and (b) scene flow from\nwide-baseline dynamic camera movement. Importantly, all\n6", "clean_text": "nally, we initialize our network with MapAnything weights [32]. For each training batch, we sample up to 4 views from the datasets and train on 1 H100 node for 100 epochs. Losses: Any4D is trained using a combination of geometric and motion losses based on the type of annotation available. Ray directions representing the camera intrinsics and quaternions are scale-agnostic, and therefore can be supervised via simple regression losses: Lrays ≜ N X i=1 ∥Ri −˜Ri∥ (5) Lrotation ≜ N X i=1 min(∥qi −˜qi∥, ∥−qi + ˜qi∥). (6) On the other hand, geometric quantities such as camera translations ti, ray depths Di and scene flow Fi are predicted in a scale-normalized coordinate frame. Following prior work [32, 38, 80], we use the ground-truth validity masks Vi and pointmaps Xi and compute the ground-truth scale as the average euclidean distance of valid points with respect to the world origin (given by the first view camera frame): z = ∥{Xi[Vi]}N i ∥/ PN i Vi. To compute scaleinvariant losses, we also compute a scale factor derived from our predictions ˜z = ∥{ ˜Xi[Vi]}N i ∥/ PN i Vi: Ltrans ≜ N X i ti zi − ˜ti ˜zi , (7) Ldepth ≜ N X i flog \u0012Di zi \u0013 −flog ˜Di ˜zi ! (8) where flog(x) ≜(x/∥x∥) log(1 + ∥x∥) converts quantities to log-space for numerical stability. A pointmap loss is also applied to the composed geometric predictions as follows: Lpm ≜ N X i flog \u0012Xi zi \u0013 −flog ˜Xi ˜zi ! (9) Similarly, scene flow is also supervised in a scaleinvariant manner. We find that scene flow loss is dominated by static points since most of the scene is static. Therefore, we find it is crucial to calculate a static-dynamic motion mask M from the ground truth scene flow, and upweight the scene flow loss in the dynamic regions by 10x more compared to static regions: Lsf ≜ N X i M · flog \u0012Fi zi \u0013 −flog ˜Fi ˜zi ! (10) Finally, the predicted metric scale factor ˜s is also supervised in the log space as follows: Lscale ≜∥flog(z) − flog(˜s · sg(˜z)∥, where sg denotes the stop-gradient operation and prevents the scale supervision from affecting other predicted quantities. The final loss is expressed as: L = Ltrans + Lrot + Lrays + Ldepth + Lsf + Lmask (11) 4. Results & Analysis We evaluate Any4D on diverse benchmarking setups specifically designed for allocentric 4D reconstruction, and compare against state-of-the-art (SOTA) methods. 3D Tracking: There is a lack of standard and unified benchmarks for evaluating 4D reconstruction in the existing literature. To create allocentric 3D tracking benchmarks, we follow [16] and repurpose existing 3D tracking benchmark, particularly TAPVID-3D [35]. However, TAPVID-3D have their own limitations: the Aria Digital Twin (ADT) sequences are largely static, Parallel-Studio sequences contain fixed-camera viewpoints, while the DriveTrack sequences are extremely sparse. Hence, we choose to drop ADT, and keep Parallel-Studio and Drive-Track benchmarking sequences. We also add unseen held-out sequences from Dynamic Replica [28] and a zero-shot dataset LSFOdyssey [76], both of which contain camera motion along with 3D tracking labels. The final benchmark contains ∼170 sequences across 4 datasets of up to 64 frames in length. We evaluate Any4D against SOTA 3D trackers SpatialTrackerV2 [87] and St4RTrack [16]. We also compose 3D reconstruction models [32, 38, 77, 92] with 2D tracks from CoTracker3 [29] for comparison. We use standard benchmarking protocols [16, 35, 69, 87] to evaluate the quality of our 4D reconstruction. Following [16], we first perform median-scaling to align to metric space. We report average percent of points within delta for 3D points after motion (APD) and inlier percentage τ for scene flow. We also report End Point Error (EPE) for 3D points after motion (dynamic points) and 3D scene flow vectors. APD and τ are defined as: APD = X i,t 1 · \u0010 Pi,t −˜Pi,t < δ3D \u0011 (12) τ = X i,t 1 · \u0010 Fi,t −˜Fi,t < 0.1m \u0011 (13) where ˜Pi represents the predicted 3D point after motion and ˜Fi is the corresponding scene flow vector at time t. For APD, we use thresholds δ3D ∈{0.1, 0.3, 0.5, 1.0} m. As evident in Tab. 1, Any4D shows state-of-the-art performance across all datasets. Furthermore, it is 15× faster than the closest performing method, SpatialTrackerV2. This is further reinforced qualitatively in Fig. 4. Dense Scene Flow: We construct allocentric scene flow benchmarks by repurposing 2 egocentric scene flow benchmarking datasets: VKITTI-2 [7] and Kubric-4D [72]. While scene flow in VKITTI-2 is limited to small consecutive frame motion, we can simulate scene flow across 60 frames and 16 camera viewpoints from Kubric4D (GCD). Hence we create 2 variants for Kubric4D (GCD): (a) scene flow from static camera movement and (b) scene flow from wide-baseline dynamic camera movement. Importantly, all 6"}
{"pdf_id": "arxiv_251210935_any4d", "page": 7, "text": "Table 1. Any4D showcases state-of-the-art sparse 3D point tracking, while providing dense motion predictions and being an order\nof magnitude faster than the closest performing baseline. We report end-point error (EPE), average points within delta (APD) and inlier\nratio at 0.1m (τ) for dynamic points in the benchmark. The runtime is computed on a H100 using 50 frames as input. Best results are bold.\nDrive Track [4]\nDynamic Replica [28]\nLSFOdyssey [76]\nPStudio [35]\nDynamic Points\nScene Flow\nDynamic Points\nScene Flow\nDynamic Points\nScene Flow\nDynamic Points\nScene Flow\nMethod\nRuntime (s)\nEPE ↓\nAPD ↑\nEPE ↓\nτ ↑\nEPE ↓\nAPD ↑\nEPE ↓\nτ ↑\nEPE ↓\nAPD ↑\nEPE ↓\nτ ↑\nEPE ↓\nAPD ↑\nEPE ↓\nτ ↑\nMonST3R + CoTracker3\n146.40\n16.81\n0.44\n21.87\n0.06\n0.81\n43.34\n0.18\n25.99\n0.61\n50.96\n0.41\n43.64\n0.51\n51.87\n0.52\n21.06\nMASt3R + CoTracker3\n13.82\n17.16\n1.22\n20.01\n0.20\n0.40\n57.72\n0.23\n53.98\n0.83\n45.95\n0.62\n41.10\n0.43\n54.11\n0.43\n14.69\nVGGT + CoTracker3\n2.31\n8.30\n4.80\n11.69\n0.77\n0.26\n69.12\n0.06\n89.37\n0.47\n59.21\n0.22\n74.11\n0.26\n69.34\n0.17\n45.77\nMapAnything + CoTracker3\n0.73\n9.42\n2.45\n12.88\n0.43\n0.25\n70.51\n0.06\n89.59\n0.63\n35.51\n0.51\n58.00\n0.63\n50.85\n0.35\n58.01\nSt4RTrack\n1.12\n11.82\n1.03\n14.63\n0.10\n0.17\n80.87\n0.07\n77.90\n0.56\n48.11\n0.25\n38.31\n0.41\n53.12\n0.21\n28.46\nSpatialTrackerV2\n11.56\n5.45\n4.48\n10.63\n0.10\n0.69\n62.34\n0.06\n83.66\n0.34\n68.37\n0.09\n78.75\n0.21\n74.46\n0.14\n50.70\nAny4D\n0.50\n3.89\n7.81\n3.14\n1.83\n0.07\n93.44\n0.05\n86.99\n0.27\n71.70\n0.10\n71.41\n0.27\n67.43\n0.19\n33.57\nTable 2. Any4D achieves state-of-the-art dense scene flow estimation performance. We report end-point error (EPE), average points\nwithin delta (APD) and inlier ratio at 0.1m (τ) for dynamic points and scene flow across three datasets, where best results are bold.\nKubric-4D Dynamic Camera\nKubric-4D Static Camera\nVKITTI-2\nDynamic Points\nScene Flow\nDynamic Points\nScene Flow\nDynamic Points\nScene Flow\nMethod\nEPE ↓\nAPD ↑\nEPE ↓\nτ ↑\nEPE ↓\nAPD ↑\nEPE ↓\nτ ↑\nEPE ↓\nAPD ↑\nEPE ↓\nτ ↑\nMonST3R + SEA-RAFT\n5.23\n2.20\n3.73\n14.69\n2.26\n6.80\n1.16\n61.79\n12.31\n0.44\n1.21\n12.93\nMASt3R + SEA-RAFT\n6.35\n1.92\n1.45\n13.95\n2.85\n7.58\n1.26\n53.62\n12.25\n2.50\n13.05\n10.20\nVGGT + SEA-RAFT\n11.80\n3.60\n11.76\n14.53\n1.92\n15.01\n0.78\n86.54\n6.57\n2.61\n0.70\n37.63\nMapAnything + SEA-RAFT\n17.65\n2.67\n17.70\n9.16\n2.82\n19.99\n1.75\n73.33\n8.46\n2.42\n1.32\n13.78\nSt4RTrack\n2.44\n5.79\n1.70\n11.83\n2.61\n6.53\n0.72\n20.51\n14.71\n0.00\n0.97\n3.37\nAny4D\n1.13\n18.14\n0.17\n83.38\n1.23\n19.53\n0.10\n87.51\n4.97\n11.70\n0.04\n93.08\nvia  Allocentric Flow \nvia Egocentric Flow \nvia 3D Points after motion\nAllocentric Scene Flow Extracted Via Different Output Representations\nInput Images\nFigure 5. Scene motion parametrized as allocentric scene flow provides the cleanest 4D reconstructions. We find that other parame-\nterizations such as 3D points after motion (proposed in St4RTrack [16]) provide extreme noise on object boundaries and background.\npairs from both datasets are from held-out scenes to ensure\nthere is no data leak from the training datasets. We evaluate\nAny4D against St4RTrack which can predict dense scene\nflow, and 3D reconstruction method outputs composed with\noptical flow from SEA-RAFT [82], to calculate covisible\nscene flow. We are unable to run SpatialTrackerV2 or Co-\nTracker3 as they do not support per-pixel point queries and\nrun out-of-memory(OOM). From Tab. 2, we see that Any-\n4D outperforms baselines by 2 −3× on average on APD,\nand by even more on scene-flow metrics.\nVideo Depth:\nWe also evaluate Any4D on standard\nvideo depth benchmarks [17, 46, 54] in Tab. 3, against spe-\ncialized video depth baselines [9, 22], feed-forward + itera-\ntive optimization baselines [40, 80, 87, 92], and single-step\nfeed-forward baselines [32, 77, 79]. Any4D shows state of\nthe art video depth estimation over other single-shot feed-\nforward inference baselines while being competitive with\noptimization based and task-specific methods.\nSupport for Multi-Modal Inputs:\nSince Any4D can uti-\nlize flexible inputs for inference to enhance performance,\n7", "clean_text": "Table 1. Any4D showcases state-of-the-art sparse 3D point tracking, while providing dense motion predictions and being an order of magnitude faster than the closest performing baseline. We report end-point error (EPE), average points within delta (APD) and inlier ratio at 0.1m (τ) for dynamic points in the benchmark. The runtime is computed on a H100 using 50 frames as input. Best results are bold. Drive Track [4] Dynamic Replica [28] LSFOdyssey [76] PStudio [35] Dynamic Points Scene Flow Dynamic Points Scene Flow Dynamic Points Scene Flow Dynamic Points Scene Flow Method Runtime (s) EPE ↓ APD ↑ EPE ↓ τ ↑ EPE ↓ APD ↑ EPE ↓ τ ↑ EPE ↓ APD ↑ EPE ↓ τ ↑ EPE ↓ APD ↑ EPE ↓ τ ↑ MonST3R + CoTracker3 146.40 16.81 0.44 21.87 0.06 0.81 43.34 0.18 25.99 0.61 50.96 0.41 43.64 0.51 51.87 0.52 21.06 MASt3R + CoTracker3 13.82 17.16 1.22 20.01 0.20 0.40 57.72 0.23 53.98 0.83 45.95 0.62 41.10 0.43 54.11 0.43 14.69 VGGT + CoTracker3 2.31 8.30 4.80 11.69 0.77 0.26 69.12 0.06 89.37 0.47 59.21 0.22 74.11 0.26 69.34 0.17 45.77 MapAnything + CoTracker3 0.73 9.42 2.45 12.88 0.43 0.25 70.51 0.06 89.59 0.63 35.51 0.51 58.00 0.63 50.85 0.35 58.01 St4RTrack 1.12 11.82 1.03 14.63 0.10 0.17 80.87 0.07 77.90 0.56 48.11 0.25 38.31 0.41 53.12 0.21 28.46 SpatialTrackerV2 11.56 5.45 4.48 10.63 0.10 0.69 62.34 0.06 83.66 0.34 68.37 0.09 78.75 0.21 74.46 0.14 50.70 Any4D 0.50 3.89 7.81 3.14 1.83 0.07 93.44 0.05 86.99 0.27 71.70 0.10 71.41 0.27 67.43 0.19 33.57 Table 2. Any4D achieves state-of-the-art dense scene flow estimation performance. We report end-point error (EPE), average points within delta (APD) and inlier ratio at 0.1m (τ) for dynamic points and scene flow across three datasets, where best results are bold. Kubric-4D Dynamic Camera Kubric-4D Static Camera VKITTI-2 Dynamic Points Scene Flow Dynamic Points Scene Flow Dynamic Points Scene Flow Method EPE ↓ APD ↑ EPE ↓ τ ↑ EPE ↓ APD ↑ EPE ↓ τ ↑ EPE ↓ APD ↑ EPE ↓ τ ↑ MonST3R + SEA-RAFT 5.23 2.20 3.73 14.69 2.26 6.80 1.16 61.79 12.31 0.44 1.21 12.93 MASt3R + SEA-RAFT 6.35 1.92 1.45 13.95 2.85 7.58 1.26 53.62 12.25 2.50 13.05 10.20 VGGT + SEA-RAFT 11.80 3.60 11.76 14.53 1.92 15.01 0.78 86.54 6.57 2.61 0.70 37.63 MapAnything + SEA-RAFT 17.65 2.67 17.70 9.16 2.82 19.99 1.75 73.33 8.46 2.42 1.32 13.78 St4RTrack 2.44 5.79 1.70 11.83 2.61 6.53 0.72 20.51 14.71 0.00 0.97 3.37 Any4D 1.13 18.14 0.17 83.38 1.23 19.53 0.10 87.51 4.97 11.70 0.04 93.08 via Allocentric Flow via Egocentric Flow via 3D Points after motion Allocentric Scene Flow Extracted Via Different Output Representations Input Images Figure 5. Scene motion parametrized as allocentric scene flow provides the cleanest 4D reconstructions. We find that other parameterizations such as 3D points after motion (proposed in St4RTrack [16]) provide extreme noise on object boundaries and background. pairs from both datasets are from held-out scenes to ensure there is no data leak from the training datasets. We evaluate Any4D against St4RTrack which can predict dense scene flow, and 3D reconstruction method outputs composed with optical flow from SEA-RAFT [82], to calculate covisible scene flow. We are unable to run SpatialTrackerV2 or CoTracker3 as they do not support per-pixel point queries and run out-of-memory(OOM). From Tab. 2, we see that Any4D outperforms baselines by 2 −3× on average on APD, and by even more on scene-flow metrics. Video Depth: We also evaluate Any4D on standard video depth benchmarks [17, 46, 54] in Tab. 3, against specialized video depth baselines [9, 22], feed-forward + iterative optimization baselines [40, 80, 87, 92], and single-step feed-forward baselines [32, 77, 79]. Any4D shows state of the art video depth estimation over other single-shot feedforward inference baselines while being competitive with optimization based and task-specific methods. Support for Multi-Modal Inputs: Since Any4D can utilize flexible inputs for inference to enhance performance, 7"}
{"pdf_id": "arxiv_251210935_any4d", "page": 8, "text": "Table 3. Any4D shows state-of-the-art video depth estimation\nover other single-step feed-forward baselines. It is also com-\npetitive to iterative/optimization-based methods or ones trained\nspecifically for this task. We report the absolute relative error (rel)\nand the inlier ratio at 1.25% (δ1.25), where the best is bold.\nAverage\nBonn\nKITTI\nSintel\nMethod\nrel ↓\nδ1.25 ↑\nrel ↓\nδ1.25 ↑\nrel ↓\nδ1.25 ↑\nrel ↓\nδ1.25 ↑\na) Video Depth:\nDepthCrafter\n0.15\n85.23\n0.07\n97.90\n0.11\n88.50\n0.27\n69.30\nVDA\n0.17\n86.90\n0.05\n98.20\n0.08\n95.10\n0.37\n67.40\nb) Feed-Forward + Iterative Optimization:\nDUSt3R\n0.26\n75.83\n0.17\n83.50\n0.12\n84.90\n0.48\n59.10\nMonST3R\n0.16\n82.73\n0.06\n95.40\n0.08\n93.40\n0.34\n59.40\nMegaSAM\n0.10\n87.97\n0.04\n97.70\n0.07\n91.60\n0.18\n74.60\nSpatialTrackerV2\n0.09\n88.80\n0.03\n98.80\n0.05\n97.30\n0.20\n70.30\nc) Single-Step Feed-Forward:\nCUT3R\n0.21\n80.30\n0.07\n95.00\n0.10\n89.90\n0.47\n56.00\nVGGT\n0.13\n85.85\n0.07\n97.27\n0.09\n94.37\n0.24\n65.90\nMapAnything\n0.14\n84.97\n0.09\n94.77\n0.09\n94.26\n0.25\n65.87\nAny4D\n0.13\n86.28\n0.07\n97.27\n0.09\n93.97\n0.24\n67.59\nTable 4.\nAuxiliary inputs improve the 4D motion estima-\ntion performance of Any4D. We compare different inputs on\nboth dense scene flow (Kubric) and sparse 3D point tracking\n(LSFOdyssey) benchmarks using end-point error (EPE), average\npoints within delta (APD) and inlier ratio at 0.1m (τ), where best\nis bold. “Geometry” indicates use of depth, intrinsics and poses.\nKubric-4D Static Camera\nLSFOdyssey\nDynamic Points\nScene Flow\nDynamic Points\nScene Flow\nAny4D Inputs\nEPE ↓\nAPD ↑\nEPE ↓\nτ ↑\nEPE ↓\nAPD ↑\nEPE ↓\nτ ↑\nImages Only\n1.17\n21.33\n0.11\n86.25\n0.28\n71.47\n0.12\n68.03\nImages + Geometry\n0.23\n80.18\n0.09\n86.26\n0.19\n80.80\n0.12\n68.71\nImages + Doppler\n1.17\n21.70\n0.12\n86.90\n0.29\n71.26\n0.11\n70.32\nImages + Geometry + Doppler\n0.23\n81.72\n0.09\n87.27\n0.19\n81.10\n0.11\n71.37\nwe study improvements to scene flow on the dense Kubric-\n4D static benchmark and 3D tracking on LSFOdyssey\nbenchmark by incorporating different input modalities.\nFrom Tab. 4, we observe that adding geometry signifi-\ncantly improves APD and EPE for 3D points.\nAdding\ndoppler further improves scene-flow, with the best perfor-\nmance achieved when all modalities are provided.\nChoice of Motion Representation:\nWhile allocentric\nmotion Fallo is arguably the useful quantity for downstream\napplications, it is possible to represent the predicted scene\nflow output in 4 ways:\n• Allocentric Scene Flow: Directly predicting ˜Fallo.\n• Egocentric Scene Flow:\nPredicting egocentric scene\nflow ˜Fego, and using estimated geometry to recover al-\nlocentric motion as:\n˜Fallo = Tt→t+1\n\u0000P v\n0 + ˜Fego\n\u0001\n−p\n• 3D Points After Motion:\nPredicting view-aligned\npointmaps at time 0 and t - P v\n0 and P v\nt , and recovering\nthe allocentric motion:\n˜Fallo = P v\nt −P v\n0\n• Backprojected 2D Flow: Unprojecting optical flow to\nobtain covisible scene flow between pointmaps.\nWe systematically investigate these choices in Tab. 5 and\nFig. 5. We find that directly predicting allocentric motion\nTable 5. Allocentric scene flow is the optimal output repre-\nsentation for 4D motion. We compare different representation\ntypes on dense scene flow (Kubric) and sparse 3D point tracking\n(LSFOdyssey) using end-point error (EPE), average points within\ndelta (APD) and inlier ratio at 0.1m (τ). Best results are bold.\nKubric-4D Static Camera\nLSFOdyssey\nDynamic Points\nScene Flow\nDynamic Points\nScene Flow\nRepresentation Type\nEPE ↓\nAPD ↑\nEPE ↓\nτ ↑\nEPE ↓\nAPD ↑\nEPE ↓\nτ ↑\nBackprojected 2D Flow\n2.14\n19.44\n1.16\n75.69\n0.49\n57.21\n0.27\n70.11\n3D Points After Motion\n1.24\n17.33\n0.58\n21.84\n0.24\n69.30\n0.38\n21.87\nEgocentric Scene Flow\n1.26\n19.43\n0.12\n85.37\n0.24\n71.80\n0.14\n65.13\nAllocentric Scene Flow\n1.23\n19.53\n0.10\n87.51\n0.24\n73.95\n0.10\n71.46\nleads to optimal performance not only on scene flow, but\nsurprisingly, also on dynamic pointmaps after motion, com-\npared to directly predicting points after motion as adopted\notherwise in [16].\nLimitations:\nAlthough Any4D takes a step forward to-\nwards achieving 4D reconstruction models, we identify im-\nportant limitations. Firstly, we always calculate scene-flow\nfrom the reference (first) view to all other frames in the se-\nquence, necessitating that the object of interest should be\npresent at the start of the video. One possible way to alle-\nviate this is by training Any4D in a permutation invariant\nmanner as in [83]. Secondly, we assume perfectly simu-\nlated multi-modal input and do not account for sensor noise\n- which is hardly true for real-world deployment. Finally, as\nwith all data-driven architectures, generalization is a func-\ntion of the diversity and size of the training set. We believe\nthat Any4D’s performance on highly dynamic scenes and\nwide baselines (or low frame-rate videos) can be improved\nwith the availability of richer dynamic 3D datasets[70].\n5. Conclusion\nWe presented Any4D, a unified model that enables dense\n4D reconstruction of dynamic scenes from both monocu-\nlar and multi-modal setups. In Any4D, we chose a factor-\nized output representation of 4D scenes, which allowed the\nuse of diverse data for training at scale with partial super-\nvision for auxiliary sub-tasks, in addition to the target task\nof dense scene flow estimation. Any4D is flexible, and sup-\nports optional multi-modal inputs. Importantly, we showed\nthrough our experiments that our joint training scheme pro-\nduces generalizable view embeddings that improve perfor-\nmance whenever inputs such as depth and egocentric radial\nvelocity (doppler) may be available to support the output\nprediction quantities. Finally, due to the feed-forward na-\nture of Any4D, we saw that one can obtain dynamic scene\nestimates an order of magnitude faster than existing meth-\nods such as SpatialTrackerV2 [86], by exploiting N-view in-\nference. We believe Any4D will ultimately enable real-time\n4D scene reconstruction for applications such as Generative\nAI, AR/VR and Robotics, and serve as a foundational 4D\nreconstruction model.\n8", "clean_text": "Table 3. Any4D shows state-of-the-art video depth estimation over other single-step feed-forward baselines. It is also competitive to iterative/optimization-based methods or ones trained specifically for this task. We report the absolute relative error (rel) and the inlier ratio at 1.25% (δ1.25), where the best is bold. Average Bonn KITTI Sintel Method rel ↓ δ1.25 ↑ rel ↓ δ1.25 ↑ rel ↓ δ1.25 ↑ rel ↓ δ1.25 ↑ a) Video Depth: DepthCrafter 0.15 85.23 0.07 97.90 0.11 88.50 0.27 69.30 VDA 0.17 86.90 0.05 98.20 0.08 95.10 0.37 67.40 b) Feed-Forward + Iterative Optimization: DUSt3R 0.26 75.83 0.17 83.50 0.12 84.90 0.48 59.10 MonST3R 0.16 82.73 0.06 95.40 0.08 93.40 0.34 59.40 MegaSAM 0.10 87.97 0.04 97.70 0.07 91.60 0.18 74.60 SpatialTrackerV2 0.09 88.80 0.03 98.80 0.05 97.30 0.20 70.30 c) Single-Step Feed-Forward: CUT3R 0.21 80.30 0.07 95.00 0.10 89.90 0.47 56.00 VGGT 0.13 85.85 0.07 97.27 0.09 94.37 0.24 65.90 MapAnything 0.14 84.97 0.09 94.77 0.09 94.26 0.25 65.87 Any4D 0.13 86.28 0.07 97.27 0.09 93.97 0.24 67.59 Table 4. Auxiliary inputs improve the 4D motion estimation performance of Any4D. We compare different inputs on both dense scene flow (Kubric) and sparse 3D point tracking (LSFOdyssey) benchmarks using end-point error (EPE), average points within delta (APD) and inlier ratio at 0.1m (τ), where best is bold. “Geometry” indicates use of depth, intrinsics and poses. Kubric-4D Static Camera LSFOdyssey Dynamic Points Scene Flow Dynamic Points Scene Flow Any4D Inputs EPE ↓ APD ↑ EPE ↓ τ ↑ EPE ↓ APD ↑ EPE ↓ τ ↑ Images Only 1.17 21.33 0.11 86.25 0.28 71.47 0.12 68.03 Images + Geometry 0.23 80.18 0.09 86.26 0.19 80.80 0.12 68.71 Images + Doppler 1.17 21.70 0.12 86.90 0.29 71.26 0.11 70.32 Images + Geometry + Doppler 0.23 81.72 0.09 87.27 0.19 81.10 0.11 71.37 we study improvements to scene flow on the dense Kubric4D static benchmark and 3D tracking on LSFOdyssey benchmark by incorporating different input modalities. From Tab. 4, we observe that adding geometry significantly improves APD and EPE for 3D points. Adding doppler further improves scene-flow, with the best performance achieved when all modalities are provided. Choice of Motion Representation: While allocentric motion Fallo is arguably the useful quantity for downstream applications, it is possible to represent the predicted scene flow output in 4 ways: • Allocentric Scene Flow: Directly predicting ˜Fallo. • Egocentric Scene Flow: Predicting egocentric scene flow ˜Fego, and using estimated geometry to recover allocentric motion as: ˜Fallo = Tt→t+1 \u0000P v 0 + ˜Fego \u0001 −p • 3D Points After Motion: Predicting view-aligned pointmaps at time 0 and t - P v 0 and P v t , and recovering the allocentric motion: ˜Fallo = P v t −P v 0 • Backprojected 2D Flow: Unprojecting optical flow to obtain covisible scene flow between pointmaps. We systematically investigate these choices in Tab. 5 and Fig. 5. We find that directly predicting allocentric motion Table 5. Allocentric scene flow is the optimal output representation for 4D motion. We compare different representation types on dense scene flow (Kubric) and sparse 3D point tracking (LSFOdyssey) using end-point error (EPE), average points within delta (APD) and inlier ratio at 0.1m (τ). Best results are bold. Kubric-4D Static Camera LSFOdyssey Dynamic Points Scene Flow Dynamic Points Scene Flow Representation Type EPE ↓ APD ↑ EPE ↓ τ ↑ EPE ↓ APD ↑ EPE ↓ τ ↑ Backprojected 2D Flow 2.14 19.44 1.16 75.69 0.49 57.21 0.27 70.11 3D Points After Motion 1.24 17.33 0.58 21.84 0.24 69.30 0.38 21.87 Egocentric Scene Flow 1.26 19.43 0.12 85.37 0.24 71.80 0.14 65.13 Allocentric Scene Flow 1.23 19.53 0.10 87.51 0.24 73.95 0.10 71.46 leads to optimal performance not only on scene flow, but surprisingly, also on dynamic pointmaps after motion, compared to directly predicting points after motion as adopted otherwise in [16]. Limitations: Although Any4D takes a step forward towards achieving 4D reconstruction models, we identify important limitations. Firstly, we always calculate scene-flow from the reference (first) view to all other frames in the sequence, necessitating that the object of interest should be present at the start of the video. One possible way to alleviate this is by training Any4D in a permutation invariant manner as in [83]. Secondly, we assume perfectly simulated multi-modal input and do not account for sensor noise - which is hardly true for real-world deployment. Finally, as with all data-driven architectures, generalization is a function of the diversity and size of the training set. We believe that Any4D’s performance on highly dynamic scenes and wide baselines (or low frame-rate videos) can be improved with the availability of richer dynamic 3D datasets[70]. 5. Conclusion We presented Any4D, a unified model that enables dense 4D reconstruction of dynamic scenes from both monocular and multi-modal setups. In Any4D, we chose a factorized output representation of 4D scenes, which allowed the use of diverse data for training at scale with partial supervision for auxiliary sub-tasks, in addition to the target task of dense scene flow estimation. Any4D is flexible, and supports optional multi-modal inputs. Importantly, we showed through our experiments that our joint training scheme produces generalizable view embeddings that improve performance whenever inputs such as depth and egocentric radial velocity (doppler) may be available to support the output prediction quantities. Finally, due to the feed-forward nature of Any4D, we saw that one can obtain dynamic scene estimates an order of magnitude faster than existing methods such as SpatialTrackerV2 [86], by exploiting N-view inference. We believe Any4D will ultimately enable real-time 4D scene reconstruction for applications such as Generative AI, AR/VR and Robotics, and serve as a foundational 4D reconstruction model. 8"}
{"pdf_id": "arxiv_251210935_any4d", "page": 9, "text": "Any4D: Unified Feed-Forward Metric 4D Reconstruction\nSupplementary Material\nTable S.1. List of Datasets used to train Any4D\nDataset\nDynamic Scene Flow Domain\n# Scenes\nBlendedMVS\n✗\n✗\nOutdoor & object centric\n500\nMegaDepth\n✗\n✗\nOutdoor\n275\nScanNet++\n✗\n✗\nIndoor\n295\nVKITTI2\n✓\n✗\nAV\n40\nWaymo-DriveTrack\n✓\n✓\nAV\n1500\nGCD-Kubric\n✓\n✗\nSynthetic random objects\n5000\nCoTracker3-Kubric\n✓\n✓\nSynthetic random objects\n5000\nDynamic Replica\n✓\n✓\nSynthetic humans & animals\n500\nPoint Odyssey\n✓\n✓\nDiverse Synthetic assets\n159\nA. Training\nDatasets:\nWe train on a combination of static and dy-\nnamic datasets with varying levels of supervision. For su-\npervision geometric quantities - depth, intrinsics, and camra\nposes, all the datasets in S.1 are used.\nFor scene flow\nsupervision, we only rely on Kubric (from CoTracker3),\nPointOdyssey and Dynamic Replica, as they contain both\ndiverse camera and scene motion crucial for learning good\nscene flow. We find that VKITTI-2 sequences span mini-\nmal scene motion while data from GCD lacks good camera\ndiversity, and thus, only use them for geometry supervision.\nImplementation Details:\nWe initialize Any4D’s weights\nwith the public MapAnything checkpoint.\nThe doppler\nscene-flow encoder, and the scene-flow DPT decoder are\ninitialized and learnt from scratch. We train the entire net-\nwork with a learning rate of 1e-5, 5e-7 and 1e-4 for the\nentire network, the DINOv2 Image encoder and the Scene-\nflow DPT decoder respectively. We use a warmup of 10\nepochs, and finetune the network for a total of 100 epochs,\ncovering approximately 120k gradient steps in total on 8\nH100 GPUs. The images and respective quantities in each\nbatch cropped and resized to 518 image width, with a ran-\ndomized height-width aspect ratio between 0.5 and 3. Dur-\ning each gradient step, we sample upto 4 views from each\ndataset, with a variable batch size of upto 24 views per\nGPU. As illustrated in Fig. S.1, we find that 4-view train-\ning is critical for generalizing with multi-view inference.\nB. Benchmarking Setup Details\nFor the TAPVID-3D PStudio dataset and DriveTrack\ndatasets, we evaluated on a uniform subset of 50 sequences\nfrom all available datasets and use the first 64 frames for\nevaluation. Since the dataset is extremely sparse and each\nsequence only contains at most a few hundred point queries,\nwe use all points for benchmarking. For Drive-Track, we\nfilter 50 sequences that contain non-zero allocentric motion.\nFor the Dynamic-Replica and LSF-Odyssey datasets, we fil-\n2\n4\n8\n16\n32\n64\nNumber of Views at inference\n0.18\n0.20\n0.22\n0.24\n0.26\nScene-Flow EPE\nMulti-view finetuning ablation for multi-view generalization\nAny4D: 2-View training\nAny4D: 4-View training\nFigure S.1. 4-View training is key to enabling multi-frame gen-\neralization during inference. Any4D trained with 2 views results\nin higher EPE at higher number of input views. In contrast, the 4-\nview model exhibits stable behaviour even at 64 views.\nter out static points (i.e., points with zero allocentric mo-\ntion) and use dynamic points as queries for our benchmark-\ning, to maintain homogeneity with the 2 other datasets and\nemphasize benchmarking of dynamic elements of a scene.\nWe acknowledge that our evaluation is similar to [16].\nFigure S.2. Doppler Scene Flow is simulated as radial component\nof ego-centric scene flow.\nC. Multi-Modal Conditioning\nSimulating Doppler Velocity:\nAs shown in Fig. S.2, we\nsimulate the Doppler velocity from egocentric scene flow\nlabels. More specifically, given a 3D point ⃗p = [x, y, z] and\nits corresponding ego scene flow vector ⃗v = [∆x, ∆y, ∆z],\nthe simulated Doppler velocity vr is defined as the projec-\ntion of the motion vector into the radial direction of each\nray. This is simply the normalized vector from the origin\nof the radar to the point ⃗p. The Doppler (radial) velocity is\ncomputed as:\nvr = ⃗p · ⃗v\n∥⃗p∥= x · ∆x + y · ∆y + z · ∆z\np\nx2 + y2 + z2\n9", "clean_text": "Any4D: Unified Feed-Forward Metric 4D Reconstruction Supplementary Material Table S.1. List of Datasets used to train Any4D Dataset Dynamic Scene Flow Domain # Scenes BlendedMVS ✗ ✗ Outdoor & object centric 500 MegaDepth ✗ ✗ Outdoor 275 ScanNet++ ✗ ✗ Indoor 295 VKITTI2 ✓ ✗ AV 40 Waymo-DriveTrack ✓ ✓ AV 1500 GCD-Kubric ✓ ✗ Synthetic random objects 5000 CoTracker3-Kubric ✓ ✓ Synthetic random objects 5000 Dynamic Replica ✓ ✓ Synthetic humans & animals 500 Point Odyssey ✓ ✓ Diverse Synthetic assets 159 A. Training Datasets: We train on a combination of static and dynamic datasets with varying levels of supervision. For supervision geometric quantities - depth, intrinsics, and camra poses, all the datasets in S.1 are used. For scene flow supervision, we only rely on Kubric (from CoTracker3), PointOdyssey and Dynamic Replica, as they contain both diverse camera and scene motion crucial for learning good scene flow. We find that VKITTI-2 sequences span minimal scene motion while data from GCD lacks good camera diversity, and thus, only use them for geometry supervision. Implementation Details: We initialize Any4D’s weights with the public MapAnything checkpoint. The doppler scene-flow encoder, and the scene-flow DPT decoder are initialized and learnt from scratch. We train the entire network with a learning rate of 1e-5, 5e-7 and 1e-4 for the entire network, the DINOv2 Image encoder and the Sceneflow DPT decoder respectively. We use a warmup of 10 epochs, and finetune the network for a total of 100 epochs, covering approximately 120k gradient steps in total on 8 H100 GPUs. The images and respective quantities in each batch cropped and resized to 518 image width, with a randomized height-width aspect ratio between 0.5 and 3. During each gradient step, we sample upto 4 views from each dataset, with a variable batch size of upto 24 views per GPU. As illustrated in Fig. S.1, we find that 4-view training is critical for generalizing with multi-view inference. B. Benchmarking Setup Details For the TAPVID-3D PStudio dataset and DriveTrack datasets, we evaluated on a uniform subset of 50 sequences from all available datasets and use the first 64 frames for evaluation. Since the dataset is extremely sparse and each sequence only contains at most a few hundred point queries, we use all points for benchmarking. For Drive-Track, we filter 50 sequences that contain non-zero allocentric motion. For the Dynamic-Replica and LSF-Odyssey datasets, we fil2 4 8 16 32 64 Number of Views at inference 0.18 0.20 0.22 0.24 0.26 Scene-Flow EPE Multi-view finetuning ablation for multi-view generalization Any4D: 2-View training Any4D: 4-View training Figure S.1. 4-View training is key to enabling multi-frame generalization during inference. Any4D trained with 2 views results in higher EPE at higher number of input views. In contrast, the 4view model exhibits stable behaviour even at 64 views. ter out static points (i.e., points with zero allocentric motion) and use dynamic points as queries for our benchmarking, to maintain homogeneity with the 2 other datasets and emphasize benchmarking of dynamic elements of a scene. We acknowledge that our evaluation is similar to [16]. Figure S.2. Doppler Scene Flow is simulated as radial component of ego-centric scene flow. C. Multi-Modal Conditioning Simulating Doppler Velocity: As shown in Fig. S.2, we simulate the Doppler velocity from egocentric scene flow labels. More specifically, given a 3D point ⃗p = [x, y, z] and its corresponding ego scene flow vector ⃗v = [∆x, ∆y, ∆z], the simulated Doppler velocity vr is defined as the projection of the motion vector into the radial direction of each ray. This is simply the normalized vector from the origin of the radar to the point ⃗p. The Doppler (radial) velocity is computed as: vr = ⃗p · ⃗v ∥⃗p∥= x · ∆x + y · ∆y + z · ∆z p x2 + y2 + z2 9"}
{"pdf_id": "arxiv_251210935_any4d", "page": 10, "text": "Figure S.3. Qualitative visualizations of Any4D estimating 3D geometry and point tracking on TAPVID-3D Waymo Drive-Track\nsequences. As visible, the image-only variant (column 1) sometimes produces an offset to the scene flow at the edges. However, the\npredictions improve whenever sparse geometry (column 2) and doppler annotations are available (column 3).\nFigure S.4. Qualitative visualizations of Any4D limitations. Videos with large camera motion inducing no visual overlap of background\nor scene motion dominating the image space are common failure modes for Any4D. We believe that the availability of large-scale dense\nscene flow and 3D tracking datasets and integrating real-time optimization is key to overcoming these limitations.\nAcknowledgments\nWe thank Tarasha Khurana and Neehar Peri for their initial\ndiscussions in the project. We appreciate the help from Jeff\nTan with setting up Stereo4D (which we ended up not using\ndue to poor dataset quality). Lastly, we thank Bardienus\nDuisterhof and members of the AirLab & Deva’s Lab at\nCMU for insightful discussions and feedback on the paper.\nThis work was supported by Defense Science and Technol-\nogy Agency contract #DST000EC124000205, Bosch Research,\nand the IARPA via Department of Interior/Interior Business\nCenter (DOI/IBC) contract 140D0423C0074. The U.S. Gov-\nernment is authorized to reproduce and distribute reprints\nfor Governmental purposes notwithstanding any copyright\nannotation thereon. Disclaimer: The views and conclusions\ncontained herein are those of the authors and should not be\ninterpreted as necessarily representing the official policies\nor endorsements, either expressed or implied, of IARPA,\nDOI/IBC, or the U.S. Government. Lastly, this work was\nsupported by a hardware grant from Nvidia and used PSC\nBridges-2 through allocation cis220039p from the Advanced\nCyberinfrastructure Coordination Ecosystem: Services &\nSupport (ACCESS) program.\nReferences\n[1] Sameer Agarwal, Noah Snavely, Ian Simon, Steven M. Seitz,\nand Richard Szeliski. Building rome in a day. In 2009 IEEE\n12th International Conference on Computer Vision, pages\n72–79, 2009. 2\n[2] Manuel L´opez Antequera, Pau Gargallo, Markus Hofinger,\nSamuel Rota Bulo, Yubin Kuang, and Peter Kontschieder.\nMapillary planet-scale depth dataset. In European Confer-\nence on Computer Vision, pages 589–604. Springer, 2020.\n2\n[3] Armen Avetisyan, Christopher Xie, Henry Howard-Jenkins,\nTsun-Yi Yang, Samir Aroudj, Suvam Patra, Fuyang Zhang,\nDuncan Frost, Luke Holland, Campbell Orme, et al. Scene-\nscript: Reconstructing scenes with an autoregressive struc-\ntured language model. In European Conference on Computer\nVision, pages 247–263. Springer, 2024. 2\n[4] Arjun Balasingam, Joseph Chandler, Chenning Li, Zhoutong\nZhang, and Hari Balakrishnan. Drivetrack: A benchmark for\nlong-range point tracking in real-world videos. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 22488–22497, 2024. 5, 7\n[5] Tali Basha, Yael Moses, and Nahum Kiryati.\nMulti-view\nscene flow estimation: A view centered variational approach.\nInternational journal of computer vision, 101:6–21, 2013. 2\n10", "clean_text": "Figure S.3. Qualitative visualizations of Any4D estimating 3D geometry and point tracking on TAPVID-3D Waymo Drive-Track sequences. As visible, the image-only variant (column 1) sometimes produces an offset to the scene flow at the edges. However, the predictions improve whenever sparse geometry (column 2) and doppler annotations are available (column 3). Figure S.4. Qualitative visualizations of Any4D limitations. Videos with large camera motion inducing no visual overlap of background or scene motion dominating the image space are common failure modes for Any4D. We believe that the availability of large-scale dense scene flow and 3D tracking datasets and integrating real-time optimization is key to overcoming these limitations. Acknowledgments We thank Tarasha Khurana and Neehar Peri for their initial discussions in the project. We appreciate the help from Jeff Tan with setting up Stereo4D (which we ended up not using due to poor dataset quality). Lastly, we thank Bardienus Duisterhof and members of the AirLab & Deva’s Lab at CMU for insightful discussions and feedback on the paper. This work was supported by Defense Science and Technology Agency contract #DST000EC124000205, Bosch Research, and the IARPA via Department of Interior/Interior Business Center (DOI/IBC) contract 140D0423C0074. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DOI/IBC, or the U.S. Government. Lastly, this work was supported by a hardware grant from Nvidia and used PSC Bridges-2 through allocation cis220039p from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program. References [1] Sameer Agarwal, Noah Snavely, Ian Simon, Steven M. Seitz, and Richard Szeliski. Building rome in a day. In 2009 IEEE 12th International Conference on Computer Vision, pages 72–79, 2009. 2 [2] Manuel L´opez Antequera, Pau Gargallo, Markus Hofinger, Samuel Rota Bulo, Yubin Kuang, and Peter Kontschieder. Mapillary planet-scale depth dataset. In European Conference on Computer Vision, pages 589–604. Springer, 2020. 2 [3] Armen Avetisyan, Christopher Xie, Henry Howard-Jenkins, Tsun-Yi Yang, Samir Aroudj, Suvam Patra, Fuyang Zhang, Duncan Frost, Luke Holland, Campbell Orme, et al. Scenescript: Reconstructing scenes with an autoregressive structured language model. In European Conference on Computer Vision, pages 247–263. Springer, 2024. 2 [4] Arjun Balasingam, Joseph Chandler, Chenning Li, Zhoutong Zhang, and Hari Balakrishnan. Drivetrack: A benchmark for long-range point tracking in real-world videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22488–22497, 2024. 5, 7 [5] Tali Basha, Yael Moses, and Nahum Kiryati. Multi-view scene flow estimation: A view centered variational approach. International journal of computer vision, 101:6–21, 2013. 2 10"}
{"pdf_id": "arxiv_251210935_any4d", "page": 11, "text": "[6] Berta Bescos, Jos´e M F´acil, Javier Civera, and Jos´e Neira.\nDynaslam: Tracking, mapping, and inpainting in dynamic\nscenes. IEEE robotics and automation letters, 3(4):4076–\n4083, 2018. 2\n[7] Yohann Cabon, Naila Murray, and Martin Humenberger. Vir-\ntual kitti 2. arXiv preprint arXiv:2001.10773, 2020. 2, 5, 6\n[8] Kaihua Chen, Tarasha Khurana, and Deva Ramanan. Recon-\nstruct, inpaint, finetune: Dynamic novel-view synthesis from\nmonocular videos. arXiv preprint arXiv:2507.12646, 2025.\n1\n[9] Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zi-\nlong Huang, Jiashi Feng, and Bingyi Kang. Video depth any-\nthing: Consistent depth estimation for super-long videos. In\nProceedings of the Computer Vision and Pattern Recognition\nConference, pages 22831–22840, 2025. 7\n[10] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace,\nEkaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon,\nYuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang,\net al.\nPanda-70m: Captioning 70m videos with multiple\ncross-modality teachers. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 13320–13331, 2024. 2\n[11] Nathaniel Chodosh, Deva Ramanan, and Simon Lucey. Re-\nevaluating lidar scene flow. In Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision\n(WACV), pages 6005–6015, 2024. 2\n[12] Tri Dao.\nFlashattention-2:\nFaster attention with bet-\nter parallelism and work partitioning.\narXiv preprint\narXiv:2307.08691, 2023. 5\n[13] Frank Dellaert, Michael Kaess, et al. Factor graphs for robot\nperception. Foundations and Trends® in Robotics, 6(1-2):\n1–139, 2017. 2\n[14] Bardienus P Duisterhof, Jan Oberst, Bowen Wen, Stan\nBirchfield, Deva Ramanan, and Jeffrey Ichnowski. Rayst3r:\nPredicting novel depth maps for zero-shot object completion.\narXiv preprint arXiv:2506.05285, 2025. 2\n[15] Jakob Engel, Thomas Sch¨ops, and Daniel Cremers.\nLsd-\nslam: Large-scale direct monocular slam. In European con-\nference on computer vision, pages 834–849. Springer, 2014.\n2\n[16] Haiwen Feng, Junyi Zhang, Qianqian Wang, Yufei Ye,\nPengcheng Yu, Michael J Black, Trevor Darrell, and Angjoo\nKanazawa. St4rtrack: Simultaneous 4d reconstruction and\ntracking in the world.\narXiv preprint arXiv:2504.13152,\n2025. 2, 3, 5, 6, 7, 8, 9\n[17] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel\nUrtasun. Vision meets robotics: The kitti dataset. The in-\nternational journal of robotics research, 32(11):1231–1237,\n2013. 7\n[18] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch,\nYilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapra-\ngasam, Florian Golemo, Charles Herrmann, Thomas Kipf,\nAbhijit Kundu, Dmitry Lagun, Issam Laradji, Hsueh-\nTi (Derek) Liu, Henning Meyer, Yishu Miao, Derek\nNowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Rad-\nwan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi,\nMatan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun,\nSuhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi,\nFangcheng Zhong, and Andrea Tagliasacchi. Kubric: a scal-\nable dataset generator. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n2022. 2, 5\n[19] Jisang Han, Honggyu An, Jaewoo Jung, Takuya Narihira,\nJunyoung Seo, Kazumi Fukuda, Chaehyun Kim, Sunghwan\nHong, Yuki Mitsufuji, and Seungryong Kim. Dˆ 2ust3r: En-\nhancing 3d reconstruction with 4d pointmaps for dynamic\nscenes. arXiv preprint arXiv:2504.06264, 2025. 3\n[20] Adam W Harley, Zhaoyuan Fang, and Katerina Fragkiadaki.\nParticle video revisited: Tracking through occlusions using\npoint trajectories. In European Conference on Computer Vi-\nsion, pages 59–75. Springer, 2022. 3\n[21] Mina Henein, Jun Zhang, Robert Mahony, and Viorela Ila.\nDynamic slam: The need for speed. In 2020 IEEE Inter-\nnational Conference on Robotics and Automation (ICRA),\npages 2123–2129. IEEE, 2020. 2\n[22] Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong\nCun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter:\nGenerating consistent long depth sequences for open-world\nvideos. In Proceedings of the Computer Vision and Pattern\nRecognition Conference, pages 2005–2015, 2025. 7\n[23] Yuan-Ting Hu, Jiahong Wang, Raymond A Yeh, and Alexan-\nder G Schwing. Sail-vos 3d: A synthetic dataset and base-\nlines for object detection and 3d mesh reconstruction from\nvideo data.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 1418–\n1428, 2021. 5\n[24] Junsheng Huang, Shengyu Hao, Bocheng Hu, and Gaoang\nWang. Understanding dynamic scenes in ego centric 4d point\nclouds. arXiv preprint arXiv:2508.07251, 2025. 1\n[25] Fr´ed´eric Huguet and Fr´ed´eric Devernay.\nA variational\nmethod for scene flow estimation from stereo sequences. In\n2007 IEEE 11th International Conference on Computer Vi-\nsion, pages 1–7. IEEE, 2007. 2\n[26] Wonbong Jang, Philippe Weinzaepfel, Vincent Leroy, Lour-\ndes Agapito, and Jerome Revaud. Pow3r: Empowering un-\nconstrained 3d reconstruction with camera and scene priors.\narXiv preprint arXiv:2503.17316, 2025. 2\n[27] Linyi Jin, Richard Tucker, Zhengqi Li, David Fouhey, Noah\nSnavely, and Aleksander Holynski. Stereo4d: Learning how\nthings move in 3d from internet stereo videos. arXiv preprint,\n2024. 2, 3, 5\n[28] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia\nNeverova, Andrea Vedaldi, and Christian Rupprecht. Dy-\nnamicstereo: Consistent dynamic depth from stereo videos.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 13229–13239, 2023.\n5, 6, 7\n[29] Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia\nNeverova, Andrea Vedaldi, and Christian Rupprecht. Co-\nTracker3:\nSimpler and better point tracking by pseudo-\nlabelling real videos. In arxiv, 2024. 3, 5, 6\n[30] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia\nNeverova, Andrea Vedaldi, and Christian Rupprecht. Co-\ntracker: It is better to track together. In Proc. ECCV, 2024.\n3\n11", "clean_text": "[6] Berta Bescos, Jos´e M F´acil, Javier Civera, and Jos´e Neira. Dynaslam: Tracking, mapping, and inpainting in dynamic scenes. IEEE robotics and automation letters, 3(4):4076– 4083, 2018. 2 [7] Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. arXiv preprint arXiv:2001.10773, 2020. 2, 5, 6 [8] Kaihua Chen, Tarasha Khurana, and Deva Ramanan. Reconstruct, inpaint, finetune: Dynamic novel-view synthesis from monocular videos. arXiv preprint arXiv:2507.12646, 2025. 1 [9] Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, and Bingyi Kang. Video depth anything: Consistent depth estimation for super-long videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 22831–22840, 2025. 7 [10] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13320–13331, 2024. 2 [11] Nathaniel Chodosh, Deva Ramanan, and Simon Lucey. Reevaluating lidar scene flow. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 6005–6015, 2024. 2 [12] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. 5 [13] Frank Dellaert, Michael Kaess, et al. Factor graphs for robot perception. Foundations and Trends® in Robotics, 6(1-2): 1–139, 2017. 2 [14] Bardienus P Duisterhof, Jan Oberst, Bowen Wen, Stan Birchfield, Deva Ramanan, and Jeffrey Ichnowski. Rayst3r: Predicting novel depth maps for zero-shot object completion. arXiv preprint arXiv:2506.05285, 2025. 2 [15] Jakob Engel, Thomas Sch¨ops, and Daniel Cremers. Lsdslam: Large-scale direct monocular slam. In European conference on computer vision, pages 834–849. Springer, 2014. 2 [16] Haiwen Feng, Junyi Zhang, Qianqian Wang, Yufei Ye, Pengcheng Yu, Michael J Black, Trevor Darrell, and Angjoo Kanazawa. St4rtrack: Simultaneous 4d reconstruction and tracking in the world. arXiv preprint arXiv:2504.13152, 2025. 2, 3, 5, 6, 7, 8, 9 [17] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The international journal of robotics research, 32(11):1231–1237, 2013. 7 [18] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun, Issam Laradji, HsuehTi (Derek) Liu, Henning Meyer, Yishu Miao, Derek Nowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Radwan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi, Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun, Suhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi, Fangcheng Zhong, and Andrea Tagliasacchi. Kubric: a scalable dataset generator. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2, 5 [19] Jisang Han, Honggyu An, Jaewoo Jung, Takuya Narihira, Junyoung Seo, Kazumi Fukuda, Chaehyun Kim, Sunghwan Hong, Yuki Mitsufuji, and Seungryong Kim. Dˆ 2ust3r: Enhancing 3d reconstruction with 4d pointmaps for dynamic scenes. arXiv preprint arXiv:2504.06264, 2025. 3 [20] Adam W Harley, Zhaoyuan Fang, and Katerina Fragkiadaki. Particle video revisited: Tracking through occlusions using point trajectories. In European Conference on Computer Vision, pages 59–75. Springer, 2022. 3 [21] Mina Henein, Jun Zhang, Robert Mahony, and Viorela Ila. Dynamic slam: The need for speed. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 2123–2129. IEEE, 2020. 2 [22] Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter: Generating consistent long depth sequences for open-world videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2005–2015, 2025. 7 [23] Yuan-Ting Hu, Jiahong Wang, Raymond A Yeh, and Alexander G Schwing. Sail-vos 3d: A synthetic dataset and baselines for object detection and 3d mesh reconstruction from video data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1418– 1428, 2021. 5 [24] Junsheng Huang, Shengyu Hao, Bocheng Hu, and Gaoang Wang. Understanding dynamic scenes in ego centric 4d point clouds. arXiv preprint arXiv:2508.07251, 2025. 1 [25] Fr´ed´eric Huguet and Fr´ed´eric Devernay. A variational method for scene flow estimation from stereo sequences. In 2007 IEEE 11th International Conference on Computer Vision, pages 1–7. IEEE, 2007. 2 [26] Wonbong Jang, Philippe Weinzaepfel, Vincent Leroy, Lourdes Agapito, and Jerome Revaud. Pow3r: Empowering unconstrained 3d reconstruction with camera and scene priors. arXiv preprint arXiv:2503.17316, 2025. 2 [27] Linyi Jin, Richard Tucker, Zhengqi Li, David Fouhey, Noah Snavely, and Aleksander Holynski. Stereo4d: Learning how things move in 3d from internet stereo videos. arXiv preprint, 2024. 2, 3, 5 [28] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Dynamicstereo: Consistent dynamic depth from stereo videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13229–13239, 2023. 5, 6, 7 [29] Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. CoTracker3: Simpler and better point tracking by pseudolabelling real videos. In arxiv, 2024. 3, 5, 6 [30] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. In Proc. ECCV, 2024. 3 11"}
{"pdf_id": "arxiv_251210935_any4d", "page": 12, "text": "[31] Nikhil Keetha, Jay Karhade, Krishna Murthy Jatavallabhula,\nGengshan Yang, Sebastian Scherer, Deva Ramanan, and\nJonathon Luiten. Splatam: Splat track & map 3d gaussians\nfor dense rgb-d slam. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n21357–21366, 2024. 2\n[32] Nikhil Keetha, Norman M¨uller, Johannes Sch¨onberger,\nLorenzo Porzi,\nYuchen Zhang,\nTobias Fischer,\nArno\nKnapitsch, Duncan Zauss, Ethan Weber, Nelson Antunes,\net al. MapAnything: Universal feed-forward metric 3d re-\nconstruction. In 2026 International Conference on 3D Vision\n(3DV). IEEE, 2026. 2, 3, 4, 5, 6, 7\n[33] Georg Klein and David Murray. Parallel tracking and map-\nping for small ar workspaces. In 2007 6th IEEE and ACM\ninternational symposium on mixed and augmented reality,\npages 225–234. IEEE, 2007. 2\n[34] Johannes Kopf, Xuejian Rong, and Jia-Bin Huang.\nRo-\nbust consistent video depth estimation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 1611–1621, 2021. 2\n[35] Skanda Koppula, Ignacio Rocco, Yi Yang, Joe Heyward,\nJo˜ao Carreira, Andrew Zisserman, Gabriel Brostow, and Carl\nDoersch. Tapvid-3d: A benchmark for tracking any point in\n3d, 2024. 3, 6, 7\n[36] Suryansh Kumar, Yuchao Dai, and Hongdong Li. Monocular\ndense 3d reconstruction of a complex dynamic scene from\ntwo perspective frames. In Proceedings of the IEEE inter-\nnational conference on computer vision, pages 4649–4657,\n2017. 2\n[37] Jiahui Lei, Yijia Weng, Adam W Harley, Leonidas Guibas,\nand Kostas Daniilidis.\nMosca: Dynamic gaussian fusion\nfrom casual videos via 4d motion scaffolds. In Proceedings\nof the Computer Vision and Pattern Recognition Conference,\npages 6165–6177, 2025. 2\n[38] Vincent Leroy, Yohann Cabon, and J´erˆome Revaud. Ground-\ning image matching in 3d with mast3r. In European Confer-\nence on Computer Vision, pages 71–91. Springer, 2024. 6\n[39] Zhengqi Li and Noah Snavely. Megadepth: Learning single-\nview depth prediction from internet photos.\nIn Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 2041–2050, 2018. 5\n[40] Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang,\nLinyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holyn-\nski, and Noah Snavely. Megasam: Accurate, fast, and ro-\nbust structure and motion from casual dynamic videos. arXiv\npreprint arXiv:2412.04463, 2024. 2, 7\n[41] Chenguo Lin, Yuchen Lin, Panwang Pan, Yifan Yu, Hon-\nglei Yan, Katerina Fragkiadaki, and Yadong Mu. Movies:\nMotion-aware 4d dynamic view synthesis in one second.\narXiv preprint arXiv:2507.10065, 2025. 1, 2, 3\n[42] Xingyu Liu,\nCharles R Qi,\nand Leonidas J Guibas.\nFlownet3d: Learning scene flow in 3d point clouds. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 529–537, 2019. 2\n[43] Xinhang Liu, Yuxi Xiao, Donny Y Chen, Jiashi Feng, Yu-\nWing Tai, Chi-Keung Tang, and Bingyi Kang. Trace any-\nthing: Representing any video in 4d via trajectory fields.\narXiv preprint arXiv:2510.13802, 2025. 3\n[44] Zeyi Liu, Shuang Li, Eric Cousineau, Siyuan Feng, Ben-\njamin Burchfiel, and Shuran Song.\nGeometry-aware 4d\nvideo generation for robot manipulation.\narXiv preprint\narXiv:2507.01099, 2025. 2\n[45] Hidenobu Matsuki, Gwangbin Bae, and Andrew J Davison.\n4dtam: Non-rigid tracking and mapping via dynamic surface\ngaussians. In Proceedings of the Computer Vision and Pat-\ntern Recognition Conference, pages 26921–26932, 2025. 2\n[46] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer,\nDaniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A\nlarge dataset to train convolutional networks for disparity,\noptical flow, and scene flow estimation. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, pages 4040–4048, 2016. 7\n[47] Lukas Mehl, Jenny Schmalfuss, Azin Jahedi, Yaroslava Nali-\nvayko, and Andr´es Bruhn. Spring: A high-resolution high-\ndetail dataset and benchmark for scene flow, optical flow\nand stereo.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 4981–\n4991, 2023. 2\n[48] Moritz Menze and Andreas Geiger. Object scene flow for\nautonomous vehicles.\nIn Conference on Computer Vision\nand Pattern Recognition (CVPR), 2015. 2\n[49] Himangi Mittal, Brian Okorn, and David Held. Just go with\nthe flow: Self-supervised scene flow estimation. In Proceed-\nings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 11177–11185, 2020. 2\n[50] Raul Mur-Artal and Juan D Tard´os. Orb-slam2: An open-\nsource slam system for monocular, stereo, and rgb-d cam-\neras. IEEE transactions on robotics, 33(5):1255–1262, 2017.\n2\n[51] Tuan Duc Ngo, Peiye Zhuang, Chuang Gan, Evange-\nlos Kalogerakis, Sergey Tulyakov, Hsin-Ying Lee, and\nChaoyang Wang. Delta: Dense efficient long-range 3d track-\ning for any video. arXiv preprint arXiv:2410.24211, 2024.\n3\n[52] Dantong Niu, Yuvan Sharma, Haoru Xue, Giscard Biamby,\nJunyi Zhang, Ziteng Ji, Trevor Darrell, and Roei Herzig. Pre-\ntraining auto-regressive robotic models with 4d representa-\ntions. arXiv preprint arXiv:2502.13142, 2025. 2\n[53] Maxime Oquab, Timoth´ee Darcet, Th´eo Moutakanni, Huy\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDinov2: Learning robust visual features without supervision.\narXiv preprint arXiv:2304.07193, 2023. 4\n[54] Emanuele Palazzolo, Jens Behley, Philipp Lottes, Philippe\nGiguere, and Cyrill Stachniss. Refusion: 3d reconstruction\nin dynamic environments for rgb-d cameras exploiting resid-\nuals. In 2019 IEEE/RSJ International Conference on Intel-\nligent Robots and Systems (IROS), pages 7855–7862. IEEE,\n2019. 7\n[55] Jean-Philippe Pons, Renaud Keriven, and Olivier Faugeras.\nMulti-view stereo reconstruction and scene flow estimation\nwith a global image-based matching score.\nInternational\nJournal of Computer Vision, 72:179–193, 2007. 2\n[56] Gilles Puy, Alexandre Boulch, and Renaud Marlet. FLOT:\nScene Flow on Point Clouds Guided by Optimal Transport.\nIn European Conference on Computer Vision, 2020. 2\n12", "clean_text": "[31] Nikhil Keetha, Jay Karhade, Krishna Murthy Jatavallabhula, Gengshan Yang, Sebastian Scherer, Deva Ramanan, and Jonathon Luiten. Splatam: Splat track & map 3d gaussians for dense rgb-d slam. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21357–21366, 2024. 2 [32] Nikhil Keetha, Norman M¨uller, Johannes Sch¨onberger, Lorenzo Porzi, Yuchen Zhang, Tobias Fischer, Arno Knapitsch, Duncan Zauss, Ethan Weber, Nelson Antunes, et al. MapAnything: Universal feed-forward metric 3d reconstruction. In 2026 International Conference on 3D Vision (3DV). IEEE, 2026. 2, 3, 4, 5, 6, 7 [33] Georg Klein and David Murray. Parallel tracking and mapping for small ar workspaces. In 2007 6th IEEE and ACM international symposium on mixed and augmented reality, pages 225–234. IEEE, 2007. 2 [34] Johannes Kopf, Xuejian Rong, and Jia-Bin Huang. Robust consistent video depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1611–1621, 2021. 2 [35] Skanda Koppula, Ignacio Rocco, Yi Yang, Joe Heyward, Jo˜ao Carreira, Andrew Zisserman, Gabriel Brostow, and Carl Doersch. Tapvid-3d: A benchmark for tracking any point in 3d, 2024. 3, 6, 7 [36] Suryansh Kumar, Yuchao Dai, and Hongdong Li. Monocular dense 3d reconstruction of a complex dynamic scene from two perspective frames. In Proceedings of the IEEE international conference on computer vision, pages 4649–4657, 2017. 2 [37] Jiahui Lei, Yijia Weng, Adam W Harley, Leonidas Guibas, and Kostas Daniilidis. Mosca: Dynamic gaussian fusion from casual videos via 4d motion scaffolds. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 6165–6177, 2025. 2 [38] Vincent Leroy, Yohann Cabon, and J´erˆome Revaud. Grounding image matching in 3d with mast3r. In European Conference on Computer Vision, pages 71–91. Springer, 2024. 6 [39] Zhengqi Li and Noah Snavely. Megadepth: Learning singleview depth prediction from internet photos. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2041–2050, 2018. 5 [40] Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely. Megasam: Accurate, fast, and robust structure and motion from casual dynamic videos. arXiv preprint arXiv:2412.04463, 2024. 2, 7 [41] Chenguo Lin, Yuchen Lin, Panwang Pan, Yifan Yu, Honglei Yan, Katerina Fragkiadaki, and Yadong Mu. Movies: Motion-aware 4d dynamic view synthesis in one second. arXiv preprint arXiv:2507.10065, 2025. 1, 2, 3 [42] Xingyu Liu, Charles R Qi, and Leonidas J Guibas. Flownet3d: Learning scene flow in 3d point clouds. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 529–537, 2019. 2 [43] Xinhang Liu, Yuxi Xiao, Donny Y Chen, Jiashi Feng, YuWing Tai, Chi-Keung Tang, and Bingyi Kang. Trace anything: Representing any video in 4d via trajectory fields. arXiv preprint arXiv:2510.13802, 2025. 3 [44] Zeyi Liu, Shuang Li, Eric Cousineau, Siyuan Feng, Benjamin Burchfiel, and Shuran Song. Geometry-aware 4d video generation for robot manipulation. arXiv preprint arXiv:2507.01099, 2025. 2 [45] Hidenobu Matsuki, Gwangbin Bae, and Andrew J Davison. 4dtam: Non-rigid tracking and mapping via dynamic surface gaussians. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 26921–26932, 2025. 2 [46] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4040–4048, 2016. 7 [47] Lukas Mehl, Jenny Schmalfuss, Azin Jahedi, Yaroslava Nalivayko, and Andr´es Bruhn. Spring: A high-resolution highdetail dataset and benchmark for scene flow, optical flow and stereo. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4981– 4991, 2023. 2 [48] Moritz Menze and Andreas Geiger. Object scene flow for autonomous vehicles. In Conference on Computer Vision and Pattern Recognition (CVPR), 2015. 2 [49] Himangi Mittal, Brian Okorn, and David Held. Just go with the flow: Self-supervised scene flow estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11177–11185, 2020. 2 [50] Raul Mur-Artal and Juan D Tard´os. Orb-slam2: An opensource slam system for monocular, stereo, and rgb-d cameras. IEEE transactions on robotics, 33(5):1255–1262, 2017. 2 [51] Tuan Duc Ngo, Peiye Zhuang, Chuang Gan, Evangelos Kalogerakis, Sergey Tulyakov, Hsin-Ying Lee, and Chaoyang Wang. Delta: Dense efficient long-range 3d tracking for any video. arXiv preprint arXiv:2410.24211, 2024. 3 [52] Dantong Niu, Yuvan Sharma, Haoru Xue, Giscard Biamby, Junyi Zhang, Ziteng Ji, Trevor Darrell, and Roei Herzig. Pretraining auto-regressive robotic models with 4d representations. arXiv preprint arXiv:2502.13142, 2025. 2 [53] Maxime Oquab, Timoth´ee Darcet, Th´eo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 4 [54] Emanuele Palazzolo, Jens Behley, Philipp Lottes, Philippe Giguere, and Cyrill Stachniss. Refusion: 3d reconstruction in dynamic environments for rgb-d cameras exploiting residuals. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 7855–7862. IEEE, 2019. 7 [55] Jean-Philippe Pons, Renaud Keriven, and Olivier Faugeras. Multi-view stereo reconstruction and scene flow estimation with a global image-based matching score. International Journal of Computer Vision, 72:179–193, 2007. 2 [56] Gilles Puy, Alexandre Boulch, and Renaud Marlet. FLOT: Scene Flow on Point Clouds Guided by Optimal Transport. In European Conference on Computer Vision, 2020. 2 12"}
{"pdf_id": "arxiv_251210935_any4d", "page": 13, "text": "[57] Yuheng Qiu, Chen Wang, Wenshan Wang, Mina Henein, and\nSebastian Scherer.\nAirdos: Dynamic slam benefits from\narticulated objects.\nIn 2022 International Conference on\nRobotics and Automation (ICRA), pages 8047–8053. IEEE,\n2022. 2\n[58] Ren´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-\nsion transformers for dense prediction. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\npages 12179–12188, 2021. 2, 5\n[59] Ren´e Ranftl,\nKatrin Lasinger,\nDavid Hafner,\nKonrad\nSchindler, and Vladlen Koltun. Towards robust monocular\ndepth estimation: Mixing datasets for zero-shot cross-dataset\ntransfer. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence, 44(3), 2022. 2\n[60] Jiawei Ren, Kevin Xie, Ashkan Mirzaei, Hanxue Liang, Xi-\naohui Zeng, Karsten Kreis, Ziwei Liu, Antonio Torralba,\nSanja Fidler, Seung Wook Kim, and Huan Ling.\nL4gm:\nLarge 4d gaussian reconstruction model.\nIn Advances in\nNeural Information Processing Systems, 2024. 2\n[61] P. Sand and S. Teller. Particle video: Long-range motion es-\ntimation using point trajectories. In 2006 IEEE Computer\nSociety Conference on Computer Vision and Pattern Recog-\nnition (CVPR’06), pages 2195–2202, 2006. 3\n[62] Johannes L Schonberger and Jan-Michael Frahm. Structure-\nfrom-motion revisited.\nIn Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n4104–4113, 2016. 2\n[63] Jenny Seidenschwarz, Qunjie Zhou, Bardienus P Duisterhof,\nDeva Ramanan, and Laura Leal-Taix´e.\nDynomo: Online\npoint tracking by dynamic online monocular gaussian recon-\nstruction. In 2025 International Conference on 3D Vision\n(3DV), pages 1012–1021. IEEE, 2025. 2\n[64] Steven M Seitz, Brian Curless, James Diebel, Daniel\nScharstein, and Richard Szeliski. A comparison and evalua-\ntion of multi-view stereo reconstruction algorithms. In 2006\nIEEE computer society conference on computer vision and\npattern recognition (CVPR’06), pages 519–528. IEEE, 2006.\n2\n[65] Akash Sharma, Wei Dong, and Michael Kaess.\nCompo-\nsitional and scalable object slam.\nIn 2021 IEEE Inter-\nnational Conference on Robotics and Automation (ICRA),\npages 11626–11632. IEEE, 2021. 2\n[66] Edgar Sucar, Zihang Lai, Eldar Insafutdinov, and An-\ndrea Vedaldi.\nDynamic point maps:\nA versatile repre-\nsentation for dynamic 3d reconstruction.\narXiv preprint\narXiv:2503.16318, 2025. 3\n[67] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field\ntransforms for optical flow.\nIn Computer Vision–ECCV\n2020: 16th European Conference, Glasgow, UK, August 23–\n28, 2020, Proceedings, Part II 16, pages 402–419. Springer,\n2020. 2\n[68] Zachary Teed and Jia Deng. Droid-slam: Deep visual slam\nfor monocular, stereo, and rgb-d cameras. Advances in neu-\nral information processing systems, 34:16558–16569, 2021.\n2\n[69] Zachary Teed and Jia Deng.\nRaft-3d: Scene flow using\nrigid-motion embeddings. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2021. 2, 6\n[70] Joachim Tesch, Giorgio Becherini, Prerana Achar, Anasta-\nsios Yiannakidis, Muhammed Kocabas, Priyanka Patel, and\nMichael J. Black. BEDLAM2.0: Synthetic humans and cam-\neras in motion. In The Thirty-ninth Annual Conference on\nNeural Information Processing Systems Datasets and Bench-\nmarks Track, 2025. 8\n[71] Bill Triggs, Philip F McLauchlan, Richard I Hartley, and An-\ndrew W Fitzgibbon. Bundle adjustment—a modern synthe-\nsis. In International workshop on vision algorithms, pages\n298–372. Springer, 1999. 2\n[72] Basile Van Hoorick, Rundi Wu, Ege Ozguroglu, Kyle Sar-\ngent, Ruoshi Liu, Pavel Tokmakov, Achal Dave, Changxi\nZheng, and Carl Vondrick. Generative camera dolly: Ex-\ntreme monocular dynamic novel view synthesis.\nIn Eu-\nropean Conference on Computer Vision, pages 313–331.\nSpringer, 2024. 1, 6\n[73] Basile Van Hoorick, Rundi Wu, Ege Ozguroglu, Kyle Sar-\ngent, Ruoshi Liu, Pavel Tokmakov, Achal Dave, Changxi\nZheng, and Carl Vondrick. Generative camera dolly: Ex-\ntreme monocular dynamic novel view synthesis. In Euro-\npean Conference on Computer Vision (ECCV), 2024. 5\n[74] Kyle Vedder, Neehar Peri, Ishan Khatri, Siyi Li, Eric Eaton,\nMehmet Kocamaz, Yue Wang, Zhiding Yu, Deva Ramanan,\nand Joachim Pehserl. Neural eulerian scene flow fields. arXiv\npreprint arXiv:2410.02031, 2024. 2\n[75] Sundar Vedula, Simon Baker, Peter Rander, Robert Collins,\nand Takeo Kanade. Three-dimensional scene flow. In Pro-\nceedings of the Seventh IEEE International Conference on\nComputer Vision, pages 722–729. IEEE, 1999. 2\n[76] Bo Wang, Jian Li, Yang Yu, Li Liu, Zhenping Sun, and\nDewen Hu. Scenetracker: Long-term scene flow estimation\nnetwork. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence, 2025. 6, 7\n[77] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea\nVedaldi, Christian Rupprecht, and David Novotny.\nVggt:\nVisual geometry grounded transformer. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2025. 2, 4, 6, 7\n[78] Qianqian Wang, Vickie Ye, Hang Gao, Weijia Zeng, Jake\nAustin, Zhengqi Li, and Angjoo Kanazawa. Shape of mo-\ntion: 4d reconstruction from a single video. In International\nConference on Computer Vision (ICCV), 2025. 2\n[79] Qianqian\nWang,\nYifei\nZhang,\nAleksander\nHolynski,\nAlexei A Efros, and Angjoo Kanazawa. Continuous 3d per-\nception model with persistent state. In Proceedings of the\nComputer Vision and Pattern Recognition Conference, pages\n10510–10522, 2025. 7\n[80] Shuzhe Wang,\nVincent Leroy,\nYohann Cabon,\nBoris\nChidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vi-\nsion made easy. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n20697–20709, 2024. 2, 3, 6, 7\n[81] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma,\nMichael M Bronstein, and Justin M Solomon.\nDynamic\ngraph cnn for learning on point clouds. ACM Transactions\non Graphics (tog), 38(5):1–12, 2019. 2\n13", "clean_text": "[57] Yuheng Qiu, Chen Wang, Wenshan Wang, Mina Henein, and Sebastian Scherer. Airdos: Dynamic slam benefits from articulated objects. In 2022 International Conference on Robotics and Automation (ICRA), pages 8047–8053. IEEE, 2022. 2 [58] Ren´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the IEEE/CVF international conference on computer vision, pages 12179–12188, 2021. 2, 5 [59] Ren´e Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(3), 2022. 2 [60] Jiawei Ren, Kevin Xie, Ashkan Mirzaei, Hanxue Liang, Xiaohui Zeng, Karsten Kreis, Ziwei Liu, Antonio Torralba, Sanja Fidler, Seung Wook Kim, and Huan Ling. L4gm: Large 4d gaussian reconstruction model. In Advances in Neural Information Processing Systems, 2024. 2 [61] P. Sand and S. Teller. Particle video: Long-range motion estimation using point trajectories. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06), pages 2195–2202, 2006. 3 [62] Johannes L Schonberger and Jan-Michael Frahm. Structurefrom-motion revisited. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4104–4113, 2016. 2 [63] Jenny Seidenschwarz, Qunjie Zhou, Bardienus P Duisterhof, Deva Ramanan, and Laura Leal-Taix´e. Dynomo: Online point tracking by dynamic online monocular gaussian reconstruction. In 2025 International Conference on 3D Vision (3DV), pages 1012–1021. IEEE, 2025. 2 [64] Steven M Seitz, Brian Curless, James Diebel, Daniel Scharstein, and Richard Szeliski. A comparison and evaluation of multi-view stereo reconstruction algorithms. In 2006 IEEE computer society conference on computer vision and pattern recognition (CVPR’06), pages 519–528. IEEE, 2006. 2 [65] Akash Sharma, Wei Dong, and Michael Kaess. Compositional and scalable object slam. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 11626–11632. IEEE, 2021. 2 [66] Edgar Sucar, Zihang Lai, Eldar Insafutdinov, and Andrea Vedaldi. Dynamic point maps: A versatile representation for dynamic 3d reconstruction. arXiv preprint arXiv:2503.16318, 2025. 3 [67] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23– 28, 2020, Proceedings, Part II 16, pages 402–419. Springer, 2020. 2 [68] Zachary Teed and Jia Deng. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. Advances in neural information processing systems, 34:16558–16569, 2021. 2 [69] Zachary Teed and Jia Deng. Raft-3d: Scene flow using rigid-motion embeddings. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 2, 6 [70] Joachim Tesch, Giorgio Becherini, Prerana Achar, Anastasios Yiannakidis, Muhammed Kocabas, Priyanka Patel, and Michael J. Black. BEDLAM2.0: Synthetic humans and cameras in motion. In The Thirty-ninth Annual Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2025. 8 [71] Bill Triggs, Philip F McLauchlan, Richard I Hartley, and Andrew W Fitzgibbon. Bundle adjustment—a modern synthesis. In International workshop on vision algorithms, pages 298–372. Springer, 1999. 2 [72] Basile Van Hoorick, Rundi Wu, Ege Ozguroglu, Kyle Sargent, Ruoshi Liu, Pavel Tokmakov, Achal Dave, Changxi Zheng, and Carl Vondrick. Generative camera dolly: Extreme monocular dynamic novel view synthesis. In European Conference on Computer Vision, pages 313–331. Springer, 2024. 1, 6 [73] Basile Van Hoorick, Rundi Wu, Ege Ozguroglu, Kyle Sargent, Ruoshi Liu, Pavel Tokmakov, Achal Dave, Changxi Zheng, and Carl Vondrick. Generative camera dolly: Extreme monocular dynamic novel view synthesis. In European Conference on Computer Vision (ECCV), 2024. 5 [74] Kyle Vedder, Neehar Peri, Ishan Khatri, Siyi Li, Eric Eaton, Mehmet Kocamaz, Yue Wang, Zhiding Yu, Deva Ramanan, and Joachim Pehserl. Neural eulerian scene flow fields. arXiv preprint arXiv:2410.02031, 2024. 2 [75] Sundar Vedula, Simon Baker, Peter Rander, Robert Collins, and Takeo Kanade. Three-dimensional scene flow. In Proceedings of the Seventh IEEE International Conference on Computer Vision, pages 722–729. IEEE, 1999. 2 [76] Bo Wang, Jian Li, Yang Yu, Li Liu, Zhenping Sun, and Dewen Hu. Scenetracker: Long-term scene flow estimation network. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. 6, 7 [77] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. 2, 4, 6, 7 [78] Qianqian Wang, Vickie Ye, Hang Gao, Weijia Zeng, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of motion: 4d reconstruction from a single video. In International Conference on Computer Vision (ICCV), 2025. 2 [79] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei A Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 10510–10522, 2025. 7 [80] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 20697–20709, 2024. 2, 3, 6, 7 [81] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. Dynamic graph cnn for learning on point clouds. ACM Transactions on Graphics (tog), 38(5):1–12, 2019. 2 13"}
{"pdf_id": "arxiv_251210935_any4d", "page": 14, "text": "[82] Yihan Wang, Lahav Lipson, and Jia Deng. Sea-raft: Simple,\nefficient, accurate raft for optical flow. In European Confer-\nence on Computer Vision, pages 36–54. Springer, 2024. 7\n[83] Yifan Wang, Jianjun Zhou, Haoyi Zhu, Wenzheng Chang,\nYang Zhou, Zizun Li, Junyi Chen, Jiangmiao Pang, Chunhua\nShen, and Tong He. π3: Scalable permutation-equivariant\nvisual geometry learning. arXiv preprint arXiv:2507.13347,\n2025. 2, 8\n[84] Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi\nZheng, Jonathan T Barron, and Aleksander Holynski. Cat4d:\nCreate anything in 4d with multi-view video diffusion mod-\nels. arXiv preprint arXiv:2411.18613, 2024. 1, 2\n[85] Wenxuan Wu, Zhi Yuan Wang, Zhuwen Li, Wei Liu, and Li\nFuxin. Pointpwc-net: Cost volume on point clouds for (self-)\nsupervised scene flow estimation. In European Conference\non Computer Vision, pages 88–107. Springer, 2020. 2\n[86] Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue,\nSida Peng, Yujun Shen, and Xiaowei Zhou. Spatialtracker:\nTracking any 2d pixels in 3d space.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 20406–20417, 2024. 3, 8\n[87] Yuxi Xiao, Jianyuan Wang, Nan Xue, Nikita Karaev, Iurii\nMakarov, Bingyi Kang, Xin Zhu, Hujun Bao, Yujun Shen,\nand Xiaowei Zhou. Spatialtrackerv2: 3d point tracking made\neasy. In ICCV, 2025. 3, 5, 6, 7\n[88] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi\nFeng, and Hengshuang Zhao. Depth anything: Unleashing\nthe power of large-scale unlabeled data. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10371–10381, 2024. 2\n[89] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren,\nLei Zhou, Tian Fang, and Long Quan. Blendedmvs: A large-\nscale dataset for generalized multi-view stereo networks. In\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 1790–1799, 2020. 5\n[90] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner,\nand Angela Dai. Scannet++: A high-fidelity dataset of 3d in-\ndoor scenes. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 12–22, 2023. 5\n[91] Bowei Zhang, Lei Ke, Adam W Harley, and Katerina Fragki-\nadaki. Tapip3d: Tracking any point in persistent 3d geome-\ntry. arXiv preprint arXiv:2504.14717, 2025. 3\n[92] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jam-\npani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-\nHsuan Yang.\nMonst3r: A simple approach for estimat-\ning geometry in the presence of motion.\narXiv preprint\narXiv:2410.03825, 2024. 2, 6, 7\n[93] Songyan Zhang, Yongtao Ge, Jinyuan Tian, Guangkai Xu,\nHao Chen, Chen Lv, and Chunhua Shen. Pomato: Marry-\ning pointmap matching with temporal motion for dynamic\n3d reconstruction. arXiv preprint arXiv:2504.05692, 2025.\n3\n[94] Yuchen Zhang, Nikhil Keetha, Chenwei Lyu, Bhuvan Jhamb,\nYutian Chen, Yuheng Qiu, Jay Karhade, Shreyas Jha, Yaoyu\nHu, Deva Ramanan, et al. UFM: A simple path towards uni-\nfied dense correspondence with flow. Advances in Neural\nInformation Processing Systems, 2025. 2\n[95] Yang Zheng, Adam W. Harley, Bokui Shen, Gordon Wet-\nzstein, and Leonidas J. Guibas. Pointodyssey: A large-scale\nsynthetic dataset for long-term point tracking.\nIn ICCV,\n2023. 2, 5\n[96] Hanyu Zhou and Gim Hee Lee. Llava-4d: Embedding spa-\ntiotemporal prompt into lmms for 4d scene understanding.\narXiv preprint arXiv:2505.12253, 2025. 1\n14", "clean_text": "[82] Yihan Wang, Lahav Lipson, and Jia Deng. Sea-raft: Simple, efficient, accurate raft for optical flow. In European Conference on Computer Vision, pages 36–54. Springer, 2024. 7 [83] Yifan Wang, Jianjun Zhou, Haoyi Zhu, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Jiangmiao Pang, Chunhua Shen, and Tong He. π3: Scalable permutation-equivariant visual geometry learning. arXiv preprint arXiv:2507.13347, 2025. 2, 8 [84] Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi Zheng, Jonathan T Barron, and Aleksander Holynski. Cat4d: Create anything in 4d with multi-view video diffusion models. arXiv preprint arXiv:2411.18613, 2024. 1, 2 [85] Wenxuan Wu, Zhi Yuan Wang, Zhuwen Li, Wei Liu, and Li Fuxin. Pointpwc-net: Cost volume on point clouds for (self-) supervised scene flow estimation. In European Conference on Computer Vision, pages 88–107. Springer, 2020. 2 [86] Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, and Xiaowei Zhou. Spatialtracker: Tracking any 2d pixels in 3d space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20406–20417, 2024. 3, 8 [87] Yuxi Xiao, Jianyuan Wang, Nan Xue, Nikita Karaev, Iurii Makarov, Bingyi Kang, Xin Zhu, Hujun Bao, Yujun Shen, and Xiaowei Zhou. Spatialtrackerv2: 3d point tracking made easy. In ICCV, 2025. 3, 5, 6, 7 [88] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10371–10381, 2024. 2 [89] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan. Blendedmvs: A largescale dataset for generalized multi-view stereo networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1790–1799, 2020. 5 [90] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: A high-fidelity dataset of 3d indoor scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12–22, 2023. 5 [91] Bowei Zhang, Lei Ke, Adam W Harley, and Katerina Fragkiadaki. Tapip3d: Tracking any point in persistent 3d geometry. arXiv preprint arXiv:2504.14717, 2025. 3 [92] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and MingHsuan Yang. Monst3r: A simple approach for estimating geometry in the presence of motion. arXiv preprint arXiv:2410.03825, 2024. 2, 6, 7 [93] Songyan Zhang, Yongtao Ge, Jinyuan Tian, Guangkai Xu, Hao Chen, Chen Lv, and Chunhua Shen. Pomato: Marrying pointmap matching with temporal motion for dynamic 3d reconstruction. arXiv preprint arXiv:2504.05692, 2025. 3 [94] Yuchen Zhang, Nikhil Keetha, Chenwei Lyu, Bhuvan Jhamb, Yutian Chen, Yuheng Qiu, Jay Karhade, Shreyas Jha, Yaoyu Hu, Deva Ramanan, et al. UFM: A simple path towards unified dense correspondence with flow. Advances in Neural Information Processing Systems, 2025. 2 [95] Yang Zheng, Adam W. Harley, Bokui Shen, Gordon Wetzstein, and Leonidas J. Guibas. Pointodyssey: A large-scale synthetic dataset for long-term point tracking. In ICCV, 2023. 2, 5 [96] Hanyu Zhou and Gim Hee Lee. Llava-4d: Embedding spatiotemporal prompt into lmms for 4d scene understanding. arXiv preprint arXiv:2505.12253, 2025. 1 14"}
