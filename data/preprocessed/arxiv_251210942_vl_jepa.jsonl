{"pdf_id": "arxiv_251210942_vl_jepa", "page": 1, "text": "VL-JEPA: Joint Embedding Predictive Architecture for\nVision-language\nDelong Chen1,2,*\nMustafa Shukor1,3,*\nThÃ©o Moutakanni1,*\nWilly Chung1,3,*\nJade Yu1,\nTejaswi Kasarla1,\nAllen Bolourchi1,\nYann LeCun1,4,\nPascale Fung1,2\n1 Meta FAIR\n2 HKUST\n3 Sorbonne UniversitÃ©\n4 NYU\n* Equal contribution\ndelong.chen@connect.ust.hk\nWe introduce VL-JEPA, a vision-language model built on a Joint Embedding Predictive Architecture (JEPA).\nInstead of autoregressively generating tokens as in classical VLMs, VL-JEPA predicts continuous embeddings of\nthe target texts. By learning in an abstract representation space, the model focuses on task-relevant semantics\nwhile abstracting away surface-level linguistic variability. In a strictly controlled comparison against standard\ntoken-space VLM training with the same vision encoder and training data, VL-JEPA achieves stronger performance\nwhile having 50% fewer trainable parameters. At inference time, a lightweight text decoder is invoked only when\nneeded to translate VL-JEPA predicted embeddings into text. We show that VL-JEPA natively supports selective\ndecoding that reduces the number of decoding operations by âˆ¼2.85Ã— while maintaining similar performance\ncompared to non-adaptive uniform decoding. Beyond generation, the VL-JEPAâ€™s embedding space naturally\nsupports open-vocabulary classification, text-to-video retrieval, and discriminative VQA without any architecture\nmodification. On eight video classification and eight video retrieval datasets, the average performance VL-JEPA\nsurpasses that of CLIP, SigLIP2, and Perception Encoder. At the same time, the model achieves comparable\nperformance as classical VLMs (InstructBLIP, QwenVL) on four VQA datasets: GQA, TallyQA, POPE and POPEv2,\ndespite only having 1.6B parameters.\n1\nIntroduction\nOne of the most important aspects of advanced machine\nintelligence is the ability to understand the physical world\nthat surrounds us. This ability enables AI systems to learn,\nreason, plan and act in the real world in order to assist\nhumans [LeCun, 2022]. Intelligent systems that need to\nact in the real world includes wearable devices and robots\n[Fung et al., 2025]. Machine learning tasks that make up for\nthis ability include captioning, retrieval, visual question\nanswering, action tracking, reasoning and planning etc\n[Bordes et al., 2024, Chen et al., 2025b]. Systems for such\nreal-world applications must have real-time response with\nlow latency and inference cost.\nCurrently, the common approach to achieve these tasks\nis to use large token-generative Vision Language Models\n(VLMs) [Liu et al., 2023, Dai et al., 2023, Alayrac et al.,\n2022, Chen et al., 2024b, Cho et al., 2025, Chen et al., 2022],\nwhich takes visual input ğ‘‹ğ‘‰, textual query ğ‘‹ğ‘„to generate\ndesired textual response ğ‘Œautoregressively in token space,\ni.e., (ğ‘‹ğ‘‰, ğ‘‹ğ‘„) â†¦â†’ğ‘Œ. This is straightforward but inadequate\nfor two main reasons. First, VLMs are expensive to de-\nvelop, because they are trained to generate responses ğ‘Œ\nto queries by capturing both task-relevant semantics with\ntask-irrelevant surface linguistic features such as words\nchoice, style or paraphrasing. During training, VLMs\nmust model both aspects, which results in unnecessary\nPredictor\nL\nXQ\nXV\nY\nX-Encoder\nY-Encoder\nSY\nTextual\nQuery\nVisual\nInput\nTextual\nTarget\nÅœY\nSV\nY-Decoder\nFigure 1. VL-JEPA model architecture\ncomputing effort spent producing diverse token sequences\nthat ultimately do not impact the correctness of the output.\nSecond, real-time tasks involving live streaming video (e.g.,\nlive action tracking) require sparse and selective decoding\n(e.g.,, emitting a description only when a new event occurs)\n[Zhou et al., 2024]. However, VLMs rely on autoregressive\ntoken-by-token decoding, which must be completed be-\nfore revealing the underlying semantics of ğ‘Œ. This process\nintroduces unnecessary latency and hampers the ability\nto update semantics dynamically in real time.\nThis paper introduces the Joint Embedding Predictive\nArchitecture for Vision-Language (VL-JEPA), turning ex-\npensive learning of data-space token generation into more\narXiv:2512.10942v1  [cs.CV]  11 Dec 2025", "clean_text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language Delong Chen1,2,* Mustafa Shukor1,3,* ThÃ©o Moutakanni1,* Willy Chung1,3,* Jade Yu1, Tejaswi Kasarla1, Allen Bolourchi1, Yann LeCun1,4, Pascale Fung1,2 1 Meta FAIR 2 HKUST 3 Sorbonne UniversitÃ© 4 NYU * Equal contribution delong.chen@connect.ust.hk We introduce VL-JEPA, a vision-language model built on a Joint Embedding Predictive Architecture (JEPA). Instead of autoregressively generating tokens as in classical VLMs, VL-JEPA predicts continuous embeddings of the target texts. By learning in an abstract representation space, the model focuses on task-relevant semantics while abstracting away surface-level linguistic variability. In a strictly controlled comparison against standard token-space VLM training with the same vision encoder and training data, VL-JEPA achieves stronger performance while having 50% fewer trainable parameters. At inference time, a lightweight text decoder is invoked only when needed to translate VL-JEPA predicted embeddings into text. We show that VL-JEPA natively supports selective decoding that reduces the number of decoding operations by âˆ¼2.85Ã— while maintaining similar performance compared to non-adaptive uniform decoding. Beyond generation, the VL-JEPAâ€™s embedding space naturally supports open-vocabulary classification, text-to-video retrieval, and discriminative VQA without any architecture modification. On eight video classification and eight video retrieval datasets, the average performance VL-JEPA surpasses that of CLIP, SigLIP2, and Perception Encoder. At the same time, the model achieves comparable performance as classical VLMs (InstructBLIP, QwenVL) on four VQA datasets: GQA, TallyQA, POPE and POPEv2, despite only having 1.6B parameters. 1 Introduction One of the most important aspects of advanced machine intelligence is the ability to understand the physical world that surrounds us. This ability enables AI systems to learn, reason, plan and act in the real world in order to assist humans [LeCun, 2022]. Intelligent systems that need to act in the real world includes wearable devices and robots [Fung et al., 2025]. Machine learning tasks that make up for this ability include captioning, retrieval, visual question answering, action tracking, reasoning and planning etc [Bordes et al., 2024, Chen et al., 2025b]. Systems for such real-world applications must have real-time response with low latency and inference cost. Currently, the common approach to achieve these tasks is to use large token-generative Vision Language Models (VLMs) [Liu et al., 2023, Dai et al., 2023, Alayrac et al., 2022, Chen et al., 2024b, Cho et al., 2025, Chen et al., 2022], which takes visual input ğ‘‹ğ‘‰, textual query ğ‘‹ğ‘„to generate desired textual response ğ‘Œautoregressively in token space, i.e., (ğ‘‹ğ‘‰, ğ‘‹ğ‘„) â†¦â†’ğ‘Œ. This is straightforward but inadequate for two main reasons. First, VLMs are expensive to develop, because they are trained to generate responses ğ‘Œ to queries by capturing both task-relevant semantics with task-irrelevant surface linguistic features such as words choice, style or paraphrasing. During training, VLMs must model both aspects, which results in unnecessary Predictor L XQ XV Y X-Encoder Y-Encoder SY Textual Query Visual Input Textual Target ÅœY SV Y-Decoder Figure 1. VL-JEPA model architecture computing effort spent producing diverse token sequences that ultimately do not impact the correctness of the output. Second, real-time tasks involving live streaming video (e.g., live action tracking) require sparse and selective decoding (e.g.,, emitting a description only when a new event occurs) [Zhou et al., 2024]. However, VLMs rely on autoregressive token-by-token decoding, which must be completed before revealing the underlying semantics of ğ‘Œ. This process introduces unnecessary latency and hampers the ability to update semantics dynamically in real time. This paper introduces the Joint Embedding Predictive Architecture for Vision-Language (VL-JEPA), turning expensive learning of data-space token generation into more arXiv:2512.10942v1 [cs.CV] 11 Dec 2025"}
{"pdf_id": "arxiv_251210942_vl_jepa", "page": 2, "text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language\nSec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion\nefficient latent-space semantic prediction. As illustrated in\nFig. 1, the model employs x-encoder to map vision inputs\nğ‘‹ğ‘‰into embedding ğ‘†ğ‘‰, a y-encoder to map the textual\ntarget ğ‘Œinto an embedding ğ‘†ğ‘Œ, and a predictor that learns\nthe mapping (ğ‘†ğ‘‰, ğ‘‹ğ‘„) â†¦â†’ğ‘†ğ‘Œwhere ğ‘‹ğ‘„is a textual query\n(i.e., the prompt).\nThe training objective is defined in\nthe embedding space â„’VL-JEPA = ğ·( Ë†ğ‘†ğ‘Œ, ğ‘†ğ‘Œ) instead of the\ndata space â„’VLM = ğ·( Ë†ğ‘Œ, ğ‘Œ). During inference, a y-decoder\nreads out the predicted embedding Ë†ğ‘†ğ‘Œto text space Ë†ğ‘Œ\nwhen needed.\nThanks to its non-generative nature, VL-JEPA is not\nforced to reconstruct every surface detail of ğ‘Œin the token\nspace. Instead, it only needs to predict the abstract repre-\nsentation ğ‘†ğ‘Œin the embedding space. In the raw one-hot\ntoken space, different plausible ğ‘Œoutputs for the same\ninput often appear nearly orthogonal if they donâ€™t share\noverlapping tokens. However, in the embedding space,\nthese diverse targets can be mapped to nearby points that\nshare similar semantics. This simplifies the target distri-\nbution thus makes the learning process more efficient. In\naddition, unlike VLMs, this approach eliminates the need\nfor learning language generation with a heavy decoder\nduring training, resulting in significant efficiency gains.\nThanks to its non-autoregressive nature, VL-JEPA can\nproduce continuous streams of target semantic embed-\ndings within sliding windows with minimal latency as it\nonly require a single forward pass without autoregressive\ndecoding. This is particularly advantageous for real-time\nonline applications such as live action tracking, scene\nrecognition, or planning, where the embedding stream\ncan be selectively decoded by a lightweight y-decoder,\nenabling efficient and prompt updates.\nIn this work, we empirically validate the advantages\nof VL-JEPA. We conduct a strictly controlled comparison\nagainst classical token-generative VLM [Liu et al., 2023,\nCho et al., 2025]: both setups use the same vision encoder,\nspatial resolution, frame rate, training data, batch size,\nand number of iterations, etc., with the only difference\nbeing the objective in token space or embedding space.\nUnder this matched training condition, VL-JEPA delivers\nconsistently higher performance on zero-shot captioning\nand classification while using roughly half the trainable\nparameters, indicating that embedding-space supervision\nimproves learning efficiency.\nBeyond the training phase, VL-JEPA also delivers sub-\nstantial inference-time efficiency improvement through\nselective decoding, where decoding happens only due to\nsignificant change in the predicted embedding stream.\nEmpirically, this strategy reduces the number of decod-\ning operations by âˆ¼2.85Ã— while preserving overall output\nquality measured by average CIDEr scores.\nOur final VL-JEPA models are trained in two stages: 1)\na pretraining stage using caption data to establish robust\nvision-language alignment, and 2) a supervised finetuning\n(SFT) stage that equips the model with VQA capabili-\nties. The model resulting from the first stage, denoted as\nVL-JEPABASE, is evaluated on zero-shot classification and\ntext-to-video retrieval. VL-JEPABASE outperforms CLIP\n[Radford et al., 2021], SigLIP2 [Tschannen et al., 2025], and\nPerception Encoder [Bolya et al., 2025] models in terms\nof average classification accuracy (across 8 datasets) and\nretrieval recall@1 (across 8 datasets). Following the second\nstage, the resulting VL-JEPASFT demonstrates significantly\nimproved classification performance due to its exposure\nto in-domain training data. As a unified generalist model,\nVL-JEPASFT approaches the performance of specialist mod-\nels optimized for individual benchmarks. Simultaneously,\nVL-JEPASFT exhibits effective VQA capabilities, achieving\nperformance on par with established VLM families, such\nas InstructBLIP [Dai et al., 2023] and Qwen-VL [Bai et al.,\n2023], across four datasets covering compositional visual\nreasoning [Hudson and Manning, 2019], complex object\ncounting [Acharya et al., 2019], and object hallucination\n[Li et al., 2023b, 2025b].\nIn summary, the contributions of this paper are as\nfollows:\nâ€¢ We introduce VL-JEPA, the first non-generative model\nthat can perform general-domain vision-language\ntasks in real-time, built on a joint embedding predic-\ntive architecture.\nâ€¢ We demonstrate in controlled experiments that VL-\nJEPA, trained with latent space embedding predic-\ntion, outperforms VLMs that rely on data space token\nprediction.\nâ€¢ We show that VL-JEPA delivers significant efficiency\ngains over VLMs for online video streaming appli-\ncations, thanks to its non-autoregressive design and\nnative support for selective decoding.\nâ€¢ We highlight that our VL-JEPASFT model, with an\nunified model architecture, can effectively handle a\nwide range of classification, retrieval, and VQA tasks\nat the same time.\n2\nMethodology\nWe propose VL-JEPA (Fig. 1), a model with the joint em-\nbedding predictive architecture (JEPA) for vision-language\ntasks. VL-JEPA is trained with triplets âŸ¨ğ‘‹ğ‘‰, ğ‘‹ğ‘„, ğ‘ŒâŸ©, where\nğ‘‹ğ‘‰denotes the visual input (a single image or a sequence\nof video frames), ğ‘‹ğ‘„is a textual query (i.e., a question)\nand ğ‘Œis the textual target (i.e., the answer) to be predicted.\nThe VL-JEPA comprises of four components:\n2", "clean_text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language Sec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion efficient latent-space semantic prediction. As illustrated in Fig. 1, the model employs x-encoder to map vision inputs ğ‘‹ğ‘‰into embedding ğ‘†ğ‘‰, a y-encoder to map the textual target ğ‘Œinto an embedding ğ‘†ğ‘Œ, and a predictor that learns the mapping (ğ‘†ğ‘‰, ğ‘‹ğ‘„) â†¦â†’ğ‘†ğ‘Œwhere ğ‘‹ğ‘„is a textual query (i.e., the prompt). The training objective is defined in the embedding space â„’VL-JEPA = ğ·( Ë†ğ‘†ğ‘Œ, ğ‘†ğ‘Œ) instead of the data space â„’VLM = ğ·( Ë†ğ‘Œ, ğ‘Œ). During inference, a y-decoder reads out the predicted embedding Ë†ğ‘†ğ‘Œto text space Ë†ğ‘Œ when needed. Thanks to its non-generative nature, VL-JEPA is not forced to reconstruct every surface detail of ğ‘Œin the token space. Instead, it only needs to predict the abstract representation ğ‘†ğ‘Œin the embedding space. In the raw one-hot token space, different plausible ğ‘Œoutputs for the same input often appear nearly orthogonal if they donâ€™t share overlapping tokens. However, in the embedding space, these diverse targets can be mapped to nearby points that share similar semantics. This simplifies the target distribution thus makes the learning process more efficient. In addition, unlike VLMs, this approach eliminates the need for learning language generation with a heavy decoder during training, resulting in significant efficiency gains. Thanks to its non-autoregressive nature, VL-JEPA can produce continuous streams of target semantic embeddings within sliding windows with minimal latency as it only require a single forward pass without autoregressive decoding. This is particularly advantageous for real-time online applications such as live action tracking, scene recognition, or planning, where the embedding stream can be selectively decoded by a lightweight y-decoder, enabling efficient and prompt updates. In this work, we empirically validate the advantages of VL-JEPA. We conduct a strictly controlled comparison against classical token-generative VLM [Liu et al., 2023, Cho et al., 2025]: both setups use the same vision encoder, spatial resolution, frame rate, training data, batch size, and number of iterations, etc., with the only difference being the objective in token space or embedding space. Under this matched training condition, VL-JEPA delivers consistently higher performance on zero-shot captioning and classification while using roughly half the trainable parameters, indicating that embedding-space supervision improves learning efficiency. Beyond the training phase, VL-JEPA also delivers substantial inference-time efficiency improvement through selective decoding, where decoding happens only due to significant change in the predicted embedding stream. Empirically, this strategy reduces the number of decoding operations by âˆ¼2.85Ã— while preserving overall output quality measured by average CIDEr scores. Our final VL-JEPA models are trained in two stages: 1) a pretraining stage using caption data to establish robust vision-language alignment, and 2) a supervised finetuning (SFT) stage that equips the model with VQA capabilities. The model resulting from the first stage, denoted as VL-JEPABASE, is evaluated on zero-shot classification and text-to-video retrieval. VL-JEPABASE outperforms CLIP [Radford et al., 2021], SigLIP2 [Tschannen et al., 2025], and Perception Encoder [Bolya et al., 2025] models in terms of average classification accuracy (across 8 datasets) and retrieval recall@1 (across 8 datasets). Following the second stage, the resulting VL-JEPASFT demonstrates significantly improved classification performance due to its exposure to in-domain training data. As a unified generalist model, VL-JEPASFT approaches the performance of specialist models optimized for individual benchmarks. Simultaneously, VL-JEPASFT exhibits effective VQA capabilities, achieving performance on par with established VLM families, such as InstructBLIP [Dai et al., 2023] and Qwen-VL [Bai et al., 2023], across four datasets covering compositional visual reasoning [Hudson and Manning, 2019], complex object counting [Acharya et al., 2019], and object hallucination [Li et al., 2023b, 2025b]. In summary, the contributions of this paper are as follows: â€¢ We introduce VL-JEPA, the first non-generative model that can perform general-domain vision-language tasks in real-time, built on a joint embedding predictive architecture. â€¢ We demonstrate in controlled experiments that VLJEPA, trained with latent space embedding prediction, outperforms VLMs that rely on data space token prediction. â€¢ We show that VL-JEPA delivers significant efficiency gains over VLMs for online video streaming applications, thanks to its non-autoregressive design and native support for selective decoding. â€¢ We highlight that our VL-JEPASFT model, with an unified model architecture, can effectively handle a wide range of classification, retrieval, and VQA tasks at the same time. 2 Methodology We propose VL-JEPA (Fig. 1), a model with the joint embedding predictive architecture (JEPA) for vision-language tasks. VL-JEPA is trained with triplets âŸ¨ğ‘‹ğ‘‰, ğ‘‹ğ‘„, ğ‘ŒâŸ©, where ğ‘‹ğ‘‰denotes the visual input (a single image or a sequence of video frames), ğ‘‹ğ‘„is a textual query (i.e., a question) and ğ‘Œis the textual target (i.e., the answer) to be predicted. The VL-JEPA comprises of four components: 2"}
{"pdf_id": "arxiv_251210942_vl_jepa", "page": 3, "text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language\nSec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion\nFigure 2. Left: VL-JEPA Architecture. It learns to predict the target embedding ğ‘†ğ‘Œ, instead of reconstructing the raw target ğ‘Œin token space as in\nclassical VLMs. Right: VL-JEPA Applications: It handles vision-text-to-text generation tasks (e.g., captioning) with selective decoding mechanism\nnatively supported. Furthermore, VL-JEPAâ€™s embedding space facilitates discriminative VQA, open-vocabulary classification and text-to-video\nretrieval tasks using a single unified model architecture.\n1. X-Encoder (ğ‘‹ğ‘‰â†¦â†’ğ‘†ğ‘‰) compresses high-volume\nvisual inputs to compact visual embeddingsâ€“a se-\nquence of continuous vectors analogous to â€œvisual\ntokensâ€ in classical VLMs.\n2. Predictor (âŸ¨ğ‘†ğ‘‰, ğ‘‹ğ‘„âŸ©â†¦â†’Ë†ğ‘†ğ‘Œ) is the core component\nof VL-JEPA. It maps visual embeddings to a predic-\ntion of target embedding, with a textual query as\nconditioning.\n3. Y-Encoder (ğ‘Œâ†¦â†’ğ‘†ğ‘Œ) embeds the textual target into\na continuous latent space as the prediction target.\nThe target embedding is expected to abstract away\ntask irrelevant information.\n4. Y-Decoder ( Ë†ğ‘†ğ‘Œâ†¦â†’Ë†ğ‘Œ) is not involved during the\nmain training phrase of VL-JEPA. At inference time,\nit translates the predicted embedding as human-\nreadable text when necessary.\nFig. 2 illustrates how we instantiate the VL-JEPA ar-\nchitecture in this paper. For the X-Encoder, we chose\nV-JEPA 2 [Assran et al., 2025], a Vision Transformer that\noutputs a sequence of visual tokens, which are then pro-\njected and fed into the Predictor initialized using Llama\n3 Transformer layers. Query conditioning is achieved by\ntokenizing and embedding the textual query and feeding\nthe resulting textual token embeddings into the Predictor\nalong with the visual embeddings. The outputs of the\nLlama 3 Transformer layers are pooled and projected into\nthe target embedding space produced by the Y-Encoder,\nwhich is initialized by EmbeddingGemma-300M [Vera\net al., 2025]. We provide more technical details in Â§??.\nTraining Objective. JEPA models typically optimize\ntwo objectives jointly: 1) prediction error in the embed-\nding space, and 2) additional regularization that avoids\nrepresentation collapse [Bardes et al., 2021, Balestriero\nand LeCun, 2025]. Any loss that implements these two\nproperties can be applied to VL-JEPA. Alternatively, the\nregularization term can be replaced by other anti-collapse\nstrategies, such as using an exponential moving average\n(EMA) for the Y-Encoder [Assran et al., 2025] or freezing\nthe Y-Encoder [Zhou et al., 2025].\nIn this work, we adopt the InfoNCE loss [Radford\net al., 2021] due to its maturity in the vision-language\ndomain. More advanced non-sample-contrastive regular-\nization, such as VICReg [Bardes et al., 2021] and SIGReg\n[Balestriero and LeCun, 2025] can also be applied but\nwe leave the exploration to future works. InfoNCE loss\ncan be mathematically divided [Wang and Isola, 2020]\ninto: 1) a representation alignment term that minimizes\nthe distance between normalized prediction and target\nembeddings, and 2) a uniformity regularization term that\npushes embeddings in a batch apart from each other, thus\navoiding representation collapse. We train the Predictor\nand the Y-Encoder jointly with bi-directional InfoNCE\nloss, enabling them to mutually learn from each other.\nCompared to the token-space loss used by generative\nVLMs, calculating the training loss in the embedding\nspace is beneficial due to the simplified target distribu-\ntion. Specifically, many real-world prediction tasks are\ninherently ill-posed: for the same input ğ‘‹, there may exist\nmultiple plausible targets ğ‘Œthat are all acceptable. For\nexample, given the query â€œWhat will happen here if I flip\nthis light switch down?â€, both â€œthe lamp is turned offâ€ and\nâ€œroom will go darkâ€ are valid answers. In the raw one-hot\ntoken space, however, the two sequences are orthogonal\nsince they share no overlapping tokens. But when VL-\nJEPAâ€™s Y-Encoder embeds them into nearby points (ideally\nyielding a compact unimodal distribution), the learning\ntask becomes much easier: the model no longer needs to\nfit multiple disjoint high-density regions in sparse token\nspace, but only a single coherent mode in a continuous\nembedding space.\nMulti-tasking. VL-JEPA supports diverse tasks using a\nsingle, unified architecture (Fig. 2). For vision-text-to-text\n3", "clean_text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language Sec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion Figure 2. Left: VL-JEPA Architecture. It learns to predict the target embedding ğ‘†ğ‘Œ, instead of reconstructing the raw target ğ‘Œin token space as in classical VLMs. Right: VL-JEPA Applications: It handles vision-text-to-text generation tasks (e.g., captioning) with selective decoding mechanism natively supported. Furthermore, VL-JEPAâ€™s embedding space facilitates discriminative VQA, open-vocabulary classification and text-to-video retrieval tasks using a single unified model architecture. 1. X-Encoder (ğ‘‹ğ‘‰â†¦â†’ğ‘†ğ‘‰) compresses high-volume visual inputs to compact visual embeddingsâ€“a sequence of continuous vectors analogous to â€œvisual tokensâ€ in classical VLMs. 2. Predictor (âŸ¨ğ‘†ğ‘‰, ğ‘‹ğ‘„âŸ©â†¦â†’Ë†ğ‘†ğ‘Œ) is the core component of VL-JEPA. It maps visual embeddings to a prediction of target embedding, with a textual query as conditioning. 3. Y-Encoder (ğ‘Œâ†¦â†’ğ‘†ğ‘Œ) embeds the textual target into a continuous latent space as the prediction target. The target embedding is expected to abstract away task irrelevant information. 4. Y-Decoder ( Ë†ğ‘†ğ‘Œâ†¦â†’Ë†ğ‘Œ) is not involved during the main training phrase of VL-JEPA. At inference time, it translates the predicted embedding as humanreadable text when necessary. Fig. 2 illustrates how we instantiate the VL-JEPA architecture in this paper. For the X-Encoder, we chose V-JEPA 2 [Assran et al., 2025], a Vision Transformer that outputs a sequence of visual tokens, which are then projected and fed into the Predictor initialized using Llama 3 Transformer layers. Query conditioning is achieved by tokenizing and embedding the textual query and feeding the resulting textual token embeddings into the Predictor along with the visual embeddings. The outputs of the Llama 3 Transformer layers are pooled and projected into the target embedding space produced by the Y-Encoder, which is initialized by EmbeddingGemma-300M [Vera et al., 2025]. We provide more technical details in Â§??. Training Objective. JEPA models typically optimize two objectives jointly: 1) prediction error in the embedding space, and 2) additional regularization that avoids representation collapse [Bardes et al., 2021, Balestriero and LeCun, 2025]. Any loss that implements these two properties can be applied to VL-JEPA. Alternatively, the regularization term can be replaced by other anti-collapse strategies, such as using an exponential moving average (EMA) for the Y-Encoder [Assran et al., 2025] or freezing the Y-Encoder [Zhou et al., 2025]. In this work, we adopt the InfoNCE loss [Radford et al., 2021] due to its maturity in the vision-language domain. More advanced non-sample-contrastive regularization, such as VICReg [Bardes et al., 2021] and SIGReg [Balestriero and LeCun, 2025] can also be applied but we leave the exploration to future works. InfoNCE loss can be mathematically divided [Wang and Isola, 2020] into: 1) a representation alignment term that minimizes the distance between normalized prediction and target embeddings, and 2) a uniformity regularization term that pushes embeddings in a batch apart from each other, thus avoiding representation collapse. We train the Predictor and the Y-Encoder jointly with bi-directional InfoNCE loss, enabling them to mutually learn from each other. Compared to the token-space loss used by generative VLMs, calculating the training loss in the embedding space is beneficial due to the simplified target distribution. Specifically, many real-world prediction tasks are inherently ill-posed: for the same input ğ‘‹, there may exist multiple plausible targets ğ‘Œthat are all acceptable. For example, given the query â€œWhat will happen here if I flip this light switch down?â€, both â€œthe lamp is turned offâ€ and â€œroom will go darkâ€ are valid answers. In the raw one-hot token space, however, the two sequences are orthogonal since they share no overlapping tokens. But when VLJEPAâ€™s Y-Encoder embeds them into nearby points (ideally yielding a compact unimodal distribution), the learning task becomes much easier: the model no longer needs to fit multiple disjoint high-density regions in sparse token space, but only a single coherent mode in a continuous embedding space. Multi-tasking. VL-JEPA supports diverse tasks using a single, unified architecture (Fig. 2). For vision-text-to-text 3"}
{"pdf_id": "arxiv_251210942_vl_jepa", "page": 4, "text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language\nSec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion\ngeneration tasks, such as captioning or open-ended VQA,\nthe query ğ‘‹ğ‘„is a captioning prompt or a question, and\nthe predictor learns to predict the embedding of the target\noutput, Ë†ğ‘†ğ‘Œ, which is then decoded into text. VL-JEPA\nalso supports CLIP-style open-vocabulary classification\nand discriminative VQA, where candidate label texts are\nencoded into embeddings and compared with prediction\nË†ğ‘†ğ‘Œto select the nearest match. For text-to-video retrieval,\ncandidate videos are mapped to their predicted embed-\ndings Ë†ğ‘†ğ‘Œusing a retrieval a captioning prompt, and then\nranked by similarity to the encoded textual retrieval query.\nSelective Decoding. Real-world video applications\noften require online streaming inference, such as tracking\nuser actions in smart glasses for procedural assistance\n[Chen et al., 2024c], monitoring world states for online\nplanning, navigation and robotics [Shukor et al., 2025,\nBlack et al., 2025, Song et al., 2025]. A central challenge\nis balancing two competing needs: the model must con-\ntinuously update semantics as new frames arrive, but\ncomputational efficiency and latency are critical.\nExisting VLMs typically rely on explicit memory mech-\nanisms [Zhou et al., 2024, Qian et al., 2024] to decide when\nto decode or complex KV-cache optimizations [Di et al.,\n2025] for efficiency, since autoregressive language models\nare expensive to run continuously. VL-JEPA, in contrast,\nnatively supports selective decoding. Since it predicts\na semantic answer embedding non-autoregressively, the\nmodel provides a continuous semantic stream of Ë†ğ‘†ğ‘Œthat\ncan be monitored in real time. This stream can be stabilized\nwith simple smoothing (e.g., average pooling) and decoded\nonly when a significant semantic shift is detected, such as\nwhen the local window variance exceeds a threshold. In\nthis way, VL-JEPA maintains always-on semantic monitor-\ning while avoiding unnecessary decoding, achieving both\nresponsiveness and efficiency.\n3\nImplementation of VL-JEPA\n3.1\nModel Architecture\nX-Encoder. Unless otherwise specified, we use a frozen\nV-JEPA 2 ViT-L [Assran et al., 2025] with 304M param-\neters, a self-supervised vision model that excels at both\nimage and video tasks. Each video input is uniformly\nsampled into frames at 2562 resolution. For image inputs,\nthe same image is duplicated to match the input shape.\nPredictor. The predictor is initialized with the last 8\nTransformer layers of Llama-3.2-1B, resulting in 490M\ntrainable parameters. The text tokenizer and token em-\nbedding are also from Llama-3.2-1B. We allow maximum\n512 query tokens, and put [PAD] tokens for short queries.\nWe disable the causal attention mask so that both vision\nand query embeddings can be jointly attended. Linear\nprojections connect the predictor with the vision and text\nembeddings, and average pooling on non-[PAD] tokens is\napplied to obtain the predicted target embedding.\nY-Encoder. We use EmbeddingGemma-300M [Vera et al.,\n2025] as the initialization of the Y-Encoder. We set max-\nimum context length of 512 to handle detailed captions.\nWe found that setting a learning rate multiplier of Ã—0.05 to\nall text encoder parameters improves performance, since\nthe quality of embedding prediction would be suboptimal\nin the beginning of training. Linear projection head is\napplied to both Predictor and Y-Encoder, obtaining a\nshared embedding space with 1,536 dimensions, where\nthe loss is calculated.\n3.2\nTwo-stage Training\nLarge-scale Pretraining. VL-JEPA is trained with two\nstages. The first query-free pretraining stage aims to es-\ntablish robust vision-language alignment using massive\ncaption data. We use PLM-Image-Auto [Cho et al., 2025],\nDatacomp [Gadre et al., 2023] and YFCC-100M [Thomee\net al., 2016] for image-text data. For video-text data, we\ninclude PLM-Video-Auto [Cho et al., 2025], Ego4D atomic\naction descriptions [Grauman et al., 2022], and an inter-\nnal dataset Action100M consisting captions generated on\nHowTo100M videos [Chen et al., 2025b].\nWe first do image-only training on Datacomp and YFCC-\n100M with only 1 frame per visual input, which allows us\nto use a large batch size of 24k. After 100k iterations, the\nmodel has seen 2B samples and achieved 61.6% ImageNet\nzero-shot accuracy (without prompt ensembling). Then,\nwe continue with joint image-video pretraining with 16\nframes per input. The pretraining takes 2 weeks using\n24 nodes with 8Ã—NVIDIA H200 GPUs each. We adopt\na constant learning rate of 5Ã—10âˆ’5 to facilitate extended\ntraining. We call the resulting model VL-JEPABASE and\nmeasure zero-shot classification and retreival performance\nwith this model.\nSupervised Finetuning. The second query-conditioned\nsupervised finetuning (SFT) stage empowers VL-JEPA\nVQA capabilities while maintaining the pretrained vision-\nlanguage alignment for classification and retrieval. The\ntraining data is selected from the PLM data mixture [Cho\net al., 2025], including 25M VQA samples, 2.8M captioning\nsamples, 1.8M classification samples, and downsampled\npretraining stage data to avoid catastrophic forgetting.\nWe train the model for 35k steps with a batch size of\n6k (âˆ¼2 days with 24 nodes), with cosine learning rate an-\nnealing applied to improve convergence. Since excessive\nhuman labelled data is included in this SFT data mixture,\nwe no longer emphasize zero-shot evaluation for the result-\ning VL-JEPASFT from this stage. Instead, we evaluate VQA\ncapabilities and compare it with state-of-the-art specialist\nmodels.\n4", "clean_text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language Sec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion generation tasks, such as captioning or open-ended VQA, the query ğ‘‹ğ‘„is a captioning prompt or a question, and the predictor learns to predict the embedding of the target output, Ë†ğ‘†ğ‘Œ, which is then decoded into text. VL-JEPA also supports CLIP-style open-vocabulary classification and discriminative VQA, where candidate label texts are encoded into embeddings and compared with prediction Ë†ğ‘†ğ‘Œto select the nearest match. For text-to-video retrieval, candidate videos are mapped to their predicted embeddings Ë†ğ‘†ğ‘Œusing a retrieval a captioning prompt, and then ranked by similarity to the encoded textual retrieval query. Selective Decoding. Real-world video applications often require online streaming inference, such as tracking user actions in smart glasses for procedural assistance [Chen et al., 2024c], monitoring world states for online planning, navigation and robotics [Shukor et al., 2025, Black et al., 2025, Song et al., 2025]. A central challenge is balancing two competing needs: the model must continuously update semantics as new frames arrive, but computational efficiency and latency are critical. Existing VLMs typically rely on explicit memory mechanisms [Zhou et al., 2024, Qian et al., 2024] to decide when to decode or complex KV-cache optimizations [Di et al., 2025] for efficiency, since autoregressive language models are expensive to run continuously. VL-JEPA, in contrast, natively supports selective decoding. Since it predicts a semantic answer embedding non-autoregressively, the model provides a continuous semantic stream of Ë†ğ‘†ğ‘Œthat can be monitored in real time. This stream can be stabilized with simple smoothing (e.g., average pooling) and decoded only when a significant semantic shift is detected, such as when the local window variance exceeds a threshold. In this way, VL-JEPA maintains always-on semantic monitoring while avoiding unnecessary decoding, achieving both responsiveness and efficiency. 3 Implementation of VL-JEPA 3.1 Model Architecture X-Encoder. Unless otherwise specified, we use a frozen V-JEPA 2 ViT-L [Assran et al., 2025] with 304M parameters, a self-supervised vision model that excels at both image and video tasks. Each video input is uniformly sampled into frames at 2562 resolution. For image inputs, the same image is duplicated to match the input shape. Predictor. The predictor is initialized with the last 8 Transformer layers of Llama-3.2-1B, resulting in 490M trainable parameters. The text tokenizer and token embedding are also from Llama-3.2-1B. We allow maximum 512 query tokens, and put [PAD] tokens for short queries. We disable the causal attention mask so that both vision and query embeddings can be jointly attended. Linear projections connect the predictor with the vision and text embeddings, and average pooling on non-[PAD] tokens is applied to obtain the predicted target embedding. Y-Encoder. We use EmbeddingGemma-300M [Vera et al., 2025] as the initialization of the Y-Encoder. We set maximum context length of 512 to handle detailed captions. We found that setting a learning rate multiplier of Ã—0.05 to all text encoder parameters improves performance, since the quality of embedding prediction would be suboptimal in the beginning of training. Linear projection head is applied to both Predictor and Y-Encoder, obtaining a shared embedding space with 1,536 dimensions, where the loss is calculated. 3.2 Two-stage Training Large-scale Pretraining. VL-JEPA is trained with two stages. The first query-free pretraining stage aims to establish robust vision-language alignment using massive caption data. We use PLM-Image-Auto [Cho et al., 2025], Datacomp [Gadre et al., 2023] and YFCC-100M [Thomee et al., 2016] for image-text data. For video-text data, we include PLM-Video-Auto [Cho et al., 2025], Ego4D atomic action descriptions [Grauman et al., 2022], and an internal dataset Action100M consisting captions generated on HowTo100M videos [Chen et al., 2025b]. We first do image-only training on Datacomp and YFCC100M with only 1 frame per visual input, which allows us to use a large batch size of 24k. After 100k iterations, the model has seen 2B samples and achieved 61.6% ImageNet zero-shot accuracy (without prompt ensembling). Then, we continue with joint image-video pretraining with 16 frames per input. The pretraining takes 2 weeks using 24 nodes with 8Ã—NVIDIA H200 GPUs each. We adopt a constant learning rate of 5Ã—10âˆ’5 to facilitate extended training. We call the resulting model VL-JEPABASE and measure zero-shot classification and retreival performance with this model. Supervised Finetuning. The second query-conditioned supervised finetuning (SFT) stage empowers VL-JEPA VQA capabilities while maintaining the pretrained visionlanguage alignment for classification and retrieval. The training data is selected from the PLM data mixture [Cho et al., 2025], including 25M VQA samples, 2.8M captioning samples, 1.8M classification samples, and downsampled pretraining stage data to avoid catastrophic forgetting. We train the model for 35k steps with a batch size of 6k (âˆ¼2 days with 24 nodes), with cosine learning rate annealing applied to improve convergence. Since excessive human labelled data is included in this SFT data mixture, we no longer emphasize zero-shot evaluation for the resulting VL-JEPASFT from this stage. Instead, we evaluate VQA capabilities and compare it with state-of-the-art specialist models. 4"}
{"pdf_id": "arxiv_251210942_vl_jepa", "page": 5, "text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language\nSec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion\nTable 1. Video classification and text-to-video retrieval. Best zero-shot performance in each dataset are highlighted. Samples seen = training step Ã—\neffective batch size.\nVideo Classification (Top-1 Accuracy)\nText-to-video Retrieval (Recall@1)\nModel\n# Parameters\n# Samples Seen\nZero-shot\nGeneralist Model\nAverage\nSSv2\nEK100\nEgoExo4D\nKinetics-400\nCOIN (SR)\nCOIN (TR)\nCrossTask (SR)\nCrossTask (TR)\nAverage\nMSR-VTT\nActivityNet\nDiDeMo\nMSVD\nYouCook2\nPVD-Bench\nDream-1k\nVDC-1k\nRN50\n75M\n12.8B\n21.8\n2.1\n1.5\n1.9\n41.4\n8.6\n39.0\n10.9\n68.7\n28.3\n28.7\n17.7\n24.7\n29.7\n5.1\n27.6\n47.2\n46.0\nViT-B\n124M\n12.8B\n25.3\n3.1\n1.3\n2.4\n49.5\n11.2\n47.3\n16.2\n71.5\n29.3\n31.0\n19.5\n25.7\n34.0\n6.1\n27.0\n48.5\n42.9\nCLIP\nViT-L\n389M\n12.8B\nâœ“\nâœ“\n30.9\n3.8\n3.7\n3.6\n58.3\n14.7\n63.5\n20.8\n78.5\n35.3\n35.9\n23.4\n30.7\n41.9\n7.9\n36.7\n56.8\n49.3\nViT-B\n375M\n40B\n33.9\n5.2\n2.3\n4.9\n57.8\n20.6\n69.9\n27.7\n82.9\n39.6\n40.2\n25.0\n32.1\n48.6\n13.8\n52.1\n60.9\n43.7\nViT-L\n882M\n40B\n38.7\n5.9\n4.5\n7.0\n63.6\n24.2\n78.5\n35.1\n90.8\n45.4\n41.6\n32.7\n35.1\n53.5\n19.0\n59.2\n71.6\n50.9\nSigLIP2\nViT-g\n1.9B\n40B\nâœ“\nâœ“\n39.9\n6.1\n6.1\n6.4\n68.0\n26.0\n80.4\n35.1\n90.8\n47.5\n43.4\n33.9\n38.9\n56.0\n22.2\n60.4\n73.0\n52.5\nViT-B\n448M\n58B\n37.3\n5.8\n3.3\n6.3\n65.4\n21.5\n77.1\n26.9\n91.8\n44.9\n46.5\n35.4\n35.3\n49.1\n15.2\n59.8\n68.7\n49.2\nViT-L\n671M\n58B\n42.8\n9.3\n6.0\n10.9\n73.4\n27.1\n83.3\n37.5\n95.3\n50.2\n48.9\n41.7\n40.8\n56.2\n22.5\n64.7\n75.9\n51.0\nPE-Core\nViT-G\n2.3B\n86B\nâœ“\nâœ“\n44.6\n9.0\n6.4\n13.0\n76.4\n29.0\n86.0\n40.3\n97.2\n58.1\n51.6\n49.1\n44.5\n58.7\n26.0\n77.0\n89.2\n68.5\nVL-JEPABASE\nViT-L\n1.6B\n2.0B\nâœ“\nâœ“\n46.4\n16.1\n13.3\n21.1\n57.8\n39.8\n74.4\n60.5\n88.0\n58.4\n37.6\n55.4\n49.2\n47.9\n23.1\n78.2\n88.8\n87.2\nVL-JEPASFT\nViT-L\n1.6B\n2.5B\nâœ—\nâœ“\n70.7\n68.2\n38.8\n59.5\n81.4\n60.3\n86.8\n77.1\n93.0\n59.5\n43.7\n53.8\n46.2\n49.1\n28.8\n81.1\n86.4\n86.7\nSoTA (including specialist models)\nâœ—\nâœ—\n-\n77.5\n56.4\n47.8\n92.1\n67.3\n95.3\n64.5\n96.0\n-\n62.8\n74.1\n74.2\n61.4\n28.9\n77.0\n89.2\n68.5\n4\nExperiments\n4.1\nClassification and Retrieval\nWe begin by evaluating VL-JEPAâ€™s classification and re-\ntrieval performance in Â§4.1, and benchmark VL-JEPA on\nVQA datasets in Â§4.2. We demonstrate application of VL-\nJEPA for understanding the relationship between world\nstate changes and action concepts (i.e., inverse dynamics) in\nÂ§4.3. In Â§4.4, we demonstrate the advantage of embedding\nprediction by comparing it with a token-predictive VLM\nbaseline under a strictly controlled setting. In Â§4.5, we\nevaluate the effectiveness of VL-JEPAâ€™s selective decoding,\nand show that it reduces decoding cost while maintaining\nthe performance. Next, we analyze VL-JEPAâ€™s Y-Encoder\nin Â§4.6.\nEvaluation Setup. We evaluate VL-JEPA following the\nCLIP-style evaluation protocol (see Fig.2 and Â§2 â€œMulti-\ntaskingâ€). We assess VL-JEPA on a broad suite of bench-\nmarks, including 8 classification datasets and 8 retrieval\ndatasets. For zero-shot evaluation, we compare against\ngeneralist foundation models CLIP [Radford et al., 2021],\nSigLIP2 [Tschannen et al., 2025], and Perception Encoder\n(PE-Core)[Bolya et al., 2025]. We additionally report refer-\nence numbers from specialist models that are individually op-\ntimized for each benchmark (summarized in Appendix??).\nResults. Table 1 summarizes the results. In the strict\nzero-shot setting, VL-JEPABASE achieves higher average ac-\ncuracy (46.4 vs 44.6) across the 8 classification datasets and\nhigher average recall@1 (58.4 vs 58.1) across the 8 retrieval\ndatasets than the best baseline PE-Core-G. Per-dataset\nscores show that VL-JEPABASE is particularly strong on\nmotion-centric benchmarks (SSv2, EK-100, EgoExo4D, and\nstep recognition on COIN and CrossTask), while relatively\nweaker on appearance-centric benchmarks (Kinetics-400 and\ntask recognition on COIN and CrossTask). This is due to\nVL-JEPABASE has seen substantially fewer vision-language\npairs (only 2B in comparison with PE-Core-Gâ€™s 86B). After\nsupervised finetuning, VL-JEPASFT improves significantly\nupon VL-JEPABASE since the model has seen in-domain\ntraining data. As a single generalist model, the performance\nof VL-JEPASFT is approaching specialist models optimized\nindividually for each dataset.\n4.2\nVisual Question Answering\nEvaluation Setup. We evaluate VL-JEPASFT on discrimi-\nnative VQA tasks. The inference process involves encode\ncandidate answers using the Y-Encoder and selecting the\nanswer that minimizes the distance to the predicted em-\nbedding (see Fig. 2).\nWe select four benchmarks that\nprioritize visual perception rather than knowledge and\nreasoning. We evaluate on GQA [Hudson and Manning,\n2019], a dataset for real-world visual reasoning and com-\npositional QA, reporting accuracy on the testdev-balanced\nsplit. For TallyQA [Acharya et al., 2019], which targets\ncomplex counting, we follow Chen et al. [2022] and report\nthe weighted average accuracy across the â€œsimpleâ€ and\nâ€œcomplexâ€ splits. Finally, to assess object hallucination,\nwe utilize POPE [Li et al., 2023b] and POPEv2 [Li et al.,\n2025b]. For POPE, we report the average accuracy across\nthe â€œrandomâ€, â€œpopularâ€, and â€œadversarialâ€ settings on\nMS-COCO.\nResults. Table 4.2 compares VL-JEPASFT against estab-\nlished VLM families, including BLIP-2 [Li et al., 2023a],\nInstructBLIP [Dai et al., 2023], Qwen-VL [Bai et al., 2023],\nInternVL [Chen et al., 2024d], Llava-1.5 [Vallaeys et al.,\n2024], SmolVLM [Marafioti et al., 2025], PaLI [Chen et al.,\n2022], PaliGemma [Beyer et al., 2024], and Video-LLaVA\n[Lin et al., 2024]. VL-JEPASFT outperforms many of these\n5", "clean_text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language Sec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion Table 1. Video classification and text-to-video retrieval. Best zero-shot performance in each dataset are highlighted. Samples seen = training step Ã— effective batch size. Video Classification (Top-1 Accuracy) Text-to-video Retrieval (Recall@1) Model # Parameters # Samples Seen Zero-shot Generalist Model Average SSv2 EK100 EgoExo4D Kinetics-400 COIN (SR) COIN (TR) CrossTask (SR) CrossTask (TR) Average MSR-VTT ActivityNet DiDeMo MSVD YouCook2 PVD-Bench Dream-1k VDC-1k RN50 75M 12.8B 21.8 2.1 1.5 1.9 41.4 8.6 39.0 10.9 68.7 28.3 28.7 17.7 24.7 29.7 5.1 27.6 47.2 46.0 ViT-B 124M 12.8B 25.3 3.1 1.3 2.4 49.5 11.2 47.3 16.2 71.5 29.3 31.0 19.5 25.7 34.0 6.1 27.0 48.5 42.9 CLIP ViT-L 389M 12.8B âœ“ âœ“ 30.9 3.8 3.7 3.6 58.3 14.7 63.5 20.8 78.5 35.3 35.9 23.4 30.7 41.9 7.9 36.7 56.8 49.3 ViT-B 375M 40B 33.9 5.2 2.3 4.9 57.8 20.6 69.9 27.7 82.9 39.6 40.2 25.0 32.1 48.6 13.8 52.1 60.9 43.7 ViT-L 882M 40B 38.7 5.9 4.5 7.0 63.6 24.2 78.5 35.1 90.8 45.4 41.6 32.7 35.1 53.5 19.0 59.2 71.6 50.9 SigLIP2 ViT-g 1.9B 40B âœ“ âœ“ 39.9 6.1 6.1 6.4 68.0 26.0 80.4 35.1 90.8 47.5 43.4 33.9 38.9 56.0 22.2 60.4 73.0 52.5 ViT-B 448M 58B 37.3 5.8 3.3 6.3 65.4 21.5 77.1 26.9 91.8 44.9 46.5 35.4 35.3 49.1 15.2 59.8 68.7 49.2 ViT-L 671M 58B 42.8 9.3 6.0 10.9 73.4 27.1 83.3 37.5 95.3 50.2 48.9 41.7 40.8 56.2 22.5 64.7 75.9 51.0 PE-Core ViT-G 2.3B 86B âœ“ âœ“ 44.6 9.0 6.4 13.0 76.4 29.0 86.0 40.3 97.2 58.1 51.6 49.1 44.5 58.7 26.0 77.0 89.2 68.5 VL-JEPABASE ViT-L 1.6B 2.0B âœ“ âœ“ 46.4 16.1 13.3 21.1 57.8 39.8 74.4 60.5 88.0 58.4 37.6 55.4 49.2 47.9 23.1 78.2 88.8 87.2 VL-JEPASFT ViT-L 1.6B 2.5B âœ— âœ“ 70.7 68.2 38.8 59.5 81.4 60.3 86.8 77.1 93.0 59.5 43.7 53.8 46.2 49.1 28.8 81.1 86.4 86.7 SoTA (including specialist models) âœ— âœ— 77.5 56.4 47.8 92.1 67.3 95.3 64.5 96.0 62.8 74.1 74.2 61.4 28.9 77.0 89.2 68.5 4 Experiments 4.1 Classification and Retrieval We begin by evaluating VL-JEPAâ€™s classification and retrieval performance in Â§4.1, and benchmark VL-JEPA on VQA datasets in Â§4.2. We demonstrate application of VLJEPA for understanding the relationship between world state changes and action concepts (i.e., inverse dynamics) in Â§4.3. In Â§4.4, we demonstrate the advantage of embedding prediction by comparing it with a token-predictive VLM baseline under a strictly controlled setting. In Â§4.5, we evaluate the effectiveness of VL-JEPAâ€™s selective decoding, and show that it reduces decoding cost while maintaining the performance. Next, we analyze VL-JEPAâ€™s Y-Encoder in Â§4.6. Evaluation Setup. We evaluate VL-JEPA following the CLIP-style evaluation protocol (see Fig.2 and Â§2 â€œMultitaskingâ€). We assess VL-JEPA on a broad suite of benchmarks, including 8 classification datasets and 8 retrieval datasets. For zero-shot evaluation, we compare against generalist foundation models CLIP [Radford et al., 2021], SigLIP2 [Tschannen et al., 2025], and Perception Encoder (PE-Core)[Bolya et al., 2025]. We additionally report reference numbers from specialist models that are individually optimized for each benchmark (summarized in Appendix??). Results. Table 1 summarizes the results. In the strict zero-shot setting, VL-JEPABASE achieves higher average accuracy (46.4 vs 44.6) across the 8 classification datasets and higher average recall@1 (58.4 vs 58.1) across the 8 retrieval datasets than the best baseline PE-Core-G. Per-dataset scores show that VL-JEPABASE is particularly strong on motion-centric benchmarks (SSv2, EK-100, EgoExo4D, and step recognition on COIN and CrossTask), while relatively weaker on appearance-centric benchmarks (Kinetics-400 and task recognition on COIN and CrossTask). This is due to VL-JEPABASE has seen substantially fewer vision-language pairs (only 2B in comparison with PE-Core-Gâ€™s 86B). After supervised finetuning, VL-JEPASFT improves significantly upon VL-JEPABASE since the model has seen in-domain training data. As a single generalist model, the performance of VL-JEPASFT is approaching specialist models optimized individually for each dataset. 4.2 Visual Question Answering Evaluation Setup. We evaluate VL-JEPASFT on discriminative VQA tasks. The inference process involves encode candidate answers using the Y-Encoder and selecting the answer that minimizes the distance to the predicted embedding (see Fig. 2). We select four benchmarks that prioritize visual perception rather than knowledge and reasoning. We evaluate on GQA [Hudson and Manning, 2019], a dataset for real-world visual reasoning and compositional QA, reporting accuracy on the testdev-balanced split. For TallyQA [Acharya et al., 2019], which targets complex counting, we follow Chen et al. [2022] and report the weighted average accuracy across the â€œsimpleâ€ and â€œcomplexâ€ splits. Finally, to assess object hallucination, we utilize POPE [Li et al., 2023b] and POPEv2 [Li et al., 2025b]. For POPE, we report the average accuracy across the â€œrandomâ€, â€œpopularâ€, and â€œadversarialâ€ settings on MS-COCO. Results. Table 4.2 compares VL-JEPASFT against established VLM families, including BLIP-2 [Li et al., 2023a], InstructBLIP [Dai et al., 2023], Qwen-VL [Bai et al., 2023], InternVL [Chen et al., 2024d], Llava-1.5 [Vallaeys et al., 2024], SmolVLM [Marafioti et al., 2025], PaLI [Chen et al., 2022], PaliGemma [Beyer et al., 2024], and Video-LLaVA [Lin et al., 2024]. VL-JEPASFT outperforms many of these 5"}
{"pdf_id": "arxiv_251210942_vl_jepa", "page": 6, "text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language\nSec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion\nTable 2. VQA benchmarks. We report accuracy on GQA [Hudson and Manning, 2019], TallyQA [Acharya et al., 2019], POPE [Li et al., 2023b], and\nPOPEv2 [Li et al., 2025b]. Scores lower than our model are marked in red. Scores from SmolVLM are obtained by our evaluation, while other baselines\nare reported in the literature.\nGQA: compositional visual reasoning\nTallyQA: complex object counting\nPOPE: object hallucination\nPOPEv2: object hallucination\nModel\nAccuracy\nModel\nAccuracy\nModel\nAccuracy\nModel\nAccuracy\nBLIP-2 (OPT-2.7B)\n33.9\nSmolVLM-256M\n32.3\nSmolVLM2-256M\n56.4\nSmolVLM-256M\n62.3\nBLIP-2 (FlanT5XXL)\n41.0\nSmolVLM-500M\n44.8\nSmolVLM-256M\n57.9\nLLaVA-1.5-13B\n72.7\nInstructBLIP (FlanT5XL)\n48.4\nPaLI-700M\n62.3\nLLaVA-7B\n72.9\nInternVL2-8B\n74.5\nInstructBLIP (Vicuna-13B)\n49.5\nSmolVLM-2B\n64.7\nInstructBLIP (Vicuna-13B)\n79.0\nInternVL2-26B\n76.1\nQwen-VL-Chat-7B\n57.5\nPaLI-3B\n65.8\nVideo-LLaVA (7B)\n83.4\nQwen2-VL-72B\n79.4\nQwen-VL-7B\n59.3\nInstructBLIP (Vicuna-13B)\n68.0\nSmolVLM-500M\n85.8\nSmolVLM-500M\n83.8\nInternVL-Chat (Vicuna-7B)\n59.5\nPaLI-17B\n71.9\nLLaVA-1.5-7B\n85.9\nQwen2-VL-7B\n87.0\nLLaVA-1.5 (Vicuna-7B)\n62.0\nLLaVA-1.5 (Vicuna-13B)\n72.3\nLLaVA-1.5-13B-HD\n86.3\nSmolVLM-2B\n88.8\nInternVL-Chat (Vicuna-13B)\n66.6\nPaliGemma (3B)\n76.8\nSmolVLM-2B\n87.5\nQwen2-VL-2B\n91.3\nVL-JEPASFT (1.6B)\n60.8\nVL-JEPASFT (1.6B)\n67.4\nVL-JEPASFT (1.6B)\n84.2\nVL-JEPASFT (1.6B)\n82.2\nTable 3. WorldPrediction-WM benchmark results. We compare the accuracy between large VLMs, socratic LLMs, and VL-JEPA. VL-JEPASFT\nachieves a new SoTA at 65.7%.\nVision Language Models\nSocratic LLMs (w/ Qwen2.5-VL-72B captions)\nVL-JEPA\nInternVL2.5\nQwen2.5-VL\nLlama-3.1\nLlama-4\nQwen2.5\nGPT-4o\nClaude-3.5\nGemini-2\nBASE\nSFT\n2B\n4B\n26B\n38B\n3B\n7B\n32B\n72B\n8B\n70B\n109B\n400B\n3B\n7B\n72B\nN/A\nN/A\nN/A\n1.6B\n1.6B\n20.0\n29.8\n30.2\n50.3\n21.6\n45.5\n49.0\n57.0\n48.7\n49.8\n52.7\n53.6\n44.0\n49.1\n48.5\n52.0\n53.3\n55.6\n63.9\n65.7\nbaselines despite requiring significantly less computational\nresourcesâ€“classical VLMs rely on extensively pretrained\nCLIP backbones combined with multi-stage visual instruc-\ntion tuning. In comparison, VL-JEPASFT employs a unified\narchitecture and a single embedding space to seamlessly han-\ndle VQA, classification, and retrieval (Tab. 1).\n4.3\nWorldPrediction-WM\nEvaluation Setup. We evaluate VL-JEPA on the â€œworld\nmodelingâ€ task in the WorldPrediction [Chen et al., 2025a]\nbenchmark, where the model is provided with two images\nrepresenting the initial and final world states and must\nidentify, among four candidate video clips, the action\nthat explains the observed transition. To adapt VL-JEPA,\nwe duplicate and concatenate the initial and final state\nimages to extract a state embedding, and encode each action\ncandidate into action embeddings. The model then selects\nthe candidate whose embedding is closest to the state\nembedding.\nResults. Table 3 shows accuracy comparisons. VL-\nJEPABASE attains 63.9% and VL-JEPASFT attains 65.7% top-1\naccuracy on WorldPrediction-WM, establishing a new\nstate of the art. Our VL-JEPA model not only substantially\nsurpasses existing VLMs of comparable or larger scale but\nalso exceeds the performance of frontier LLMs such as\nGPT-4o, Claude-3.5-sonnet, and Gemini-2.0.\n4.4\nEmbedding Prediction vs. Token\nPrediction: A Controlled Comparison\nEvaluation Setup. In this section, we compare VL-JEPA to\na token-generative VLM baseline under a strictly aligned\ntraining conditions. Both models use the same Perception\nEncoder [Bolya et al., 2025] (frozen ViT-L-14 with 3362\nresolution, no tiling, 16 frames per video) for vision inputs.\nWe use the same training iterations with the same effective\nbatch size of 128, same learning rate scheduler on the same\npretraining data mixture described above (Â§3). The only\ndifference is the prediction task: VL-JEPA predicts target\nembeddings [Duquenne et al., 2023] using a 0.5B predictor,\nwhereas the VLM baseline performs next-token prediction\nwith cross-entropy using a 1B LLM. For VLM, we use the\nstandard training recipe and codebase of PerceptionLM\n[Cho et al., 2025], aligning frozen vision encoder and text-\nonly LLM Llama-3.2-1B. For VL-JEPA, we initialize the\npredictor from the 8-16 layers of Llama-3.2-1B.\nWe evaluate both models at regular checkpoints through-\nout training spanning from 500K to 15M samples seen. At\neach checkpoint, we measure the performance on video\ncaptioning and video classification. For video captioning,\nwe report CIDEr scores averaged across YouCook2 [Zhou\net al., 2018], MSR-VTT [Xu et al., 2016] and PVD-Bench\n[Bolya et al., 2025]. VL-JEPA decodes the predicted embed-\ndings while VLM generates the tokens directly. For video\nclassification, we report top-5 accuracy averaged across\nCrossTask-Step, CrossTask-Task [Zhukov et al., 2019] and\nEgoExo4D [Grauman et al., 2024]. For VL-JEPA we choose\nthe candidate with lowest cosine distance to the predicted\nembedding, while for VLM we pick the class with lowest\nperplexity.\nResults. As shown in Fig. 3, both models yield compa-\nrable performance after 500K samples seen in both tasks,\nwith respectively 1.23 and 1.35 CIDEr in video captioning\nand 14.9% and 14.0% top-5 accuracy for VL-JEPA and\n6", "clean_text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language Sec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion Table 2. VQA benchmarks. We report accuracy on GQA [Hudson and Manning, 2019], TallyQA [Acharya et al., 2019], POPE [Li et al., 2023b], and POPEv2 [Li et al., 2025b]. Scores lower than our model are marked in red. Scores from SmolVLM are obtained by our evaluation, while other baselines are reported in the literature. GQA: compositional visual reasoning TallyQA: complex object counting POPE: object hallucination POPEv2: object hallucination Model Accuracy Model Accuracy Model Accuracy Model Accuracy BLIP-2 (OPT-2.7B) 33.9 SmolVLM-256M 32.3 SmolVLM2-256M 56.4 SmolVLM-256M 62.3 BLIP-2 (FlanT5XXL) 41.0 SmolVLM-500M 44.8 SmolVLM-256M 57.9 LLaVA-1.5-13B 72.7 InstructBLIP (FlanT5XL) 48.4 PaLI-700M 62.3 LLaVA-7B 72.9 InternVL2-8B 74.5 InstructBLIP (Vicuna-13B) 49.5 SmolVLM-2B 64.7 InstructBLIP (Vicuna-13B) 79.0 InternVL2-26B 76.1 Qwen-VL-Chat-7B 57.5 PaLI-3B 65.8 Video-LLaVA (7B) 83.4 Qwen2-VL-72B 79.4 Qwen-VL-7B 59.3 InstructBLIP (Vicuna-13B) 68.0 SmolVLM-500M 85.8 SmolVLM-500M 83.8 InternVL-Chat (Vicuna-7B) 59.5 PaLI-17B 71.9 LLaVA-1.5-7B 85.9 Qwen2-VL-7B 87.0 LLaVA-1.5 (Vicuna-7B) 62.0 LLaVA-1.5 (Vicuna-13B) 72.3 LLaVA-1.5-13B-HD 86.3 SmolVLM-2B 88.8 InternVL-Chat (Vicuna-13B) 66.6 PaliGemma (3B) 76.8 SmolVLM-2B 87.5 Qwen2-VL-2B 91.3 VL-JEPASFT (1.6B) 60.8 VL-JEPASFT (1.6B) 67.4 VL-JEPASFT (1.6B) 84.2 VL-JEPASFT (1.6B) 82.2 Table 3. WorldPrediction-WM benchmark results. We compare the accuracy between large VLMs, socratic LLMs, and VL-JEPA. VL-JEPASFT achieves a new SoTA at 65.7%. Vision Language Models Socratic LLMs (w/ Qwen2.5-VL-72B captions) VL-JEPA InternVL2.5 Qwen2.5-VL Llama-3.1 Llama-4 Qwen2.5 GPT-4o Claude-3.5 Gemini-2 BASE SFT 2B 4B 26B 38B 3B 7B 32B 72B 8B 70B 109B 400B 3B 7B 72B N/A N/A N/A 1.6B 1.6B 20.0 29.8 30.2 50.3 21.6 45.5 49.0 57.0 48.7 49.8 52.7 53.6 44.0 49.1 48.5 52.0 53.3 55.6 63.9 65.7 baselines despite requiring significantly less computational resourcesâ€“classical VLMs rely on extensively pretrained CLIP backbones combined with multi-stage visual instruction tuning. In comparison, VL-JEPASFT employs a unified architecture and a single embedding space to seamlessly handle VQA, classification, and retrieval (Tab. 1). 4.3 WorldPrediction-WM Evaluation Setup. We evaluate VL-JEPA on the â€œworld modelingâ€ task in the WorldPrediction [Chen et al., 2025a] benchmark, where the model is provided with two images representing the initial and final world states and must identify, among four candidate video clips, the action that explains the observed transition. To adapt VL-JEPA, we duplicate and concatenate the initial and final state images to extract a state embedding, and encode each action candidate into action embeddings. The model then selects the candidate whose embedding is closest to the state embedding. Results. Table 3 shows accuracy comparisons. VLJEPABASE attains 63.9% and VL-JEPASFT attains 65.7% top-1 accuracy on WorldPrediction-WM, establishing a new state of the art. Our VL-JEPA model not only substantially surpasses existing VLMs of comparable or larger scale but also exceeds the performance of frontier LLMs such as GPT-4o, Claude-3.5-sonnet, and Gemini-2.0. 4.4 Embedding Prediction vs. Token Prediction: A Controlled Comparison Evaluation Setup. In this section, we compare VL-JEPA to a token-generative VLM baseline under a strictly aligned training conditions. Both models use the same Perception Encoder [Bolya et al., 2025] (frozen ViT-L-14 with 3362 resolution, no tiling, 16 frames per video) for vision inputs. We use the same training iterations with the same effective batch size of 128, same learning rate scheduler on the same pretraining data mixture described above (Â§3). The only difference is the prediction task: VL-JEPA predicts target embeddings [Duquenne et al., 2023] using a 0.5B predictor, whereas the VLM baseline performs next-token prediction with cross-entropy using a 1B LLM. For VLM, we use the standard training recipe and codebase of PerceptionLM [Cho et al., 2025], aligning frozen vision encoder and textonly LLM Llama-3.2-1B. For VL-JEPA, we initialize the predictor from the 8-16 layers of Llama-3.2-1B. We evaluate both models at regular checkpoints throughout training spanning from 500K to 15M samples seen. At each checkpoint, we measure the performance on video captioning and video classification. For video captioning, we report CIDEr scores averaged across YouCook2 [Zhou et al., 2018], MSR-VTT [Xu et al., 2016] and PVD-Bench [Bolya et al., 2025]. VL-JEPA decodes the predicted embeddings while VLM generates the tokens directly. For video classification, we report top-5 accuracy averaged across CrossTask-Step, CrossTask-Task [Zhukov et al., 2019] and EgoExo4D [Grauman et al., 2024]. For VL-JEPA we choose the candidate with lowest cosine distance to the predicted embedding, while for VLM we pick the class with lowest perplexity. Results. As shown in Fig. 3, both models yield comparable performance after 500K samples seen in both tasks, with respectively 1.23 and 1.35 CIDEr in video captioning and 14.9% and 14.0% top-5 accuracy for VL-JEPA and 6"}
{"pdf_id": "arxiv_251210942_vl_jepa", "page": 7, "text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language\nSec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion\nFigure 3. Comparison of embedding prediction (VL-JEPA) and token prediction (VLM). We conduct a fair comparison of under strictly aligned\ntraining settings (encoder, data, batchsize, etc.). Left: Zero-shot video captioning CIDEr score averaged over 3 datasets and zero-shot classification\naccuracy (top-5) averaged over 3 benchmarks. Right: Comparing the trainable parameters and average inference time cost.\nFigure 4. Evaluation of selective decoding. Left: We compare uniform sampling of decoding points at fixed intervals (red) and embedding-guided\nselective decoding (blue). Performance is measured by the average CIDEr score between each annotation ğ‘¦and its closest decoded output Ë†ğ‘¦. Right:\nResults on EgoExo4D show that selective decoding achieves a Pareto improvement over uniform sampling: for the same performance level, it requires\nfewer decoding operations.\nVLM. After a few iterations, we show that VL-JEPAâ€™s per-\nformance increase is much sharper compared to VLM,\nreaching 14.7 CIDEr and 35.3% top-5 accuracy after 5M\nsamples seen. This gap remains constant as training scales\nat 15M samples with 14.8 CIDEr and 41.0% top-5 accuracy\nfor VL-JEPA, while the VLM baseline yield respectively\n7.1 CIDEr and 27.2% top-5 accuracy. This controlled com-\nparison highlights the benefit of predicting embeddings\nrather than tokens, showing both higher sample efficiency\nand stronger absolute performance.\nWe compare the inference cost of the above VL-JEPA and\nthe VLM by pre-loading 64 video frames into memory and\nrepeatedly decoding text 100 times with the same prompt,\nmeasuring the average time per sample. As shown in\nFig. 3 (right most), both models exhibit comparable latency\nwhen generating text.\nWhat differentiates our model\nfrom classical VLM is the decoupling between the prompt\nprocessing (â€œQuery Embeddingâ€) and the video encoder\n(â€œEncoder + Predictorâ€) from the text generation module\n(â€œDecoderâ€). This allows us to only use the first part of\nthe model to perform retrieval and decode text only when\nneeded (see Section 4.5 below), making our model more\nscalable for online video inference.\n4.5\nEffectiveness of Selective Decoding\nEvaluation Setup. We evaluate the effectiveness of VL-\nJEPAâ€™s embedding-guided selective decoding on long-form\nvideo streams. To this end, we design a benchmark task\nwhere the goal is to recover a temporal sequence of an-\nnotations while minimizing the number of text decoding\noperations, which dominate inference cost. As shown in\nFig. 4 (left), decoding is performed only at selected points\nalong the VL-JEPA embedding stream, yielding a sequence\nof ğ‘decoded outputs [(Ë†ğ‘¡1, Ë†ğ‘¦1), (Ë†ğ‘¡2, Ë†ğ‘¦2), . . . , (Ë†ğ‘¡ğ‘, Ë†ğ‘¦ğ‘)]. Each\nground-truth annotation [(ğ‘¡1, ğ‘¦1), (ğ‘¡2, ğ‘¦2), . . . , (ğ‘¡ğ‘‡, ğ‘¦ğ‘‡)] is\nthen aligned to its nearest decoded output in time (illus-\ntrated as â—¦Â· Â· Â· â—¦in Fig. 4), and CIDEr is computed between\nmatched pairs. We use the EgoExo4D [Grauman et al.,\n2024] validation set in procedural activity domains, which\nconsists of 218 videos with an average duration of 6 min-\nutes and about ğ‘‡= 143 atomic action annotations per\nvideo.\nAs a baseline, we consider uniform sampling, where de-\ncoding points are placed at fixed intervals regardless of the\nunderlying video content. Standard streaming VLMs are\nlimited to this strategy, whereas VL-JEPA supports a more\neffective alternative: adaptive selection of decoding points\nguided by its predicted embeddings. We apply agglom-\nerative clustering with temporal connectivity constraints\n[Murtagh and Contreras, 2012] to partition the embedding\nsequence into ğ‘segments of high intra-segment monose-\nmanticity [Chen et al., 2024a], measured by variance (i.e.,\nWard distance). The intuition is that within a semantically\ncoherent segment, decoded outputs are highly similar, so\ndecoding once per segment captures the essential infor-\nmation while greatly reducing overall decoding cost. The\n7", "clean_text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language Sec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion Figure 3. Comparison of embedding prediction (VL-JEPA) and token prediction (VLM). We conduct a fair comparison of under strictly aligned training settings (encoder, data, batchsize, etc.). Left: Zero-shot video captioning CIDEr score averaged over 3 datasets and zero-shot classification accuracy (top-5) averaged over 3 benchmarks. Right: Comparing the trainable parameters and average inference time cost. Figure 4. Evaluation of selective decoding. Left: We compare uniform sampling of decoding points at fixed intervals (red) and embedding-guided selective decoding (blue). Performance is measured by the average CIDEr score between each annotation ğ‘¦and its closest decoded output Ë†ğ‘¦. Right: Results on EgoExo4D show that selective decoding achieves a Pareto improvement over uniform sampling: for the same performance level, it requires fewer decoding operations. VLM. After a few iterations, we show that VL-JEPAâ€™s performance increase is much sharper compared to VLM, reaching 14.7 CIDEr and 35.3% top-5 accuracy after 5M samples seen. This gap remains constant as training scales at 15M samples with 14.8 CIDEr and 41.0% top-5 accuracy for VL-JEPA, while the VLM baseline yield respectively 7.1 CIDEr and 27.2% top-5 accuracy. This controlled comparison highlights the benefit of predicting embeddings rather than tokens, showing both higher sample efficiency and stronger absolute performance. We compare the inference cost of the above VL-JEPA and the VLM by pre-loading 64 video frames into memory and repeatedly decoding text 100 times with the same prompt, measuring the average time per sample. As shown in Fig. 3 (right most), both models exhibit comparable latency when generating text. What differentiates our model from classical VLM is the decoupling between the prompt processing (â€œQuery Embeddingâ€) and the video encoder (â€œEncoder + Predictorâ€) from the text generation module (â€œDecoderâ€). This allows us to only use the first part of the model to perform retrieval and decode text only when needed (see Section 4.5 below), making our model more scalable for online video inference. 4.5 Effectiveness of Selective Decoding Evaluation Setup. We evaluate the effectiveness of VLJEPAâ€™s embedding-guided selective decoding on long-form video streams. To this end, we design a benchmark task where the goal is to recover a temporal sequence of annotations while minimizing the number of text decoding operations, which dominate inference cost. As shown in Fig. 4 (left), decoding is performed only at selected points along the VL-JEPA embedding stream, yielding a sequence of ğ‘decoded outputs [(Ë†ğ‘¡1, Ë†ğ‘¦1), (Ë†ğ‘¡2, Ë†ğ‘¦2), . . . , (Ë†ğ‘¡ğ‘, Ë†ğ‘¦ğ‘)]. Each ground-truth annotation [(ğ‘¡1, ğ‘¦1), (ğ‘¡2, ğ‘¦2), . . . , (ğ‘¡ğ‘‡, ğ‘¦ğ‘‡)] is then aligned to its nearest decoded output in time (illustrated as â—¦Â· Â· Â· â—¦in Fig. 4), and CIDEr is computed between matched pairs. We use the EgoExo4D [Grauman et al., 2024] validation set in procedural activity domains, which consists of 218 videos with an average duration of 6 minutes and about ğ‘‡= 143 atomic action annotations per video. As a baseline, we consider uniform sampling, where decoding points are placed at fixed intervals regardless of the underlying video content. Standard streaming VLMs are limited to this strategy, whereas VL-JEPA supports a more effective alternative: adaptive selection of decoding points guided by its predicted embeddings. We apply agglomerative clustering with temporal connectivity constraints [Murtagh and Contreras, 2012] to partition the embedding sequence into ğ‘segments of high intra-segment monosemanticity [Chen et al., 2024a], measured by variance (i.e., Ward distance). The intuition is that within a semantically coherent segment, decoded outputs are highly similar, so decoding once per segment captures the essential information while greatly reducing overall decoding cost. The 7"}
{"pdf_id": "arxiv_251210942_vl_jepa", "page": 8, "text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language\nSec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion\nTable 4. Comparison of text-encoders performance. We report triplet-based accuracy (%) on SugarCrepe++ and VISLA datasets.\nModel\n# Params.\n(total)\n# Params.\n(text encoder)\nSugarCrepe++ [Dumpala et al., 2024a]\nVISLA [Dumpala et al., 2024b]\nAverage\nReplace\nAttribute\nReplace\nObject\nReplace\nRelation\nSwap\nAttribute\nSwap\nObject\nAverage\nGeneric\nSpatial\nCLIP\nViT-L\n389M\n85M\n44.5\n56.7\n83.0\n42.5\n27.0\n13.5\n34.5\n37.6\n31.3\nSigLIP2\nViT-g\n1.9B\n708M\n56.5\n66.9\n74.4\n52.1\n58.4\n30.6\n40.4\n48.7\n32.1\nPE-Core\nViT-G\n2.3B\n537M\n58.6\n73.6\n90.6\n48.9\n53.2\n26.5\n38.3\n45.2\n31.4\nVL-JEPABASE\nViT-L\n1.6B\n300M\n63.9\n72.2\n90.1\n52.2\n62.9\n42.0\n42.9\n49.8\n35.9\nVL-JEPASFT\nViT-L\n1.6B\n300M\n58.4\n68.5\n90.9\n47.4\n55.4\n29.8\n39.5\n44.8\n34.2\nTable 5. Ablation studies results. The default setting adopted by VL-JEPA is marked in blue . We calculate Â±delta within each group of ablations\nin comparison with the default setting.\nClassification\n(Accuracy)\nRetrieval\n(Recall@1)\nVQA\n(Accuracy)\nVL-JEPASFT\n59.1\n70.6\n53.2\n(a) Effectiveness of pretraining stage on caption data\nw/ Pretraining\n49.0\n47.5\n46.1\nw/o Pretraining\n27.3\n(-21.7)\n30.2\n(-17.3)\n42.5\n(-3.6)\n(b) Learning rate multiplier for Y-Encoder\nmultiplier = 0.05\n27.3\n30.2\n42.5\nmultiplier = 1.00\n23.7\n(-3.6)\n28.8\n(-1.4)\n40.7\n(-1.8)\nmultiplier = 0.10\n26.9\n(-0.4)\n30.2\n(-0.0)\n42.9\n(+0.4)\nmultiplier = 0.01\n25.6\n(-1.7)\n27.7\n(-2.5)\n41.0\n(-1.5)\nmultiplier = 0.00\n20.0\n(-7.3)\n25.9\n(-4.3)\n41.4\n(-1.1)\n(c) Loss function (with no projection head on top frozen text encoder)\nInfoNCE\n23.3\n30.3\n44.3\nCosine\n16.5\n(-6.8)\n20.2\n(-10.1)\n46.6\n(+2.3)\nL1\n14.8\n(-8.5)\n15.5\n(-14.8)\n41.9\n(-2.4)\nL2\n13.5\n(-9.8)\n11.7\n(-18.6)\n43.7\n(-0.6)\nClassification\n(Accuracy)\nRetrieval\n(Recall@1)\nVQA\n(Accuracy)\n(d) Predictor architecture and initialization\nLayer 8-16\n27.3\n30.2\n42.5\nLayer 0-2\n24.3\n(-3.0)\n27.8\n(-2.4)\n40.1\n(-2.4)\nLayer 0-4\n25.1\n(-2.2)\n28.9\n(-1.3)\n43.6\n(+1.1)\nLayer 0-8\n27.2\n(-0.1)\n29.3\n(-0.9)\n43.4\n(+0.9)\nLayer 0-16\n27.4\n(+0.1)\n31.0\n(+0.8)\n45.5\n(+3.0)\nw/o Bi-direction Attention\n26.7\n(-0.6)\n31.2\n(+1.0)\n40.6\n(-1.9)\nw/o Llama-3 Initialization\n28.1\n(+0.8)\n30.4\n(+0.2)\n40.6\n(-1.9)\n(e) Y-Encoder (trainable linear projection on top of frozen text encoder)\nEmbeddingGemma-300M\n19.5\n24.1\n42.5\nQwen3-Embedding-0.6B\n24.5\n(+5.0)\n24.5\n(+0.4)\n41.5\n(-1.0)\nQwen3-Embedding-4B\n27.7\n(+8.2)\n26.6\n(+2.5)\n38.1\n(-4.4)\nQwen3-Embedding-8B\n29.6\n(+10.1)\n29.5\n(+5.4)\n41.9\n(-0.6)\nPEcore-B (356M)\n29.4\n(+9.9)\n34.5\n(+10.4)\n35.9\n(-6.6)\nPEcore-L (356M)\n29.0\n(+9.5)\n34.2\n(+10.1)\n42.9\n(+0.4)\nPEcore-G (539M)\n33.9\n(+14.4)\n32.0\n(+7.9)\n41.8\n(-0.7)\nmidpoint of each segment is then chosen as the decoding\npoint, and decoding is performed either from the exact\nembedding or from the average-pooled embedding within\nthe segment.\nResults. As shown in Fig. 4 (right), we sweep the av-\nerage decoding frequency from 2.0 Hz down to 0.01 Hz\n(i.e., average intervals between consecutive decoding op-\nerations from 0.5s to 100s) by adjusting either the stride\nof uniform sampling or the number of clusters in adap-\ntive selection. Across the entire range, adaptive selection\nconsistently Pareto-dominates uniform sampling. In par-\nticular, selective decoding at 0.35 Hz (i.e., âˆ¼2.85s interval)\nmatches the performance of uniform decoding at 1 Hz,\nreducing decoding cost by âˆ¼2.85Ã—. We further observe\nthat average pooling provides consistent gains for both\nstrategies, since it provides denoising and stabilization on\nembeddings prior feeding into the decoder.\n4.6\nEvaluation of Y-Encoder\nEvaluation Setup. We evaluate whether the JEPA architec-\nture improves the Y-Encoder by following the uni-modal\ntext-only (TOT) evaluation setup. We use the hard-negative\nbenchmarks SugarCrepe++ [Dumpala et al., 2024a] and\nVISLA [Dumpala et al., 2024b]. These datasets test sensitiv-\nity to semantic and lexical changes in image descriptions.\nEach dataset contains triplets: two semantically similar de-\nscriptions of the same image (ğ‘1 and ğ‘2), and one negative\ndescription (ğ‘›) created by altering attributes, relations, or\nobjects. We compare Y-Encoders from different models\nby computing the cosine similarity for all description pairs.\nWe check that the similarity between positives ğ‘ ğ‘–ğ‘š(ğ‘1, ğ‘2)\nis higher than both the similarity between each positive\nand the negative ğ‘ ğ‘–ğ‘š(ğ‘1, ğ‘›) and ğ‘ ğ‘–ğ‘š(ğ‘2, ğ‘›). We report\naccuracy (%) across all samples.\nResults. Table 4 shows the performance of different\nmodels on text hard-negative benchmarks. VL-JEPABASE\nachieves a micro average accuracy of 63.9% on Sugar-\nCrepe++ and 42.9% on VISLA. This is higher than the best\nother models: PE-Core scores 58.6% on SugarCrepe++ and\nSigLIP2 scores 40.4% on VISLA. The finetuned VL-JEPASFT\nmodel also achieves competitive results, with 58.4% on\nSugarCrepe++ and 39.5% on VISLA. These results indicate\nthat VL-JEPABASE has a Y-Encoder that is more resilient to\ntext hard-negatives.\n4.7\nAblation Study\nEvaluation Setup. We study different design choices for\nVL-JEPA. Here we train all ablation models on the SFT stage\n8", "clean_text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language Sec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion Table 4. Comparison of text-encoders performance. We report triplet-based accuracy (%) on SugarCrepe++ and VISLA datasets. Model # Params. (total) # Params. (text encoder) SugarCrepe++ [Dumpala et al., 2024a] VISLA [Dumpala et al., 2024b] Average Replace Attribute Replace Object Replace Relation Swap Attribute Swap Object Average Generic Spatial CLIP ViT-L 389M 85M 44.5 56.7 83.0 42.5 27.0 13.5 34.5 37.6 31.3 SigLIP2 ViT-g 1.9B 708M 56.5 66.9 74.4 52.1 58.4 30.6 40.4 48.7 32.1 PE-Core ViT-G 2.3B 537M 58.6 73.6 90.6 48.9 53.2 26.5 38.3 45.2 31.4 VL-JEPABASE ViT-L 1.6B 300M 63.9 72.2 90.1 52.2 62.9 42.0 42.9 49.8 35.9 VL-JEPASFT ViT-L 1.6B 300M 58.4 68.5 90.9 47.4 55.4 29.8 39.5 44.8 34.2 Table 5. Ablation studies results. The default setting adopted by VL-JEPA is marked in blue . We calculate Â±delta within each group of ablations in comparison with the default setting. Classification (Accuracy) Retrieval (Recall@1) VQA (Accuracy) VL-JEPASFT 59.1 70.6 53.2 (a) Effectiveness of pretraining stage on caption data w/ Pretraining 49.0 47.5 46.1 w/o Pretraining 27.3 (-21.7) 30.2 (-17.3) 42.5 (-3.6) (b) Learning rate multiplier for Y-Encoder multiplier = 0.05 27.3 30.2 42.5 multiplier = 1.00 23.7 (-3.6) 28.8 (-1.4) 40.7 (-1.8) multiplier = 0.10 26.9 (-0.4) 30.2 (-0.0) 42.9 (+0.4) multiplier = 0.01 25.6 (-1.7) 27.7 (-2.5) 41.0 (-1.5) multiplier = 0.00 20.0 (-7.3) 25.9 (-4.3) 41.4 (-1.1) (c) Loss function (with no projection head on top frozen text encoder) InfoNCE 23.3 30.3 44.3 Cosine 16.5 (-6.8) 20.2 (-10.1) 46.6 (+2.3) L1 14.8 (-8.5) 15.5 (-14.8) 41.9 (-2.4) L2 13.5 (-9.8) 11.7 (-18.6) 43.7 (-0.6) Classification (Accuracy) Retrieval (Recall@1) VQA (Accuracy) (d) Predictor architecture and initialization Layer 8-16 27.3 30.2 42.5 Layer 0-2 24.3 (-3.0) 27.8 (-2.4) 40.1 (-2.4) Layer 0-4 25.1 (-2.2) 28.9 (-1.3) 43.6 (+1.1) Layer 0-8 27.2 (-0.1) 29.3 (-0.9) 43.4 (+0.9) Layer 0-16 27.4 (+0.1) 31.0 (+0.8) 45.5 (+3.0) w/o Bi-direction Attention 26.7 (-0.6) 31.2 (+1.0) 40.6 (-1.9) w/o Llama-3 Initialization 28.1 (+0.8) 30.4 (+0.2) 40.6 (-1.9) (e) Y-Encoder (trainable linear projection on top of frozen text encoder) EmbeddingGemma-300M 19.5 24.1 42.5 Qwen3-Embedding-0.6B 24.5 (+5.0) 24.5 (+0.4) 41.5 (-1.0) Qwen3-Embedding-4B 27.7 (+8.2) 26.6 (+2.5) 38.1 (-4.4) Qwen3-Embedding-8B 29.6 (+10.1) 29.5 (+5.4) 41.9 (-0.6) PEcore-B (356M) 29.4 (+9.9) 34.5 (+10.4) 35.9 (-6.6) PEcore-L (356M) 29.0 (+9.5) 34.2 (+10.1) 42.9 (+0.4) PEcore-G (539M) 33.9 (+14.4) 32.0 (+7.9) 41.8 (-0.7) midpoint of each segment is then chosen as the decoding point, and decoding is performed either from the exact embedding or from the average-pooled embedding within the segment. Results. As shown in Fig. 4 (right), we sweep the average decoding frequency from 2.0 Hz down to 0.01 Hz (i.e., average intervals between consecutive decoding operations from 0.5s to 100s) by adjusting either the stride of uniform sampling or the number of clusters in adaptive selection. Across the entire range, adaptive selection consistently Pareto-dominates uniform sampling. In particular, selective decoding at 0.35 Hz (i.e., âˆ¼2.85s interval) matches the performance of uniform decoding at 1 Hz, reducing decoding cost by âˆ¼2.85Ã—. We further observe that average pooling provides consistent gains for both strategies, since it provides denoising and stabilization on embeddings prior feeding into the decoder. 4.6 Evaluation of Y-Encoder Evaluation Setup. We evaluate whether the JEPA architecture improves the Y-Encoder by following the uni-modal text-only (TOT) evaluation setup. We use the hard-negative benchmarks SugarCrepe++ [Dumpala et al., 2024a] and VISLA [Dumpala et al., 2024b]. These datasets test sensitivity to semantic and lexical changes in image descriptions. Each dataset contains triplets: two semantically similar descriptions of the same image (ğ‘1 and ğ‘2), and one negative description (ğ‘›) created by altering attributes, relations, or objects. We compare Y-Encoders from different models by computing the cosine similarity for all description pairs. We check that the similarity between positives ğ‘ ğ‘–ğ‘š(ğ‘1, ğ‘2) is higher than both the similarity between each positive and the negative ğ‘ ğ‘–ğ‘š(ğ‘1, ğ‘›) and ğ‘ ğ‘–ğ‘š(ğ‘2, ğ‘›). We report accuracy (%) across all samples. Results. Table 4 shows the performance of different models on text hard-negative benchmarks. VL-JEPABASE achieves a micro average accuracy of 63.9% on SugarCrepe++ and 42.9% on VISLA. This is higher than the best other models: PE-Core scores 58.6% on SugarCrepe++ and SigLIP2 scores 40.4% on VISLA. The finetuned VL-JEPASFT model also achieves competitive results, with 58.4% on SugarCrepe++ and 39.5% on VISLA. These results indicate that VL-JEPABASE has a Y-Encoder that is more resilient to text hard-negatives. 4.7 Ablation Study Evaluation Setup. We study different design choices for VL-JEPA. Here we train all ablation models on the SFT stage 8"}
{"pdf_id": "arxiv_251210942_vl_jepa", "page": 9, "text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language\nSec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion\ndata for 10K steps with a batch size of 512 (5M samples seen)\nand constant learning rate. We report average classification\ntop-1 accuracy of 8 datasets (Tab. 1), average text-to-video\nretrieval recall@1 of 8 datasets (Tab. 1), and average VQA\naccuracy of 4 datasets (CLEVR, GQA, TallyQA simple and\ncomplex). We report the results in Tab. 5.\nResults. (a) Pretraining. Dropping the first query-free\npretraining stage on image and video captions significantly\nhurt performance, especially on classification (-21.7) and\nretrieval (-17.3). (b) LR Multiplier. The sweet point of\nlearning rate multiplier to the Y-Encoder is around 0.05\nto 0.10.\nEither faster or slower learning degrades the\nperformance. (c) Loss Function. InfoNCE generally give\nsuperior performance compared to cosine, L1, and L2\nlosses, with the only exception being cosine loss outper-\nform InfoNCE on VQA. However, only InfoNCE has the\nanti-collapse regularization and can be applied with un-\nfrozen Y-Encoder. (d) Predictor. In terms of predictor size,\nmore layers yield better performance, especially on VQA\nperformance. We also see that if using the original causal\nattention instead of updating to bi-direction attention hurt\nVQA performance (-1.9), since query tokens are appended\nafter visual tokens, and visual tokens are no longer able to\nattend to query tokens. Finally, we also see that LLama-3\ninitialization is beneficial to VQA performance, although\nvision-language alignment (classification and retrieval) is\na bit worse compared to randomly initialized Transformer\nlayers. (e) Y-Encoder. We tried different text encoder as the\nY-Encoder, and confirmed that VL-JEPA works well with\nother embedding models than EmbeddingGemma-300M.\nGenerally, larger encoder leads to better performance, with\nvisually aligned text encoders (PE models) has significant\nadvantage in classification and retrieval.\n5\nRelated Works\nJEPA Models. JEPA model learns by predicting the rep-\nresentation of a target input ğ‘Œfrom the representation of\na context input ğ‘‹. Early instantiations include I-JEPA for\nimage encoding [Assran et al., 2023] and V-JEPA for video\nencoding [Bardes et al., 2023], which demonstrated the\neffectiveness of this objective over pixel reconstruction ap-\nproach in their respective modality. Recent JEPA work falls\ninto two categories. One category of work emphasizes bet-\nter unimodal representation learning [Assran et al., 2023,\nBardes et al., 2023, Fei et al., 2023] or cross-modal align-\nment [Lei et al., 2025, Jose et al., 2025]. The other direction\ntargets world modeling, where pretrained encoders are\nfrozen and action-conditioned predictors are trained for\nconditional prediction of state representations [Zhou et al.,\n2025, Baldassarre et al., 2025, Assran et al., 2025]. This\nhas shown good results but remains limited to narrow\ndomains like mazes or robotic pick-and-place. Our pro-\nposed VL-JEPA is the first designed for general-purpose\nvisionâ€“language tasks. It performs conditional latent pre-\ndiction over vision and text, and preserves efficiency while\nenabling flexible, multitask architecture.\nVision Language Models. Existing vision-language\nmodels largely fall into two families: (1) CLIP-style models\nwith a non-predictive joint-embedding architecture (JEA)\n[Radford et al., 2021, Zhai et al., 2023, Bolya et al., 2025,\nLiu et al., 2024, Chen et al., 2023] encode images and\ntexts independently into a common latent space, ğ‘‹ğ‘‰â†¦â†’ğ‘†ğ‘‰\nand ğ‘Œâ†¦â†’ğ‘†ğ‘Œ.\nBy minimizing â„’CLIP = ğ·(ğ‘†ğ‘‰, ğ‘†ğ‘Œ) with\na contrastive loss (e.g., InfoNCE), CLIP learns aligned\nrepresentations that support zero-shot classification and\nvisionâ€“language retrieval; (2) Generative VLMs [Liu et al.,\n2023, Chen et al., 2022, Dai et al., 2023, Alayrac et al., 2022,\nChen et al., 2024b, Cho et al., 2025, Beyer et al., 2024]\nconnect a vision encoder [Radford et al., 2021, Fini et al.,\n2025] with a language model (e.g., LLM). They are typically\ntrained with â„’VLM = ğ·( Ë†ğ‘Œ, ğ‘Œ), i.e., next token prediction\nwith cross-entropy loss, and can learn to handle various\nvision-text-to-text generation tasks such as VQA.\nTable 6. Task coverage comparison.\nCLIP\nVLM\nVL-JEPA\nGeneration\nâœ—\nâœ“\nâœ“\nRetrieval\nâœ“\nâœ—\nâœ“\nOur proposed VL-JEPA integrates the architectural ad-\nvantages and task coverage of both CLIPs and VLMs\n(Table 6). Since VL-JEPA learns in embedding space, it\ncan leverage web-scale noisy imageâ€“text pairs [Jia et al.,\n2021], yielding strong open-domain features. On the other\nhand, VL-JEPA supports conditional generation tasks with\na readout text decoder. Meanwhile, compared to genera-\ntive VLMs that optimize directly in data space, VL-JEPA is\nmore efficient at learning in the latent space. In addition,\nit is also more efficient for online inference, as it allows\nnaturally selective decoding.\nEfficient Vision Language Models. The growing size\nand training cost of VLMs has motivated efforts to improve\nefficiency. On the training side, strong performance can\nbe achieved by updating only a subset of parameters,\nsuch as the visionâ€“language connector [Tsimpoukelli et al.,\n2021, Alayrac et al., 2022, Vallaeys et al., 2024, Shukor\net al., 2023, Koh et al., 2023, Merullo et al., 2022, Dai\net al., 2023]. At inference, efficiency is pursued through\npruning parameters or visual tokens [Cao et al., 2023,\nShukor and Cord, 2024, Vasu et al., 2025]. For real-time\nuse cases, recent work explores small VLMs [Yao et al.,\n2024, Marafioti et al., 2025] and heuristics to reduce query\nfrequency in asynchronous inference [Shukor et al., 2025].\nLatent-space Language Modeling. Current state-of-\nthe-art LLMs are trained to decode and reason in text space\n9", "clean_text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language Sec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion data for 10K steps with a batch size of 512 (5M samples seen) and constant learning rate. We report average classification top-1 accuracy of 8 datasets (Tab. 1), average text-to-video retrieval recall@1 of 8 datasets (Tab. 1), and average VQA accuracy of 4 datasets (CLEVR, GQA, TallyQA simple and complex). We report the results in Tab. 5. Results. (a) Pretraining. Dropping the first query-free pretraining stage on image and video captions significantly hurt performance, especially on classification (-21.7) and retrieval (-17.3). (b) LR Multiplier. The sweet point of learning rate multiplier to the Y-Encoder is around 0.05 to 0.10. Either faster or slower learning degrades the performance. (c) Loss Function. InfoNCE generally give superior performance compared to cosine, L1, and L2 losses, with the only exception being cosine loss outperform InfoNCE on VQA. However, only InfoNCE has the anti-collapse regularization and can be applied with unfrozen Y-Encoder. (d) Predictor. In terms of predictor size, more layers yield better performance, especially on VQA performance. We also see that if using the original causal attention instead of updating to bi-direction attention hurt VQA performance (-1.9), since query tokens are appended after visual tokens, and visual tokens are no longer able to attend to query tokens. Finally, we also see that LLama-3 initialization is beneficial to VQA performance, although vision-language alignment (classification and retrieval) is a bit worse compared to randomly initialized Transformer layers. (e) Y-Encoder. We tried different text encoder as the Y-Encoder, and confirmed that VL-JEPA works well with other embedding models than EmbeddingGemma-300M. Generally, larger encoder leads to better performance, with visually aligned text encoders (PE models) has significant advantage in classification and retrieval. 5 Related Works JEPA Models. JEPA model learns by predicting the representation of a target input ğ‘Œfrom the representation of a context input ğ‘‹. Early instantiations include I-JEPA for image encoding [Assran et al., 2023] and V-JEPA for video encoding [Bardes et al., 2023], which demonstrated the effectiveness of this objective over pixel reconstruction approach in their respective modality. Recent JEPA work falls into two categories. One category of work emphasizes better unimodal representation learning [Assran et al., 2023, Bardes et al., 2023, Fei et al., 2023] or cross-modal alignment [Lei et al., 2025, Jose et al., 2025]. The other direction targets world modeling, where pretrained encoders are frozen and action-conditioned predictors are trained for conditional prediction of state representations [Zhou et al., 2025, Baldassarre et al., 2025, Assran et al., 2025]. This has shown good results but remains limited to narrow domains like mazes or robotic pick-and-place. Our proposed VL-JEPA is the first designed for general-purpose visionâ€“language tasks. It performs conditional latent prediction over vision and text, and preserves efficiency while enabling flexible, multitask architecture. Vision Language Models. Existing vision-language models largely fall into two families: (1) CLIP-style models with a non-predictive joint-embedding architecture (JEA) [Radford et al., 2021, Zhai et al., 2023, Bolya et al., 2025, Liu et al., 2024, Chen et al., 2023] encode images and texts independently into a common latent space, ğ‘‹ğ‘‰â†¦â†’ğ‘†ğ‘‰ and ğ‘Œâ†¦â†’ğ‘†ğ‘Œ. By minimizing â„’CLIP = ğ·(ğ‘†ğ‘‰, ğ‘†ğ‘Œ) with a contrastive loss (e.g., InfoNCE), CLIP learns aligned representations that support zero-shot classification and visionâ€“language retrieval; (2) Generative VLMs [Liu et al., 2023, Chen et al., 2022, Dai et al., 2023, Alayrac et al., 2022, Chen et al., 2024b, Cho et al., 2025, Beyer et al., 2024] connect a vision encoder [Radford et al., 2021, Fini et al., 2025] with a language model (e.g., LLM). They are typically trained with â„’VLM = ğ·( Ë†ğ‘Œ, ğ‘Œ), i.e., next token prediction with cross-entropy loss, and can learn to handle various vision-text-to-text generation tasks such as VQA. Table 6. Task coverage comparison. CLIP VLM VL-JEPA Generation âœ— âœ“ âœ“ Retrieval âœ“ âœ— âœ“ Our proposed VL-JEPA integrates the architectural advantages and task coverage of both CLIPs and VLMs (Table 6). Since VL-JEPA learns in embedding space, it can leverage web-scale noisy imageâ€“text pairs [Jia et al., 2021], yielding strong open-domain features. On the other hand, VL-JEPA supports conditional generation tasks with a readout text decoder. Meanwhile, compared to generative VLMs that optimize directly in data space, VL-JEPA is more efficient at learning in the latent space. In addition, it is also more efficient for online inference, as it allows naturally selective decoding. Efficient Vision Language Models. The growing size and training cost of VLMs has motivated efforts to improve efficiency. On the training side, strong performance can be achieved by updating only a subset of parameters, such as the visionâ€“language connector [Tsimpoukelli et al., 2021, Alayrac et al., 2022, Vallaeys et al., 2024, Shukor et al., 2023, Koh et al., 2023, Merullo et al., 2022, Dai et al., 2023]. At inference, efficiency is pursued through pruning parameters or visual tokens [Cao et al., 2023, Shukor and Cord, 2024, Vasu et al., 2025]. For real-time use cases, recent work explores small VLMs [Yao et al., 2024, Marafioti et al., 2025] and heuristics to reduce query frequency in asynchronous inference [Shukor et al., 2025]. Latent-space Language Modeling. Current state-ofthe-art LLMs are trained to decode and reason in text space 9"}
{"pdf_id": "arxiv_251210942_vl_jepa", "page": 10, "text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language\nSec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion\nusing autoregressive generation and chain-of-thought\nprompting [Wei et al., 2022]. Text-space LLMs have rapidly\nimproved and now achieve strong results on a wide range\nof benchmarks. However, the discrete nature of their rea-\nsoning trace may limit both speed and performance in\nthe long term. Several works have explored latent-space\nLLMs that process or reason in latent space, such as Large\nConcept Models [Barrault et al., 2024] and COCONUT\n[Hao et al., 2024]. These models focus on unimodal latent-\nspace reasoning. With VL-JEPA, our goal is to align vision\nand text representations in a shared multi-modal latent\nspace. This approach aims to enable better abstractions\nand improve both the performance and speed of vision-\nlanguage models (VLMs). We hope VL-JEPA will serve as\na foundation for future work on multi-modal latent space\nreasoning, including visual chain-of-thought methods [Li\net al., 2025a].\n6\nConclusion\nWe have presented VL-JEPA, a new visionâ€“language model\nbuilt upon the joint embedding predictive architecture.\nBy shifting supervision from discrete token space to con-\ntinuous semantic embedding space, VL-JEPA simplifies\nthe learning target, avoids redundant modeling of sur-\nface linguistic variability, and enables non-autoregressive\nprediction. Through controlled experiments, we show\nthat VL-JEPA outperforms generative VLMs trained with\ncross-entropy loss under matched training data budget,\nwhile achieving superior training efficiency and signifi-\ncantly lower inference latency. Beyond generation tasks,\nthe embedding-based design further allows VL-JEPA to\nhandle open-vocabulary classification and cross-modal\nretrieval within a single unified architecture. Its ability\nto emit continuous semantic embeddings also makes it\nparticularly well suited for real-time video applications,\nwhere selective decoding can improve both responsive-\nness and efficiency. In this work, we demonstrated the\nadvantages of VL-JEPA over standard VLMs, particularly\nin computational efficiency, streaming applications, and\nvideo-language tasks. Our goal at this stage, is not to\npropose a universal alternative to VLMs, as this would\nrequire broader evaluation on tasks such as reasoning, tool\nuse, and agentic behaviors where current token generative\nVLMs excel. Finally, although our results show clear bene-\nfits from scaling parameters and dataset size, we did not\nfully explore this direction, leaving it for future work.\nAcknowledgments\nWe would like to thank Yejin Bang, Adrien Bardes, LoÃ¯c\nBarrault, Lucas Beyer, Quentin Garrido, JoÃ£o Maria Janeiro,\nYifu Qiu, Koustuv Sinha, Basile Terver, and FranÃ§ois Yvon\nfor providing valuable feedback and support to this work.\nReferences\nManoj Acharya, Kushal Kafle, and Christopher Kanan.\nTallyqa: Answering complex counting questions. In\nProceedings of the AAAI conference on artificial intelligence,\nvolume 33, pages 8076â€“8084, 2019.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur\nMensch, Katherine Millican, Malcolm Reynolds, et al.\nFlamingo: a visual language model for few-shot learn-\ning. Advances in neural information processing systems, 35:\n23716â€“23736, 2022.\nMahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bo-\njanowski, Pascal Vincent, Michael Rabbat, Yann LeCun,\nand Nicolas Ballas. Self-supervised learning from im-\nages with a joint-embedding predictive architecture. In\nProceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 15619â€“15629, 2023.\nMido Assran, Adrien Bardes, David Fan, Quentin Garrido,\nRussell Howes, Matthew Muckley, Ammar Rizvi, Claire\nRoberts, Koustuv Sinha, Artem Zholus, et al. V-jepa\n2: Self-supervised video models enable understanding,\nprediction and planning. arXiv preprint arXiv:2506.09985,\n2025.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\nXiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, et al. Qwen technical report. arXiv preprint\narXiv:2309.16609, 2023.\nFederico Baldassarre, Marc Szafraniec, Basile Terver, Vasil\nKhalidov, Francisco Massa, Yann LeCun, Patrick Labatut,\nMaximilian Seitzer, and Piotr Bojanowski. Back to the\nfeatures: Dino as a foundation for video world models.\narXiv preprint arXiv:2507.19468, 2025.\nRandall Balestriero and Yann LeCun. Lejepa: Provable and\nscalable self-supervised learning without the heuristics.\narXiv preprint arXiv:2511.08544, 2025.\nAdrien Bardes, Jean Ponce, and Yann LeCun.\nVicreg:\nVariance-invariance-covariance regularization for self-\nsupervised learning. arXiv preprint arXiv:2105.04906,\n2021.\nAdrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen,\nMichael Rabbat, Yann LeCun, Mido Assran, and Nico-\nlas Ballas. V-jepa: Latent video prediction for visual\nrepresentation learning. 2023.\nLoÃ¯c Barrault, Paul-Ambroise Duquenne, Maha Elbayad,\nArtyom Kozhevnikov, Belen Alastruey, Pierre Andrews,\nMariano Coria, Guillaume Couairon, Marta R Costa-\njussÃ , David Dale, et al. Large concept models: Language\n10", "clean_text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language Sec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion using autoregressive generation and chain-of-thought prompting [Wei et al., 2022]. Text-space LLMs have rapidly improved and now achieve strong results on a wide range of benchmarks. However, the discrete nature of their reasoning trace may limit both speed and performance in the long term. Several works have explored latent-space LLMs that process or reason in latent space, such as Large Concept Models [Barrault et al., 2024] and COCONUT [Hao et al., 2024]. These models focus on unimodal latentspace reasoning. With VL-JEPA, our goal is to align vision and text representations in a shared multi-modal latent space. This approach aims to enable better abstractions and improve both the performance and speed of visionlanguage models (VLMs). We hope VL-JEPA will serve as a foundation for future work on multi-modal latent space reasoning, including visual chain-of-thought methods [Li et al., 2025a]. 6 Conclusion We have presented VL-JEPA, a new visionâ€“language model built upon the joint embedding predictive architecture. By shifting supervision from discrete token space to continuous semantic embedding space, VL-JEPA simplifies the learning target, avoids redundant modeling of surface linguistic variability, and enables non-autoregressive prediction. Through controlled experiments, we show that VL-JEPA outperforms generative VLMs trained with cross-entropy loss under matched training data budget, while achieving superior training efficiency and significantly lower inference latency. Beyond generation tasks, the embedding-based design further allows VL-JEPA to handle open-vocabulary classification and cross-modal retrieval within a single unified architecture. Its ability to emit continuous semantic embeddings also makes it particularly well suited for real-time video applications, where selective decoding can improve both responsiveness and efficiency. In this work, we demonstrated the advantages of VL-JEPA over standard VLMs, particularly in computational efficiency, streaming applications, and video-language tasks. Our goal at this stage, is not to propose a universal alternative to VLMs, as this would require broader evaluation on tasks such as reasoning, tool use, and agentic behaviors where current token generative VLMs excel. Finally, although our results show clear benefits from scaling parameters and dataset size, we did not fully explore this direction, leaving it for future work. Acknowledgments We would like to thank Yejin Bang, Adrien Bardes, LoÃ¯c Barrault, Lucas Beyer, Quentin Garrido, JoÃ£o Maria Janeiro, Yifu Qiu, Koustuv Sinha, Basile Terver, and FranÃ§ois Yvon for providing valuable feedback and support to this work. References Manoj Acharya, Kushal Kafle, and Christopher Kanan. Tallyqa: Answering complex counting questions. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 8076â€“8084, 2019. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in neural information processing systems, 35: 23716â€“23736, 2022. Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with a joint-embedding predictive architecture. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15619â€“15629, 2023. Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al. V-jepa 2: Self-supervised video models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09985, 2025. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Federico Baldassarre, Marc Szafraniec, Basile Terver, Vasil Khalidov, Francisco Massa, Yann LeCun, Patrick Labatut, Maximilian Seitzer, and Piotr Bojanowski. Back to the features: Dino as a foundation for video world models. arXiv preprint arXiv:2507.19468, 2025. Randall Balestriero and Yann LeCun. Lejepa: Provable and scalable self-supervised learning without the heuristics. arXiv preprint arXiv:2511.08544, 2025. Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization for selfsupervised learning. arXiv preprint arXiv:2105.04906, 2021. Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mido Assran, and Nicolas Ballas. V-jepa: Latent video prediction for visual representation learning. 2023. LoÃ¯c Barrault, Paul-Ambroise Duquenne, Maha Elbayad, Artyom Kozhevnikov, Belen Alastruey, Pierre Andrews, Mariano Coria, Guillaume Couairon, Marta R CostajussÃ , David Dale, et al. Large concept models: Language 10"}
{"pdf_id": "arxiv_251210942_vl_jepa", "page": 11, "text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language\nSec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion\nmodeling in a sentence representation space. arXiv\npreprint arXiv:2412.08821, 2024.\nLucas Beyer, Andreas Steiner, AndrÃ© Susano Pinto, Alexan-\nder Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neu-\nmann, Ibrahim Alabdulmohsin, Michael Tschannen,\nEmanuele Bugliarello, et al. Paligemma: A versatile 3b\nvlm for transfer. arXiv preprint arXiv:2407.07726, 2024.\nKevin Black, Manuel Y Galliker, and Sergey Levine. Real-\ntime execution of action chunking flow policies. arXiv\npreprint arXiv:2506.07339, 2025.\nDaniel Bolya, Po-Yao Huang, Peize Sun, Jang Hyun Cho,\nAndrea Madotto, Chen Wei, Tengyu Ma, Jiale Zhi,\nJathushan Rajasegaran, Hanoona Rasheed, et al. Percep-\ntion encoder: The best visual embeddings are not at the\noutput of the network. arXiv preprint arXiv:2504.13181,\n2025.\nFlorian Bordes, Richard Yuanzhe Pang, Anurag Ajay,\nAlexander C Li, Adrien Bardes, Suzanne Petryk, Oscar\nMaÃ±as, Zhiqiu Lin, Anas Mahmoud, Bargav Jayaraman,\net al. An introduction to vision-language modeling.\narXiv preprint arXiv:2405.17247, 2024.\nQingqing Cao, Bhargavi Paranjape, and Hannaneh Ha-\njishirzi. Pumer: Pruning and merging tokens for efficient\nvision language models. arXiv preprint arXiv:2305.17530,\n2023.\nDelong Chen, Zhao Wu, Fan Liu, Zaiquan Yang, Shaoqiu\nZheng, Ying Tan, and Erjin Zhou. Protoclip: Proto-\ntypical contrastive language image pretraining. IEEE\nTransactions on Neural Networks and Learning Systems,\n2023.\nDelong Chen, Samuel CahyawÄ³aya, Jianfeng Liu, Baoyuan\nWang, and Pascale Fung. Subobject-level image tok-\nenization. arXiv preprint arXiv:2402.14327, 2024a.\nDelong Chen, Jianfeng Liu, Wenliang Dai, and Baoyuan\nWang. Visual instruction tuning with polite flamingo.\nIn Proceedings of the aaai conference on artificial intelligence,\nvolume 38, pages 17745â€“17753, 2024b.\nDelong Chen, Willy Chung, Yejin Bang, Ziwei Ji, and Pas-\ncale Fung. Worldprediction: A benchmark for high-level\nworld modeling and long-horizon procedural planning.\narXiv preprint arXiv:2506.04363, 2025a.\nDelong Chen, Theo Moutakanni, Willy Chung, Yejin Bang,\nZiwei Ji, Allen Bolourchi, and Pascale Fung. Planning\nwith reasoning using vision language world model.\narXiv preprint arXiv:2509.02722, 2025b.\nJoya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong\nLin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao,\nDongxing Mao, and Mike Zheng Shou. Videollm-online:\nOnline video large language model for streaming video.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 18407â€“18418, 2024c.\nXi Chen, Xiao Wang, Soravit Changpinyo, Anthony J\nPiergiovanni, Piotr Padlewski, Daniel Salz, Sebastian\nGoodman, Adam Grycner, Basil Mustafa, Lucas Beyer,\net al. Pali: A jointly-scaled multilingual language-image\nmodel. arXiv preprint arXiv:2209.06794, 2022.\nZhe Chen, Jiannan Wu, Wenhai Wang, WeÄ³ie Su, Guo Chen,\nSen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu,\nLewei Lu, et al. Internvl: Scaling up vision foundation\nmodels and aligning for generic visual-linguistic tasks.\nIn Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 24185â€“24198, 2024d.\nJang Hyun Cho, Andrea Madotto, Effrosyni Mavroudi,\nTriantafyllos Afouras, Tushar Nagarajan, Muhammad\nMaaz, Yale Song, Tengyu Ma, Shuming Hu, Suyog\nJain, et al. Perceptionlm: Open-access data and mod-\nels for detailed visual understanding. arXiv preprint\narXiv:2504.13180, 2025.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong,\nJunqi Zhao, Weisheng Wang, Boyang Li, Pascale N\nFung, and Steven Hoi. Instructblip: Towards general-\npurpose vision-language models with instruction tuning.\nAdvances in neural information processing systems, 36:49250â€“\n49267, 2023.\nShangzhe Di, Zhelun Yu, Guanghao Zhang, Haoyuan\nLi, Tao Zhong, Hao Cheng, Bolin Li, Wanggui He,\nFangxun Shu, and Hao Jiang. Streaming video question-\nanswering with in-context video kv-cache retrieval.\narXiv preprint arXiv:2503.00540, 2025.\nSri Harsha Dumpala, Aman Jaiswal, Chandramouli Sastry,\nEvangelos Milios, Sageev Oore, and Hassan Sajjad. Sug-\narcrepe++ dataset: vision-language model sensitivity to\nsemantic and lexical alterations. In Proceedings of the 38th\nInternational Conference on Neural Information Processing\nSystems, NIPS â€™24, Red Hook, NY, USA, 2024a. Curran\nAssociates Inc. ISBN 9798331314385.\nSri Harsha Dumpala, Aman Jaiswal, Chandramouli Sastry,\nEvangelos Milios, Sageev Oore, and Hassan Sajjad. Visla\nbenchmark: Evaluating embedding sensitivity to seman-\ntic and lexical alterations. arXiv preprint arXiv:2404.16365,\n2024b.\nPaul-Ambroise Duquenne, Holger Schwenk, and BenoÃ®t\nSagot. Sonar: sentence-level multimodal and language-\n11", "clean_text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language Sec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion modeling in a sentence representation space. arXiv preprint arXiv:2412.08821, 2024. Lucas Beyer, Andreas Steiner, AndrÃ© Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: A versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. Kevin Black, Manuel Y Galliker, and Sergey Levine. Realtime execution of action chunking flow policies. arXiv preprint arXiv:2506.07339, 2025. Daniel Bolya, Po-Yao Huang, Peize Sun, Jang Hyun Cho, Andrea Madotto, Chen Wei, Tengyu Ma, Jiale Zhi, Jathushan Rajasegaran, Hanoona Rasheed, et al. Perception encoder: The best visual embeddings are not at the output of the network. arXiv preprint arXiv:2504.13181, 2025. Florian Bordes, Richard Yuanzhe Pang, Anurag Ajay, Alexander C Li, Adrien Bardes, Suzanne Petryk, Oscar MaÃ±as, Zhiqiu Lin, Anas Mahmoud, Bargav Jayaraman, et al. An introduction to vision-language modeling. arXiv preprint arXiv:2405.17247, 2024. Qingqing Cao, Bhargavi Paranjape, and Hannaneh Hajishirzi. Pumer: Pruning and merging tokens for efficient vision language models. arXiv preprint arXiv:2305.17530, 2023. Delong Chen, Zhao Wu, Fan Liu, Zaiquan Yang, Shaoqiu Zheng, Ying Tan, and Erjin Zhou. Protoclip: Prototypical contrastive language image pretraining. IEEE Transactions on Neural Networks and Learning Systems, 2023. Delong Chen, Samuel CahyawÄ³aya, Jianfeng Liu, Baoyuan Wang, and Pascale Fung. Subobject-level image tokenization. arXiv preprint arXiv:2402.14327, 2024a. Delong Chen, Jianfeng Liu, Wenliang Dai, and Baoyuan Wang. Visual instruction tuning with polite flamingo. In Proceedings of the aaai conference on artificial intelligence, volume 38, pages 17745â€“17753, 2024b. Delong Chen, Willy Chung, Yejin Bang, Ziwei Ji, and Pascale Fung. Worldprediction: A benchmark for high-level world modeling and long-horizon procedural planning. arXiv preprint arXiv:2506.04363, 2025a. Delong Chen, Theo Moutakanni, Willy Chung, Yejin Bang, Ziwei Ji, Allen Bolourchi, and Pascale Fung. Planning with reasoning using vision language world model. arXiv preprint arXiv:2509.02722, 2025b. Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, and Mike Zheng Shou. Videollm-online: Online video large language model for streaming video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18407â€“18418, 2024c. Xi Chen, Xiao Wang, Soravit Changpinyo, Anthony J Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794, 2022. Zhe Chen, Jiannan Wu, Wenhai Wang, WeÄ³ie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 24185â€“24198, 2024d. Jang Hyun Cho, Andrea Madotto, Effrosyni Mavroudi, Triantafyllos Afouras, Tushar Nagarajan, Muhammad Maaz, Yale Song, Tengyu Ma, Shuming Hu, Suyog Jain, et al. Perceptionlm: Open-access data and models for detailed visual understanding. arXiv preprint arXiv:2504.13180, 2025. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, and Steven Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning. Advances in neural information processing systems, 36:49250â€“ 49267, 2023. Shangzhe Di, Zhelun Yu, Guanghao Zhang, Haoyuan Li, Tao Zhong, Hao Cheng, Bolin Li, Wanggui He, Fangxun Shu, and Hao Jiang. Streaming video questionanswering with in-context video kv-cache retrieval. arXiv preprint arXiv:2503.00540, 2025. Sri Harsha Dumpala, Aman Jaiswal, Chandramouli Sastry, Evangelos Milios, Sageev Oore, and Hassan Sajjad. Sugarcrepe++ dataset: vision-language model sensitivity to semantic and lexical alterations. In Proceedings of the 38th International Conference on Neural Information Processing Systems, NIPS â€™24, Red Hook, NY, USA, 2024a. Curran Associates Inc. ISBN 9798331314385. Sri Harsha Dumpala, Aman Jaiswal, Chandramouli Sastry, Evangelos Milios, Sageev Oore, and Hassan Sajjad. Visla benchmark: Evaluating embedding sensitivity to semantic and lexical alterations. arXiv preprint arXiv:2404.16365, 2024b. Paul-Ambroise Duquenne, Holger Schwenk, and BenoÃ®t Sagot. Sonar: sentence-level multimodal and language11"}
{"pdf_id": "arxiv_251210942_vl_jepa", "page": 12, "text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language\nSec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion\nagnostic representations. arXiv preprint arXiv:2308.11466,\n2023.\nZhengcong Fei, Mingyuan Fan, and Junshi Huang. A-jepa:\nJoint-embedding predictive architecture can listen. arXiv\npreprint arXiv:2311.15830, 2023.\nEnrico Fini, Mustafa Shukor, Xiujun Li, Philipp Dufter,\nMichal Klein, David Haldimann, Sai Aitharaju, Vic-\ntor G Turrisi da Costa, Louis BÃ©thune, Zhe Gan, et al.\nMultimodal autoregressive pre-training of large vision\nencoders. In Proceedings of the Computer Vision and Pattern\nRecognition Conference, pages 9641â€“9654, 2025.\nPascale Fung, Yoram Bachrach, Asli Celikyilmaz, Ka-\nmalika Chaudhuri, Delong Chen, Willy Chung, Em-\nmanuel Dupoux, Hongyu Gong, HervÃ© JÃ©gou, Alessan-\ndro Lazaric, et al. Embodied ai agents: Modeling the\nworld. arXiv preprint arXiv:2506.22355, 2025.\nSamir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan\nHayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,\nMitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al.\nDatacomp: In search of the next generation of multi-\nmodal datasets. Advances in Neural Information Processing\nSystems, 36:27092â€“27112, 2023.\nKristen Grauman, Andrew Westbury, Eugene Byrne,\nZachary Chavis, Antonino Furnari, Rohit Girdhar, Jack-\nson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al.\nEgo4d: Around the world in 3,000 hours of egocentric\nvideo. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pages 18995â€“19012,\n2022.\nKristen Grauman, Andrew Westbury, Lorenzo Torresani,\nKris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar\nAshutosh, VÄ³ay Baiyya, Siddhant Bansal, Bikram Boote,\net al. Ego-exo4d: Understanding skilled human activity\nfrom first-and third-person perspectives. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 19383â€“19400, 2024.\nShibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting\nHu, Jason Weston, and Yuandong Tian. Training large\nlanguage models to reason in a continuous latent space.\narXiv preprint arXiv:2412.06769, 2024.\nDrew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and composi-\ntional question answering. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages\n6700â€“6709, 2019.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and\nTom Duerig. Scaling up visual and vision-language\nrepresentation learning with noisy text supervision. In\nInternational conference on machine learning, pages 4904â€“\n4916. PMLR, 2021.\nCÄ³o Jose, ThÃ©o Moutakanni, Dahyun Kang, Federico Bal-\ndassarre, TimothÃ©e Darcet, Hu Xu, Daniel Li, Marc\nSzafraniec, MichaÃ«l Ramamonjisoa, Maxime Oquab,\net al.\nDinov2 meets text: A unified framework for\nimage-and pixel-level vision-language alignment. In\nProceedings of the Computer Vision and Pattern Recognition\nConference, pages 24905â€“24916, 2025.\nJing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.\nGrounding language models to images for multimodal\ninputs and outputs. In International Conference on Machine\nLearning, pages 17283â€“17300. PMLR, 2023.\nYann LeCun. A path towards autonomous machine intelli-\ngence. Open Review, 62(1):1â€“62, 2022.\nHongyang Lei, Xiaolong Cheng, Qi Qin, Dan Wang,\nHuazhen Huang, Qingqing Gu, Yetao Wu, and Luo\nJi. M3-jepa: Multimodal alignment via multi-gate moe\nbased on the joint-embedding predictive architecture. In\nForty-second International Conference on Machine Learning,\n2025.\nChengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia,\nShaoguang Mao, Li Dong, Ivan VuliÄ‡, and Furu\nWei. Imagine while reasoning in space: Multimodal\nvisualization-of-thought. arXiv preprint arXiv:2501.07542,\n2025a.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models. In\nInternational conference on machine learning, pages 19730â€“\n19742. PMLR, 2023a.\nYifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin\nZhao, and Ji-Rong Wen. Evaluating object hallucina-\ntion in large vision-language models. arXiv preprint\narXiv:2305.10355, 2023b.\nYifan Li, Kun Zhou, Wayne Xin Zhao, Lei Fang, and Ji-Rong\nWen. Analyzing and mitigating object hallucination: A\ntraining bias perspective. arXiv preprint arXiv:2508.04567,\n2025b.\nBin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng\nJin, and Li Yuan. Video-llava: Learning united visual\nrepresentation by alignment before projection. In Pro-\nceedings of the 2024 Conference on Empirical Methods in\nNatural Language Processing, pages 5971â€“5984, 2024.\n12", "clean_text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language Sec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion agnostic representations. arXiv preprint arXiv:2308.11466, 2023. Zhengcong Fei, Mingyuan Fan, and Junshi Huang. A-jepa: Joint-embedding predictive architecture can listen. arXiv preprint arXiv:2311.15830, 2023. Enrico Fini, Mustafa Shukor, Xiujun Li, Philipp Dufter, Michal Klein, David Haldimann, Sai Aitharaju, Victor G Turrisi da Costa, Louis BÃ©thune, Zhe Gan, et al. Multimodal autoregressive pre-training of large vision encoders. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 9641â€“9654, 2025. Pascale Fung, Yoram Bachrach, Asli Celikyilmaz, Kamalika Chaudhuri, Delong Chen, Willy Chung, Emmanuel Dupoux, Hongyu Gong, HervÃ© JÃ©gou, Alessandro Lazaric, et al. Embodied ai agents: Modeling the world. arXiv preprint arXiv:2506.22355, 2025. Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. Advances in Neural Information Processing Systems, 36:27092â€“27112, 2023. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 18995â€“19012, 2022. Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, VÄ³ay Baiyya, Siddhant Bansal, Bikram Boote, et al. Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19383â€“19400, 2024. Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in a continuous latent space. arXiv preprint arXiv:2412.06769, 2024. Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6700â€“6709, 2019. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pages 4904â€“ 4916. PMLR, 2021. CÄ³o Jose, ThÃ©o Moutakanni, Dahyun Kang, Federico Baldassarre, TimothÃ©e Darcet, Hu Xu, Daniel Li, Marc Szafraniec, MichaÃ«l Ramamonjisoa, Maxime Oquab, et al. Dinov2 meets text: A unified framework for image-and pixel-level vision-language alignment. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 24905â€“24916, 2025. Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. Grounding language models to images for multimodal inputs and outputs. In International Conference on Machine Learning, pages 17283â€“17300. PMLR, 2023. Yann LeCun. A path towards autonomous machine intelligence. Open Review, 62(1):1â€“62, 2022. Hongyang Lei, Xiaolong Cheng, Qi Qin, Dan Wang, Huazhen Huang, Qingqing Gu, Yetao Wu, and Luo Ji. M3-jepa: Multimodal alignment via multi-gate moe based on the joint-embedding predictive architecture. In Forty-second International Conference on Machine Learning, 2025. Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan VuliÄ‡, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-of-thought. arXiv preprint arXiv:2501.07542, 2025a. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 19730â€“ 19742. PMLR, 2023a. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023b. Yifan Li, Kun Zhou, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. Analyzing and mitigating object hallucination: A training bias perspective. arXiv preprint arXiv:2508.04567, 2025b. Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 5971â€“5984, 2024. 12"}
{"pdf_id": "arxiv_251210942_vl_jepa", "page": 13, "text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language\nSec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion\nFan Liu, Delong Chen, Zhangqingyun Guan, Xiaocong\nZhou, Jiale Zhu, Qiaolin Ye, Liyong Fu, and Jun Zhou.\nRemoteclip: A vision language foundation model for\nremote sensing. IEEE Transactions on Geoscience and\nRemote Sensing, 62:1â€“16, 2024.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. Visual instruction tuning. Advances in neural infor-\nmation processing systems, 36:34892â€“34916, 2023.\nAndrÃ©s Marafioti, Orr Zohar, Miquel FarrÃ©, Merve Noyan,\nElie Bakouch, Pedro Cuenca, Cyril Zakka, Loubna Ben\nAllal, Anton Lozhkov, Nouamane Tazi, et al. Smolvlm:\nRedefining small and efficient multimodal models. arXiv\npreprint arXiv:2504.05299, 2025.\nJack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie\nPavlick. Linearly mapping from image to text space.\narXiv preprint arXiv:2209.15162, 2022.\nFionn Murtagh and Pedro Contreras. Algorithms for hier-\narchical clustering: an overview. Wiley interdisciplinary\nreviews: data mining and knowledge discovery, 2(1):86â€“97,\n2012.\nRui Qian, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Shuan-\ngrui Ding, Dahua Lin, and Jiaqi Wang. Streaming long\nvideo understanding with large language models. Ad-\nvances in Neural Information Processing Systems, 37:119336â€“\n119360, 2024.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.\nLearning transferable visual models from natural lan-\nguage supervision. In International conference on machine\nlearning, pages 8748â€“8763. PmLR, 2021.\nMustafa Shukor and Matthieu Cord. Skipping computa-\ntions in multimodal llms. arXiv preprint arXiv:2410.09454,\n2024.\nMustafa Shukor, Corentin Dancette, and Matthieu Cord.\nep-alm: Efficient perceptual augmentation of language\nmodels. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 22056â€“22069, 2023.\nMustafa Shukor, Dana Aubakirova, Francesco Capuano,\nPepÄ³n KooÄ³mans, Steven Palma, Adil Zouitine, Michel\nAractingi, Caroline Pascal, Martino Russi, Andres\nMarafioti, et al.\nSmolvla: A vision-language-action\nmodel for affordable and efficient robotics. arXiv preprint\narXiv:2506.01844, 2025.\nWenxuan Song, Jiayi Chen, Pengxiang Ding, Han Zhao,\nWei Zhao, Zhide Zhong, Zongyuan Ge, Jun Ma, and\nHaoang Li. Accelerating vision-language-action model\nintegrated with action chunking via parallel decoding.\narXiv preprint arXiv:2503.02310, 2025.\nBart Thomee, David A Shamma, Gerald Friedland, Ben-\njamin Elizalde, Karl Ni, Douglas Poland, Damian Borth,\nand Li-Jia Li. Yfcc100m: The new data in multimedia\nresearch. Communications of the ACM, 59(2):64â€“73, 2016.\nMichael Tschannen, Alexey Gritsenko, Xiao Wang, Muham-\nmad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil\nParthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil\nMustafa, et al. Siglip 2: Multilingual vision-language en-\ncoders with improved semantic understanding, localiza-\ntion, and dense features. arXiv preprint arXiv:2502.14786,\n2025.\nMaria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Es-\nlami, Oriol Vinyals, and Felix Hill. Multimodal few-shot\nlearning with frozen language models. Advances in\nNeural Information Processing Systems, 34:200â€“212, 2021.\nThÃ©ophane Vallaeys, Mustafa Shukor, Matthieu Cord, and\nJakob Verbeek. Improved baselines for data-efficient\nperceptual augmentation of llms. In European Conference\non Computer Vision, pages 369â€“387. Springer, 2024.\nPavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang\nLi, Cem Koc, Nate True, Albert Antony, Gokula San-\nthanam, James Gabriel, Peter Grasch, Oncel Tuzel, et al.\nFastvlm: Efficient vision encoding for vision language\nmodels. In Proceedings of the Computer Vision and Pattern\nRecognition Conference, pages 19769â€“19780, 2025.\nHenrique Schechter Vera, Sahil Dua, Biao Zhang, Daniel\nSalz, Ryan Mullins, Sindhu Raghuram Panyam, Sara\nSmoot, Iftekhar Naim, Joe Zou, Feiyang Chen, et al.\nEmbeddinggemma: Powerful and lightweight text rep-\nresentations. arXiv preprint arXiv:2509.20354, 2025.\nTongzhou Wang and Phillip Isola. Understanding con-\ntrastive representation learning through alignment and\nuniformity on the hypersphere. In International conference\non machine learning, pages 9929â€“9939. PMLR, 2020.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large\nlanguage models. Advances in neural information process-\ning systems, 35:24824â€“24837, 2022.\nJun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A\nlarge video description dataset for bridging video and\nlanguage. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 5288â€“5296, 2016.\nYuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo\nCui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao,\n13", "clean_text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language Sec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion Fan Liu, Delong Chen, Zhangqingyun Guan, Xiaocong Zhou, Jiale Zhu, Qiaolin Ye, Liyong Fu, and Jun Zhou. Remoteclip: A vision language foundation model for remote sensing. IEEE Transactions on Geoscience and Remote Sensing, 62:1â€“16, 2024. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:34892â€“34916, 2023. AndrÃ©s Marafioti, Orr Zohar, Miquel FarrÃ©, Merve Noyan, Elie Bakouch, Pedro Cuenca, Cyril Zakka, Loubna Ben Allal, Anton Lozhkov, Nouamane Tazi, et al. Smolvlm: Redefining small and efficient multimodal models. arXiv preprint arXiv:2504.05299, 2025. Jack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie Pavlick. Linearly mapping from image to text space. arXiv preprint arXiv:2209.15162, 2022. Fionn Murtagh and Pedro Contreras. Algorithms for hierarchical clustering: an overview. Wiley interdisciplinary reviews: data mining and knowledge discovery, 2(1):86â€“97, 2012. Rui Qian, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Shuangrui Ding, Dahua Lin, and Jiaqi Wang. Streaming long video understanding with large language models. Advances in Neural Information Processing Systems, 37:119336â€“ 119360, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748â€“8763. PmLR, 2021. Mustafa Shukor and Matthieu Cord. Skipping computations in multimodal llms. arXiv preprint arXiv:2410.09454, 2024. Mustafa Shukor, Corentin Dancette, and Matthieu Cord. ep-alm: Efficient perceptual augmentation of language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22056â€“22069, 2023. Mustafa Shukor, Dana Aubakirova, Francesco Capuano, PepÄ³n KooÄ³mans, Steven Palma, Adil Zouitine, Michel Aractingi, Caroline Pascal, Martino Russi, Andres Marafioti, et al. Smolvla: A vision-language-action model for affordable and efficient robotics. arXiv preprint arXiv:2506.01844, 2025. Wenxuan Song, Jiayi Chen, Pengxiang Ding, Han Zhao, Wei Zhao, Zhide Zhong, Zongyuan Ge, Jun Ma, and Haoang Li. Accelerating vision-language-action model integrated with action chunking via parallel decoding. arXiv preprint arXiv:2503.02310, 2025. Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):64â€“73, 2016. Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems, 34:200â€“212, 2021. ThÃ©ophane Vallaeys, Mustafa Shukor, Matthieu Cord, and Jakob Verbeek. Improved baselines for data-efficient perceptual augmentation of llms. In European Conference on Computer Vision, pages 369â€“387. Springer, 2024. Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokula Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, et al. Fastvlm: Efficient vision encoding for vision language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 19769â€“19780, 2025. Henrique Schechter Vera, Sahil Dua, Biao Zhang, Daniel Salz, Ryan Mullins, Sindhu Raghuram Panyam, Sara Smoot, Iftekhar Naim, Joe Zou, Feiyang Chen, et al. Embeddinggemma: Powerful and lightweight text representations. arXiv preprint arXiv:2509.20354, 2025. Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International conference on machine learning, pages 9929â€“9939. PMLR, 2020. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824â€“24837, 2022. Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288â€“5296, 2016. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, 13"}
{"pdf_id": "arxiv_251210942_vl_jepa", "page": 14, "text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language\nSec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion\nZhihui He, et al. Minicpm-v: A gpt-4v level mllm on\nyour phone. arXiv preprint arXiv:2408.01800, 2024.\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and\nLucas Beyer.\nSigmoid loss for language image pre-\ntraining. In Proceedings of the IEEE/CVF international\nconference on computer vision, pages 11975â€“11986, 2023.\nGaoyue Zhou, Hengkai Pan, Yann LeCun, and Lerrel Pinto.\nDino-wm: World models on pre-trained visual features\nenable zero-shot planning. In Forty-second International\nConference on Machine Learning, 2025.\nLuowei Zhou, Chenliang Xu, and Jason Corso. Towards\nautomatic learning of procedures from web instructional\nvideos. In Proceedings of the AAAI conference on artificial\nintelligence, volume 32, 2018.\nXingyi Zhou, Anurag Arnab, Shyamal Buch, Shen Yan,\nAustin Myers, Xuehan Xiong, Arsha Nagrani, and\nCordelia Schmid. Streaming dense video captioning.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 18243â€“18252, 2024.\nDimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gok-\nberk Cinbis, David Fouhey, Ivan Laptev, and Josef Sivic.\nCross-task weakly supervised learning from instruc-\ntional videos. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 3537â€“\n3545, 2019.\n14", "clean_text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language Sec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion Zhihui He, et al. Minicpm-v: A gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pretraining. In Proceedings of the IEEE/CVF international conference on computer vision, pages 11975â€“11986, 2023. Gaoyue Zhou, Hengkai Pan, Yann LeCun, and Lerrel Pinto. Dino-wm: World models on pre-trained visual features enable zero-shot planning. In Forty-second International Conference on Machine Learning, 2025. Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. Xingyi Zhou, Anurag Arnab, Shyamal Buch, Shen Yan, Austin Myers, Xuehan Xiong, Arsha Nagrani, and Cordelia Schmid. Streaming dense video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18243â€“18252, 2024. Dimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gokberk Cinbis, David Fouhey, Ivan Laptev, and Josef Sivic. Cross-task weakly supervised learning from instructional videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3537â€“ 3545, 2019. 14"}
