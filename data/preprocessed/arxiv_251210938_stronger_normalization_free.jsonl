{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 1, "text": "Stronger Normalization-Free Transformers\nMingzhi Chen1\nTaiming Lu1\nJiachen Zhu2\nMingjie Sun3\nZhuang Liu1\n1Princeton University\n2NYU\n3Carnegie Mellon University\n...\n82.8%\n82.5%\n82.4%\n82.3%\n82.2%\n82.2%\n81.6%\n81.6%\n...\nViT ImageNet acc\nreplace with\nPoint-wise functions\npoint-wise functions\nNorm Layer\nAttention / FFN\n-0.5\n0.5\n-1\n1\n0\n-2\n-1\n0\n1\n2\na) We search point-wise functions of different shapes as norm layer replacement.\nLayerNorm:\nğ‘¥âˆ’ğœ‡\nâˆš\nğœ2+ğœ–\nDyT: tanh(ğ›¼ğ‘¥)\nDerf: erf(ğ›¼ğ‘¥+ ğ‘ )\nb) Formulation of LayerNorm (LN), DyT, and Derf (ours).\nmethod\nViT acc (â†‘)\nDiT FID (â†“)\nDNA acc (â†‘)\nLN\n82.3%\n45.91\n86.9%\nDyT\n82.5%\n45.66\n86.9%\nDerf\n82.8%\n43.94\n87.3%\nc) Performance across domains.\nFigure 1 We introduce Dynamic erf (Derf), a point-wise function, that outperforms normalization layers and\nother point-wise functions. (a) We identify the feasible function shape for replacing the normalization layer and\npropose a large set of point-wise functions within this space. Evaluating all candidates, we identify and introduce\nDerf as the strongest choice. (b) LayerNorm, DyT (Zhu et al., 2025), and Derf operate in fundamentally different\nways: with channels ğ¶and tokens ğ‘‡, LayerNorm normalizes each channel across the token axis, whereas DyT and\nDerf apply independent scalar mappings to each element. (c) Across ImagenNet-1K classification and generation,\nand DNA sequence modeling, Derf consistently outperforms LayerNorm and DyT. Derf demonstrates that a\npoint-wise function can not only replace normalization but also surpass it.\nAbstract\nAlthough normalization layers have long been viewed as indispensable components of deep learning\narchitectures, the recent introduction of Dynamic Tanh (DyT) (Zhu et al., 2025) has demonstrated\nthat alternatives are possible. The point-wise function DyT constrains extreme values for stable\nconvergence and reaches normalization-level performance; this work seeks further for function designs\nthat can surpass it. We first study how the intrinsic properties of point-wise functions influence\ntraining and performance. Building on these findings, we conduct a large-scale search for a more\neffective function design. Through this exploration, we introduce Derf(ğ‘¥) = erf(ğ›¼ğ‘¥+ ğ‘ ), where erf(ğ‘¥)\nis the rescaled Gaussian cumulative distribution function, and identify it as the most performant\ndesign. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including\nvisual recognition and generation, speech representation, and DNA sequence modeling. Our findings\nsuggest that the performance gains of Derf largely stem from its improved generalization rather than\nstronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for\nnormalization-free Transformer architectures. Our code is available at this link.\n1\narXiv:2512.10938v1  [cs.LG]  11 Dec 2025", "clean_text": "Stronger Normalization-Free Transformers Mingzhi Chen1 Taiming Lu1 Jiachen Zhu2 Mingjie Sun3 Zhuang Liu1 1Princeton University 2NYU 3Carnegie Mellon University ... 82.8% 82.5% 82.4% 82.3% 82.2% 82.2% 81.6% 81.6% ... ViT ImageNet acc replace with Point-wise functions point-wise functions Norm Layer Attention / FFN -0.5 0.5 -1 1 0 -2 -1 0 1 2 a) We search point-wise functions of different shapes as norm layer replacement. LayerNorm: ğ‘¥âˆ’ğœ‡ âˆš ğœ2+ğœ– DyT: tanh(ğ›¼ğ‘¥) Derf: erf(ğ›¼ğ‘¥+ ğ‘ ) b) Formulation of LayerNorm (LN), DyT, and Derf (ours). method ViT acc (â†‘) DiT FID (â†“) DNA acc (â†‘) LN 82.3% 45.91 86.9% DyT 82.5% 45.66 86.9% Derf 82.8% 43.94 87.3% c) Performance across domains. Figure 1 We introduce Dynamic erf (Derf), a point-wise function, that outperforms normalization layers and other point-wise functions. (a) We identify the feasible function shape for replacing the normalization layer and propose a large set of point-wise functions within this space. Evaluating all candidates, we identify and introduce Derf as the strongest choice. (b) LayerNorm, DyT (Zhu et al., 2025), and Derf operate in fundamentally different ways: with channels ğ¶and tokens ğ‘‡, LayerNorm normalizes each channel across the token axis, whereas DyT and Derf apply independent scalar mappings to each element. (c) Across ImagenNet-1K classification and generation, and DNA sequence modeling, Derf consistently outperforms LayerNorm and DyT. Derf demonstrates that a point-wise function can not only replace normalization but also surpass it. Abstract Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) (Zhu et al., 2025) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce Derf(ğ‘¥) = erf(ğ›¼ğ‘¥+ ğ‘ ), where erf(ğ‘¥) is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including visual recognition and generation, speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures. Our code is available at this link. 1 arXiv:2512.10938v1 [cs.LG] 11 Dec 2025"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 2, "text": "1 Introduction\nNormalization layers have become a critical component in modern deep neural networks. Since the invention\nof Batch Normalization (Ioffe and Szegedy, 2015), more and more variants have been developed to adapt\nnormalization to various architectures and model modalities (Ba et al., 2016; Salimans and Kingma, 2016;\nUlyanov et al., 2016; Wu and He, 2018; Zhang and Sennrich, 2019). By regulating the distribution of\nintermediate activations, normalization layers have long demonstrated their strong capability in stabilizing\ntraining and accelerating model convergence (Santurkar et al., 2018; Bjorck et al., 2018).\nDue to the inherent formulation of normalization layers, they heavily rely on activation statistics during\ntraining. This introduces additional memory access and synchronization overhead (Zhang and Sennrich,\n2019; Chen et al., 2020; Yang et al., 2022). Moreover, some normalization methods are highly sensitive to\nbatch size, and inappropriate batch settings can lead to unstable training (Wu and He, 2018; Lian and Liu,\n2019; Singh and Krishnan, 2020). These issues motivate recent efforts to develop normalization-free methods.\nAmong these attempts, Dynamic Tanh (Zhu et al., 2025), an S-shaped point-wise function, has emerged as a\nsimple yet effective drop-in replacement for normalization layers. This work has established the foundation\nfor point-wise functions that match the performance of normalization layers, yet functions that can surpass\nthem remain unexplored. In this work, we aim to discover point-wise functions that outperform normalization\nlayers to push toward stronger Transformer architectures (Vaswani et al., 2017; Dosovitskiy, 2021).\nWe first systematically study how the intrinsic properties of point-wise functions affect the training dynamics\nand final performance. Specifically, we focus on four fundamental and representative function properties:\nzero-centeredness, boundedness, center sensitivity, and monotonicity. Each property is independently examined\nthrough controlled experiments on a diverse set of point-wise functions to assess its impact on the training\nresult. This analysis isolates a subset of point-wise functions as effective normalization replacements and\nyields a concrete design principle for normalization-free Transformers.\nGuided by these principles, we identify a set of promising point-wise functions that have the potential to\nsurpass the performance of normalization layers. Within this set, we empirically search for the optimal\ndesigns, among which Dynamic erf (Derf) emerges as a simple yet the most performant function (Figure 1a).\nDerf augments erf(ğ‘¥) with learnable parameters, where the error function erf(ğ‘¥) is an S-shaped, rescaled\ncumulative distribution of a standard Gaussian around zero.\nWe evaluate Derf spanning multiple modalities (vision, language, speech, and DNA sequences); covering various\ntasks (classification, generation, and sequence modeling), under different training paradigms (supervised and\nself-supervised). Across all these settings, Derf consistently surpasses LayerNorm, RMSNorm, and Dynamic\nTanh (Figure 1b). To pinpoint the source of these gains, we measure the training loss in evaluation mode\nafter optimization. Derf exhibits higher training loss than normalization-based models, indicating that its\nsuperior performance stems from stronger generalization rather than enhanced fitting capacity. Overall, our\nwork demonstrates that well-designed point-wise functions can outperform normalization layers.\n2 Background\nNormalization layers. Normalization layers have become pivotal components of modern neural networks.\nAmong the various normalization techniques, Batch Normalization (BN) (Ioffe and Szegedy, 2015), Layer\nNormalization (LN) (Ba et al., 2016), and Root Mean Square Normalization (RMSNorm) (Zhang and Sennrich,\n2019) are the three most widely used in deep learning models.\nğ‘¦= ğ›¾*\nğ‘¥âˆ’ğœ‡\nâˆš\nğœ2 + ğœ–\n+ ğ›½\n(1)\nAll normalization methods adhere to a unified paradigm, formalized in Equation 1, where activations within\neach group are centered and scaled by their mean ğœ‡and standard deviation ğœ(with ğœ–for numerical stability)\nto maintain consistent scale and stable gradient flow. The main distinction among different normalization\nmethods lies in how the activations are grouped when computing ğœ‡and ğœ. For example, LN computes the\nstatistics along the channel dimension for each token independently. Given a token representation ğ‘¥âˆˆRğ¶, the\nmean and variance are computed as Equation 2, where ğ¶denotes the number of hidden features (channels).\n2", "clean_text": "1 Introduction Normalization layers have become a critical component in modern deep neural networks. Since the invention of Batch Normalization (Ioffe and Szegedy, 2015), more and more variants have been developed to adapt normalization to various architectures and model modalities (Ba et al., 2016; Salimans and Kingma, 2016; Ulyanov et al., 2016; Wu and He, 2018; Zhang and Sennrich, 2019). By regulating the distribution of intermediate activations, normalization layers have long demonstrated their strong capability in stabilizing training and accelerating model convergence (Santurkar et al., 2018; Bjorck et al., 2018). Due to the inherent formulation of normalization layers, they heavily rely on activation statistics during training. This introduces additional memory access and synchronization overhead (Zhang and Sennrich, 2019; Chen et al., 2020; Yang et al., 2022). Moreover, some normalization methods are highly sensitive to batch size, and inappropriate batch settings can lead to unstable training (Wu and He, 2018; Lian and Liu, 2019; Singh and Krishnan, 2020). These issues motivate recent efforts to develop normalization-free methods. Among these attempts, Dynamic Tanh (Zhu et al., 2025), an S-shaped point-wise function, has emerged as a simple yet effective drop-in replacement for normalization layers. This work has established the foundation for point-wise functions that match the performance of normalization layers, yet functions that can surpass them remain unexplored. In this work, we aim to discover point-wise functions that outperform normalization layers to push toward stronger Transformer architectures (Vaswani et al., 2017; Dosovitskiy, 2021). We first systematically study how the intrinsic properties of point-wise functions affect the training dynamics and final performance. Specifically, we focus on four fundamental and representative function properties: zero-centeredness, boundedness, center sensitivity, and monotonicity. Each property is independently examined through controlled experiments on a diverse set of point-wise functions to assess its impact on the training result. This analysis isolates a subset of point-wise functions as effective normalization replacements and yields a concrete design principle for normalization-free Transformers. Guided by these principles, we identify a set of promising point-wise functions that have the potential to surpass the performance of normalization layers. Within this set, we empirically search for the optimal designs, among which Dynamic erf (Derf) emerges as a simple yet the most performant function (Figure 1a). Derf augments erf(ğ‘¥) with learnable parameters, where the error function erf(ğ‘¥) is an S-shaped, rescaled cumulative distribution of a standard Gaussian around zero. We evaluate Derf spanning multiple modalities (vision, language, speech, and DNA sequences); covering various tasks (classification, generation, and sequence modeling), under different training paradigms (supervised and self-supervised). Across all these settings, Derf consistently surpasses LayerNorm, RMSNorm, and Dynamic Tanh (Figure 1b). To pinpoint the source of these gains, we measure the training loss in evaluation mode after optimization. Derf exhibits higher training loss than normalization-based models, indicating that its superior performance stems from stronger generalization rather than enhanced fitting capacity. Overall, our work demonstrates that well-designed point-wise functions can outperform normalization layers. 2 Background Normalization layers. Normalization layers have become pivotal components of modern neural networks. Among the various normalization techniques, Batch Normalization (BN) (Ioffe and Szegedy, 2015), Layer Normalization (LN) (Ba et al., 2016), and Root Mean Square Normalization (RMSNorm) (Zhang and Sennrich, 2019) are the three most widely used in deep learning models. ğ‘¦= ğ›¾* ğ‘¥âˆ’ğœ‡ âˆš ğœ2 + ğœ– + ğ›½ (1) All normalization methods adhere to a unified paradigm, formalized in Equation 1, where activations within each group are centered and scaled by their mean ğœ‡and standard deviation ğœ(with ğœ–for numerical stability) to maintain consistent scale and stable gradient flow. The main distinction among different normalization methods lies in how the activations are grouped when computing ğœ‡and ğœ. For example, LN computes the statistics along the channel dimension for each token independently. Given a token representation ğ‘¥âˆˆRğ¶, the mean and variance are computed as Equation 2, where ğ¶denotes the number of hidden features (channels). 2"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 3, "text": "Zero-centered\nCentered\nNot centered\nBounded\nBounded\nNot bounded\nCenter Sensitive\nSensitive\nNot sensitive\nMonotonic\nMonotonic\nNot monotonic\nFigure 2 Key properties of point-wise function. The four properties: zero-centeredness, boundedness, center sensitivity,\nand monotonicity collectively characterize functional behavior on activations and influence training dynamics. Blue\ncurves represent functions that satisfy each property, while red curves violate them.\nDue to its per-token normalization, LN is particularly well-suited for Transformer architectures, where\nactivations across tokens exhibit diverse statistics.\nğœ‡= 1\nğ¶\nğ¶\nâˆ‘ï¸\nğ‘˜=1\nğ‘¥ğ‘˜,\nğœ2 = 1\nğ¶\nğ¶\nâˆ‘ï¸\nğ‘˜=1\n(ğ‘¥ğ‘˜âˆ’ğœ‡)2,\n(2)\nPoint-wise functions. The strong reliance of normalization layers on activation statistics has motivated\nfurther exploration of statistics-free methods (He and Hofmann, 2024; Heimersheim, 2024; Jha and Reagen,\n2024; Zhu et al., 2025). Among these approaches, point-wise functions (Zhu et al., 2025) have emerged as\nsimple yet effective alternatives to traditional normalization methods. Unlike normalization, a point-wise\nfunction applies the same parametric mapping ğ‘“(ğ‘¥; ğœƒ) to each activation independently. The parameters ğœƒ\nare fixed or learned, rather than being computed from batch-, token-, or channel-level statistics. A recent\nstudy (Zhu et al., 2025) introduces the Dynamic Tanh (DyT) function (Equation 3), where ğ›¼is a learnable\nparameter. This design is motivated by the observation that Layer Normalization often produces an S-shaped\ninput-output mapping in practice. The saturating nature of the tanh function squashes extreme activations,\nthereby fulfilling a role analogous to the re-centering and re-scaling effects of normalization layers.\nDyT(ğ‘¥) = ğ›¾* tanh(ğ›¼ğ‘¥) + ğ›½\n(3)\nWhile DyT has shown similar empirical performance to normalization layers across various Transformer\narchitectures, a comprehensive analysis of the design space for these statistics-free operators remains missing.\nIn this work, we target at the optimal form of the point-wise function as normalization replacement. We identify\nthe function properties crucial for convergence and performance, and then we introduce Derf, a point-wise\nfunction consistently surpassing normalization layers rather than merely matching their performance.\n3 Function Property Analysis\nTraining Transformers without normalization requires understanding the factors that make a point-wise\nfunction stable and effective as a replacement. In this section, we examine four essential properties: zero-\ncenteredness, boundedness, center sensitivity, and monotonicity (see Figure 2). These properties collectively\ncharacterize the fundamental shape of point-wise functions and their behavior on activations. By isolating the\nimpact of each property, we explore its influence on optimization and final performance.\nTo investigate these properties, we replace each normalization layer with a point-wise function of the form:\nğ‘¦= ğ›¾Â· ğ‘“(ğ›¼ğ‘¥) + ğ›½,\n(4)\nwhere ğ‘“(Â·) denotes the chosen base function with learnable ğ›¼rescaling the input. ğ›¾and ğ›½are affine parameters,\nsimilar to those in normalization layers. We begin with three base functions: tanh(ğ‘¥), erf(ğ‘¥), and arctan(ğ‘¥).\nIn subsequent experiments, we modify these functions with controlled transformations to examine the impact\nof each property. All experiments are conducted with ViT-Base (Dosovitskiy, 2021), and top-1 accuracy on\nImageNet-1K (Deng et al., 2009) is reported. In Appendix A, we provide more detailed training results.\n3", "clean_text": "Zero-centered Centered Not centered Bounded Bounded Not bounded Center Sensitive Sensitive Not sensitive Monotonic Monotonic Not monotonic Figure 2 Key properties of point-wise function. The four properties: zero-centeredness, boundedness, center sensitivity, and monotonicity collectively characterize functional behavior on activations and influence training dynamics. Blue curves represent functions that satisfy each property, while red curves violate them. Due to its per-token normalization, LN is particularly well-suited for Transformer architectures, where activations across tokens exhibit diverse statistics. ğœ‡= 1 ğ¶ ğ¶ âˆ‘ï¸ ğ‘˜=1 ğ‘¥ğ‘˜, ğœ2 = 1 ğ¶ ğ¶ âˆ‘ï¸ ğ‘˜=1 (ğ‘¥ğ‘˜âˆ’ğœ‡)2, (2) Point-wise functions. The strong reliance of normalization layers on activation statistics has motivated further exploration of statistics-free methods (He and Hofmann, 2024; Heimersheim, 2024; Jha and Reagen, 2024; Zhu et al., 2025). Among these approaches, point-wise functions (Zhu et al., 2025) have emerged as simple yet effective alternatives to traditional normalization methods. Unlike normalization, a point-wise function applies the same parametric mapping ğ‘“(ğ‘¥; ğœƒ) to each activation independently. The parameters ğœƒ are fixed or learned, rather than being computed from batch-, token-, or channel-level statistics. A recent study (Zhu et al., 2025) introduces the Dynamic Tanh (DyT) function (Equation 3), where ğ›¼is a learnable parameter. This design is motivated by the observation that Layer Normalization often produces an S-shaped input-output mapping in practice. The saturating nature of the tanh function squashes extreme activations, thereby fulfilling a role analogous to the re-centering and re-scaling effects of normalization layers. DyT(ğ‘¥) = ğ›¾* tanh(ğ›¼ğ‘¥) + ğ›½ (3) While DyT has shown similar empirical performance to normalization layers across various Transformer architectures, a comprehensive analysis of the design space for these statistics-free operators remains missing. In this work, we target at the optimal form of the point-wise function as normalization replacement. We identify the function properties crucial for convergence and performance, and then we introduce Derf, a point-wise function consistently surpassing normalization layers rather than merely matching their performance. 3 Function Property Analysis Training Transformers without normalization requires understanding the factors that make a point-wise function stable and effective as a replacement. In this section, we examine four essential properties: zerocenteredness, boundedness, center sensitivity, and monotonicity (see Figure 2). These properties collectively characterize the fundamental shape of point-wise functions and their behavior on activations. By isolating the impact of each property, we explore its influence on optimization and final performance. To investigate these properties, we replace each normalization layer with a point-wise function of the form: ğ‘¦= ğ›¾Â· ğ‘“(ğ›¼ğ‘¥) + ğ›½, (4) where ğ‘“(Â·) denotes the chosen base function with learnable ğ›¼rescaling the input. ğ›¾and ğ›½are affine parameters, similar to those in normalization layers. We begin with three base functions: tanh(ğ‘¥), erf(ğ‘¥), and arctan(ğ‘¥). In subsequent experiments, we modify these functions with controlled transformations to examine the impact of each property. All experiments are conducted with ViT-Base (Dosovitskiy, 2021), and top-1 accuracy on ImageNet-1K (Deng et al., 2009) is reported. In Appendix A, we provide more detailed training results. 3"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 4, "text": "3.1 Zero-centeredness\nZero-centeredness means that the functionâ€™s outputs are balanced around zero, with positive and negative\nvalues of similar magnitude and symmetry. Because normalization layers inherently recenter activations to the\norigin for stabilizing gradients, maintaining this property could reduce internal covariate shifts and promote\nsmoother gradient flow during training.\nSetup. Under the ViT setup, we manipulate the centering of the functions. For each base function, we\nconsider two types of shifts: horizontal and vertical, defined in Equation 5. In this form, ğœ†horiz and ğœ†vert\nrespectively denote the magnitudes of horizontal and vertical shifts. For both types of shifts, we vary ğœ†over\n{Â± 1\n2, Â±1, Â±2} to examine how increasing deviation from zero-centeredness affects the functionâ€™s behavior. All\nother training settings remain unchanged.\nğ‘“horiz(ğ‘¥) = ğ‘“(ğ‘¥+ ğœ†horiz),\nğ‘“vert(ğ‘¥) = ğ‘“(ğ‘¥) + ğœ†vert,\n(5)\nResults. As shown in Table 1, the results are consistent across different base functions: for horizontal shifts,\nperformance remains largely comparable to the zero-centered base function when |ğœ†horiz| â‰¤0.5. However, as\n|ğœ†horiz| increases, performance gradually degrades, and training diverges when |ğœ†horiz| â‰¥2. Similarly, vertical\nshifts consistently lead to a decline in performance as |ğœ†vert| grows with training failure once |ğœ†vert| â‰¥2. These\nresults show that zero-centeredness is a requirement for stable convergence and effective training.\nfunction\nshift type\n-2\n-1\n-0.5\n-0.1\nğœ†= 0\n+0.1\n+0.5\n+1\n+2\nerf(ğ‘¥)\nhorizontal\nÃ—\n82.0%\n82.5%\n82.6%\n82.6%\n82.7%\n82.5%\n82.1%\nÃ—\nvertical\nÃ—\n81.8%\n82.3%\n82.4%\n82.6%\n82.5%\n82.3%\n81.6%\nÃ—\ntanh(ğ‘¥)\nhorizontal\nÃ—\n82.1%\n82.5%\n82.6%\n82.5%\n82.6%\n82.4%\n82.2%\nÃ—\nvertical\nÃ—\n81.5%\n81.9%\n82.4%\n82.5%\n82.3%\n81.9%\n81.4%\nÃ—\narctan(ğ‘¥)\nhorizontal\nÃ—\n81.9%\n82.3%\n82.3%\n82.3%\n82.4%\n82.2%\n82.0%\nÃ—\nvertical\nÃ—\n81.4%\n81.9%\n82.2%\n82.3%\n82.3%\n82.0%\n81.2%\nÃ—\nTable 1 Results of zero-centeredness on ViT-Base. Horizontal shift corresponds to modifying the input as ğ‘“(ğ›¼ğ‘¥Â± ğœ†),\nwhile vertical shift adds or subtracts a constant to the output as ğ‘“(ğ›¼ğ‘¥) Â± ğœ†. â€œÃ—â€ indicates training failure.\n3.2 Boundedness\nBoundedness refers to the property of a function whose output is constrained within a finite range. Formally,\na function ğ‘“(Â·) is bounded if there exist constants ğ‘, ğ‘âˆˆR such that ğ‘â‰¤ğ‘“(ğ‘¥) â‰¤ğ‘for all ğ‘¥in its domain. This\nensures that activations remain finite and do not accumulate variance across layers. Unbounded functions, in\ncontrast, may induce signal explosion and gradient instability.\nSetup. Under the same ViT setup, we study the role of boundedness with two methods. Firstly, we select\nthree inherently unbounded S-shaped functions (e.g., arcsinh(ğ‘¥)) and compare them with their clamped\nversions shown in Equation 6, where ğ‘“ğ‘¢(ğ‘¥) denotes the unbounded point-wise function, and ğœ†is a chosen\nvalue specifying the clipping range.\nğ‘¦= clip(ğ‘“ğ‘¢(ğ‘¥), âˆ’ğœ†ğ‘¢, ğœ†ğ‘¢),\n(6)\nSecondly, we gradually transition bounded functions (e.g., erf(ğ‘¥)) toward unbounded linear form, defined in\nEquation 7, where ğ‘“ğ‘denotes a bounded point-wise function, and ğœ†controls how quickly the function becomes\nunbounded. We vary ğœ†ğ‘¢over {0.5, 0.8, 1.0, 2.0, 3.0, 5.0} in the first method and ğœ†ğ‘over {0.01, 0.1, 0.5} for the\nsecond. The original unmodified function is also included as a baseline.\nğ‘¦= (1 âˆ’ğœ†)ğ‘“ğ‘(ğ‘¥) + ğœ†ğ‘ğ‘¥,\nğœ†ğ‘âˆˆ(0, 1).\n(7)\nResults. For the first method, among the three unbounded functions in Table 2, only arcsinh(ğ‘¥) and logsign(ğ‘¥)\nconverge effectively, while linear(ğ‘¥) does not. For the convergent functions, their clipped versions consistently\n4", "clean_text": "3.1 Zero-centeredness Zero-centeredness means that the functionâ€™s outputs are balanced around zero, with positive and negative values of similar magnitude and symmetry. Because normalization layers inherently recenter activations to the origin for stabilizing gradients, maintaining this property could reduce internal covariate shifts and promote smoother gradient flow during training. Setup. Under the ViT setup, we manipulate the centering of the functions. For each base function, we consider two types of shifts: horizontal and vertical, defined in Equation 5. In this form, ğœ†horiz and ğœ†vert respectively denote the magnitudes of horizontal and vertical shifts. For both types of shifts, we vary ğœ†over {Â± 1 2, Â±1, Â±2} to examine how increasing deviation from zero-centeredness affects the functionâ€™s behavior. All other training settings remain unchanged. ğ‘“horiz(ğ‘¥) = ğ‘“(ğ‘¥+ ğœ†horiz), ğ‘“vert(ğ‘¥) = ğ‘“(ğ‘¥) + ğœ†vert, (5) Results. As shown in Table 1, the results are consistent across different base functions: for horizontal shifts, performance remains largely comparable to the zero-centered base function when |ğœ†horiz| â‰¤0.5. However, as |ğœ†horiz| increases, performance gradually degrades, and training diverges when |ğœ†horiz| â‰¥2. Similarly, vertical shifts consistently lead to a decline in performance as |ğœ†vert| grows with training failure once |ğœ†vert| â‰¥2. These results show that zero-centeredness is a requirement for stable convergence and effective training. function shift type -2 -1 -0.5 -0.1 ğœ†= 0 +0.1 +0.5 +1 +2 erf(ğ‘¥) horizontal Ã— 82.0% 82.5% 82.6% 82.6% 82.7% 82.5% 82.1% Ã— vertical Ã— 81.8% 82.3% 82.4% 82.6% 82.5% 82.3% 81.6% Ã— tanh(ğ‘¥) horizontal Ã— 82.1% 82.5% 82.6% 82.5% 82.6% 82.4% 82.2% Ã— vertical Ã— 81.5% 81.9% 82.4% 82.5% 82.3% 81.9% 81.4% Ã— arctan(ğ‘¥) horizontal Ã— 81.9% 82.3% 82.3% 82.3% 82.4% 82.2% 82.0% Ã— vertical Ã— 81.4% 81.9% 82.2% 82.3% 82.3% 82.0% 81.2% Ã— Table 1 Results of zero-centeredness on ViT-Base. Horizontal shift corresponds to modifying the input as ğ‘“(ğ›¼ğ‘¥Â± ğœ†), while vertical shift adds or subtracts a constant to the output as ğ‘“(ğ›¼ğ‘¥) Â± ğœ†. â€œÃ—â€ indicates training failure. 3.2 Boundedness Boundedness refers to the property of a function whose output is constrained within a finite range. Formally, a function ğ‘“(Â·) is bounded if there exist constants ğ‘, ğ‘âˆˆR such that ğ‘â‰¤ğ‘“(ğ‘¥) â‰¤ğ‘for all ğ‘¥in its domain. This ensures that activations remain finite and do not accumulate variance across layers. Unbounded functions, in contrast, may induce signal explosion and gradient instability. Setup. Under the same ViT setup, we study the role of boundedness with two methods. Firstly, we select three inherently unbounded S-shaped functions (e.g., arcsinh(ğ‘¥)) and compare them with their clamped versions shown in Equation 6, where ğ‘“ğ‘¢(ğ‘¥) denotes the unbounded point-wise function, and ğœ†is a chosen value specifying the clipping range. ğ‘¦= clip(ğ‘“ğ‘¢(ğ‘¥), âˆ’ğœ†ğ‘¢, ğœ†ğ‘¢), (6) Secondly, we gradually transition bounded functions (e.g., erf(ğ‘¥)) toward unbounded linear form, defined in Equation 7, where ğ‘“ğ‘denotes a bounded point-wise function, and ğœ†controls how quickly the function becomes unbounded. We vary ğœ†ğ‘¢over {0.5, 0.8, 1.0, 2.0, 3.0, 5.0} in the first method and ğœ†ğ‘over {0.01, 0.1, 0.5} for the second. The original unmodified function is also included as a baseline. ğ‘¦= (1 âˆ’ğœ†)ğ‘“ğ‘(ğ‘¥) + ğœ†ğ‘ğ‘¥, ğœ†ğ‘âˆˆ(0, 1). (7) Results. For the first method, among the three unbounded functions in Table 2, only arcsinh(ğ‘¥) and logsign(ğ‘¥) converge effectively, while linear(ğ‘¥) does not. For the convergent functions, their clipped versions consistently 4"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 5, "text": "3\n6\n9\n12\n15\n18\n6\n12\n18\n0\nlinear(x)\npower23(x)\nlogquad(x)\narcsinh(x)\nlogsign(x)\nFigure 3 Visualization of several unbounded point-wise functions on the\npositive half-axis, illustrating their different growth rates. ğ‘ğ‘Ÿğ‘ğ‘ ğ‘–ğ‘›â„(ğ‘¥)\nrefers to its standard analytical form. The remaining functions are defined\nas linear(ğ‘¥) = ğ‘¥, power23(ğ‘¥) = ğ‘¥\n2\n3 , logsign(ğ‘¥) = sign(ğ‘¥) ln(|ğ‘¥| + 1),\nsmoothsign(ğ‘¥) =\nğ‘¥\n1+|ğ‘¥|, and logquad(ğ‘¥) = sign(ğ‘¥) ln(ğ‘¥2+1). Among them,\nlogquad(ğ‘¥) shows the fastest growth that still ensures stable convergence.\nğœ†ğ‘¢\narcsinh(ğ‘¥)\nlogsign(ğ‘¥)\nlinear(ğ‘¥)\nâˆ’\n82.2%\n82.2%\nÃ—\n0.5\n82.3%\n82.4%\n82.1%\n0.8\n82.3%\n82.4%\n82.2%\n1.0\n82.4%\n82.4%\n82.2%\n2.0\n82.4%\n82.4%\n82.1%\n3.0\n82.4%\n82.3%\n82.1%\n5.0\n82.3%\n82.3%\n82.0%\nTable 2 Results of clamping for bound-\nedness on ViT-Base. Clipped version of\nunbounded functions consistently achieves\nbetter performance than unbounded base-\nlines. â€œâˆ’â€ denotes the original unmodified\nfunction. â€œÃ—â€ indicates training failure.\noutperform the unbounded baselines across all tested ğœ†values. These results indicate that incorporating\nboundedness can improve optimization and result in better performance. For the second, as shown in Table 3,\nthe results are consistent with clipping the intrinsic unbounded functions: the unbounded variant yields\nslightly lower accuracy than the bounded baseline.\nğœ†ğ‘\nerf(ğ‘¥)\ntanh(ğ‘¥)\narctan(ğ‘¥)\nisru(ğ‘¥)\nâˆ’\n82.6%\n82.5%\n82.4%\n82.3%\n0.01\n82.4%\n82.4%\n82.1%\n82.2%\n0.1\n82.3%\n82.3%\n82.1%\n82.1%\n0.5\nÃ—\nÃ—\nÃ—\nÃ—\nTable 3 Results of removing boundedness on ViT-Base. Performance decreases as the function is less bounded. â€œâˆ’â€\ndenotes the original function without modification and â€œÃ—â€ donotes training failure.\nLimitation of growth rate. From Table 2 and Table 3, we observe that there is an upper limit on their\nacceptable growth rate. Large growth rates often lead to training failure. To determine this limit, we evaluate\na family of inherently unbounded functions with varying growth rates, as illustrated in Figure 3. Among them,\nlogquad(ğ‘¥) exhibits the fastest growth that still allows training convergence (see Table 4). Functions with\nfaster growth, such as linear(ğ‘¥) and power23(ğ‘¥), tend to cause optimization divergence in the early stages of\ntraining. This failure occurs because rapidly growing functions fail to suppress variance effectively, leading to\nlarge gradient norms at the start of optimization.\nlogsign(ğ‘¥)\narcsinh(ğ‘¥)\nlogquad(ğ‘¥)\npower23(ğ‘¥)\nlinear(ğ‘¥)\n82.2%\n82.2%\n82.1%\nÃ—\nÃ—\nTable 4 Results of unbounded functions with different growth rates on ViT-Base. Point-wise functions have a growth\nrate upper bound, with logquad(ğ‘¥) being the fastest function that still converges. â€œÃ—â€ indicates training failure.\n3.3 Center Sensitivity\nWe use center sensitivity to characterize how quickly a point-wise function becomes responsive to input\nvariations around zero. Without center sensitivity, a function is locally flat around the origin, returning zero\nor near-zero over a finite interval. The region around zero is particularly important, as most activations tend\nto concentrate near the origin during training. Consequently, the responsiveness of a function in this area\ndirectly influences how effectively small signals can propagate through the network.\n5", "clean_text": "3 6 9 12 15 18 6 12 18 0 linear(x) power23(x) logquad(x) arcsinh(x) logsign(x) Figure 3 Visualization of several unbounded point-wise functions on the positive half-axis, illustrating their different growth rates. ğ‘ğ‘Ÿğ‘ğ‘ ğ‘–ğ‘›â„(ğ‘¥) refers to its standard analytical form. The remaining functions are defined as linear(ğ‘¥) = ğ‘¥, power23(ğ‘¥) = ğ‘¥ 2 3 , logsign(ğ‘¥) = sign(ğ‘¥) ln(|ğ‘¥| + 1), smoothsign(ğ‘¥) = ğ‘¥ 1+|ğ‘¥|, and logquad(ğ‘¥) = sign(ğ‘¥) ln(ğ‘¥2+1). Among them, logquad(ğ‘¥) shows the fastest growth that still ensures stable convergence. ğœ†ğ‘¢ arcsinh(ğ‘¥) logsign(ğ‘¥) linear(ğ‘¥) âˆ’ 82.2% 82.2% Ã— 0.5 82.3% 82.4% 82.1% 0.8 82.3% 82.4% 82.2% 1.0 82.4% 82.4% 82.2% 2.0 82.4% 82.4% 82.1% 3.0 82.4% 82.3% 82.1% 5.0 82.3% 82.3% 82.0% Table 2 Results of clamping for boundedness on ViT-Base. Clipped version of unbounded functions consistently achieves better performance than unbounded baselines. â€œâˆ’â€ denotes the original unmodified function. â€œÃ—â€ indicates training failure. outperform the unbounded baselines across all tested ğœ†values. These results indicate that incorporating boundedness can improve optimization and result in better performance. For the second, as shown in Table 3, the results are consistent with clipping the intrinsic unbounded functions: the unbounded variant yields slightly lower accuracy than the bounded baseline. ğœ†ğ‘ erf(ğ‘¥) tanh(ğ‘¥) arctan(ğ‘¥) isru(ğ‘¥) âˆ’ 82.6% 82.5% 82.4% 82.3% 0.01 82.4% 82.4% 82.1% 82.2% 0.1 82.3% 82.3% 82.1% 82.1% 0.5 Ã— Ã— Ã— Ã— Table 3 Results of removing boundedness on ViT-Base. Performance decreases as the function is less bounded. â€œâˆ’â€ denotes the original function without modification and â€œÃ—â€ donotes training failure. Limitation of growth rate. From Table 2 and Table 3, we observe that there is an upper limit on their acceptable growth rate. Large growth rates often lead to training failure. To determine this limit, we evaluate a family of inherently unbounded functions with varying growth rates, as illustrated in Figure 3. Among them, logquad(ğ‘¥) exhibits the fastest growth that still allows training convergence (see Table 4). Functions with faster growth, such as linear(ğ‘¥) and power23(ğ‘¥), tend to cause optimization divergence in the early stages of training. This failure occurs because rapidly growing functions fail to suppress variance effectively, leading to large gradient norms at the start of optimization. logsign(ğ‘¥) arcsinh(ğ‘¥) logquad(ğ‘¥) power23(ğ‘¥) linear(ğ‘¥) 82.2% 82.2% 82.1% Ã— Ã— Table 4 Results of unbounded functions with different growth rates on ViT-Base. Point-wise functions have a growth rate upper bound, with logquad(ğ‘¥) being the fastest function that still converges. â€œÃ—â€ indicates training failure. 3.3 Center Sensitivity We use center sensitivity to characterize how quickly a point-wise function becomes responsive to input variations around zero. Without center sensitivity, a function is locally flat around the origin, returning zero or near-zero over a finite interval. The region around zero is particularly important, as most activations tend to concentrate near the origin during training. Consequently, the responsiveness of a function in this area directly influences how effectively small signals can propagate through the network. 5"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 6, "text": "Setup. Since center sensitivity is difficult to isolate independently, we approximate it using a controllable\nnear-zero inactive region. Under the same ViT setup, we modify each base function to incorporate a symmetric\nflat region around the origin with a sensitivity scale ğœ†> 0 to control the extent of this region. Specifically,\nfor inputs in the range ğ‘¥âˆˆ[âˆ’ğœ†, ğœ†], we enforce ğ‘“(ğ‘¥) = 0 and smoothly shift the positive and negative parts\noutward for |ğ‘¥| > ğœ†to ensure continuity at the boundaries. A smaller ğœ†results in a narrower flat region and\nhigher sensitivity near zero, while a larger ğœ†leads to lower sensitivity. We vary ğœ†over {0.1, 0.5, 1.0, 2.0, 3.0}\nacross three base functions.\nResults. As shown in Table 5, the best performance is achieved at ğœ†= 0. As ğœ†increases, the performance\nconsistently degrades. This trend is not very clear when ğœ†â‰¤0.5, but once ğœ†exceeds 1.0, the degradation\nbecomes much more obvious. Finally, when ğœ†â‰¥3.0, the training process diverges at an early stage.\nfunction\nğœ†= 0\n0.1\n0.5\n1.0\n2.0\n3.0\nerf(ğ‘¥)\n82.6%\n82.5%\n82.5%\n82.1%\n81.3%\nÃ—\ntanh(ğ‘¥)\n82.5%\n82.5%\n82.4%\n82.1%\n81.1%\nÃ—\narctan(ğ‘¥)\n82.3%\n82.3%\n82.1%\n81.8%\n80.9%\nÃ—\nTable 5 Results of center sensitivity (ğœ†) on ViT-Base. â€œÃ—â€ indicates training failure. The best performance is achieved\nwhen no flat region is given, showing the importance of center sensitivity.\n3.4 Monotonicity\nMonotonicity ensures a functionâ€™s output consistently increases (or decreases) as the input increases, preserving\nthe relative order of inputs throughout the transformation. Non-monotonic functions may disrupt the relative\nordering of activations. Furthermore, since a non-monotonic function necessarily has regions where its\nderivative changes sign, it may also produce flipped gradient signals during training.\nSetup.\nEach base function selected can serve as the monotonically increasing case, while its negated\ncounterpart is defined as ğ‘“neg(ğ‘¥) = âˆ’ğ‘“(ğ‘¥), representing the monotonically decreasing variant. As non-\nmonotonic comparisons, we include hump-shaped functions and oscillatory functions (see Figure 4) to examine\nhow violations of monotonicity influence the training performance. To control potential confounding factors,\nwe rescale each function so that its output range matches that of the monotonic functions. After rescaling, all\nfunctions are aligned in terms of zero-centeredness, boundedness, and center sensitivity.\n-0.5\n1.0\n0.5\n0.0\n-1.0\n8\n-4\n0\n4\n-8\nFigure 4 Visualization of point-wise functions\nwith different monotonicity behaviors. erf(ğ‘¥) and\nsin(ğ‘¥) refer to their standard form. The remain-\ning functions are defined as negerf(ğ‘¥) = âˆ’erf(ğ‘¥),\ndampx(ğ‘¥) =\n2ğ‘¥\n1+ğ‘¥2 , dampexp(ğ‘¥) = 2.72ğ‘¥Â· ğ‘’âˆ’|ğ‘¥|\nfunction\nğ‘“(ğ‘¥)\nğ‘“neg(ğ‘¥)\nerf(ğ‘¥)\n82.6%\n82.5%\ntanh(ğ‘¥)\n82.5%\n82.5%\narctan(ğ‘¥)\n82.3%\n82.2%\n(a) Monotonic\nfunction\nğ‘“(ğ‘¥)\nsin(ğ‘¥)\n81.6%\ndampx(ğ‘¥)\n80.7%\ndampexp(ğ‘¥)\n81.2%\n(b) Non-monotonic\nTable 6 Results of monotonicity on ViT-Base. Monotonic func-\ntions consistently achieve better performance than their negated\nversions and other non-monotonic functions, whether hump-\nshaped or oscillatory.\nThis identifies monotonicity as a key\nproperty for effective learning.\nResults. As shown in Table 6, both increasing and decreasing monotonic functions train stably and achieve\nhigh accuracy. In contrast, non-monotonic functions, whether hump-shaped or oscillatory, consistently perform\nworse than monotonic functions and lead to a clear drop in final accuracy. These results highlight monotonicity\nas a key property for point-wise functions to ensure effective learning.\n6", "clean_text": "Setup. Since center sensitivity is difficult to isolate independently, we approximate it using a controllable near-zero inactive region. Under the same ViT setup, we modify each base function to incorporate a symmetric flat region around the origin with a sensitivity scale ğœ†> 0 to control the extent of this region. Specifically, for inputs in the range ğ‘¥âˆˆ[âˆ’ğœ†, ğœ†], we enforce ğ‘“(ğ‘¥) = 0 and smoothly shift the positive and negative parts outward for |ğ‘¥| > ğœ†to ensure continuity at the boundaries. A smaller ğœ†results in a narrower flat region and higher sensitivity near zero, while a larger ğœ†leads to lower sensitivity. We vary ğœ†over {0.1, 0.5, 1.0, 2.0, 3.0} across three base functions. Results. As shown in Table 5, the best performance is achieved at ğœ†= 0. As ğœ†increases, the performance consistently degrades. This trend is not very clear when ğœ†â‰¤0.5, but once ğœ†exceeds 1.0, the degradation becomes much more obvious. Finally, when ğœ†â‰¥3.0, the training process diverges at an early stage. function ğœ†= 0 0.1 0.5 1.0 2.0 3.0 erf(ğ‘¥) 82.6% 82.5% 82.5% 82.1% 81.3% Ã— tanh(ğ‘¥) 82.5% 82.5% 82.4% 82.1% 81.1% Ã— arctan(ğ‘¥) 82.3% 82.3% 82.1% 81.8% 80.9% Ã— Table 5 Results of center sensitivity (ğœ†) on ViT-Base. â€œÃ—â€ indicates training failure. The best performance is achieved when no flat region is given, showing the importance of center sensitivity. 3.4 Monotonicity Monotonicity ensures a functionâ€™s output consistently increases (or decreases) as the input increases, preserving the relative order of inputs throughout the transformation. Non-monotonic functions may disrupt the relative ordering of activations. Furthermore, since a non-monotonic function necessarily has regions where its derivative changes sign, it may also produce flipped gradient signals during training. Setup. Each base function selected can serve as the monotonically increasing case, while its negated counterpart is defined as ğ‘“neg(ğ‘¥) = âˆ’ğ‘“(ğ‘¥), representing the monotonically decreasing variant. As nonmonotonic comparisons, we include hump-shaped functions and oscillatory functions (see Figure 4) to examine how violations of monotonicity influence the training performance. To control potential confounding factors, we rescale each function so that its output range matches that of the monotonic functions. After rescaling, all functions are aligned in terms of zero-centeredness, boundedness, and center sensitivity. -0.5 1.0 0.5 0.0 -1.0 8 -4 0 4 -8 Figure 4 Visualization of point-wise functions with different monotonicity behaviors. erf(ğ‘¥) and sin(ğ‘¥) refer to their standard form. The remaining functions are defined as negerf(ğ‘¥) = âˆ’erf(ğ‘¥), dampx(ğ‘¥) = 2ğ‘¥ 1+ğ‘¥2 , dampexp(ğ‘¥) = 2.72ğ‘¥Â· ğ‘’âˆ’|ğ‘¥| function ğ‘“(ğ‘¥) ğ‘“neg(ğ‘¥) erf(ğ‘¥) 82.6% 82.5% tanh(ğ‘¥) 82.5% 82.5% arctan(ğ‘¥) 82.3% 82.2% (a) Monotonic function ğ‘“(ğ‘¥) sin(ğ‘¥) 81.6% dampx(ğ‘¥) 80.7% dampexp(ğ‘¥) 81.2% (b) Non-monotonic Table 6 Results of monotonicity on ViT-Base. Monotonic functions consistently achieve better performance than their negated versions and other non-monotonic functions, whether humpshaped or oscillatory. This identifies monotonicity as a key property for effective learning. Results. As shown in Table 6, both increasing and decreasing monotonic functions train stably and achieve high accuracy. In contrast, non-monotonic functions, whether hump-shaped or oscillatory, consistently perform worse than monotonic functions and lead to a clear drop in final accuracy. These results highlight monotonicity as a key property for point-wise functions to ensure effective learning. 6"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 7, "text": "function\nalias\ntop-1 acc â†‘\nFID â†“\nViT-Base\nDiT-B/4\nDiT-L/4\nâ€“\nLayerNorm\n82.3%\n64.93\n45.91\n2ğœ‹âˆ’1/2âˆ«ï¸€ğ‘¥\n0 ğ‘’âˆ’ğ‘¡2 ğ‘‘ğ‘¡\nerf(ğ‘¥)\n82.8%\n63.23\n43.94\n(ğ‘’ğ‘¥âˆ’ğ‘’âˆ’ğ‘¥)(ğ‘’ğ‘¥+ ğ‘’âˆ’ğ‘¥)âˆ’1\ntanh(ğ‘¥)\n82.6%\n63.71\n45.48\nsin(clip(ğ‘¥, âˆ’ğœ‹\n2 , ğœ‹\n2 ))\nsatursin(ğ‘¥)\n82.6%\n63.90\n44.83\nclip (ln(ğ‘¥+\nâˆš\nğ‘¥2 + 1), âˆ’1, 1)\narcsinhclip(ğ‘¥)\n82.5%\n64.72\n45.48\nğ‘¥(ğ‘¥2 + 1)âˆ’1/2\nisru(ğ‘¥)\n82.3%\n65.72\n45.93\nsign(ğ‘¥) ((1 âˆ’ğ‘’âˆ’âˆš\n|ğ‘¥|))\nexproot(ğ‘¥)\n82.4%\n65.20\n46.91\nclip(ğ‘¥, âˆ’1, 1)\nlinearclip(ğ‘¥)\n82.3%\n66.08\n45.49\nâˆ’sign(ğ‘¥) ((ğ‘’âˆ’|ğ‘¥| âˆ’1))\nexpsign(ğ‘¥)\n82.2%\n64.85\n45.82\nclip (sign(ğ‘¥) ln(|ğ‘¥| + 1), âˆ’1, 1)\nlogsignclip(ğ‘¥)\n82.4%\n65.59\n46.34\nğ‘¥(\nâˆš\nğ‘¥2 + 1 + 1)âˆ’1\nrelsign(ğ‘¥)\n82.3%\n68.42\n48.33\narctan(ğ‘¥)\narctan(ğ‘¥)\n82.4%\n67.07\n46.62\nğ‘¥(1 + |ğ‘¥|)âˆ’1\nsmoothsign(ğ‘¥)\n82.4%\n68.84\n47.29\nclip (sign(ğ‘¥) ln(ğ‘¥2 + 1), âˆ’1, 1)\nlogquadclip(ğ‘¥)\n82.2%\n65.92\n47.12\nclip (sign(ğ‘¥) |ğ‘¥|2/3, âˆ’1, 1)\npower23clip(ğ‘¥)\n82.1%\n66.11\n46.47\nsign(ğ‘¥) ln(|ğ‘¥| + 1) (ln(|ğ‘¥| + 1) + 1)âˆ’1\nsaturlog(ğ‘¥)\n81.8%\n68.23\n47.44\nğ‘¥3(|ğ‘¥|3 + 1)âˆ’1\ncubsign(ğ‘¥)\n81.4%\n70.22\n49.16\nTable 7 Top-1 accuracy on ViT-Base and image generation quality (FID) on DiT-B/4 and DiT-L/4. Different\nfunctions show noticeable differences in performance. Among all the point-wise functions and LayerNorm, erf(ğ‘¥) shows\nthe best performance in both top-1 accuracy and FID. Visualization of each function is included in Appendix B.\n4 Function Search\nFrom the previous section, we observe that functions that are near zero-centered, bounded, center-sensitive\n(responsive to input variations around zero), and monotonic (increasing or decreasing) tend to yield better\noptimization performance. Building upon these insights, we start to construct our function set from widely used\nscalar functions and cumulative distribution functions (CDFs), including polynomial, rational, exponential,\nlogarithmic, and trigonometric forms. We then generate variants via simple transformations such as translation,\nscaling, mirroring, rotation, and clipping. Functions that satisfy our four function properties after these\ntransformations are retained as the candidate subset used in the search. For example, we transform the\nunbounded function arcsinh(ğ‘¥) by clipping it to the range [âˆ’1, 1], limiting it to a finite range and conforming\nto all four principles. In Appendix B, we provide further details about how we obtain these candidate functions.\nWithin this set, we evaluate their performance, and Derf emerges as the most effective function.\nSetup. We conduct an empirical search on two representative vision architectures: Vision Transformer\n(ViT-Base) (Dosovitskiy, 2021) and Diffusion Transformer (DiT-B/4 and DiT-L/4) (Peebles and Xie, 2023).\nModels are trained on ImageNet-1K (Deng et al., 2009) under their default training settings. For ViT, model\nperformance is measured using top-1 accuracy on the ImageNet-1K validation set. For DiT, we follow the\nstandard ImageNet reference batch evaluation and report the FrÃ©chet Inception Distance (FID) as the metric.\nFormulation. We quantitatively evaluate a set of functions under the constraint of our function properties,\nas illustrated in Figure 5. Each point-wise function is instantiated in a unified form in Equation 8, where\nğ‘“(Â·) denotes a candidate point-wise function, with learnable parameter ğ‘ and ğ›¼recentering and rescaling the\ninput. The parameters ğ›¾and ğ›½follow the same role as in standard normalization layers. We introduce a\nlearnable shift parameter ğ‘ , as it improves the final performance to varying degrees across different functions.\nDetailed ablation results on the effect of ğ‘ are provided in Section 7.1.\nğ‘¦= ğ›¾* ğ‘“(ğ›¼ğ‘¥+ ğ‘ ) + ğ›½,\n(8)\n7", "clean_text": "function alias top-1 acc â†‘ FID â†“ ViT-Base DiT-B/4 DiT-L/4 â€“ LayerNorm 82.3% 64.93 45.91 2ğœ‹âˆ’1/2âˆ«ï¸€ğ‘¥ 0 ğ‘’âˆ’ğ‘¡2 ğ‘‘ğ‘¡ erf(ğ‘¥) 82.8% 63.23 43.94 (ğ‘’ğ‘¥âˆ’ğ‘’âˆ’ğ‘¥)(ğ‘’ğ‘¥+ ğ‘’âˆ’ğ‘¥)âˆ’1 tanh(ğ‘¥) 82.6% 63.71 45.48 sin(clip(ğ‘¥, âˆ’ğœ‹ 2 , ğœ‹ 2 )) satursin(ğ‘¥) 82.6% 63.90 44.83 clip (ln(ğ‘¥+ âˆš ğ‘¥2 + 1), âˆ’1, 1) arcsinhclip(ğ‘¥) 82.5% 64.72 45.48 ğ‘¥(ğ‘¥2 + 1)âˆ’1/2 isru(ğ‘¥) 82.3% 65.72 45.93 sign(ğ‘¥) ((1 âˆ’ğ‘’âˆ’âˆš |ğ‘¥|)) exproot(ğ‘¥) 82.4% 65.20 46.91 clip(ğ‘¥, âˆ’1, 1) linearclip(ğ‘¥) 82.3% 66.08 45.49 âˆ’sign(ğ‘¥) ((ğ‘’âˆ’|ğ‘¥| âˆ’1)) expsign(ğ‘¥) 82.2% 64.85 45.82 clip (sign(ğ‘¥) ln(|ğ‘¥| + 1), âˆ’1, 1) logsignclip(ğ‘¥) 82.4% 65.59 46.34 ğ‘¥( âˆš ğ‘¥2 + 1 + 1)âˆ’1 relsign(ğ‘¥) 82.3% 68.42 48.33 arctan(ğ‘¥) arctan(ğ‘¥) 82.4% 67.07 46.62 ğ‘¥(1 + |ğ‘¥|)âˆ’1 smoothsign(ğ‘¥) 82.4% 68.84 47.29 clip (sign(ğ‘¥) ln(ğ‘¥2 + 1), âˆ’1, 1) logquadclip(ğ‘¥) 82.2% 65.92 47.12 clip (sign(ğ‘¥) |ğ‘¥|2/3, âˆ’1, 1) power23clip(ğ‘¥) 82.1% 66.11 46.47 sign(ğ‘¥) ln(|ğ‘¥| + 1) (ln(|ğ‘¥| + 1) + 1)âˆ’1 saturlog(ğ‘¥) 81.8% 68.23 47.44 ğ‘¥3(|ğ‘¥|3 + 1)âˆ’1 cubsign(ğ‘¥) 81.4% 70.22 49.16 Table 7 Top-1 accuracy on ViT-Base and image generation quality (FID) on DiT-B/4 and DiT-L/4. Different functions show noticeable differences in performance. Among all the point-wise functions and LayerNorm, erf(ğ‘¥) shows the best performance in both top-1 accuracy and FID. Visualization of each function is included in Appendix B. 4 Function Search From the previous section, we observe that functions that are near zero-centered, bounded, center-sensitive (responsive to input variations around zero), and monotonic (increasing or decreasing) tend to yield better optimization performance. Building upon these insights, we start to construct our function set from widely used scalar functions and cumulative distribution functions (CDFs), including polynomial, rational, exponential, logarithmic, and trigonometric forms. We then generate variants via simple transformations such as translation, scaling, mirroring, rotation, and clipping. Functions that satisfy our four function properties after these transformations are retained as the candidate subset used in the search. For example, we transform the unbounded function arcsinh(ğ‘¥) by clipping it to the range [âˆ’1, 1], limiting it to a finite range and conforming to all four principles. In Appendix B, we provide further details about how we obtain these candidate functions. Within this set, we evaluate their performance, and Derf emerges as the most effective function. Setup. We conduct an empirical search on two representative vision architectures: Vision Transformer (ViT-Base) (Dosovitskiy, 2021) and Diffusion Transformer (DiT-B/4 and DiT-L/4) (Peebles and Xie, 2023). Models are trained on ImageNet-1K (Deng et al., 2009) under their default training settings. For ViT, model performance is measured using top-1 accuracy on the ImageNet-1K validation set. For DiT, we follow the standard ImageNet reference batch evaluation and report the FrÃ©chet Inception Distance (FID) as the metric. Formulation. We quantitatively evaluate a set of functions under the constraint of our function properties, as illustrated in Figure 5. Each point-wise function is instantiated in a unified form in Equation 8, where ğ‘“(Â·) denotes a candidate point-wise function, with learnable parameter ğ‘ and ğ›¼recentering and rescaling the input. The parameters ğ›¾and ğ›½follow the same role as in standard normalization layers. We introduce a learnable shift parameter ğ‘ , as it improves the final performance to varying degrees across different functions. Detailed ablation results on the effect of ğ‘ are provided in Section 7.1. ğ‘¦= ğ›¾* ğ‘“(ğ›¼ğ‘¥+ ğ‘ ) + ğ›½, (8) 7"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 8, "text": "0.5\n1\n1.5\n2\n2.5\n3\n0.25\n0.5\n0.75\n1\n0\nsatursin(x)\nerf(x)\ntanh(x)\nisru(x)\nexpsign(x)\narctan(x)\nsmoothsign(x)\nrelsign(x)\n0.5\n1\n1.5\n2\n2.5\n3\n0.25\n0.5\n0.75\n1\n0\npower23clip(x)\nlinearclip(x)\narcsinhclip(x)\nlogquadclip(x)\nlogsignclip(x)\ncubsign(x)\nexproot(x)\nsaturlog(x)\nFigure 5 Visualization of candidate point-wise functions on the positive half-axis. All functions are self-symmetric\nwith respect to the origin.\nQuantitative evaluation. As shown in Table 7, even though these S-shaped functions appear highly similar\nin form, their empirical training results show noticeable differences in final performance. Among all the\npoint-wise functions, erf(ğ‘¥) with the introduced transformations stands out as the best-performing function,\nconsistently surpassing all other candidates and the baseline normalization layers.\n5 Dynamic erf (Derf)\nFrom the search, we identify erf(ğ‘¥) as the most performant point-wise function. The error function erf(Â·) is\nclosely related to the cumulative distribution function (CDF) of a standard Gaussian distribution. Specifically,\nerf(ğ‘¥) can be defined by Equation 9. In our setup, erf(ğ‘¥) is in the form augmented with learnable parameters,\nwhich we introduce as Derf, Dynamic erf. Given an input tensor ğ‘¥, a Derf layer is defined in Equation 10,\nwhere both the shift ğ‘ and the scale ğ›¼are learnable scalars. ğ›¾and ğ›½are learnable per-channel vectors. To\nintegrate Derf into a transformer-based architecture, we replace each normalization layer with a corresponding\nDerf layer. In particular, the pre-attention, the pre-FFN, and the final normalization layers are all substituted\nin a one-to-one manner, ensuring consistent incorporation of Derf across the entire model.\nerf(ğ‘¥) =\n2\nâˆšğœ‹\nâˆ«ï¸ğ‘¥\n0\nğ‘’âˆ’ğ‘¡2ğ‘‘ğ‘¡\n(9)\nDerf(ğ‘¥) = ğ›¾erf(ğ›¼ğ‘¥+ ğ‘ ) + ğ›½\n(10)\nParameter initialization. We initialize ğ›¾to an all-one vector and ğ›½to an all-zero vector following the same\nstrategy as in standard normalization layers. For the additional scalar parameters introduced by Derf, the\nscaling parameter ğ›¼is initialized to 0.5, while the shift parameter ğ‘ is initialized to 0. Unless otherwise\nspecified, these initialization settings are adopted throughout all experiments.\n6 Experiments\nWe evaluate the effectiveness of Derf across various transformer-based and a few other modern architectures.\nFor each model, we replace the original normalization layers with DyT and Derf, following the standard\ntraining and evaluation protocols, as detailed in Appendix C. Across all tested architectures, Derf consistently\nachieves stronger performance over the baseline normalization methods and DyT. Besides each modelâ€™s default\nnormalization, we also report results with other common normalization methods in Appendix D.\nVision Transformers. We train ViT-Base and ViT-Large models (Dosovitskiy, 2021) on ImageNet-1K (Deng\net al., 2009) using LayerNorm (LN), DyT, and Derf for comparison. Table 8 reports the top-1 classification\naccuracy. Compared to LN and DyT, Derf achieves clearly higher top-1 accuracy.\n8", "clean_text": "0.5 1 1.5 2 2.5 3 0.25 0.5 0.75 1 0 satursin(x) erf(x) tanh(x) isru(x) expsign(x) arctan(x) smoothsign(x) relsign(x) 0.5 1 1.5 2 2.5 3 0.25 0.5 0.75 1 0 power23clip(x) linearclip(x) arcsinhclip(x) logquadclip(x) logsignclip(x) cubsign(x) exproot(x) saturlog(x) Figure 5 Visualization of candidate point-wise functions on the positive half-axis. All functions are self-symmetric with respect to the origin. Quantitative evaluation. As shown in Table 7, even though these S-shaped functions appear highly similar in form, their empirical training results show noticeable differences in final performance. Among all the point-wise functions, erf(ğ‘¥) with the introduced transformations stands out as the best-performing function, consistently surpassing all other candidates and the baseline normalization layers. 5 Dynamic erf (Derf) From the search, we identify erf(ğ‘¥) as the most performant point-wise function. The error function erf(Â·) is closely related to the cumulative distribution function (CDF) of a standard Gaussian distribution. Specifically, erf(ğ‘¥) can be defined by Equation 9. In our setup, erf(ğ‘¥) is in the form augmented with learnable parameters, which we introduce as Derf, Dynamic erf. Given an input tensor ğ‘¥, a Derf layer is defined in Equation 10, where both the shift ğ‘ and the scale ğ›¼are learnable scalars. ğ›¾and ğ›½are learnable per-channel vectors. To integrate Derf into a transformer-based architecture, we replace each normalization layer with a corresponding Derf layer. In particular, the pre-attention, the pre-FFN, and the final normalization layers are all substituted in a one-to-one manner, ensuring consistent incorporation of Derf across the entire model. erf(ğ‘¥) = 2 âˆšğœ‹ âˆ«ï¸ğ‘¥ 0 ğ‘’âˆ’ğ‘¡2ğ‘‘ğ‘¡ (9) Derf(ğ‘¥) = ğ›¾erf(ğ›¼ğ‘¥+ ğ‘ ) + ğ›½ (10) Parameter initialization. We initialize ğ›¾to an all-one vector and ğ›½to an all-zero vector following the same strategy as in standard normalization layers. For the additional scalar parameters introduced by Derf, the scaling parameter ğ›¼is initialized to 0.5, while the shift parameter ğ‘ is initialized to 0. Unless otherwise specified, these initialization settings are adopted throughout all experiments. 6 Experiments We evaluate the effectiveness of Derf across various transformer-based and a few other modern architectures. For each model, we replace the original normalization layers with DyT and Derf, following the standard training and evaluation protocols, as detailed in Appendix C. Across all tested architectures, Derf consistently achieves stronger performance over the baseline normalization methods and DyT. Besides each modelâ€™s default normalization, we also report results with other common normalization methods in Appendix D. Vision Transformers. We train ViT-Base and ViT-Large models (Dosovitskiy, 2021) on ImageNet-1K (Deng et al., 2009) using LayerNorm (LN), DyT, and Derf for comparison. Table 8 reports the top-1 classification accuracy. Compared to LN and DyT, Derf achieves clearly higher top-1 accuracy. 8"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 9, "text": "model\nLN\nDyT\nDerf\nÎ”LN\nÎ”DyT\nViT-B\n82.3%\n82.5%\n82.8%\nâ†‘0.5%\nâ†‘0.3%\nViT-L\n83.1%\n83.6%\n83.8%\nâ†‘0.7%\nâ†‘0.2%\nTable 8 Supervised classification accuracy on ImageNet-1K. Derf achieves higher top-1 accuracy than both LN and\nDyT on different model sizes, demonstrating its effectiveness in vision transformer architectures.\nDiffusion Transformers. We train three Diffusion Transformer (DiT) (Peebles and Xie, 2023) models on\nImageNet-1K (Deng et al., 2009). Consistent with the original DiT setup, the affine parameters in the\nnormalization layers are retained for class conditioning across LN, DyT, and Derf. After training, we evaluate\nthe FID scores using the standard ImageNet â€œreference batchâ€ to measure image generation quality, as\nreported in Table 9. Derf achieves a clear improvement in FID compared to both LayerNorm and DyT.\nmodel\nLN\nDyT\nDerf\nÎ”LN\nÎ”DyT\nDiT-B/4\n64.93\n63.94\n63.23\nâ†“1.70\nâ†“0.71\nDiT-L/4\n45.91\n45.66\n43.94\nâ†“1.97\nâ†“1.72\nDiT-XL/2\n19.94\n20.83\n18.92\nâ†“1.02\nâ†“1.91\nTable 9 Image generation quality (FID) on ImageNet. Lower FID indicates better image generation quality. Derf\nachieves lower FID scores than both LN and DyT across all DiT model sizes.\nSpeech models. We train two wav2vec 2.0 Transformer models (Baevski et al., 2020) on the LibriSpeech\ndataset (Panayotov et al., 2015) for speech representation learning. We report the final validation loss in\nTable 10. Compared to LayerNorm and DyT, Derf yields lower validation loss on different model sizes.\nmodel\nLN\nDyT\nDerf\nÎ”LN\nÎ”DyT\nwav2vec 2.0 Base\n1.95\n1.95\n1.93\nâ†“0.02\nâ†“0.02\nwav2vec 2.0 Large\n1.92\n1.91\n1.90\nâ†“0.02\nâ†“0.01\nTable 10 Speech pretraining validation loss on the LibriSpeech dataset. Derf achieves lower validation loss than both\nLN and DyT across two wav2vec 2.0 models, indicating its better representation quality.\nDNA models. For the long-range DNA sequence modeling task, we pretrain the HyenaDNA model (Nguyen\net al., 2023) and the Caduceus model (Schiff et al., 2024) using the human reference genome from (GRCh38,\n2013). Model evaluation is conducted on the GenomicBenchmarks dataset (GreÅ¡ovÃ¡ et al., 2023). We report\nthe averaged accuracy over all subtasks. As shown in Table 11, Derf surpasses both normalization layers and\nDyT in performance, demonstrating its robustness in genomic sequence modeling.\nmodel\nNorm\nDyT\nDerf\nÎ”Norm\nÎ”DyT\nHyena\n85.2%\n85.2%\n85.7%\nâ†‘0.5%\nâ†‘0.5%\nCaduceus\n86.9%\n86.9%\n87.3%\nâ†‘0.4%\nâ†‘0.4%\nTable 11 DNA classification accuracy on the GenomicBenchmarks dataset, averaged over each subtask. Each model\nis evaluated with its default normalization layer (LN for Heyna, RMSNorm for Caduceus). Derf consistently achieves\nhigher accuracy than both normalization layers and DyT, indicating its effectiveness in DNA model.\n9", "clean_text": "model LN DyT Derf Î”LN Î”DyT ViT-B 82.3% 82.5% 82.8% â†‘0.5% â†‘0.3% ViT-L 83.1% 83.6% 83.8% â†‘0.7% â†‘0.2% Table 8 Supervised classification accuracy on ImageNet-1K. Derf achieves higher top-1 accuracy than both LN and DyT on different model sizes, demonstrating its effectiveness in vision transformer architectures. Diffusion Transformers. We train three Diffusion Transformer (DiT) (Peebles and Xie, 2023) models on ImageNet-1K (Deng et al., 2009). Consistent with the original DiT setup, the affine parameters in the normalization layers are retained for class conditioning across LN, DyT, and Derf. After training, we evaluate the FID scores using the standard ImageNet â€œreference batchâ€ to measure image generation quality, as reported in Table 9. Derf achieves a clear improvement in FID compared to both LayerNorm and DyT. model LN DyT Derf Î”LN Î”DyT DiT-B/4 64.93 63.94 63.23 â†“1.70 â†“0.71 DiT-L/4 45.91 45.66 43.94 â†“1.97 â†“1.72 DiT-XL/2 19.94 20.83 18.92 â†“1.02 â†“1.91 Table 9 Image generation quality (FID) on ImageNet. Lower FID indicates better image generation quality. Derf achieves lower FID scores than both LN and DyT across all DiT model sizes. Speech models. We train two wav2vec 2.0 Transformer models (Baevski et al., 2020) on the LibriSpeech dataset (Panayotov et al., 2015) for speech representation learning. We report the final validation loss in Table 10. Compared to LayerNorm and DyT, Derf yields lower validation loss on different model sizes. model LN DyT Derf Î”LN Î”DyT wav2vec 2.0 Base 1.95 1.95 1.93 â†“0.02 â†“0.02 wav2vec 2.0 Large 1.92 1.91 1.90 â†“0.02 â†“0.01 Table 10 Speech pretraining validation loss on the LibriSpeech dataset. Derf achieves lower validation loss than both LN and DyT across two wav2vec 2.0 models, indicating its better representation quality. DNA models. For the long-range DNA sequence modeling task, we pretrain the HyenaDNA model (Nguyen et al., 2023) and the Caduceus model (Schiff et al., 2024) using the human reference genome from (GRCh38, 2013). Model evaluation is conducted on the GenomicBenchmarks dataset (GreÅ¡ovÃ¡ et al., 2023). We report the averaged accuracy over all subtasks. As shown in Table 11, Derf surpasses both normalization layers and DyT in performance, demonstrating its robustness in genomic sequence modeling. model Norm DyT Derf Î”Norm Î”DyT Hyena 85.2% 85.2% 85.7% â†‘0.5% â†‘0.5% Caduceus 86.9% 86.9% 87.3% â†‘0.4% â†‘0.4% Table 11 DNA classification accuracy on the GenomicBenchmarks dataset, averaged over each subtask. Each model is evaluated with its default normalization layer (LN for Heyna, RMSNorm for Caduceus). Derf consistently achieves higher accuracy than both normalization layers and DyT, indicating its effectiveness in DNA model. 9"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 10, "text": "Language models. We pretrain a GPT-2 (124M) model on the OpenWebText dataset and report the validation\nloss in Table 12. For DyT and Derf, we additionally finetune the initialization of the learnable parameter ğ›¼.\nWe observe that Derf achieves comparable performance to LN, while clearly outperforming DyT.\nmodel\nLN\nDyT\nDerf\nÎ”LN\nÎ”DyT\nGPT-2\n2.94\n2.97\n2.94\n0.00\nâ†“0.03\nTable 12 GPT-2 validation loss on the OpenWebText dataset. Derf matches the performance of LN while achieving\nlower validation loss than DyT.\n6.1 Stronger Generalization or Better Fitting?\nGiven Derfâ€™s superior performance, we aim to determine whether the gains arise from improved fitting capacity\nor stronger generalization. To this end, we compare the training loss of models respectively trained with\nnormalization layers, DyT, and Derf. Since lower training loss indicates stronger fitting ability, this comparison\nhelps us assess whether Derf improves optimization or enhances generalization.\nSetup. We compute training losses across diverse architectures and scales. To measure fitting capacity fairly,\nwe do not use the loss during optimization, which is confounded by stochastic regularization (e.g., stochastic\ndepth (Huang et al., 2016a)) and train-time augmentations. Instead, after training, we switch to evaluation\nmode, disable stochastic depth (when present), adopt the test-time preprocessing pipeline, and compute the\nloss on the training set. This yields a fair estimate of each modelâ€™s fitting capacity. In Appendix E, we provide\nthe detailed procedure for computing the evaluation-mode training loss for each model.\nResults. Across all architectures and scales, both Derf and DyT result in higher training loss than normalization-\nbased models, with Derf generally yielding slightly lower training loss than DyT, as shown in Table 13. This\nconsistent pattern indicates that neither Derf nor DyT improves fitting capacity over normalization layers.\nmodel\nNorm\nDerf\nDyT\nViT-B\n0.2623\n0.2681\n0.2714\nViT-L\n0.2034\n0.2066\n0.2083\nDiT-B\n0.1531\n0.1533\n0.1535\nDiT-L\n0.1501\n0.1510\n0.1518\nDiT-XL\n0.1432\n0.1436\n0.1440\nwav2vec 2.0 B\n1.8509\n1.8821\n1.8946\nwav2vec 2.0 L\n1.8241\n1.8563\n1.8641\nHyena\n1.1297\n1.1526\n1.1631\nCaduceus\n0.8917\n0.9129\n0.9203\nGPT-2\n2.9478\n2.9702\n2.9822\nTable 13 Evaluation-mode training loss of normalization layers (Norm), Derf, and DyT after optimization. Bolded\nindicates the lowest loss, and underlined means the second-lowest loss. Across all model architectures, the training loss\nfollows the relation: Norm < Derf < DyT. Both DyT and Derf exhibit higher training loss than normalization layers,\nwhile Derf achieves slightly lower loss than DyT.\nDiscussion. Despite the reduced fitting capacity, Derf delivers consistent performance gains across all evaluated\ntasks. We hypothesize that these gains arise primarily from both better generalization than normalization\nlayers and stronger fitting capacity than DyT.\nFirstly, point-wise functions promote stronger generalization. Although Derf yields higher training loss, it\nachieves superior downstream performance, indicating that its benefits stem not from improved fitting but\nfrom enhanced generalization. This difference likely originates from the contrasting operational principles\nbetween normalization layers and point-wise functions. Normalization layers adapt their transformation\n10", "clean_text": "Language models. We pretrain a GPT-2 (124M) model on the OpenWebText dataset and report the validation loss in Table 12. For DyT and Derf, we additionally finetune the initialization of the learnable parameter ğ›¼. We observe that Derf achieves comparable performance to LN, while clearly outperforming DyT. model LN DyT Derf Î”LN Î”DyT GPT-2 2.94 2.97 2.94 0.00 â†“0.03 Table 12 GPT-2 validation loss on the OpenWebText dataset. Derf matches the performance of LN while achieving lower validation loss than DyT. 6.1 Stronger Generalization or Better Fitting? Given Derfâ€™s superior performance, we aim to determine whether the gains arise from improved fitting capacity or stronger generalization. To this end, we compare the training loss of models respectively trained with normalization layers, DyT, and Derf. Since lower training loss indicates stronger fitting ability, this comparison helps us assess whether Derf improves optimization or enhances generalization. Setup. We compute training losses across diverse architectures and scales. To measure fitting capacity fairly, we do not use the loss during optimization, which is confounded by stochastic regularization (e.g., stochastic depth (Huang et al., 2016a)) and train-time augmentations. Instead, after training, we switch to evaluation mode, disable stochastic depth (when present), adopt the test-time preprocessing pipeline, and compute the loss on the training set. This yields a fair estimate of each modelâ€™s fitting capacity. In Appendix E, we provide the detailed procedure for computing the evaluation-mode training loss for each model. Results. Across all architectures and scales, both Derf and DyT result in higher training loss than normalizationbased models, with Derf generally yielding slightly lower training loss than DyT, as shown in Table 13. This consistent pattern indicates that neither Derf nor DyT improves fitting capacity over normalization layers. model Norm Derf DyT ViT-B 0.2623 0.2681 0.2714 ViT-L 0.2034 0.2066 0.2083 DiT-B 0.1531 0.1533 0.1535 DiT-L 0.1501 0.1510 0.1518 DiT-XL 0.1432 0.1436 0.1440 wav2vec 2.0 B 1.8509 1.8821 1.8946 wav2vec 2.0 L 1.8241 1.8563 1.8641 Hyena 1.1297 1.1526 1.1631 Caduceus 0.8917 0.9129 0.9203 GPT-2 2.9478 2.9702 2.9822 Table 13 Evaluation-mode training loss of normalization layers (Norm), Derf, and DyT after optimization. Bolded indicates the lowest loss, and underlined means the second-lowest loss. Across all model architectures, the training loss follows the relation: Norm < Derf < DyT. Both DyT and Derf exhibit higher training loss than normalization layers, while Derf achieves slightly lower loss than DyT. Discussion. Despite the reduced fitting capacity, Derf delivers consistent performance gains across all evaluated tasks. We hypothesize that these gains arise primarily from both better generalization than normalization layers and stronger fitting capacity than DyT. Firstly, point-wise functions promote stronger generalization. Although Derf yields higher training loss, it achieves superior downstream performance, indicating that its benefits stem not from improved fitting but from enhanced generalization. This difference likely originates from the contrasting operational principles between normalization layers and point-wise functions. Normalization layers adapt their transformation 10"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 11, "text": "based on training statistics, allowing them to dynamically fit activation distributions throughout training.\nIn contrast, point-wise functions are controlled by only a small set of learnable scalar parameters (e.g., ğ›¼\nfor DyT and ğ›¼, ğ‘ for Derf) that do not adapt to activation statistics after training. They apply the same\ntransformation regardless of activation distribution. This limited adaptability constrains overfitting and\neffectively serves as an implicit regularizer, leading to improved generalization.\nSecondly, Derf exhibits stronger fitting power than DyT. It achieves lower training loss while retaining the\nimplicit regularization of point-wise functions, combining higher fitting capacity with strong generalization to\noutperform both DyT and normalization-based models.\n7 Analysis\nIn this section, we begin with two ablation studies examining the influence of the learnable shift parameter ğ‘ \non the training results, followed by an analysis of an approximation of Derf.\n7.1 Effect of s\nRemoving ğ‘ . We investigate the effect of the learnable scalar parameter ğ‘ by removing it from the point-wise\nfunction. As shown in Table 14, introducing this learnable shift consistently improves the overall training\nperformance, and the degree of improvement varies across different functions. The stronger results of erf(ğ‘¥)\nover tanh(ğ‘¥) indicate that Derf surpasses DyT not only because of the shift ğ‘ .\ntop-1 acc â†‘\nFID â†“\nfunction\nwithout ğ‘ \nwith ğ‘ \nwithout ğ‘ \nwith ğ‘ \nerf(ğ‘¥)\n82.6%\n82.8%\n63.39\n63.23\ntanh(ğ‘¥)\n82.5%\n82.6%\n63.94\n63.71\nsatursin(ğ‘¥)\n82.4%\n82.6%\n65.28\n63.90\nisru(ğ‘¥)\n82.2%\n82.3%\n66.14\n65.72\narctan(ğ‘¥)\n82.3%\n82.4%\n67.41\n67.07\narcsinhclip(ğ‘¥)\n82.4%\n82.5%\n65.19\n64.72\nTable 14 Ablation study of ğ‘ . Top-1 accuracy on ViT-Base and FID score on DiT-B/4, comparing models with and\nwithout ğ‘ . ğ‘ improves the overall training performance, while its effect varies across different point-wise functions.\nScalar vs. vector ğ‘ . We further examine whether using a per-channel vector parameter instead of a scalar ğ‘ \nleads to any performance improvement. As shown in Table 15, across all three point-wise functions, the choice\nbetween a scalar and a per-channel vector shows no significant impact on the final performance. Therefore,\nwe adopt the scalar form of ğ‘ for efficiency and simplicity during training.\nfunction\nvector\nscalar\nerf(ğ‘¥)\n82.8%\n82.8%\narctan(ğ‘¥)\n82.5%\n82.4%\narcsinhclip(ğ‘¥)\n82.5%\n82.5%\nTable 15 Top-1 accuracy of scalar vs. vector ğ‘ on\nViT-Base. Using either a scalar or a per-channel vector\nfor the parameter ğ‘ yields nearly identical performance.\nfunction\nViT-B\nViT-L\nDiT-B\nDiT-L\ntanh(ğ‘¥)\n82.6%\n83.6%\n63.71\n45.48\ntanh(ğœ€ğ‘¥)\n82.7%\n83.7%\n63.88\n45.13\nerf(ğ‘¥)\n82.8%\n83.8%\n63.23\n43.94\nTable 16 Top-1 accuracy of tanh(ğœ€ğ‘¥) on ViT and DiT.\ntanh(ğœ€ğ‘¥) yields a comparable or slightly improved perfor-\nmance over tanh(ğ‘¥) but still remains below erf(ğ‘¥).\n11", "clean_text": "based on training statistics, allowing them to dynamically fit activation distributions throughout training. In contrast, point-wise functions are controlled by only a small set of learnable scalar parameters (e.g., ğ›¼ for DyT and ğ›¼, ğ‘ for Derf) that do not adapt to activation statistics after training. They apply the same transformation regardless of activation distribution. This limited adaptability constrains overfitting and effectively serves as an implicit regularizer, leading to improved generalization. Secondly, Derf exhibits stronger fitting power than DyT. It achieves lower training loss while retaining the implicit regularization of point-wise functions, combining higher fitting capacity with strong generalization to outperform both DyT and normalization-based models. 7 Analysis In this section, we begin with two ablation studies examining the influence of the learnable shift parameter ğ‘  on the training results, followed by an analysis of an approximation of Derf. 7.1 Effect of s Removing ğ‘ . We investigate the effect of the learnable scalar parameter ğ‘ by removing it from the point-wise function. As shown in Table 14, introducing this learnable shift consistently improves the overall training performance, and the degree of improvement varies across different functions. The stronger results of erf(ğ‘¥) over tanh(ğ‘¥) indicate that Derf surpasses DyT not only because of the shift ğ‘ . top-1 acc â†‘ FID â†“ function without ğ‘  with ğ‘  without ğ‘  with ğ‘  erf(ğ‘¥) 82.6% 82.8% 63.39 63.23 tanh(ğ‘¥) 82.5% 82.6% 63.94 63.71 satursin(ğ‘¥) 82.4% 82.6% 65.28 63.90 isru(ğ‘¥) 82.2% 82.3% 66.14 65.72 arctan(ğ‘¥) 82.3% 82.4% 67.41 67.07 arcsinhclip(ğ‘¥) 82.4% 82.5% 65.19 64.72 Table 14 Ablation study of ğ‘ . Top-1 accuracy on ViT-Base and FID score on DiT-B/4, comparing models with and without ğ‘ . ğ‘ improves the overall training performance, while its effect varies across different point-wise functions. Scalar vs. vector ğ‘ . We further examine whether using a per-channel vector parameter instead of a scalar ğ‘  leads to any performance improvement. As shown in Table 15, across all three point-wise functions, the choice between a scalar and a per-channel vector shows no significant impact on the final performance. Therefore, we adopt the scalar form of ğ‘ for efficiency and simplicity during training. function vector scalar erf(ğ‘¥) 82.8% 82.8% arctan(ğ‘¥) 82.5% 82.4% arcsinhclip(ğ‘¥) 82.5% 82.5% Table 15 Top-1 accuracy of scalar vs. vector ğ‘ on ViT-Base. Using either a scalar or a per-channel vector for the parameter ğ‘ yields nearly identical performance. function ViT-B ViT-L DiT-B DiT-L tanh(ğ‘¥) 82.6% 83.6% 63.71 45.48 tanh(ğœ€ğ‘¥) 82.7% 83.7% 63.88 45.13 erf(ğ‘¥) 82.8% 83.8% 63.23 43.94 Table 16 Top-1 accuracy of tanh(ğœ€ğ‘¥) on ViT and DiT. tanh(ğœ€ğ‘¥) yields a comparable or slightly improved performance over tanh(ğ‘¥) but still remains below erf(ğ‘¥). 11"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 12, "text": "7.2 Approximating Derf\nGiven the superior performance of erf(ğ‘¥) over tanh(ğ‘¥), we approximate erf(ğ‘¥) by scaling tanh(ğ‘¥) and examine\nwhether this modification can lead to performance improvement. We introduce a fixed coefficient ğœ€and use\ntanh(ğœ€ğ‘¥), where ğœ€is obtained by minimizing the following objective:\nmin\nğœ€\nâˆ«ï¸+âˆ\nâˆ’âˆ\nâƒ’âƒ’tanh(ğœ€ğ‘¥) âˆ’erf(ğ‘¥)\nâƒ’âƒ’ğ‘‘ğ‘¥.\n(11)\nThe optimal value is found to be ğœ€â‰ˆ1.205. As shown in Table 16, tanh(ğœ€ğ‘¥) achieves a comparable or slightly\nimproved performance over the original tanh(ğ‘¥), while still performing worse than erf(ğ‘¥). This indicates that\nsimply scaling tanh(ğ‘¥) is insufficient to match the behavior or performance of erf(ğ‘¥).\n8 Related Work\nNormalization layers. Since the introduction of Batch Normalization (BN) (Ioffe and Szegedy, 2015), various\nnormalization methods have been proposed to better stabilize training. To address BNâ€™s limitations with\nsmall batches, several alternatives (Salimans and Kingma, 2016; Wu and He, 2018; Yan et al., 2020; Shen\net al., 2020; Singh and Krishnan, 2020) have been explored. In parallel, LayerNorm (Ba et al., 2016; Nguyen\nand Salazar, 2019; Xu et al., 2019; Xiong et al., 2020) and RMSNorm (Zhang and Sennrich, 2019) were\ndesigned for RNN (Hochreiter and Schmidhuber, 1997) and Transformer architectures (Vaswani et al., 2017).\nTask-specific variants (Ulyanov et al., 2016; Wu and He, 2018; Shen et al., 2020) further adapt normalization\nto applications such as object detection and style transfer.\nMechanisms of normalization. A series of studies has investigated how normalization layers contribute to\nmodel convergence. From an optimization perspective, normalization stabilizes gradient flow (Balduzzi et al.,\n2017; Daneshmand et al., 2020; Lubana et al., 2021), reduces sensitivity to initialization (Zhang et al., 2019;\nDe and Smith, 2020; Shao et al., 2020), and implicitly tunes learning rates (Arora et al., 2019; Tanaka and\nKunin, 2021). It has also been shown to smooth the loss landscape (Santurkar et al., 2018; Bjorck et al., 2018;\nKarakida et al., 2019) and reduce sharpness (Lyu et al., 2022; Dai et al., 2023; Mueller et al., 2023), promoting\nmore stable optimization dynamics. Understanding these underlying functionalities provides valuable guidance\nfor designing normalization-free training methods.\nNormalization-free methods. Building on this understanding of normalization, recent work explores how to\nachieve stable convergence without normalization. One line of work operates at the parameter and optimization\nlevel, using tailored initialization schemes (Bachlechner et al., 2021; De and Smith, 2020; Zhang et al., 2019),\nself-normalizing activations (Klambauer et al., 2017), weight normalization (Salimans and Kingma, 2016; Brock\net al., 2021a), or adaptive gradient clipping (Brock et al., 2021b) to maintain stable gradient propagation.\nAnother line of work modifies the architecture through structural simplifications (He and Hofmann, 2024) and\nSoftmax-only formulations (Jha and Reagen, 2024). More recently, point-wise functions such as Dynamic Tanh\n(Zhu et al., 2025) have been proposed, with theoretical analyses revealing their similarity to normalization\noperations (Stollenwerk, 2025). Unlike previous methods that aim to match the performance of normalization\nlayers, Derf consistently delivers stronger performance across diverse models.\n9 Conclusion\nIn this work, we demonstrate that well-designed point-wise functions do not merely match the performance of\nnormalization layers, but can surpass them. By revisiting the design space of point-wise functions, we identify\nzero-centeredness, boundedness, center sensitivity, and monotonicity as four key properties that enable strong\nperformance in Transformer-based models. Among the functions satisfying these properties, Derf stands out\nas the most effective design: it consistently outperforms normalization-based methods and another notable\npoint-wise function, DyT, across a wide range of modalities and tasks. Its simplicity and strong empirical\nperformance make Derf a compelling replacement for normalization layers in many Transformer architectures.\n12", "clean_text": "7.2 Approximating Derf Given the superior performance of erf(ğ‘¥) over tanh(ğ‘¥), we approximate erf(ğ‘¥) by scaling tanh(ğ‘¥) and examine whether this modification can lead to performance improvement. We introduce a fixed coefficient ğœ€and use tanh(ğœ€ğ‘¥), where ğœ€is obtained by minimizing the following objective: min ğœ€ âˆ«ï¸+âˆ âˆ’âˆ âƒ’âƒ’tanh(ğœ€ğ‘¥) âˆ’erf(ğ‘¥) âƒ’âƒ’ğ‘‘ğ‘¥. (11) The optimal value is found to be ğœ€â‰ˆ1.205. As shown in Table 16, tanh(ğœ€ğ‘¥) achieves a comparable or slightly improved performance over the original tanh(ğ‘¥), while still performing worse than erf(ğ‘¥). This indicates that simply scaling tanh(ğ‘¥) is insufficient to match the behavior or performance of erf(ğ‘¥). 8 Related Work Normalization layers. Since the introduction of Batch Normalization (BN) (Ioffe and Szegedy, 2015), various normalization methods have been proposed to better stabilize training. To address BNâ€™s limitations with small batches, several alternatives (Salimans and Kingma, 2016; Wu and He, 2018; Yan et al., 2020; Shen et al., 2020; Singh and Krishnan, 2020) have been explored. In parallel, LayerNorm (Ba et al., 2016; Nguyen and Salazar, 2019; Xu et al., 2019; Xiong et al., 2020) and RMSNorm (Zhang and Sennrich, 2019) were designed for RNN (Hochreiter and Schmidhuber, 1997) and Transformer architectures (Vaswani et al., 2017). Task-specific variants (Ulyanov et al., 2016; Wu and He, 2018; Shen et al., 2020) further adapt normalization to applications such as object detection and style transfer. Mechanisms of normalization. A series of studies has investigated how normalization layers contribute to model convergence. From an optimization perspective, normalization stabilizes gradient flow (Balduzzi et al., 2017; Daneshmand et al., 2020; Lubana et al., 2021), reduces sensitivity to initialization (Zhang et al., 2019; De and Smith, 2020; Shao et al., 2020), and implicitly tunes learning rates (Arora et al., 2019; Tanaka and Kunin, 2021). It has also been shown to smooth the loss landscape (Santurkar et al., 2018; Bjorck et al., 2018; Karakida et al., 2019) and reduce sharpness (Lyu et al., 2022; Dai et al., 2023; Mueller et al., 2023), promoting more stable optimization dynamics. Understanding these underlying functionalities provides valuable guidance for designing normalization-free training methods. Normalization-free methods. Building on this understanding of normalization, recent work explores how to achieve stable convergence without normalization. One line of work operates at the parameter and optimization level, using tailored initialization schemes (Bachlechner et al., 2021; De and Smith, 2020; Zhang et al., 2019), self-normalizing activations (Klambauer et al., 2017), weight normalization (Salimans and Kingma, 2016; Brock et al., 2021a), or adaptive gradient clipping (Brock et al., 2021b) to maintain stable gradient propagation. Another line of work modifies the architecture through structural simplifications (He and Hofmann, 2024) and Softmax-only formulations (Jha and Reagen, 2024). More recently, point-wise functions such as Dynamic Tanh (Zhu et al., 2025) have been proposed, with theoretical analyses revealing their similarity to normalization operations (Stollenwerk, 2025). Unlike previous methods that aim to match the performance of normalization layers, Derf consistently delivers stronger performance across diverse models. 9 Conclusion In this work, we demonstrate that well-designed point-wise functions do not merely match the performance of normalization layers, but can surpass them. By revisiting the design space of point-wise functions, we identify zero-centeredness, boundedness, center sensitivity, and monotonicity as four key properties that enable strong performance in Transformer-based models. Among the functions satisfying these properties, Derf stands out as the most effective design: it consistently outperforms normalization-based methods and another notable point-wise function, DyT, across a wide range of modalities and tasks. Its simplicity and strong empirical performance make Derf a compelling replacement for normalization layers in many Transformer architectures. 12"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 13, "text": "Acknowledgments\nWe gratefully acknowledge the use of the Neuronic GPU computing cluster maintained by the Department of\nComputer Science at Princeton University. This work was substantially performed using Princeton Research\nComputing resources, a consortium led by the Princeton Institute for Computational Science and Engineering\n(PICSciE) and Research Computing at Princeton University. This work is also supported by the computational\nresources generously provided by Googleâ€™s TPU Research Cloud program.\nReferences\nSanjeev Arora, Zhiyuan Li, and Kaifeng Lyu. Theoretical analysis of auto rate-tuning by batch normalization.\nICLR, 2019.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\nLayer normalization.\narXiv preprint\narXiv:1607.06450, 2016.\nThomas Bachlechner, Bodhisattwa Prasad Majumder, Henry Mao, Gary Cottrell, and Julian McAuley. Rezero\nis all you need: Fast convergence at large depth. In UAI, 2021.\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for\nself-supervised learning of speech representations. In NeurIPS, 2020.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.\nDavid Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The\nshattered gradients problem: If resnets are the answer, then what is the question? In ICML, 2017.\nNils Bjorck, Carla P Gomes, Bart Selman, and Kilian Q Weinberger. Understanding batch normalization. In\nNeurIPS, 2018.\nAndrew Brock, Soham De, and Samuel L Smith. Characterizing signal propagation to close the performance\ngap in unnormalized resnets. ICLR, 2021a.\nAndrew Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-scale image\nrecognition without normalization. In ICML, 2021b.\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\nEnd-to-end object detection with transformers. In ECCV, 2020.\nZhaodong Chen, Lei Deng, Guoqi Li, Jiawei Sun, Xing Hu, Ling Liang, Yufei Ding, and Yuan Xie. Effective\nand efficient batch normalization using a few uncorrelated data for statistics estimation. IEEE Transactions\non Neural Networks and Learning Systems, 2020.\nYan Dai, Kwangjun Ahn, and Suvrit Sra. The crucial role of normalization in sharpness-aware minimization.\nIn NeurIPS, 2023.\nHadi Daneshmand, Jonas Kohler, Francis Bach, Thomas Hofmann, and Aurelien Lucchi. Batch normalization\nprovably avoids ranks collapse for randomly initialised deep networks. In NeurIPS, 2020.\nSoham De and Sam Smith. Batch normalization biases residual blocks towards the identity function in deep\nnetworks. In NeurIPS, 2020.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In CVPR, 2009.\nAlexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\n13", "clean_text": "Acknowledgments We gratefully acknowledge the use of the Neuronic GPU computing cluster maintained by the Department of Computer Science at Princeton University. This work was substantially performed using Princeton Research Computing resources, a consortium led by the Princeton Institute for Computational Science and Engineering (PICSciE) and Research Computing at Princeton University. This work is also supported by the computational resources generously provided by Googleâ€™s TPU Research Cloud program. References Sanjeev Arora, Zhiyuan Li, and Kaifeng Lyu. Theoretical analysis of auto rate-tuning by batch normalization. ICLR, 2019. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Thomas Bachlechner, Bodhisattwa Prasad Majumder, Henry Mao, Gary Cottrell, and Julian McAuley. Rezero is all you need: Fast convergence at large depth. In UAI, 2021. Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations. In NeurIPS, 2020. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? In ICML, 2017. Nils Bjorck, Carla P Gomes, Bart Selman, and Kilian Q Weinberger. Understanding batch normalization. In NeurIPS, 2018. Andrew Brock, Soham De, and Samuel L Smith. Characterizing signal propagation to close the performance gap in unnormalized resnets. ICLR, 2021a. Andrew Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-scale image recognition without normalization. In ICML, 2021b. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020. Zhaodong Chen, Lei Deng, Guoqi Li, Jiawei Sun, Xing Hu, Ling Liang, Yufei Ding, and Yuan Xie. Effective and efficient batch normalization using a few uncorrelated data for statistics estimation. IEEE Transactions on Neural Networks and Learning Systems, 2020. Yan Dai, Kwangjun Ahn, and Suvrit Sra. The crucial role of normalization in sharpness-aware minimization. In NeurIPS, 2023. Hadi Daneshmand, Jonas Kohler, Francis Bach, Thomas Hofmann, and Aurelien Lucchi. Batch normalization provably avoids ranks collapse for randomly initialised deep networks. In NeurIPS, 2020. Soham De and Sam Smith. Batch normalization biases residual blocks towards the identity function in deep networks. In NeurIPS, 2020. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 13"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 14, "text": "Ensembl GRCh38. p13 (genome reference consortium human build 38), insdc assembly, 2013.\nKatarÃ­na GreÅ¡ovÃ¡, Vlastimil Martinek, David ÄŒechÃ¡k, Petr Å imeÄek, and Panagiotis Alexiou. Genomic\nbenchmarks: a collection of datasets for genomic sequence classification. BMC Genomic Data, 2023.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma,\nPeiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement\nlearning. arXiv preprint arXiv:2501.12948, 2025.\nBobby He and Thomas Hofmann. Simplifying transformer blocks. ICLR, 2024.\nStefan Heimersheim. You can remove gpt2â€™s layernorm by fine-tuning. arXiv preprint arXiv:2409.13710, 2024.\nSepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural Computation, 1997.\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Weinberger. Deep networks with stochastic depth,\n2016a.\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic\ndepth. In ECCV, 2016b.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. In ICML, 2015.\nNandan Kumar Jha and Brandon Reagen. Aero: Softmax-only llms for efficient private inference. arXiv\npreprint arXiv:2410.13060, 2024.\nRyo Karakida, Shotaro Akaho, and Shun-ichi Amari. The normalization method for alleviating pathological\nsharpness in wide neural networks. In NeurIPS, 2019.\nGÃ¼nter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural\nnetworks. In NeurIPS, 2017.\nXiangru Lian and Ji Liu. Revisit batch normalization: New understanding and refinement via composition\noptimization. In AISTATS, 2019.\nAixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan,\nDamai Dai, Daya Guo, et al. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language\nmodel. arXiv preprint arXiv:2405.04434, 2024.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021.\nZhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet\nfor the 2020s. In CVPR, 2022.\nEkdeep S Lubana, Robert Dick, and Hidenori Tanaka. Beyond batchnorm: Towards a unified understanding\nof normalization in deep learning. In NeurIPS, 2021.\nKaifeng Lyu, Zhiyuan Li, and Sanjeev Arora. Understanding the generalization benefit of normalization\nlayers: Sharpness reduction. In NeurIPS, 2022.\nMaximilian Mueller, Tiffany Vlaar, David Rolnick, and Matthias Hein. Normalization layers are all that\nsharpness-aware minimization needs. In NeurIPS, 2023.\nEric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Michael Wornow, Callum Birch-Sykes, Stefano\nMassaroli, Aman Patel, Clayton Rabideau, Yoshua Bengio, et al. Hyenadna: Long-range genomic sequence\nmodeling at single nucleotide resolution. In NeurIPS, 2023.\nToan Q Nguyen and Julian Salazar. Transformers without tears: Improving the normalization of self-attention.\nIWSLT, 2019.\n14", "clean_text": "Ensembl GRCh38. p13 (genome reference consortium human build 38), insdc assembly, 2013. KatarÃ­na GreÅ¡ovÃ¡, Vlastimil Martinek, David ÄŒechÃ¡k, Petr Å imeÄek, and Panagiotis Alexiou. Genomic benchmarks: a collection of datasets for genomic sequence classification. BMC Genomic Data, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Bobby He and Thomas Hofmann. Simplifying transformer blocks. ICLR, 2024. Stefan Heimersheim. You can remove gpt2â€™s layernorm by fine-tuning. arXiv preprint arXiv:2409.13710, 2024. Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural Computation, 1997. Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Weinberger. Deep networks with stochastic depth, 2016a. Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In ECCV, 2016b. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. Nandan Kumar Jha and Brandon Reagen. Aero: Softmax-only llms for efficient private inference. arXiv preprint arXiv:2410.13060, 2024. Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari. The normalization method for alleviating pathological sharpness in wide neural networks. In NeurIPS, 2019. GÃ¼nter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural networks. In NeurIPS, 2017. Xiangru Lian and Ji Liu. Revisit batch normalization: New understanding and refinement via composition optimization. In AISTATS, 2019. Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021. Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In CVPR, 2022. Ekdeep S Lubana, Robert Dick, and Hidenori Tanaka. Beyond batchnorm: Towards a unified understanding of normalization in deep learning. In NeurIPS, 2021. Kaifeng Lyu, Zhiyuan Li, and Sanjeev Arora. Understanding the generalization benefit of normalization layers: Sharpness reduction. In NeurIPS, 2022. Maximilian Mueller, Tiffany Vlaar, David Rolnick, and Matthias Hein. Normalization layers are all that sharpness-aware minimization needs. In NeurIPS, 2023. Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Michael Wornow, Callum Birch-Sykes, Stefano Massaroli, Aman Patel, Clayton Rabideau, Yoshua Bengio, et al. Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution. In NeurIPS, 2023. Toan Q Nguyen and Julian Salazar. Transformers without tears: Improving the normalization of self-attention. IWSLT, 2019. 14"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 15, "text": "Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on\npublic domain audio books. In ICASSP, 2015.\nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\nJMLR, 2020.\nTim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate training\nof deep neural networks. In NeurIPS, 2016.\nShibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normalization\nhelp optimization? In NeurIPS, 2018.\nYair Schiff, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. Caduceus:\nBi-directional equivariant long-range dna sequence modeling. In ICML, 2024.\nJie Shao, Kai Hu, Changhu Wang, Xiangyang Xue, and Bhiksha Raj. Is normalization indispensable for\ntraining deep neural network? In NeurIPS, 2020.\nSheng Shen, Zhewei Yao, Amir Gholami, Michael Mahoney, and Kurt Keutzer. Powernorm: Rethinking batch\nnormalization in transformers. In ICML, 2020.\nSaurabh Singh and Shankar Krishnan. Filter response normalization layer: Eliminating batch dependence in\nthe training of deep neural networks. In CVPR, 2020.\nFelix Stollenwerk.\nThe mathematical relationship between layer normalization and dynamic activation\nfunctions. arXiv preprint arXiv:2503.21708, 2025.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the\ninception architecture for computer vision. In CVPR, 2016.\nHidenori Tanaka and Daniel Kunin. Noetherâ€™s learning dynamics: Role of symmetry breaking in neural\nnetworks. In NeurIPS, 2021.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix,\nBaptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and\nfine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\nDmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for\nfast stylization. arXiv preprint arXiv:1607.08022, 2016.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\nYuxin Wu and Kaiming He. Group normalization. In ECCV, 2018.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan,\nLiwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In ICML, 2020.\nJingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin. Understanding and improving\nlayer normalization. In NeurIPS, 2019.\nJunjie Yan, Ruosi Wan, Xiangyu Zhang, Wei Zhang, Yichen Wei, and Jian Sun. Towards stabilizing batch\nstatistics in backward propagation of batch normalization. ICLR, 2020.\n15", "clean_text": "Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In ICASSP, 2015. William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR, 2020. Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In NeurIPS, 2016. Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normalization help optimization? In NeurIPS, 2018. Yair Schiff, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. Caduceus: Bi-directional equivariant long-range dna sequence modeling. In ICML, 2024. Jie Shao, Kai Hu, Changhu Wang, Xiangyang Xue, and Bhiksha Raj. Is normalization indispensable for training deep neural network? In NeurIPS, 2020. Sheng Shen, Zhewei Yao, Amir Gholami, Michael Mahoney, and Kurt Keutzer. Powernorm: Rethinking batch normalization in transformers. In ICML, 2020. Saurabh Singh and Shankar Krishnan. Filter response normalization layer: Eliminating batch dependence in the training of deep neural networks. In CVPR, 2020. Felix Stollenwerk. The mathematical relationship between layer normalization and dynamic activation functions. arXiv preprint arXiv:2503.21708, 2025. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In CVPR, 2016. Hidenori Tanaka and Daniel Kunin. Noetherâ€™s learning dynamics: Role of symmetry breaking in neural networks. In NeurIPS, 2021. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. Yuxin Wu and Kaiming He. Group normalization. In ECCV, 2018. Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In ICML, 2020. Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin. Understanding and improving layer normalization. In NeurIPS, 2019. Junjie Yan, Ruosi Wan, Xiangyu Zhang, Wei Zhang, Yichen Wei, and Jian Sun. Towards stabilizing batch statistics in backward propagation of batch normalization. ICLR, 2020. 15"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 16, "text": "An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li,\nDayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024.\nQiming Yang, Kai Zhang, Chaoxiang Lan, Zhi Yang, Zheyang Li, Wenming Tan, Jun Xiao, and Shiliang Pu.\nUnified normalization for accelerating and stabilizing transformers. In ACM MM, 2022.\nSangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix:\nRegularization strategy to train strong classifiers with localizable features. In ICCV, 2019.\nBiao Zhang and Rico Sennrich. Root mean square layer normalization. In NeurIPS, 2019.\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk\nminimization. 2018.\nHongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without normalization.\nICLR, 2019.\nZhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In\nAAAI, 2020.\nJiachen Zhu, Xinlei Chen, Kaiming He, Yann LeCun, and Zhuang Liu. Transformers without normalization.\nIn CVPR, 2025.\nAppendix\nA Property Analysis Details\nIn this section, we provided detailed explanation and visualization on how different function properties affect\nmodel training.\nA.1 Zero-centeredness\nWe plot the training curves for ğœ†horiz and ğœ†vert with values {0, 0.1, 1} in Figure 6. The trends are consistent\nwith those observed in top-1 accuracy on ImageNet-1K. For horizontal shifts, the training loss with ğœ†horiz = 0.1\nnearly overlaps with that of ğœ†horiz = 0, and even reaches a slightly lower loss. In contrast, vertical shifts\nexhibit a monotonic pattern: increasing ğœ†vert consistently raises the training loss, suggesting reduced fitting\ncapacity under larger vertical shift.\n0\n50\n100\n150\n200\n250\n300\nepoch\n3\n4\n5\n6\n7\nloss\nÎ» = 0\nÎ» = 0.1\nÎ» = 1\n(a) Horizontal shift\n0\n50\n100\n150\n200\n250\n300\nepoch\n3\n4\n5\n6\n7\nloss\nÎ» = 0\nÎ» = 0.1\nÎ» = 1\n(b) Vertical shift\nFigure 6 Training loss curve for horizontal and vertical shifts on the base point-wise function erf(ğ‘¥). The trends are\nconsistent with the patterns observed in top-1 accuracy on ImageNet-1K.\nA.2 Center Sensitivity\nWe visualize the training losses obtained as ğœ†varies over {0, 0.1, 0.5, 1.0, 2.0} on the base point-wise function\nerf(ğ‘¥). As shown in Figure 7, training loss shows a clear monotonic trend: larger ğœ†consistently leads to\nhigher loss, indicating that the width of the flat zone directly limits the modelâ€™s fitting capacity.\n16", "clean_text": "An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Qiming Yang, Kai Zhang, Chaoxiang Lan, Zhi Yang, Zheyang Li, Wenming Tan, Jun Xiao, and Shiliang Pu. Unified normalization for accelerating and stabilizing transformers. In ACM MM, 2022. Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In ICCV, 2019. Biao Zhang and Rico Sennrich. Root mean square layer normalization. In NeurIPS, 2019. Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. 2018. Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without normalization. ICLR, 2019. Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In AAAI, 2020. Jiachen Zhu, Xinlei Chen, Kaiming He, Yann LeCun, and Zhuang Liu. Transformers without normalization. In CVPR, 2025. Appendix A Property Analysis Details In this section, we provided detailed explanation and visualization on how different function properties affect model training. A.1 Zero-centeredness We plot the training curves for ğœ†horiz and ğœ†vert with values {0, 0.1, 1} in Figure 6. The trends are consistent with those observed in top-1 accuracy on ImageNet-1K. For horizontal shifts, the training loss with ğœ†horiz = 0.1 nearly overlaps with that of ğœ†horiz = 0, and even reaches a slightly lower loss. In contrast, vertical shifts exhibit a monotonic pattern: increasing ğœ†vert consistently raises the training loss, suggesting reduced fitting capacity under larger vertical shift. 0 50 100 150 200 250 300 epoch 3 4 5 6 7 loss Î» = 0 Î» = 0.1 Î» = 1 (a) Horizontal shift 0 50 100 150 200 250 300 epoch 3 4 5 6 7 loss Î» = 0 Î» = 0.1 Î» = 1 (b) Vertical shift Figure 6 Training loss curve for horizontal and vertical shifts on the base point-wise function erf(ğ‘¥). The trends are consistent with the patterns observed in top-1 accuracy on ImageNet-1K. A.2 Center Sensitivity We visualize the training losses obtained as ğœ†varies over {0, 0.1, 0.5, 1.0, 2.0} on the base point-wise function erf(ğ‘¥). As shown in Figure 7, training loss shows a clear monotonic trend: larger ğœ†consistently leads to higher loss, indicating that the width of the flat zone directly limits the modelâ€™s fitting capacity. 16"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 17, "text": "0\n50\n100\n150\n200\n250\n300\nepoch\n3\n4\n5\n6\n7\nloss\nÎ» = 0\nÎ» = 0.1\nÎ» = 0.5\nÎ» = 1.0\nÎ» = 2.0\nFigure 7 Training loss curve for different center\nsensitivity (controlled by ğœ†). A larger ğœ†leads to\nhigher training loss and poorer fitting ability.\n0\n50\n100\n150\n200\n250\n300\nepoch\n3\n4\n5\n6\n7\nloss\nerf(x)\nsin(x)\nnegerf(x)\ndampx(x)\nFigure 8 Training loss curve for different monotonicity. Mono-\ntonic functions consistently achieve lower training loss than\nnon-monotonic functions.\nA.3 Monotonicity\nWe plot the training losses of four functions with distinct monotonicity patterns: the monotonically increasing\nerf(ğ‘¥), the monotonically decreasing negerf(ğ‘¥), the hump-shaped dampx(ğ‘¥), and the oscillatory sin(ğ‘¥). As\nshown in Figure 8, both increasing and decreasing monotonic functions achieve clearly lower training loss,\nindicating stronger fitting capacity. In contrast, the non-monotonic functions exhibit higher training loss.\nThis behavior aligns closely with the top-1 accuracy trends observed on ImageNet-1K.\nB Function Search Details\nIn function search, a wide variety of common functional forms are systematically explored under the constraint\nof our function properties. The candidates range from polynomial and rational functions to the trigonometric\nand hyperbolic families, as well as various cumulative distribution functions. Beyond these common functional\nforms, we also experiment with their variants through translation, scaling, concatenation, and clipping.\nWe categorize all candidate functions (see Table 7) into four groups: natural functions, transformed basic\nfunctions, clipped unbounded functions, and canonical ratio functions, and present detailed descriptions and\nvisualizations of how each group is constructed.\nNatural functions. This category consists of three functions: erf(ğ‘¥), tanh(ğ‘¥), and arctan(ğ‘¥). As shown in\nFigure 9, these functions naturally satisfy all the function properties, including zero-centeredness, boundedness,\ncenter sensitivity, and monotonicity. Among them, only arctan(ğ‘¥) is rescaled so that all three functions have\ntheir ranges unified to [âˆ’1, 1].\nTransformed basic functions. This category consists of six functions: satursin(ğ‘¥), expsign(ğ‘¥), exproot(ğ‘¥),\nrelsign(ğ‘¥), isru(ğ‘¥), and cubsign(ğ‘¥). These functions are constructed by starting from simple and commonly\nused primitives, such as power functions and polynomial forms. Through transformations including translation,\nscaling, and rotation, we reshape their original structures so that they satisfy all four function properties\nwhile preserving the qualitative behavior of the underlying base functions, as shown in Figure 10.\nClipped unbounded functions. This category consists of five functions: logsign(ğ‘¥), logquad(ğ‘¥), arcsinh(ğ‘¥),\npower23(ğ‘¥), and linear(ğ‘¥). These functions inherently satisfy zero-centeredness and center sensitivity. For\nlogsignclip(ğ‘¥), logquad(ğ‘¥), and power23clip(ğ‘¥), either due to domain asymmetry or because the original form\nis not monotonic, we construct the negative branch by mirroring the positive side around the origin to ensure\nmonotonicity, as shown in Figure 11. To additionally enforce boundedness, we clip their outputs to the interval\n[âˆ’1, 1], which leads to improved performance in practice.\nCanonical ratio functions. This category consists of two functions: saturlog(ğ‘¥) and smoothsign(ğ‘¥). Both\nfunctions are constructed using the canonical ratio form\nğ‘“(ğ‘¥)\n|ğ‘“(ğ‘¥)|+1, which naturally enforces boundedness\nand monotonicity. By selecting ğ‘“(ğ‘¥) to be an odd, zero-centered base function, the resulting ratio form\nautomatically satisfies zero-centeredness and center sensitivity as well. As shown in Figure 12, this construction\nyields smooth saturating behaviors that remain stable across a wide input range.\n17", "clean_text": "0 50 100 150 200 250 300 epoch 3 4 5 6 7 loss Î» = 0 Î» = 0.1 Î» = 0.5 Î» = 1.0 Î» = 2.0 Figure 7 Training loss curve for different center sensitivity (controlled by ğœ†). A larger ğœ†leads to higher training loss and poorer fitting ability. 0 50 100 150 200 250 300 epoch 3 4 5 6 7 loss erf(x) sin(x) negerf(x) dampx(x) Figure 8 Training loss curve for different monotonicity. Monotonic functions consistently achieve lower training loss than non-monotonic functions. A.3 Monotonicity We plot the training losses of four functions with distinct monotonicity patterns: the monotonically increasing erf(ğ‘¥), the monotonically decreasing negerf(ğ‘¥), the hump-shaped dampx(ğ‘¥), and the oscillatory sin(ğ‘¥). As shown in Figure 8, both increasing and decreasing monotonic functions achieve clearly lower training loss, indicating stronger fitting capacity. In contrast, the non-monotonic functions exhibit higher training loss. This behavior aligns closely with the top-1 accuracy trends observed on ImageNet-1K. B Function Search Details In function search, a wide variety of common functional forms are systematically explored under the constraint of our function properties. The candidates range from polynomial and rational functions to the trigonometric and hyperbolic families, as well as various cumulative distribution functions. Beyond these common functional forms, we also experiment with their variants through translation, scaling, concatenation, and clipping. We categorize all candidate functions (see Table 7) into four groups: natural functions, transformed basic functions, clipped unbounded functions, and canonical ratio functions, and present detailed descriptions and visualizations of how each group is constructed. Natural functions. This category consists of three functions: erf(ğ‘¥), tanh(ğ‘¥), and arctan(ğ‘¥). As shown in Figure 9, these functions naturally satisfy all the function properties, including zero-centeredness, boundedness, center sensitivity, and monotonicity. Among them, only arctan(ğ‘¥) is rescaled so that all three functions have their ranges unified to [âˆ’1, 1]. Transformed basic functions. This category consists of six functions: satursin(ğ‘¥), expsign(ğ‘¥), exproot(ğ‘¥), relsign(ğ‘¥), isru(ğ‘¥), and cubsign(ğ‘¥). These functions are constructed by starting from simple and commonly used primitives, such as power functions and polynomial forms. Through transformations including translation, scaling, and rotation, we reshape their original structures so that they satisfy all four function properties while preserving the qualitative behavior of the underlying base functions, as shown in Figure 10. Clipped unbounded functions. This category consists of five functions: logsign(ğ‘¥), logquad(ğ‘¥), arcsinh(ğ‘¥), power23(ğ‘¥), and linear(ğ‘¥). These functions inherently satisfy zero-centeredness and center sensitivity. For logsignclip(ğ‘¥), logquad(ğ‘¥), and power23clip(ğ‘¥), either due to domain asymmetry or because the original form is not monotonic, we construct the negative branch by mirroring the positive side around the origin to ensure monotonicity, as shown in Figure 11. To additionally enforce boundedness, we clip their outputs to the interval [âˆ’1, 1], which leads to improved performance in practice. Canonical ratio functions. This category consists of two functions: saturlog(ğ‘¥) and smoothsign(ğ‘¥). Both functions are constructed using the canonical ratio form ğ‘“(ğ‘¥) |ğ‘“(ğ‘¥)|+1, which naturally enforces boundedness and monotonicity. By selecting ğ‘“(ğ‘¥) to be an odd, zero-centered base function, the resulting ratio form automatically satisfies zero-centeredness and center sensitivity as well. As shown in Figure 12, this construction yields smooth saturating behaviors that remain stable across a wide input range. 17"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 18, "text": "-3\n-2\n-1\n0\n1\n2\n3\n-1\n-0.5\n0\n0.5\n1\nerf(x)\ntanh(x)\narctan(x)\nFigure 9 Visualization of natural functions.\n4\n2\n0\n2\n4\n1.0\n0.5\n0.0\n0.5\n1.0\nOriginal Function\nsin(x)\n4\n2\n0\n2\n4\n1.0\n0.5\n0.0\n0.5\n1.0\nAfter Transformation\nsatursin(x)\n2\n0\n2\n4\n0\n1\n2\n3\n4\n5\neâˆ’x\n4\n2\n0\n2\n4\n1.0\n0.5\n0.0\n0.5\n1.0\nexpsign(x)\n0\n2\n4\n6\n8\n0.00\n0.25\n0.50\n0.75\n1.00\neâˆ’\npx\n4\n2\n0\n2\n4\n1.0\n0.5\n0.0\n0.5\n1.0\nexproot(x)\n4\n2\n0\n2\n4\n1\n0\n1\n2\n3\n4\np\n1 + x2\n4\n2\n0\n2\n4\n1.0\n0.5\n0.0\n0.5\n1.0\nrelsign(x)\n0\n1\n2\n3\n4\n0.0\n0.5\n1.0\n1.5\n(1 + x2)âˆ’1\n4\n2\n0\n2\n4\n1.0\n0.5\n0.0\n0.5\n1.0\nisru(x)\n4\n2\n0\n2\n4\n20\n10\n0\n10\n20\nxâˆ’3\n4\n2\n0\n2\n4\n1.0\n0.5\n0.0\n0.5\n1.0\ncubsign(x)\nFigure 10 Visualization of transformed basic functions.\n0\n1\n2\n3\n4\n0\n1\n2\n3\nOriginal Function\nln(x + 1)\n4\n2\n0\n2\n4\n1.0\n0.5\n0.0\n0.5\n1.0\nAfter Transformation\nlogsignclip(x)\n0\n1\n2\n3\n4\n0\n1\n2\n3\nln(x2 + 1)\n4\n2\n0\n2\n4\n1.0\n0.5\n0.0\n0.5\n1.0\nlogquadclip(x)\n4\n2\n0\n2\n4\n2\n1\n0\n1\n2\narcsinh(x)\n4\n2\n0\n2\n4\n1.0\n0.5\n0.0\n0.5\n1.0\narcsinhclip(x)\n0\n1\n2\n3\n4\n0\n1\n2\n3\nx2/3\n4\n2\n0\n2\n4\n1.0\n0.5\n0.0\n0.5\n1.0\npower23clip(x)\n4\n2\n0\n2\n4\n4\n2\n0\n2\n4\nx\n4\n2\n0\n2\n4\n1.0\n0.5\n0.0\n0.5\n1.0\nlinearclip(x)\nFigure 11 Visualization of clipped unbounded functions.\n4\n2\n0\n2\n4\n4\n2\n0\n2\n4\nOriginal Function\nx\n4\n2\n0\n2\n4\n1.0\n0.5\n0.0\n0.5\n1.0\nAfter Transformation\nsmoothsign(x)\n8\n4\n0\n4\n8\n2\n1\n0\n1\n2\nsign(x) Â· ln(|x| + 1)\n4\n2\n0\n2\n4\n1.0\n0.5\n0.0\n0.5\n1.0\nsaturlog(x)\nFigure 12 Visualization of canonical ratio functions.\n18", "clean_text": "-3 -2 -1 0 1 2 3 -1 -0.5 0 0.5 1 erf(x) tanh(x) arctan(x) Figure 9 Visualization of natural functions. 4 2 0 2 4 1.0 0.5 0.0 0.5 1.0 Original Function sin(x) 4 2 0 2 4 1.0 0.5 0.0 0.5 1.0 After Transformation satursin(x) 2 0 2 4 0 1 2 3 4 5 eâˆ’x 4 2 0 2 4 1.0 0.5 0.0 0.5 1.0 expsign(x) 0 2 4 6 8 0.00 0.25 0.50 0.75 1.00 eâˆ’ px 4 2 0 2 4 1.0 0.5 0.0 0.5 1.0 exproot(x) 4 2 0 2 4 1 0 1 2 3 4 p 1 + x2 4 2 0 2 4 1.0 0.5 0.0 0.5 1.0 relsign(x) 0 1 2 3 4 0.0 0.5 1.0 1.5 (1 + x2)âˆ’1 4 2 0 2 4 1.0 0.5 0.0 0.5 1.0 isru(x) 4 2 0 2 4 20 10 0 10 20 xâˆ’3 4 2 0 2 4 1.0 0.5 0.0 0.5 1.0 cubsign(x) Figure 10 Visualization of transformed basic functions. 0 1 2 3 4 0 1 2 3 Original Function ln(x + 1) 4 2 0 2 4 1.0 0.5 0.0 0.5 1.0 After Transformation logsignclip(x) 0 1 2 3 4 0 1 2 3 ln(x2 + 1) 4 2 0 2 4 1.0 0.5 0.0 0.5 1.0 logquadclip(x) 4 2 0 2 4 2 1 0 1 2 arcsinh(x) 4 2 0 2 4 1.0 0.5 0.0 0.5 1.0 arcsinhclip(x) 0 1 2 3 4 0 1 2 3 x2/3 4 2 0 2 4 1.0 0.5 0.0 0.5 1.0 power23clip(x) 4 2 0 2 4 4 2 0 2 4 x 4 2 0 2 4 1.0 0.5 0.0 0.5 1.0 linearclip(x) Figure 11 Visualization of clipped unbounded functions. 4 2 0 2 4 4 2 0 2 4 Original Function x 4 2 0 2 4 1.0 0.5 0.0 0.5 1.0 After Transformation smoothsign(x) 8 4 0 4 8 2 1 0 1 2 sign(x) Â· ln(|x| + 1) 4 2 0 2 4 1.0 0.5 0.0 0.5 1.0 saturlog(x) Figure 12 Visualization of canonical ratio functions. 18"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 19, "text": "C Experimental Settings\nVision Transformers. For all supervised classification experiments on ImageNet-1K, we adopt the training\nconfigurations summarized in Table 17. ViT-B and ViT-L share the same hyperparameters, except that ViT-L\nemploys a modified AdamW momentum setting with (ğ›½1=0.9, ğ›½2=0.95) and a higher stochastic depth rate of\n0.5.\nconfig\nvalue\noptimizer\nAdamW\nbase learning rate\n4e-3\nweight decay\n0.05\noptimizer momentum\nğ›½1=0.9, ğ›½2=0.999 (ğµ), 0.95 (ğ¿)\neffective batch size\n4096\nlearning rate schedule\ncosine decay\nwarmup epochs\n20\ntraining epochs\n300\naugmentation\nrand-m9-mstd0.5-inc1\nlabel smoothing (Szegedy et al., 2016)\n0.1\nmixup (Zhang et al., 2018)\n0.8\ncutmix (Yun et al., 2019)\n1.0\nrandom erase (Zhong et al., 2020)\n0.25\ndrop path (Huang et al., 2016b)\n0.15 (B), 0.5 (L)\nexp. moving average (EMA)\n0.9999\nTable 17 Training Configurations of ViT.\nDiffusion Transformers. We use the official implementation (Peebles and Xie, 2023) to train all DiT model\nsizes as shown in Table 18. We observe that the default learning rate is suboptimal for the models in this\nwork. For both the search function experiments and the final evaluation of Derf, we go through three learning\nrates, 1 Ã— 10âˆ’4, 2 Ã— 10âˆ’4, and 4 Ã— 10âˆ’4, for all models, whether they use LayerNorm or a point-wise function,\nand report the best result. We also observe that the zero initialization negatively affects the performance of\nDerf models and other point-wise function models. Therefore, we retain the zero initialization for LN models\nbut remove it for the other models.\nconfig\nvalue\noptimizer\nAdamW\nbase learning rate\n{1e-4, 2e-4, 4e-4}\nweight decay\n0\noptimizer momentum\nğ›½1=0.9, ğ›½2=0.999\neffective batch size\n256\nlearning rate schedule\nconstant\ntraining epochs\n80\nexp. moving average (EMA)\n0.9999\nTable 18 Training Configurations of DiT.\nSpeech models. For both wav2vec 2.0 models, we retain the first GroupNorm layer and the LayerNorm\nlocated after the convolutional feature extractor, since both primarily serve as data normalization to handle\nthe unnormalized input data. We use the official implementation (Baevski et al., 2020) for both the Base and\nLarge models, keeping all hyperparameters identical to the original setup, as shown in Table 19. The only\nchange we make is running all modelsâ€”whether normalization-based or point-wise-function-basedâ€”in fp32\nprecision instead of the default bf16. We report the final validation loss.\nDNA models. For both the HyenaDNA model (Nguyen et al., 2023) and the Caduceus model (Schiff et al.,\n2024), we directly follow their official implementations without modifying hyperparameters, as shown in\nTable 20. In particular, Hyena uses LayerNorm and Caduceus uses RMSNorm. For our evaluation, we replace\neach modelâ€™s original normalization layer with Derf and report the average accuracy across all tasks.\nLanguage models. For the GPT-2 (124M) model, we follow the hyperparameters as shown in Table 21.\nFor Derf and DyT, we configure the ğ›¼initialization separately for the point-wise function layer following\n19", "clean_text": "C Experimental Settings Vision Transformers. For all supervised classification experiments on ImageNet-1K, we adopt the training configurations summarized in Table 17. ViT-B and ViT-L share the same hyperparameters, except that ViT-L employs a modified AdamW momentum setting with (ğ›½1=0.9, ğ›½2=0.95) and a higher stochastic depth rate of 0.5. config value optimizer AdamW base learning rate 4e-3 weight decay 0.05 optimizer momentum ğ›½1=0.9, ğ›½2=0.999 (ğµ), 0.95 (ğ¿) effective batch size 4096 learning rate schedule cosine decay warmup epochs 20 training epochs 300 augmentation rand-m9-mstd0.5-inc1 label smoothing (Szegedy et al., 2016) 0.1 mixup (Zhang et al., 2018) 0.8 cutmix (Yun et al., 2019) 1.0 random erase (Zhong et al., 2020) 0.25 drop path (Huang et al., 2016b) 0.15 (B), 0.5 (L) exp. moving average (EMA) 0.9999 Table 17 Training Configurations of ViT. Diffusion Transformers. We use the official implementation (Peebles and Xie, 2023) to train all DiT model sizes as shown in Table 18. We observe that the default learning rate is suboptimal for the models in this work. For both the search function experiments and the final evaluation of Derf, we go through three learning rates, 1 Ã— 10âˆ’4, 2 Ã— 10âˆ’4, and 4 Ã— 10âˆ’4, for all models, whether they use LayerNorm or a point-wise function, and report the best result. We also observe that the zero initialization negatively affects the performance of Derf models and other point-wise function models. Therefore, we retain the zero initialization for LN models but remove it for the other models. config value optimizer AdamW base learning rate {1e-4, 2e-4, 4e-4} weight decay 0 optimizer momentum ğ›½1=0.9, ğ›½2=0.999 effective batch size 256 learning rate schedule constant training epochs 80 exp. moving average (EMA) 0.9999 Table 18 Training Configurations of DiT. Speech models. For both wav2vec 2.0 models, we retain the first GroupNorm layer and the LayerNorm located after the convolutional feature extractor, since both primarily serve as data normalization to handle the unnormalized input data. We use the official implementation (Baevski et al., 2020) for both the Base and Large models, keeping all hyperparameters identical to the original setup, as shown in Table 19. The only change we make is running all modelsâ€”whether normalization-based or point-wise-function-basedâ€”in fp32 precision instead of the default bf16. We report the final validation loss. DNA models. For both the HyenaDNA model (Nguyen et al., 2023) and the Caduceus model (Schiff et al., 2024), we directly follow their official implementations without modifying hyperparameters, as shown in Table 20. In particular, Hyena uses LayerNorm and Caduceus uses RMSNorm. For our evaluation, we replace each modelâ€™s original normalization layer with Derf and report the average accuracy across all tasks. Language models. For the GPT-2 (124M) model, we follow the hyperparameters as shown in Table 21. For Derf and DyT, we configure the ğ›¼initialization separately for the point-wise function layer following 19"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 20, "text": "config\nvalue\noptimizer\nAdam\nlearning rate\n5e-4 (B), 3e-4 (L)\nweight decay\n0.01\noptimizer momentum\nğ›½1=0.9, ğ›½2=0.98\nmax tokens\n1400000 (B), 1200000 (L)\nlearning rate schedule\npolynomial decay\nwarmup updates\n32000 (B), 20000 (L)\nmax updates\n400000 (B), 250000 (L)\ndropout (input to encoder)\n0.1\ndropout (target features)\n0.1\ndropout (transformer)\n0.0 (B), 0.1 (L)\nlayer dropout\n0.05 (B), 0.2 (L)\nfeature grad mult\n0.1\nlatent temp\n[2,0.5,0.999995] (B), [2.0,0.1,0.999995] (L)\nmax sample size\n250000 (B), 320000 (L)\nTable 19 Training Configurations of wav2vec 2.0.\nconfig\nvalue\noptimizer\nAdamW\nlearning rate\n6e-4 (H), 8e-3 (C)\nsequence length\n1024 (H), 131072 (C)\neffective batch size\n1024 (H), 8 (C)\ntraining steps\n10000 (H), 50000 (C)\nRC augmentation\ntrue (H), false (C)\nMLM probability\n0.0 (H), 0.15 (C)\nbidirectional\nfalse (H), true (C)\nTable 20 Training Configurations of HyenaDNA and Caduceus. H denotes HyenaDNA, C denotes Caduceus.\nthe attention layer and for the other point-wise function layers. We try multiple combinations of these\ninitialization settings and report the best validation loss.\nconfig\nvalue\noptimizer\nAdamW\nbase learning rate\n6e-4\nweight decay\n0.1\noptimizer momentum\nğ›½1=0.9, ğ›½2=0.95\ngradient clipping\n1.0\nblock size\n1024\ngradient accumulation steps\n40\neffective batch size\n491,520\nlearning rate schedule\ncosine decay\nwarmup iterations\n2,000\ntraining iterations\n300,000\ndropout\n0.0\nmixed precision\nbf16\nTable 21 Training Configurations of GPT-2 (124M).\nD Additional Results\nBeyond evaluating each model with its default normalization layer (typically LN), we additionally test\nRMSNorm and GroupNorm (GN) to enable a more complete comparison. RMSNorm is widely used in modern\nlarge language models, including T5 (Raffel et al., 2020), LLaMA (Touvron et al., 2023a,b; Dubey et al.,\n2024), Qwen (Bai et al., 2023; Yang et al., 2024), and DeepSeek (Liu et al., 2024; Guo et al., 2025), while\nGN is employed in several vision architectures, including ConvNeXt (Liu et al., 2022), DETR (Carion et al.,\n2020), and Swin Transformer (Liu et al., 2021).\nAll evaluations follow the same experimental settings described in the previous section. These additional\nresults show that Derf not only surpasses the default choices used in each model, but also outperforms the\n20", "clean_text": "config value optimizer Adam learning rate 5e-4 (B), 3e-4 (L) weight decay 0.01 optimizer momentum ğ›½1=0.9, ğ›½2=0.98 max tokens 1400000 (B), 1200000 (L) learning rate schedule polynomial decay warmup updates 32000 (B), 20000 (L) max updates 400000 (B), 250000 (L) dropout (input to encoder) 0.1 dropout (target features) 0.1 dropout (transformer) 0.0 (B), 0.1 (L) layer dropout 0.05 (B), 0.2 (L) feature grad mult 0.1 latent temp [2,0.5,0.999995] (B), [2.0,0.1,0.999995] (L) max sample size 250000 (B), 320000 (L) Table 19 Training Configurations of wav2vec 2.0. config value optimizer AdamW learning rate 6e-4 (H), 8e-3 (C) sequence length 1024 (H), 131072 (C) effective batch size 1024 (H), 8 (C) training steps 10000 (H), 50000 (C) RC augmentation true (H), false (C) MLM probability 0.0 (H), 0.15 (C) bidirectional false (H), true (C) Table 20 Training Configurations of HyenaDNA and Caduceus. H denotes HyenaDNA, C denotes Caduceus. the attention layer and for the other point-wise function layers. We try multiple combinations of these initialization settings and report the best validation loss. config value optimizer AdamW base learning rate 6e-4 weight decay 0.1 optimizer momentum ğ›½1=0.9, ğ›½2=0.95 gradient clipping 1.0 block size 1024 gradient accumulation steps 40 effective batch size 491,520 learning rate schedule cosine decay warmup iterations 2,000 training iterations 300,000 dropout 0.0 mixed precision bf16 Table 21 Training Configurations of GPT-2 (124M). D Additional Results Beyond evaluating each model with its default normalization layer (typically LN), we additionally test RMSNorm and GroupNorm (GN) to enable a more complete comparison. RMSNorm is widely used in modern large language models, including T5 (Raffel et al., 2020), LLaMA (Touvron et al., 2023a,b; Dubey et al., 2024), Qwen (Bai et al., 2023; Yang et al., 2024), and DeepSeek (Liu et al., 2024; Guo et al., 2025), while GN is employed in several vision architectures, including ConvNeXt (Liu et al., 2022), DETR (Carion et al., 2020), and Swin Transformer (Liu et al., 2021). All evaluations follow the same experimental settings described in the previous section. These additional results show that Derf not only surpasses the default choices used in each model, but also outperforms the 20"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 21, "text": "other normalization alternatives we evaluate.\nVision Transformers. For both ViT-Base and ViT-Large (Dosovitskiy, 2021), the default normalization layer\nis LayerNorm. To complement the results, we also evaluate RMSNorm (Zhang and Sennrich, 2019) and GN\n(Wu and He, 2018) as additional replacements in Table 22. Compared to all other methods, Derf achieves\nclearly higher top-1 accuracy, demonstrating its effectiveness in vision transformer architectures.\nmodel\nLN\nDyT\nDerf\nRMSNorm\nGN\nViT-B\n82.3%\n82.5%\n82.8%\n82.4%\n82.5%\nViT-L\n83.1%\n83.6%\n83.8%\n83.0%\n83.1%\nTable 22 Supervised classification accuracy on ImageNet-1K. Derf achieves higher top-1 accuracy than all other\nmethods on different model sizes.\nDiffusion Transformers. For DiT models (Peebles and Xie, 2023), we additionally evaluate RMSNorm (Zhang\nand Sennrich, 2019) as an alternative normalization layer and compare its performance with LN, DyT, and\nDerf. As shown in Table 23, Derf achieves a clear improvement in FID compared to all other methods.\nmodel\nLN\nDyT\nDerf\nRMSNorm\nDiT-B/4\n64.93\n63.94\n63.23\n65.08\nDiT-L/4\n45.91\n45.66\n43.94\n45.02\nDiT-XL/2\n19.94\n20.83\n18.92\n20.76\nTable 23 Image generation quality (FID) on ImageNet. Lower FID indicates better image generation quality. Derf\nachieves lower FID scores than all other methods across different DiT models.\nSpeech models. For two wav2vec 2.0 Transformer models (Baevski et al., 2020), we additionally evaluate\nRMSNorm (Zhang and Sennrich, 2019) as an alternative normalization layer and compare its performance\nwith LN, DyT, and Derf in Table 24. Compared to other methods, Derf yields lower validation loss on different\nmodel sizes\nmodel\nLN\nDyT\nDerf\nRMSNorm\nwav2vec 2.0 Base\n1.95\n1.95\n1.93\n1.95\nwav2vec 2.0 Large\n1.92\n1.91\n1.90\n1.93\nTable 24 Speech pretraining validation loss on the LibriSpeech dataset. Derf achieves lower validation loss than all\nother methods across two wav2vec 2.0 models.\nDNA models. For the HyenaDNA model (Nguyen et al., 2023) and the Caduceus model (Schiff et al., 2024),\nwe additionally evaluate both LayerNorm and RMSNorm for each architecture, regardless of their default\nchoices, and compare their performance with DyT and Derf in Table 25.\nmodel\nLN\nDyT\nDerf\nRMSNorm\nHyena\n85.2%\n85.2%\n85.7%\n85.2%\nCaduceus\n87.0%\n86.9%\n87.3%\n86.9%\nTable 25 DNA classification accuracy on the GenomicBenchmarks dataset, averaged over each subtask.\nDerf\nconsistently outperforms other methods across two different DNA models.\nLanguage models. For the GPT-2 (124M) model, we additionally evaluate RMSNorm (Zhang and Sennrich,\n2019) for a more complete comparison of normalization choices.\nAs shown in Table 26, Derf achieves\ncomparable performance to both LN and RMSNorm, while clearly outperforming DyT.\n21", "clean_text": "other normalization alternatives we evaluate. Vision Transformers. For both ViT-Base and ViT-Large (Dosovitskiy, 2021), the default normalization layer is LayerNorm. To complement the results, we also evaluate RMSNorm (Zhang and Sennrich, 2019) and GN (Wu and He, 2018) as additional replacements in Table 22. Compared to all other methods, Derf achieves clearly higher top-1 accuracy, demonstrating its effectiveness in vision transformer architectures. model LN DyT Derf RMSNorm GN ViT-B 82.3% 82.5% 82.8% 82.4% 82.5% ViT-L 83.1% 83.6% 83.8% 83.0% 83.1% Table 22 Supervised classification accuracy on ImageNet-1K. Derf achieves higher top-1 accuracy than all other methods on different model sizes. Diffusion Transformers. For DiT models (Peebles and Xie, 2023), we additionally evaluate RMSNorm (Zhang and Sennrich, 2019) as an alternative normalization layer and compare its performance with LN, DyT, and Derf. As shown in Table 23, Derf achieves a clear improvement in FID compared to all other methods. model LN DyT Derf RMSNorm DiT-B/4 64.93 63.94 63.23 65.08 DiT-L/4 45.91 45.66 43.94 45.02 DiT-XL/2 19.94 20.83 18.92 20.76 Table 23 Image generation quality (FID) on ImageNet. Lower FID indicates better image generation quality. Derf achieves lower FID scores than all other methods across different DiT models. Speech models. For two wav2vec 2.0 Transformer models (Baevski et al., 2020), we additionally evaluate RMSNorm (Zhang and Sennrich, 2019) as an alternative normalization layer and compare its performance with LN, DyT, and Derf in Table 24. Compared to other methods, Derf yields lower validation loss on different model sizes model LN DyT Derf RMSNorm wav2vec 2.0 Base 1.95 1.95 1.93 1.95 wav2vec 2.0 Large 1.92 1.91 1.90 1.93 Table 24 Speech pretraining validation loss on the LibriSpeech dataset. Derf achieves lower validation loss than all other methods across two wav2vec 2.0 models. DNA models. For the HyenaDNA model (Nguyen et al., 2023) and the Caduceus model (Schiff et al., 2024), we additionally evaluate both LayerNorm and RMSNorm for each architecture, regardless of their default choices, and compare their performance with DyT and Derf in Table 25. model LN DyT Derf RMSNorm Hyena 85.2% 85.2% 85.7% 85.2% Caduceus 87.0% 86.9% 87.3% 86.9% Table 25 DNA classification accuracy on the GenomicBenchmarks dataset, averaged over each subtask. Derf consistently outperforms other methods across two different DNA models. Language models. For the GPT-2 (124M) model, we additionally evaluate RMSNorm (Zhang and Sennrich, 2019) for a more complete comparison of normalization choices. As shown in Table 26, Derf achieves comparable performance to both LN and RMSNorm, while clearly outperforming DyT. 21"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 22, "text": "model\nLN\nDyT\nDerf\nRMSNorm\nGPT-2\n2.94\n2.97\n2.94\n2.95\nTable 26 GPT-2 validation loss on the OpenWebText dataset. Derf matches the performances of both LayerNorm\nand RMSNorm while achieving lower validation loss than DyT.\nE Loss Calculation Details\nVision Transformers. For ViT models, we measure fitting capacity under a deterministic evaluation setup.\nWe switch the model to evaluation mode, disable drop-path, mixup, cutmix, label smoothing, and all data\naugmentations, and apply only the standard test-time preprocessing (center crop and normalize). The\ncross-entropy loss is then computed on the training set and averaged over all samples.\nDiffusion Transformers. For DiT models, we evaluate fitting capacity by switching the model to evaluation\nmode. We apply the standard test-time preprocessing (center crop, random horizontal flip, and normalize).\nSince DiT does not employ drop-path, no stochastic regularization needs to be disabled. We then compute\nthe diffusion MSE loss over the first 100 training batches and report the average.\nOther models. For all other models, wav2vec 2.0, HyenaDNA, Caduceus, and GPT2, we simply apply the\nsame procedure: use the standard test-time preprocessing, disable drop-path or dropout when present, and\ncompute the training loss over the full training set, reporting the average.\n22", "clean_text": "model LN DyT Derf RMSNorm GPT-2 2.94 2.97 2.94 2.95 Table 26 GPT-2 validation loss on the OpenWebText dataset. Derf matches the performances of both LayerNorm and RMSNorm while achieving lower validation loss than DyT. E Loss Calculation Details Vision Transformers. For ViT models, we measure fitting capacity under a deterministic evaluation setup. We switch the model to evaluation mode, disable drop-path, mixup, cutmix, label smoothing, and all data augmentations, and apply only the standard test-time preprocessing (center crop and normalize). The cross-entropy loss is then computed on the training set and averaged over all samples. Diffusion Transformers. For DiT models, we evaluate fitting capacity by switching the model to evaluation mode. We apply the standard test-time preprocessing (center crop, random horizontal flip, and normalize). Since DiT does not employ drop-path, no stochastic regularization needs to be disabled. We then compute the diffusion MSE loss over the first 100 training batches and report the average. Other models. For all other models, wav2vec 2.0, HyenaDNA, Caduceus, and GPT2, we simply apply the same procedure: use the standard test-time preprocessing, disable drop-path or dropout when present, and compute the training loss over the full training set, reporting the average. 22"}
