{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 1, "text": "Bidirectional Normalizing Flow: From Data to Noise and Back\nYiyang Lu1,2,∗,†,‡ Qiao Sun1,∗,† Xianbang Wang1,∗Zhicheng Jiang1 Hanhong Zhao1 Kaiming He1\n∗Equal technical contribution\n†Project lead\n1MIT\n2Tsinghua University\nAbstract\nNormalizing Flows (NFs) have been established as a prin-\ncipled framework for generative modeling. Standard NFs\nconsist of a forward process and a reverse process: the for-\nward process maps data to noise, while the reverse process\ngenerates samples by inverting it. Typical NF forward trans-\nformations are constrained by explicit invertibility, ensuring\nthat the reverse process can serve as their exact analytic\ninverse. Recent developments in TARFlow and its variants\nhave revitalized NF methods by combining Transformers and\nautoregressive flows, but have also exposed causal decoding\nas a major bottleneck. In this work, we introduce Bidirec-\ntional Normalizing Flow (BiFlow), a framework that re-\nmoves the need for an exact analytic inverse. BiFlow learns\na reverse model that approximates the underlying noise-to-\ndata inverse mapping, enabling more flexible loss functions\nand architectures. Experiments on ImageNet demonstrate\nthat BiFlow, compared to its causal decoding counterpart,\nimproves generation quality while accelerating sampling\nby up to two orders of magnitude. BiFlow yields state-of-\nthe-art results among NF-based methods and competitive\nperformance among single-evaluation (“1-NFE”) methods.\nFollowing recent encouraging progress on NFs, we hope our\nwork will draw further attention to this classical paradigm.\n1. Introduction\nNormalizing Flows (NFs) are a long-standing family\nof generative models [45, 10, 30]. They contain two pro-\ncesses: a forward process that learns to transform data into\nnoise, and a reverse process that generates samples by in-\nverting this transformation. A notable property of NFs is\nthat the underlying flow trajectories from data to noise are\nlearned rather than imposed. This differs from their modern\ncontinuous-time counterparts [7], such as Flow Matching\n(FM) [36, 37, 1], whose ground-truth trajectories are pre-\ndetermined via time-scheduling. However, this advantage\nof NFs comes at the cost of increased learning difficulty,\ntypically leading to more demanding constraints on forward\narchitectures and objective formulations.\n‡Work done as an intern at MIT.\n(a) Standard Normalizing Flow:\nexplicit inverse.\n(b) Bidirectional Normalizing Flow:\nlearned inverse.\nFigure 1. Conceptual comparison between standard Normalizing\nFlows and our proposed Bidirectional Normalizing Flow (BiFlow).\nInstead of constraining the forward model F to be explicitly in-\nvertible and using its exact analytic inverse for generation, BiFlow\nintroduces a learnable reverse model G that approximates this in-\nverse through our hidden alignment objective. This design frees\nBiFlow from architectural constraints and enables flexible loss de-\nsign, allowing for efficient generation with improved quality in a\nsingle forward pass.\nThe standard NF paradigm [45, 10] requires the reverse\nprocess to be the exact analytic inverse of the forward pro-\ncess (Fig. 1a). This requirement restricts the range of forward\nmodel architectures that can be employed, as the model must\nbe explicitly invertible and its Jacobian determinant must be\ncomputable, tractable, and differentiable. Existing work on\nNFs [45, 10, 57, 30, 41, 29] have largely focused on design-\ning compound forward functions that satisfy these require-\nments. Despite these diverse attempts, NF-based methods re-\nmain limited in their ability to use powerful, general-purpose\narchitectures (e.g., U-Nets [47] or Vision Transformers [12]),\nin contrast to many modern generative model families.\nRecently, the gap between NFs and other generative mod-\nels has been largely closed by TARFlow [65] and its exten-\nsions [21]. TARFlow has effectively integrated Transformers\n[58] with autoregressive flows [30, 41] into the NF paradigm.\nThis design allows NF methods to benefit from the power-\nful Transformers, substantially mitigating a major limitation\nof traditional NFs. However, to maintain computable and\ntractable Jacobian determinants, TARFlow decomposes the\nforward process into a long chain (e.g., thousands of steps)\n1\narXiv:2512.10953v1  [cs.LG]  11 Dec 2025", "clean_text": "Bidirectional Normalizing Flow: From Data to Noise and Back Yiyang Lu1,2,∗,†,‡ Qiao Sun1,∗,† Xianbang Wang1,∗Zhicheng Jiang1 Hanhong Zhao1 Kaiming He1 ∗Equal technical contribution †Project lead 1MIT 2Tsinghua University Abstract Normalizing Flows (NFs) have been established as a principled framework for generative modeling. Standard NFs consist of a forward process and a reverse process: the forward process maps data to noise, while the reverse process generates samples by inverting it. Typical NF forward transformations are constrained by explicit invertibility, ensuring that the reverse process can serve as their exact analytic inverse. Recent developments in TARFlow and its variants have revitalized NF methods by combining Transformers and autoregressive flows, but have also exposed causal decoding as a major bottleneck. In this work, we introduce Bidirectional Normalizing Flow (BiFlow), a framework that removes the need for an exact analytic inverse. BiFlow learns a reverse model that approximates the underlying noise-todata inverse mapping, enabling more flexible loss functions and architectures. Experiments on ImageNet demonstrate that BiFlow, compared to its causal decoding counterpart, improves generation quality while accelerating sampling by up to two orders of magnitude. BiFlow yields state-ofthe-art results among NF-based methods and competitive performance among single-evaluation (“1-NFE”) methods. Following recent encouraging progress on NFs, we hope our work will draw further attention to this classical paradigm. 1. Introduction Normalizing Flows (NFs) are a long-standing family of generative models [45, 10, 30]. They contain two processes: a forward process that learns to transform data into noise, and a reverse process that generates samples by inverting this transformation. A notable property of NFs is that the underlying flow trajectories from data to noise are learned rather than imposed. This differs from their modern continuous-time counterparts [7], such as Flow Matching (FM) [36, 37, 1], whose ground-truth trajectories are predetermined via time-scheduling. However, this advantage of NFs comes at the cost of increased learning difficulty, typically leading to more demanding constraints on forward architectures and objective formulations. ‡Work done as an intern at MIT. (a) Standard Normalizing Flow: explicit inverse. (b) Bidirectional Normalizing Flow: learned inverse. Figure 1. Conceptual comparison between standard Normalizing Flows and our proposed Bidirectional Normalizing Flow (BiFlow). Instead of constraining the forward model F to be explicitly invertible and using its exact analytic inverse for generation, BiFlow introduces a learnable reverse model G that approximates this inverse through our hidden alignment objective. This design frees BiFlow from architectural constraints and enables flexible loss design, allowing for efficient generation with improved quality in a single forward pass. The standard NF paradigm [45, 10] requires the reverse process to be the exact analytic inverse of the forward process (Fig. 1a). This requirement restricts the range of forward model architectures that can be employed, as the model must be explicitly invertible and its Jacobian determinant must be computable, tractable, and differentiable. Existing work on NFs [45, 10, 57, 30, 41, 29] have largely focused on designing compound forward functions that satisfy these requirements. Despite these diverse attempts, NF-based methods remain limited in their ability to use powerful, general-purpose architectures (e.g., U-Nets [47] or Vision Transformers [12]), in contrast to many modern generative model families. Recently, the gap between NFs and other generative models has been largely closed by TARFlow [65] and its extensions [21]. TARFlow has effectively integrated Transformers [58] with autoregressive flows [30, 41] into the NF paradigm. This design allows NF methods to benefit from the powerful Transformers, substantially mitigating a major limitation of traditional NFs. However, to maintain computable and tractable Jacobian determinants, TARFlow decomposes the forward process into a long chain (e.g., thousands of steps) 1 arXiv:2512.10953v1 [cs.LG] 11 Dec 2025"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 2, "text": "0.001\n0.01\n0.1\n0.5\nReverse Process Time (s, log-scale)\n0\n2\n4\n6\n8\nFID-50K (w/ CFG)\n2.39\n4.54\n6.83\n2.1 points\n better\n697× Faster\n4.4 points\n better\n224× Faster\nimproved TARFlow\nBiFlow\nFigure 2. BiFlow surpasses our improved TARFlow baseline by\na wide margin in generation quality, despite using a base-size\nmodel versus an extra-large model, and it achieves markedly faster\nsampling as well. The x-axis denotes the wall-clock time (log scale)\nfor generating one image on 8 v4 TPU cores. VAE decoding is\nomitted from this figure; comprehensive inference cost comparison\nappears in Tab. 3.\nof autoregressive operations. The resulting explicit inverse\ntherefore requires a large number of causal steps at inference\ntime, which is difficult to parallelize. This design not only\nslows down sampling, but also retains the undesirable archi-\ntectural constraints during inference, e.g., the reverse model\ncannot perform feedforward, non-causal attention.\nIn this work, we introduce Bidirectional Normalizing\nFlow (BiFlow), a framework in which both the forward and\nreverse processes are learned. In our framework, the designs\nof the forward and reverse processes are decoupled: the\nforward process can be any NF model Fθ that is computable,\ntractable, and easy to learn (e.g., an improved TARFlow),\nwhile the reverse process learns a separate model Gϕ to\napproximate its inverse (Fig. 1b). In contrast to the explicit\ninverse, our reverse model is highly flexible: it can be a\nfeedforward, non-causal Transformer that is both expressive\nand efficient to run, naturally enabling high-quality, single\nfunction evaluation (1-NFE) generation.\nLearning the reverse model Gϕ is not merely a form of dis-\ntillation, even though we use a pre-trained forward model Fθ:\nin fact, our learned reverse model Gϕ can outperform the ex-\nplicit inverse of Fθ. Compared to distilling the noise-to-data\ntrajectories, we find that aligning the intermediate hidden\nstates yields results even better than the explicit inverse. In\naddition, our learnable reverse model can naturally eliminate\nthe extra step of score-based denoising in TARFlow, sim-\nplifying and accelerating inference while improving quality.\nSuch a “what-you-see-is-what-you-get” property further en-\nables the use of perceptual loss [68], which is impossible or\ndifficult to leverage with an explicit inverse. Putting these\nfactors together, our learned reverse model can substantially\noutperform its explicit-inverse counterpart.\nWe report competitive results on the ImageNet 256×256\ngeneration. Comparing with an improved TARFlow (which\nis also the forward model for BiFlow), BiFlow achieves an\nFID of 2.39 using a DiT-B size [42] model, while being two\norders of magnitude faster (see Fig. 2; detailed in Tab. 3).\nThis not only sets a new state-of-the-art result among NF-\nbased methods, but also represents a strong 1-NFE result in\ncomparison with other generative model families.\nFollowing the progress established by TARFlow and ex-\ntensions, our work on BiFlow further unleashes the potential\nof NFs as a strong competitor among modern generative\nmodel families. Our findings indicate that the NF principle of\nlearning the forward trajectories, rather than pre-scheduling\nthem, can be advantageous and need not introduce inference-\ntime limitations. Considering that modern Flow Matching\nmethods are continuous-time NFs with pre-scheduled tra-\njectories, we hope our study will shed light on the potential\nsynergy among these related methods.\n2. Related Work\nNormalizing Flows. Normalizing Flows (NFs) have long\nserved as a principled framework for probabilistic genera-\ntive modeling. Over the past decade, extensive research has\nfocused on enhancing the expressivity and scalability of NFs\nunder the constraint of invertible transformations. Planar\nflows [45] and NICE [10] pioneered the use of simple re-\nversible mappings to construct deep generative models. Real\nNVP [11] and Glow [29] extended this framework with non-\nvolume-preserving transformations and convolutional archi-\ntectures. IAF [30] and MAF [41] introduced autoregressive\nflows to improve expressivity while maintaining tractable\nlikelihoods. TARFlow [65] and STARFlow [21] further re-\nvitalized the NF family by incorporating Transformer into\nautoregressive flows. They demonstrated significant gains\nin generation quality and scalability, reaffirming NFs as a\ncompetitive paradigm in modern generative modeling.\nDespite these advances, standard NFs still inherit limita-\ntions from their invertibility requirement. In particular, au-\ntoregressive flow formulations impose strict causal ordering\nand sequential dependencies, which constrain architectural\ndesign and lead to slow inference.\nContinuous Normalizing Flows. Continuous Normalizing\nFlows (CNFs) [20, 14, 19] generalize discrete flows by mod-\neling transformations as continuous-time dynamics governed\nby ordinary differential equations (ODEs) [7]. CNFs enable\nmore flexible architectures and tractable likelihood compu-\ntation via numerical ODE simulations. FM [36, 37, 13]\nreformulates the explicit maximum-likelihood training ob-\njective into an equivalent implicit objective. Diffusion mod-\nels [25, 51, 9] can be interpreted as a special case of Flow\nMatching with stochastic dynamics, achieving impressive\nfidelity and scalability. Despite their empirical success, the\n2", "clean_text": "0.001 0.01 0.1 0.5 Reverse Process Time (s, log-scale) 0 2 4 6 8 FID-50K (w/ CFG) 2.39 4.54 6.83 2.1 points better 697× Faster 4.4 points better 224× Faster improved TARFlow BiFlow Figure 2. BiFlow surpasses our improved TARFlow baseline by a wide margin in generation quality, despite using a base-size model versus an extra-large model, and it achieves markedly faster sampling as well. The x-axis denotes the wall-clock time (log scale) for generating one image on 8 v4 TPU cores. VAE decoding is omitted from this figure; comprehensive inference cost comparison appears in Tab. 3. of autoregressive operations. The resulting explicit inverse therefore requires a large number of causal steps at inference time, which is difficult to parallelize. This design not only slows down sampling, but also retains the undesirable architectural constraints during inference, e.g., the reverse model cannot perform feedforward, non-causal attention. In this work, we introduce Bidirectional Normalizing Flow (BiFlow), a framework in which both the forward and reverse processes are learned. In our framework, the designs of the forward and reverse processes are decoupled: the forward process can be any NF model Fθ that is computable, tractable, and easy to learn (e.g., an improved TARFlow), while the reverse process learns a separate model Gϕ to approximate its inverse (Fig. 1b). In contrast to the explicit inverse, our reverse model is highly flexible: it can be a feedforward, non-causal Transformer that is both expressive and efficient to run, naturally enabling high-quality, single function evaluation (1-NFE) generation. Learning the reverse model Gϕ is not merely a form of distillation, even though we use a pre-trained forward model Fθ: in fact, our learned reverse model Gϕ can outperform the explicit inverse of Fθ. Compared to distilling the noise-to-data trajectories, we find that aligning the intermediate hidden states yields results even better than the explicit inverse. In addition, our learnable reverse model can naturally eliminate the extra step of score-based denoising in TARFlow, simplifying and accelerating inference while improving quality. Such a “what-you-see-is-what-you-get” property further enables the use of perceptual loss [68], which is impossible or difficult to leverage with an explicit inverse. Putting these factors together, our learned reverse model can substantially outperform its explicit-inverse counterpart. We report competitive results on the ImageNet 256×256 generation. Comparing with an improved TARFlow (which is also the forward model for BiFlow), BiFlow achieves an FID of 2.39 using a DiT-B size [42] model, while being two orders of magnitude faster (see Fig. 2; detailed in Tab. 3). This not only sets a new state-of-the-art result among NFbased methods, but also represents a strong 1-NFE result in comparison with other generative model families. Following the progress established by TARFlow and extensions, our work on BiFlow further unleashes the potential of NFs as a strong competitor among modern generative model families. Our findings indicate that the NF principle of learning the forward trajectories, rather than pre-scheduling them, can be advantageous and need not introduce inferencetime limitations. Considering that modern Flow Matching methods are continuous-time NFs with pre-scheduled trajectories, we hope our study will shed light on the potential synergy among these related methods. 2. Related Work Normalizing Flows. Normalizing Flows (NFs) have long served as a principled framework for probabilistic generative modeling. Over the past decade, extensive research has focused on enhancing the expressivity and scalability of NFs under the constraint of invertible transformations. Planar flows [45] and NICE [10] pioneered the use of simple reversible mappings to construct deep generative models. Real NVP [11] and Glow [29] extended this framework with nonvolume-preserving transformations and convolutional architectures. IAF [30] and MAF [41] introduced autoregressive flows to improve expressivity while maintaining tractable likelihoods. TARFlow [65] and STARFlow [21] further revitalized the NF family by incorporating Transformer into autoregressive flows. They demonstrated significant gains in generation quality and scalability, reaffirming NFs as a competitive paradigm in modern generative modeling. Despite these advances, standard NFs still inherit limitations from their invertibility requirement. In particular, autoregressive flow formulations impose strict causal ordering and sequential dependencies, which constrain architectural design and lead to slow inference. Continuous Normalizing Flows. Continuous Normalizing Flows (CNFs) [20, 14, 19] generalize discrete flows by modeling transformations as continuous-time dynamics governed by ordinary differential equations (ODEs) [7]. CNFs enable more flexible architectures and tractable likelihood computation via numerical ODE simulations. FM [36, 37, 13] reformulates the explicit maximum-likelihood training objective into an equivalent implicit objective. Diffusion models [25, 51, 9] can be interpreted as a special case of Flow Matching with stochastic dynamics, achieving impressive fidelity and scalability. Despite their empirical success, the 2"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 3, "text": "TARFlow Block\nTARFlow Block\nFigure 3. Illustration on the autoregressive inference process of\nTARFlow. In each block, each token is transformed one by one,\ndepending on previous tokens. This is repeated for a sequence\nwith length 256 for a 32×32 input with patch size 2, and is further\nrepeated for all blocks (e.g., 8 blocks). Altogether, TARFlow infer-\nence requires 8×256 sequential function evaluations.\nimplicit formulation of FM and diffusion models sacrifices\nthe learnable bidirectional mapping that characterizes NFs.\n3. Background: Normalizing Flows\nNormalizing Flows (NFs) are a class of generative models\nthat establish a bijective transformation between a Gaussian\nprior distribution p0 and a complex data distribution pdata.\nAn NF consists of a forward process and a reverse process.\nGiven a data sample x ∈RD ∼pdata, the forward process\nF maps it into the Gaussian prior space z = F(x). The\nmodel assigns the data likelihood p(x) through the change-\nof-variables formula. Training is performed by optimizing F\nto maximize the log-likelihood log p(x) over data samples.\nClassical NF requires the forward process F to be explic-\nitly invertible for exact likelihood computation and efficient\nsampling. Once trained, its exact inverse, F−1, can be used\nfor generation by transforming Gaussian noise back to the\ndata space, i.e., x = F−1(z) where z ∼p0.\nIn practice, to enhance expressiveness, the forward pro-\ncess is commonly constructed as a composition of multiple\nsimpler bijective transformations F := fB−1 ◦· · · ◦f1 ◦f0\n(◦denotes function composition). Under this formulation,\nthe log-likelihood objective becomes\nlog p(x) = log p0(z) +\nX\ni\nlog\n\f\f\f\fdet ∂fi(xi)\n∂xi\n\f\f\f\f ,\n(1)\nwith x0 = x and xi+1 = fi(xi). Here, det(·) denotes\nthe determinant operator. Designing transformations that\nyield computable and differentiable determinant has been a\nkey consideration in prior NF formulations. This require-\nment motivates specialized designs such as affine coupling\n[10, 11] and autoregressive flows [30, 41], which preserve\ntractable Jacobians.\nImportantly, while the log-determinant term in Eq. (1)\nrequires the forward process F to be invertible, it does not\nnecessitate an explicitly invertible formulation. The explicit\ninverse is only required at inference time, where we need to\nmap samples from prior back to the data space.\nTARFlow. TARFlow [65] integrates Transformer architec-\ntures into autoregressive flows (AF), substantially improv-\ning their expressiveness and scalability. The core idea in\nAF is to further decompose each sub-transformation fi, pa-\nrameterized by a block, into T steps, where T denotes the\nsequence length of the input tokens. Each step transforms\nthe i-th token only conditioned on its predecessors, which\ncan naturally be realized through Transformer layers with\ncausal masks. To capture bidirectional context, AF flips\nthe sequence order in alternating blocks. By combining ex-\npressive Transformer architectures with autoregressive flows,\nTARFlow successfully revives NF to remain competitive\nwith today’s state-of-the-art generative models.\nHowever, AF parameterization introduces asymmetry\nbetween training and sampling.\nSimilar to next-token-\nprediction language models, although likelihood evaluation\nand training can be parallelized efficiently, sampling must\nproceed sequentially due to the autoregressive nature, as\nillustrated in Fig. 3. In practice, this requires performing,\ne.g., thousands of (8×256) inverse transformations one after\nanother, resulting in substantial inference latency.\n4. Bidirectional Normalizing Flow\nWe propose a Bidirectional Normalizing Flow (BiFlow)\nframework, which has: (i) a forward model Fθ that trans-\nforms data samples into pure noise, and (ii) a learnable,\nseparate reverse model Gϕ that approximates its inverse,\nmapping noise back to the data space. Training is performed\nin two stages: first, similar to classical NF, we train the\nforward model using maximum likelihood estimation; then,\nkeeping the forward model fixed, we train the reverse model\nto approximate its inverse mapping.\nNotably, our reverse model Gϕ is not constrained by ex-\nplicit invertibility. As a result, this allows us to design the\nreverse model with arbitrary architectures (e.g., bidirectional\nattention-based Transformers) and training objectives. Next,\nwe discuss the formulation, objectives, and learning dynam-\nics of the reverse process.\n4.1. Learning to Approximate the Inverse\nGiven a pre-trained forward model Fθ, our goal is to\noptimize a reverse model Gϕ that approximates its inverse.\nWe consider three strategies: (i) naive distillation; (ii) hidden\ndistillation; (iii) hidden alignment, as approaches to learning\nthe reverse model. Fig. 4 illustrates the differences among\nthese methods, as we describe next.\nNaive Distillation. A straightforward strategy is to impose\na direct distillation loss:\nLnaive(x) = D\n\u0000x, x′\u0001\n,\n3", "clean_text": "TARFlow Block TARFlow Block Figure 3. Illustration on the autoregressive inference process of TARFlow. In each block, each token is transformed one by one, depending on previous tokens. This is repeated for a sequence with length 256 for a 32×32 input with patch size 2, and is further repeated for all blocks (e.g., 8 blocks). Altogether, TARFlow inference requires 8×256 sequential function evaluations. implicit formulation of FM and diffusion models sacrifices the learnable bidirectional mapping that characterizes NFs. 3. Background: Normalizing Flows Normalizing Flows (NFs) are a class of generative models that establish a bijective transformation between a Gaussian prior distribution p0 and a complex data distribution pdata. An NF consists of a forward process and a reverse process. Given a data sample x ∈RD ∼pdata, the forward process F maps it into the Gaussian prior space z = F(x). The model assigns the data likelihood p(x) through the changeof-variables formula. Training is performed by optimizing F to maximize the log-likelihood log p(x) over data samples. Classical NF requires the forward process F to be explicitly invertible for exact likelihood computation and efficient sampling. Once trained, its exact inverse, F−1, can be used for generation by transforming Gaussian noise back to the data space, i.e., x = F−1(z) where z ∼p0. In practice, to enhance expressiveness, the forward process is commonly constructed as a composition of multiple simpler bijective transformations F := fB−1 ◦· · · ◦f1 ◦f0 (◦denotes function composition). Under this formulation, the log-likelihood objective becomes log p(x) = log p0(z) + X i log det ∂fi(xi) ∂xi , (1) with x0 = x and xi+1 = fi(xi). Here, det(·) denotes the determinant operator. Designing transformations that yield computable and differentiable determinant has been a key consideration in prior NF formulations. This requirement motivates specialized designs such as affine coupling [10, 11] and autoregressive flows [30, 41], which preserve tractable Jacobians. Importantly, while the log-determinant term in Eq. (1) requires the forward process F to be invertible, it does not necessitate an explicitly invertible formulation. The explicit inverse is only required at inference time, where we need to map samples from prior back to the data space. TARFlow. TARFlow [65] integrates Transformer architectures into autoregressive flows (AF), substantially improving their expressiveness and scalability. The core idea in AF is to further decompose each sub-transformation fi, parameterized by a block, into T steps, where T denotes the sequence length of the input tokens. Each step transforms the i-th token only conditioned on its predecessors, which can naturally be realized through Transformer layers with causal masks. To capture bidirectional context, AF flips the sequence order in alternating blocks. By combining expressive Transformer architectures with autoregressive flows, TARFlow successfully revives NF to remain competitive with today’s state-of-the-art generative models. However, AF parameterization introduces asymmetry between training and sampling. Similar to next-tokenprediction language models, although likelihood evaluation and training can be parallelized efficiently, sampling must proceed sequentially due to the autoregressive nature, as illustrated in Fig. 3. In practice, this requires performing, e.g., thousands of (8×256) inverse transformations one after another, resulting in substantial inference latency. 4. Bidirectional Normalizing Flow We propose a Bidirectional Normalizing Flow (BiFlow) framework, which has: (i) a forward model Fθ that transforms data samples into pure noise, and (ii) a learnable, separate reverse model Gϕ that approximates its inverse, mapping noise back to the data space. Training is performed in two stages: first, similar to classical NF, we train the forward model using maximum likelihood estimation; then, keeping the forward model fixed, we train the reverse model to approximate its inverse mapping. Notably, our reverse model Gϕ is not constrained by explicit invertibility. As a result, this allows us to design the reverse model with arbitrary architectures (e.g., bidirectional attention-based Transformers) and training objectives. Next, we discuss the formulation, objectives, and learning dynamics of the reverse process. 4.1. Learning to Approximate the Inverse Given a pre-trained forward model Fθ, our goal is to optimize a reverse model Gϕ that approximates its inverse. We consider three strategies: (i) naive distillation; (ii) hidden distillation; (iii) hidden alignment, as approaches to learning the reverse model. Fig. 4 illustrates the differences among these methods, as we describe next. Naive Distillation. A straightforward strategy is to impose a direct distillation loss: Lnaive(x) = D \u0000x, x′\u0001 , 3"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 4, "text": "(a) Naive Distillation.\n(b) Hidden Distillation.\n(c) Hidden Alignment.\nFigure 4. Comparison of three approaches for learning the reverse process. Each ◦marks a position where the model returns to the same\ndimension as input x. Blue arrows with D refer to distance loss terms. Our hidden alignment strategy (Fig. 4c) combines the strengths of\nFig. 4a and Fig. 4b, leveraging the entire trajectory for supervision without repeatedly returning to input space.\nwhere x is a data sample, x′ = Gϕ(Fθ(x)) is the recon-\nstructed data, and D denotes a distance metric (e.g., L2\ndistance). The reverse model is trained to minimize the\nreconstruction error on data samples (see Fig. 4a).\nThis simple approach provides supervision only at the fi-\nnal output, which may be insufficient for effectively training\nthe reverse model. Directly mapping pure noise to data in\none step is highly under-constrained, making it difficult for\nthe reverse network to learn a reliable inverse from a single\nreconstruction loss.\nHidden Distillation. A typical NF is composed of a se-\nquence of simple sub-transformations, i.e., Fθ = fB−1 ◦\n· · · ◦f1 ◦f0, where each fi is a transformation block and B\ndenotes the total number of blocks. We can strengthen the\ntraining signal by leveraging the full sequence of intermedi-\nate states generated along the forward trajectory.\nAs illustrated in Fig. 4b, starting from x ∼pdata, the\nforward model produces a trajectory of intermediate hidden\nstates {xi} with z = Fθ(x) as the final output prior. Analo-\ngously, we also design the reverse model to be composed of\nB blocks, generating a reverse trajectory {hi} from z. We\ndistill the reverse model by enforcing the two trajectories to\nbe close. Formally, the loss is defined as:\nLhidden(x) =\nX\ni\nD\n\u0000xi, hi\u0001\n,\nwhere h0 corresponds to the reconstructed output x′. Option-\nally, each term can be assigned a distinct weighting factor.\nThis formulation encourages the reverse model to invert each\nsub-transformation individually, which could help guide the\nreverse model to invert the mapping Fθ step by step. The in-\ntermediate hidden states {xi} serve as auxiliary supervision\nfor learning the correspondence between x and z.\nAlthough this hidden distillation strategy provides more\nsupervision than naive distillation, it introduces structural\nconstraints on model design. Since each intermediate state\nxi has the same dimensionality as the input, the reverse\nmodel is forced to repeatedly project features down to the\ninput space and then back up into the hidden space. This\nrigid requirement restricts architectural flexibility, ultimately\nlimiting the model’s effectiveness.\nHidden Alignment. We propose a more flexible strategy,\ntermed hidden alignment. Crucially, it leverages the full for-\nward trajectory for supervision while relaxing the restrictive\nrequirement in hidden distillation that intermediate hidden\nstates must lie in the input space.\nAs shown in Fig. 4c, we extract intermediate hidden states\n{hi} from the reverse model Gϕ. Unlike hidden distillation,\nwhich enforces each hi to directly match its input-space\ncounterpart xi, we introduce a set of learnable projection\nheads {φi} to align the projected representations φi(hi) with\nthe corresponding forward states xi. The training objective\nthen becomes:\nLalign(x) =\nX\ni\nD\n\u0000xi, φi(hi)\n\u0001\n,\n(2)\nwhere h0 = x′ and φ0 is the identity mapping.\nThis simple modification allows the reverse model to\nbenefit from full trajectory supervision while maintaining\narchitectural and representational flexibility. By decoupling\nthe representation space from the input token space, hidden\nalignment avoids the potential semantic distortion caused by\nrepeated projections.\n4.2. Eliminating Score-based Denoising\nExisting state-of-the-art NFs such as TARFlow [65] devi-\nate from standard flow-based modeling in that they learn\na noise-perturbed distribution and then denoise the out-\nput. Specifically, during training, TARFlow takes a noise-\nperturbed input ˜x = x + σϵ, where ϵ ∼N(0, I), and during\ninference, TARFlow first generates ˜x = F−1\nθ (z), then per-\nforms an additional score-based denoising step:\nx ←˜x + σ2 ∇˜x log p(˜x),\n(3)\nas illustrated in Fig. 5a, where the score term is computed\nvia a forward-backward pass. This post-processing almost\ndoubles the inference cost, becoming a clear computational\nbottleneck for efficient generation.\nLearned Denoising. We eliminate the explicit score-based\ndenoising step by integrating denoising directly into the re-\nverse model. As illustrated in Fig. 5b, we extend the forward\ntrajectory from ˜x to z by appending the clean data x at its\nstart, and extend the reverse model with one additional block\nh0 →x′ that learns denoising jointly with the inverse. The\n4", "clean_text": "(a) Naive Distillation. (b) Hidden Distillation. (c) Hidden Alignment. Figure 4. Comparison of three approaches for learning the reverse process. Each ◦marks a position where the model returns to the same dimension as input x. Blue arrows with D refer to distance loss terms. Our hidden alignment strategy (Fig. 4c) combines the strengths of Fig. 4a and Fig. 4b, leveraging the entire trajectory for supervision without repeatedly returning to input space. where x is a data sample, x′ = Gϕ(Fθ(x)) is the reconstructed data, and D denotes a distance metric (e.g., L2 distance). The reverse model is trained to minimize the reconstruction error on data samples (see Fig. 4a). This simple approach provides supervision only at the final output, which may be insufficient for effectively training the reverse model. Directly mapping pure noise to data in one step is highly under-constrained, making it difficult for the reverse network to learn a reliable inverse from a single reconstruction loss. Hidden Distillation. A typical NF is composed of a sequence of simple sub-transformations, i.e., Fθ = fB−1 ◦ · · · ◦f1 ◦f0, where each fi is a transformation block and B denotes the total number of blocks. We can strengthen the training signal by leveraging the full sequence of intermediate states generated along the forward trajectory. As illustrated in Fig. 4b, starting from x ∼pdata, the forward model produces a trajectory of intermediate hidden states {xi} with z = Fθ(x) as the final output prior. Analogously, we also design the reverse model to be composed of B blocks, generating a reverse trajectory {hi} from z. We distill the reverse model by enforcing the two trajectories to be close. Formally, the loss is defined as: Lhidden(x) = X i D \u0000xi, hi\u0001 , where h0 corresponds to the reconstructed output x′. Optionally, each term can be assigned a distinct weighting factor. This formulation encourages the reverse model to invert each sub-transformation individually, which could help guide the reverse model to invert the mapping Fθ step by step. The intermediate hidden states {xi} serve as auxiliary supervision for learning the correspondence between x and z. Although this hidden distillation strategy provides more supervision than naive distillation, it introduces structural constraints on model design. Since each intermediate state xi has the same dimensionality as the input, the reverse model is forced to repeatedly project features down to the input space and then back up into the hidden space. This rigid requirement restricts architectural flexibility, ultimately limiting the model’s effectiveness. Hidden Alignment. We propose a more flexible strategy, termed hidden alignment. Crucially, it leverages the full forward trajectory for supervision while relaxing the restrictive requirement in hidden distillation that intermediate hidden states must lie in the input space. As shown in Fig. 4c, we extract intermediate hidden states {hi} from the reverse model Gϕ. Unlike hidden distillation, which enforces each hi to directly match its input-space counterpart xi, we introduce a set of learnable projection heads {φi} to align the projected representations φi(hi) with the corresponding forward states xi. The training objective then becomes: Lalign(x) = X i D \u0000xi, φi(hi) \u0001 , (2) where h0 = x′ and φ0 is the identity mapping. This simple modification allows the reverse model to benefit from full trajectory supervision while maintaining architectural and representational flexibility. By decoupling the representation space from the input token space, hidden alignment avoids the potential semantic distortion caused by repeated projections. 4.2. Eliminating Score-based Denoising Existing state-of-the-art NFs such as TARFlow [65] deviate from standard flow-based modeling in that they learn a noise-perturbed distribution and then denoise the output. Specifically, during training, TARFlow takes a noiseperturbed input ˜x = x + σϵ, where ϵ ∼N(0, I), and during inference, TARFlow first generates ˜x = F−1 θ (z), then performs an additional score-based denoising step: x ←˜x + σ2 ∇˜x log p(˜x), (3) as illustrated in Fig. 5a, where the score term is computed via a forward-backward pass. This post-processing almost doubles the inference cost, becoming a clear computational bottleneck for efficient generation. Learned Denoising. We eliminate the explicit score-based denoising step by integrating denoising directly into the reverse model. As illustrated in Fig. 5b, we extend the forward trajectory from ˜x to z by appending the clean data x at its start, and extend the reverse model with one additional block h0 →x′ that learns denoising jointly with the inverse. The 4"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 5, "text": "(a) TARFlow: explicit denoising.\n(b) BiFlow: learned denoising.\nFigure 5. Incorporating the denoising step into our hidden align-\nment framework. The reverse model is extended with an additional\nblock dedicated to denoising. Our learned denoising eliminates the\nneed for calculating the score function through a whole forward-\nbackward pass, incurring only a single additional block forward.\nresulting reverse network, with one extra block for denoising,\nmaps z to a clean sample x′ in a single pass. As such, our\nreverse model directly learns the correspondence between z\nand the clean data x directly, rather than the noisy data ˜x.\nThe training process follows the same objective as Eq. (2),\nwith a reconstruction loss on (x, x′) and hidden alignment\nlosses on intermediate states. By integrating denoising into\nthe reverse process itself, BiFlow achieves a unified learned\nformulation for generation, where inverse and denoising are\nseamlessly coupled within a single direct generative model,\neliminating the need for any extra refinement step.\n4.3. Distance Metric\nBiFlow provides a flexible supervised-learning frame-\nwork for tackling the generation problem. This flexibility\nstems from two key properties of BiFlow: (i) 1-NFE genera-\ntion — the learned reverse model produces a sample x′ in a\nsingle forward pass, so generated samples are directly acces-\nsible during training; and (ii) explicit pairing — the forward\nprocess establishes a direct correspondence between data x\nand noise z, serving as training pairs for the reverse model.\nTogether, these properties realize a what-you-see-is-what-\nyou-get training regime: generated samples are available for\nimmediate loss evaluation and backpropagation, enabling\nrich semantic supervision signals.\nOur framework is highly flexible in the choice of loss\nfunctions: almost any distance metric can be used, and mul-\ntiple metrics can be combined. Our default choice for the\ndistance metric D in Eq. (2) is simply mean squared error\n(MSE). To enhance realism, we further apply perceptual loss\nat the final VAE-decoded image, while intermediate hidden\nstates remain aligned by MSE. In this work, we adopt both\nVGG [50] and ConvNeXt V2 [61] feature spaces for per-\nceptual loss (our implementation for VGG features follows\nLPIPS [68]). As in prior work [16, 52, 17], all loss terms\ncan be adaptively re-weighted during training. Details are\nprovided in Appendix B.3.\n4.4. Norm Control\nThe intermediate states produced by the forward model\nare unconstrained under the NF formulation, often exhibiting\nlarge norm fluctuations across blocks (see Fig. 8a). These\nvariations can lead to imbalanced supervision when using\nmagnitude-sensitive losses such as MSE for reverse-model\ntraining. To mitigate this issue, we introduce two comple-\nmentary norm-control strategies applied to the forward and\nreverse models to ensure stable and consistent supervision\nstrength (details in Appendix B.3).\nOn the forward model, we clip the output parameters of\neach transformation fi within a fixed range [−c, c], limiting\nexcessive scaling and stabilizing intermediate state norms\nwithout compromising expressiveness. On the reverse model,\nwe normalize each intermediate state before performing hid-\nden alignment, which equalizes the contribution across tra-\njectory depth and promotes scale-invariant learning.\n4.5. BiFlow with Guidance\nClassifier-free guidance (CFG) [24] was originally pro-\nposed for diffusion models to control the trade-off between\nsample diversity and fidelity. Due to its effectiveness, it has\nbeen widely adopted in diffusion-based generative models.\nFollowing this success, recent Normalizing Flows [65, 21]\nand autoregressive models [56, 35] also incorporate CFG to\nfurther improve generation quality.\nCFG can be seamlessly integrated into BiFlow’s infer-\nence process by extrapolating conditional and unconditional\npredictions of Gϕ at each hidden state hi, i.e.,\nhi+1 = (1 + wi) Gi\nϕ(hi | c) −wi Gi\nϕ(hi),\n(4)\nwhere c is the class condition and wi is the guidance scale\n(our w definition follows the original CFG formulation [24],\ni.e., w = 0 is w/o CFG). The subscript i indicates that wi can\ndiffer among blocks, supporting CFG interval [31]. More\nresults are provided in Appendix C.2.\nDirectly applying CFG doubles the computational cost\nduring inference, since each guided block requires two for-\nward passes. To alleviate this, following [5, 55], we incor-\nporate CFG into the training stage, enabling inference with\nonly one function evaluation (1-NFE) while preserving the\nbenefits of guidance. Additionally, to retain the flexibility\nof adjusting guidance scales at inference time, we allow the\nreverse model to leverage CFG scale as condition [39, 18].\nBy training the model with a range of guidance scales, Bi-\nFlow can generate outputs corresponding to various guidance\nstrengths within a single forward pass. Further details are\nprovided in Appendix B.2.\n5", "clean_text": "(a) TARFlow: explicit denoising. (b) BiFlow: learned denoising. Figure 5. Incorporating the denoising step into our hidden alignment framework. The reverse model is extended with an additional block dedicated to denoising. Our learned denoising eliminates the need for calculating the score function through a whole forwardbackward pass, incurring only a single additional block forward. resulting reverse network, with one extra block for denoising, maps z to a clean sample x′ in a single pass. As such, our reverse model directly learns the correspondence between z and the clean data x directly, rather than the noisy data ˜x. The training process follows the same objective as Eq. (2), with a reconstruction loss on (x, x′) and hidden alignment losses on intermediate states. By integrating denoising into the reverse process itself, BiFlow achieves a unified learned formulation for generation, where inverse and denoising are seamlessly coupled within a single direct generative model, eliminating the need for any extra refinement step. 4.3. Distance Metric BiFlow provides a flexible supervised-learning framework for tackling the generation problem. This flexibility stems from two key properties of BiFlow: (i) 1-NFE generation — the learned reverse model produces a sample x′ in a single forward pass, so generated samples are directly accessible during training; and (ii) explicit pairing — the forward process establishes a direct correspondence between data x and noise z, serving as training pairs for the reverse model. Together, these properties realize a what-you-see-is-whatyou-get training regime: generated samples are available for immediate loss evaluation and backpropagation, enabling rich semantic supervision signals. Our framework is highly flexible in the choice of loss functions: almost any distance metric can be used, and multiple metrics can be combined. Our default choice for the distance metric D in Eq. (2) is simply mean squared error (MSE). To enhance realism, we further apply perceptual loss at the final VAE-decoded image, while intermediate hidden states remain aligned by MSE. In this work, we adopt both VGG [50] and ConvNeXt V2 [61] feature spaces for perceptual loss (our implementation for VGG features follows LPIPS [68]). As in prior work [16, 52, 17], all loss terms can be adaptively re-weighted during training. Details are provided in Appendix B.3. 4.4. Norm Control The intermediate states produced by the forward model are unconstrained under the NF formulation, often exhibiting large norm fluctuations across blocks (see Fig. 8a). These variations can lead to imbalanced supervision when using magnitude-sensitive losses such as MSE for reverse-model training. To mitigate this issue, we introduce two complementary norm-control strategies applied to the forward and reverse models to ensure stable and consistent supervision strength (details in Appendix B.3). On the forward model, we clip the output parameters of each transformation fi within a fixed range [−c, c], limiting excessive scaling and stabilizing intermediate state norms without compromising expressiveness. On the reverse model, we normalize each intermediate state before performing hidden alignment, which equalizes the contribution across trajectory depth and promotes scale-invariant learning. 4.5. BiFlow with Guidance Classifier-free guidance (CFG) [24] was originally proposed for diffusion models to control the trade-off between sample diversity and fidelity. Due to its effectiveness, it has been widely adopted in diffusion-based generative models. Following this success, recent Normalizing Flows [65, 21] and autoregressive models [56, 35] also incorporate CFG to further improve generation quality. CFG can be seamlessly integrated into BiFlow’s inference process by extrapolating conditional and unconditional predictions of Gϕ at each hidden state hi, i.e., hi+1 = (1 + wi) Gi ϕ(hi | c) −wi Gi ϕ(hi), (4) where c is the class condition and wi is the guidance scale (our w definition follows the original CFG formulation [24], i.e., w = 0 is w/o CFG). The subscript i indicates that wi can differ among blocks, supporting CFG interval [31]. More results are provided in Appendix C.2. Directly applying CFG doubles the computational cost during inference, since each guided block requires two forward passes. To alleviate this, following [5, 55], we incorporate CFG into the training stage, enabling inference with only one function evaluation (1-NFE) while preserving the benefits of guidance. Additionally, to retain the flexibility of adjusting guidance scales at inference time, we allow the reverse model to leverage CFG scale as condition [39, 18]. By training the model with a range of guidance scales, BiFlow can generate outputs corresponding to various guidance strengths within a single forward pass. Further details are provided in Appendix B.2. 5"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 6, "text": "5. Experiments\nExperiment Settings. Our experiments are conducted on\nclass-conditional ImageNet [8] generation at 256×256 reso-\nlution. We evaluate Fr´echet Inception Distance (FID) [23]\nand Inception Score (IS) [48] on 50000 generated images.\nFollowing [46, 13, 21], we implement our models on the\nlatent space of a pre-trained VAE tokenizer. For ImageNet\n256×256, the tokenizer maps images to a 32×32×4 latent\nrepresentation, serving as the input and output domain of our\nmodels.\nImproved TARFlow as Baseline. Our BiFlow framework\nbuilds upon TARFlow [65] as our forward model. We in-\ntroduce several modifications to the original TARFlow to\nenhance stability and performance. Specifically, we replace\nadditive conditioning with in-context conditioning [42] and\napply the norm control strategy in Sec. 4.4, while omitting\nSTARFlow-specific components such as deep-shallow de-\nsign, decoder finetuning, and customized CFG. We denote\nthis enhanced version as improved TARFlow (iTARFlow).\nAs shown in the table below, it achieves substantial gains\nover the original TARFlow, both with or without CFG, es-\ntablishing a strong baseline for BiFlow.\nMethod\nFID (↓)\n# Params\nw/o CFG\nw/ CFG\nlatent TARFlow-B/2 *\n59.43\n10.89\n118M\n+ in-context conditioning\n53.87\n8.25\n120M\n+ 160 epochs →960 epochs\n45.48\n7.05\n120M\n+ norm control (iTARFlow)\n44.46\n6.83\n120M\nConfigurations. Our reverse model adopts a ViT backbone\nwith modern Transformer components [53, 66] and multi-\ntoken in-context conditioning [18]. We name our model as\nBiFlow-B/2, where B/2 indicates a base-sized model with\npatch size 2, resulting in a sequence length of 256. In our\nablation studies, we choose an iTARFlow as our forward\nmodel and train the reverse model with the forward model\nfixed. Unless otherwise specified, our ablations employ the\nadaptive-weighted MSE, while final comparisons in Tab. 4\nincorporate perceptual distance mentioned in Sec. 4.3 for\noptimal performance. Details are provided in Appendix A.\n5.1. Ablation: Learning to Approximate the Inverse\nWe evaluate three strategies for learning the reverse\nmodel, as described in Sec. 4.1, and report generation qual-\nity (FID in Tab. 1) as well as reconstruction error (see Ap-\npendix C.1).\nThe naive distillation approach, trained with a simple\nMSE objective, already outperforms the exact inverse base-\nline, indicating that a learned reverse model is a practical\nand competitive alternative to the analytic inverse.\nHidden distillation supervises the reverse model using\nthe entire forward trajectory. However, repeated projections\n*The latent TARFlow-B/2 is our TARFlow reproduction in VAE latent.\nFID (↓)\nattention\nexact inverse\n44.46\ncausal\nnaive distillation\n43.41 −1.05\nbidirect\nhidden distillation\n55.00 +10.54\nbidirect\nhidden alignment\n36.93 −7.53\nbidirect\nTable 1. Reverse learning method. Naive distillation can exceed\nthe exact inverse with a simple MSE objective. Our hidden align-\nment yields the best result among the three strategies. (Settings:\nBiFlow-B/2, 160 epochs, adaptive weighted MSE loss, w/o CFG)\nbetween representation and input spaces cause information\nloss and limit architectural expressiveness. This results in\ndegraded performance compared to the naive distillation.\nOur proposed hidden alignment method removes the re-\npeated projections inherent in hidden distillation while re-\ntaining full trajectory-level supervision, thereby preserving\nboth architectural flexibility and representational richness. It\nachieves the best performance among the three strategies and\nsurpasses the exact inverse by a clear margin in generation\nquality. These results collectively demonstrate that hidden\nalignment is an effective and robust strategy for learning an\napproximate inverse in BiFlow.\n5.2. Other Ablations\nWe ablate several key design choices in BiFlow and ana-\nlyze their impact on performance in Tab. 2.\nBiFlow with Guidance. BiFlow is conditioned on the CFG\nscale and learns across a range of CFG scales during training.\nThis enables 1-NFE inference while preserving the benefits\nand flexibility of guidance. As shown in Tab. 2a, compared to\nstandard CFG approach, our training-time CFG mechanism\nreduces inference cost by half while achieving better FID.\nLearned Denoising. Tab. 2b demonstrates the effective-\nness of our learned denoising strategy. By jointly training\ndenoising with the inverse, our learned one-block denoiser\nimproves generation quality over the score-based denoising\nused in TARFlow. Moreover, our approach introduces only a\nsingle additional block, whereas TARFlow’s score-based de-\nnoising requires an extra forward-backward pass (incurring\n15.8× flops). This substantially reduces inference overhead.\nNorm Control. We introduce two norm control strategies in\nSec. 4.4 and evaluate their effectiveness in Tab. 2c. Apply-\ning either strategy alleviates imbalance in MSE loss across\nblocks, thereby enhancing performance. We provide visual-\nizations of the norm statistics in Appendix C.4.\nDistance Metric. Our framework supports various distance\nmetric designs. As shown in Tab. 2d, incorporating percep-\ntual distance [68, 61] at the image end can largely improve\ngeneration quality. Notably, when both VGG and ConvNeXt\nfeatures are used for the perceptual loss, the optimal guid-\nance scale in Eq. (4) for this model is close to 0.0, resulting\nin performance similar to no-CFG setting. This suggests\nthese features already provide strong class-discriminative\ninformation. More results are provided in Appendix C.3.\n6", "clean_text": "5. Experiments Experiment Settings. Our experiments are conducted on class-conditional ImageNet [8] generation at 256×256 resolution. We evaluate Fr´echet Inception Distance (FID) [23] and Inception Score (IS) [48] on 50000 generated images. Following [46, 13, 21], we implement our models on the latent space of a pre-trained VAE tokenizer. For ImageNet 256×256, the tokenizer maps images to a 32×32×4 latent representation, serving as the input and output domain of our models. Improved TARFlow as Baseline. Our BiFlow framework builds upon TARFlow [65] as our forward model. We introduce several modifications to the original TARFlow to enhance stability and performance. Specifically, we replace additive conditioning with in-context conditioning [42] and apply the norm control strategy in Sec. 4.4, while omitting STARFlow-specific components such as deep-shallow design, decoder finetuning, and customized CFG. We denote this enhanced version as improved TARFlow (iTARFlow). As shown in the table below, it achieves substantial gains over the original TARFlow, both with or without CFG, establishing a strong baseline for BiFlow. Method FID (↓) # Params w/o CFG w/ CFG latent TARFlow-B/2 * 59.43 10.89 118M + in-context conditioning 53.87 8.25 120M + 160 epochs →960 epochs 45.48 7.05 120M + norm control (iTARFlow) 44.46 6.83 120M Configurations. Our reverse model adopts a ViT backbone with modern Transformer components [53, 66] and multitoken in-context conditioning [18]. We name our model as BiFlow-B/2, where B/2 indicates a base-sized model with patch size 2, resulting in a sequence length of 256. In our ablation studies, we choose an iTARFlow as our forward model and train the reverse model with the forward model fixed. Unless otherwise specified, our ablations employ the adaptive-weighted MSE, while final comparisons in Tab. 4 incorporate perceptual distance mentioned in Sec. 4.3 for optimal performance. Details are provided in Appendix A. 5.1. Ablation: Learning to Approximate the Inverse We evaluate three strategies for learning the reverse model, as described in Sec. 4.1, and report generation quality (FID in Tab. 1) as well as reconstruction error (see Appendix C.1). The naive distillation approach, trained with a simple MSE objective, already outperforms the exact inverse baseline, indicating that a learned reverse model is a practical and competitive alternative to the analytic inverse. Hidden distillation supervises the reverse model using the entire forward trajectory. However, repeated projections *The latent TARFlow-B/2 is our TARFlow reproduction in VAE latent. FID (↓) attention exact inverse 44.46 causal naive distillation 43.41 −1.05 bidirect hidden distillation 55.00 +10.54 bidirect hidden alignment 36.93 −7.53 bidirect Table 1. Reverse learning method. Naive distillation can exceed the exact inverse with a simple MSE objective. Our hidden alignment yields the best result among the three strategies. (Settings: BiFlow-B/2, 160 epochs, adaptive weighted MSE loss, w/o CFG) between representation and input spaces cause information loss and limit architectural expressiveness. This results in degraded performance compared to the naive distillation. Our proposed hidden alignment method removes the repeated projections inherent in hidden distillation while retaining full trajectory-level supervision, thereby preserving both architectural flexibility and representational richness. It achieves the best performance among the three strategies and surpasses the exact inverse by a clear margin in generation quality. These results collectively demonstrate that hidden alignment is an effective and robust strategy for learning an approximate inverse in BiFlow. 5.2. Other Ablations We ablate several key design choices in BiFlow and analyze their impact on performance in Tab. 2. BiFlow with Guidance. BiFlow is conditioned on the CFG scale and learns across a range of CFG scales during training. This enables 1-NFE inference while preserving the benefits and flexibility of guidance. As shown in Tab. 2a, compared to standard CFG approach, our training-time CFG mechanism reduces inference cost by half while achieving better FID. Learned Denoising. Tab. 2b demonstrates the effectiveness of our learned denoising strategy. By jointly training denoising with the inverse, our learned one-block denoiser improves generation quality over the score-based denoising used in TARFlow. Moreover, our approach introduces only a single additional block, whereas TARFlow’s score-based denoising requires an extra forward-backward pass (incurring 15.8× flops). This substantially reduces inference overhead. Norm Control. We introduce two norm control strategies in Sec. 4.4 and evaluate their effectiveness in Tab. 2c. Applying either strategy alleviates imbalance in MSE loss across blocks, thereby enhancing performance. We provide visualizations of the norm statistics in Appendix C.4. Distance Metric. Our framework supports various distance metric designs. As shown in Tab. 2d, incorporating perceptual distance [68, 61] at the image end can largely improve generation quality. Notably, when both VGG and ConvNeXt features are used for the perceptual loss, the optimal guidance scale in Eq. (4) for this model is close to 0.0, resulting in performance similar to no-CFG setting. This suggests these features already provide strong class-discriminative information. More results are provided in Appendix C.3. 6"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 7, "text": "FID, w/o CFG\nFID, w/ CFG\ninference-time CFG\n36.93 (NFE=1)\n6.90 (NFE=2)\n→training-time CFG\n31.88 (NFE=1)\n6.79 (NFE=1)\n(a) BiFlow with guidance. Conditioning on the CFG scale during training\nimproves FID both w/ and w/o CFG while preserving flexible 1-NFE infer-\nence. The baseline w/o CFG is final results in Tab. 1.\nFID, w/o CFG\nFID, w/ CFG\nlearned denoise\n31.88\n6.79\n→no denoise\n100.51\n26.20\n→score-based denoise\n42.62\n10.98\n(b) Learned denoising. Our learned denoising scheme is effective. Com-\npared to score-based denoising in TARFlow, it eliminates an extra forward-\nbackward calculation, and unifies the denoising step into our framework.\nFID, w/o CFG\nFID, w/ CFG\nnorm control: clip\n31.88\n6.79\nnorm control: none\n45.54\n12.33\nnorm control: traj.\n34.88\n8.03\n(c) Norm control. Either clipping the forward model’s output or normaliz-\ning the forward trajectory improves generation quality by ensuring balanced\nsupervision strength across blocks.\nFID, w/o CFG\nFID, w/ CFG\nMSE\n31.88\n6.79\n+ LPIPS\n14.15\n4.91\n+ LPIPS + ConvNeXt\n2.46\n2.46\n(d) Distance metric. Our framework enables a flexible design of distance\nmetrics. Incorporating perceptual distance improves generation quality.\nTable 2. Ablation study on ImageNet 256×256 generation. FID-\n50K with 1-NFE is reported by default. (Settings: BiFlow-B/2, 160\nepochs. By default: adaptive weighted MSE loss without perceptual\nloss, training-time CFG.)\nScaling Behavior. We investigate the scaling behavior of\nBiFlow under different distance metrics, using iTARFlow\nof corresponding size as forward models. We summarize\npreliminary results in the table below.\nFID, w/ CFG\nB\nXL\nMSE\n6.79\n4.61\n+ LPIPS\n4.91\n3.36\n+ LPIPS + ConvNeXt\n2.46\n2.57\nOverall, BiFlow exhibits clear gains from increased\nmodel capacity when trained without the ConvNeXt-based\nperceptual loss. However, after incorporating ConvNeXt\nfeatures, further scaling yields diminishing returns, with FID\nimprovements gradually saturating. We hypothesize this\nbehavior may be related to overfitting, as evidenced by an\nincrease in FID during training. A comprehensive investiga-\ntion of BiFlow’s scaling behavior is left for future work.\n5.3. BiFlow vs. improved TARFlow\nWe compare our learned reverse model (BiFlow) with\nthe exact analytic inverse baseline (improved TARFlow) of\nthe forward process. In Tab. 3, we benchmark in terms of\ngeneration quality (FID score) and inference efficiency (flops\nand wall-clock time for generating a single image). Details\nof our benchmarking setup are provided in Appendix A.\nBiFlow\nimproved TARFlow\nB/2\nB/2\nM/2\nL/2\nXL/2\nFID\n2.39\n6.83\n5.22\n4.82\n4.54\n# Params\n133M\n120M\n296M\n448M\n690M\nGflops\n38\n152\n363\n552\n836\nWall-clock time (ms)\nTPU\n0.29+1.3\n65+1.3\n85+1.3\n165+1.3\n202+1.3\nGPU\n2.15+2.7\n129+2.7\n208+2.7\n349+2.7\n400+2.7\nCPU\n80+240\n9040+240\n16200+240\n20400+240\n26300+240\nWall-clock speedup, BiFlow-B/2 vs. iTARFlow: (VAE excluded, see also Fig. 2)\nTPU\n-\n224×\n293×\n569×\n697×\nGPU\n-\n60×\n97×\n162×\n186×\nCPU\n-\n113×\n203×\n255×\n329×\nWall-clock speedup, BiFlow-B/2 vs. iTARFlow: (VAE included)\nTPU\n-\n42×\n54×\n105×\n128×\nGPU\n-\n27×\n43×\n73×\n83×\nCPU\n-\n29×\n51×\n65×\n83×\nTable 3. Comparison between BiFlow and iTARFlow baseline.\nWe report both generation quality (FID-50K) and inference cost\nper image. All wall-clock time measurements are reported as\n“generator + VAE decoding”. Compared to iTARFlow, BiFlow\nachieves one to two orders of magnitude faster sampling on TPU,\nGPU, and CPU, while attaining superior generation quality. (The\nVAE decoder contains 49M parameters and requires 308 Gflops.)\nExperiments show that our BiFlow-B/2 surpasses the\nexact inverse of the improved TARFlow-XL/2 baseline in\ngeneration quality. Remarkably, BiFlow requires only a\nsingle function evaluation (1-NFE), compared to 256×2\nsequential decoding steps for the autoregressive inference\nof the exact analytic inverse — resulting in up to a 42×\nspeedup for models of similar size on TPU.\nWhy can a learned inverse outperform the exact inverse?\nOur reverse model Gϕ is trained to reconstruct real images\ndirectly, rather than to replicate synthetic samples produced\nby the exact inverse as in conventional distillation. This\nencourages its predictions to align more closely with the true\ndata distribution. In addition, Gϕ is optimized end-to-end\nwith the forward map fixed, learning to directly transform\nnoise into clean data. This joint optimization can help the\nmodel to learn a stable and globally consistent mapping.\nWhy is a learned inverse significantly faster than the exact\ninverse? From an algorithmic perspective, two key improve-\nments reduce the computational cost of BiFlow. First, Bi-\nFlow eliminates the score-based denoising step required\nby the exact inverse of TARFlow, removing a major com-\nputational bottleneck. Second, we integrate CFG into the\ntraining stage, effectively halving the inference cost com-\npared to applying CFG during sampling. Together, these two\nimprovements reduce the flops by roughly 4×.\nFrom an architectural perspective, the autoregressive de-\nsign of TARFlow imposes inherent limitations on parallelism\nduring inference. Our bidirectional attention Transformer\ndesign allows for fully parallelized computation across the\nsequence dimension, which leads to significant speedups\non modern accelerators. Notably, due to the efficiency of\nBiFlow, the VAE decoder has become a dominant computa-\ntional overhead, which is outside the scope of this work.\n7", "clean_text": "FID, w/o CFG FID, w/ CFG inference-time CFG 36.93 (NFE=1) 6.90 (NFE=2) →training-time CFG 31.88 (NFE=1) 6.79 (NFE=1) (a) BiFlow with guidance. Conditioning on the CFG scale during training improves FID both w/ and w/o CFG while preserving flexible 1-NFE inference. The baseline w/o CFG is final results in Tab. 1. FID, w/o CFG FID, w/ CFG learned denoise 31.88 6.79 →no denoise 100.51 26.20 →score-based denoise 42.62 10.98 (b) Learned denoising. Our learned denoising scheme is effective. Compared to score-based denoising in TARFlow, it eliminates an extra forwardbackward calculation, and unifies the denoising step into our framework. FID, w/o CFG FID, w/ CFG norm control: clip 31.88 6.79 norm control: none 45.54 12.33 norm control: traj. 34.88 8.03 (c) Norm control. Either clipping the forward model’s output or normalizing the forward trajectory improves generation quality by ensuring balanced supervision strength across blocks. FID, w/o CFG FID, w/ CFG MSE 31.88 6.79 + LPIPS 14.15 4.91 + LPIPS + ConvNeXt 2.46 2.46 (d) Distance metric. Our framework enables a flexible design of distance metrics. Incorporating perceptual distance improves generation quality. Table 2. Ablation study on ImageNet 256×256 generation. FID50K with 1-NFE is reported by default. (Settings: BiFlow-B/2, 160 epochs. By default: adaptive weighted MSE loss without perceptual loss, training-time CFG.) Scaling Behavior. We investigate the scaling behavior of BiFlow under different distance metrics, using iTARFlow of corresponding size as forward models. We summarize preliminary results in the table below. FID, w/ CFG B XL MSE 6.79 4.61 + LPIPS 4.91 3.36 + LPIPS + ConvNeXt 2.46 2.57 Overall, BiFlow exhibits clear gains from increased model capacity when trained without the ConvNeXt-based perceptual loss. However, after incorporating ConvNeXt features, further scaling yields diminishing returns, with FID improvements gradually saturating. We hypothesize this behavior may be related to overfitting, as evidenced by an increase in FID during training. A comprehensive investigation of BiFlow’s scaling behavior is left for future work. 5.3. BiFlow vs. improved TARFlow We compare our learned reverse model (BiFlow) with the exact analytic inverse baseline (improved TARFlow) of the forward process. In Tab. 3, we benchmark in terms of generation quality (FID score) and inference efficiency (flops and wall-clock time for generating a single image). Details of our benchmarking setup are provided in Appendix A. BiFlow improved TARFlow B/2 B/2 M/2 L/2 XL/2 FID 2.39 6.83 5.22 4.82 4.54 # Params 133M 120M 296M 448M 690M Gflops 38 152 363 552 836 Wall-clock time (ms) TPU 0.29+1.3 65+1.3 85+1.3 165+1.3 202+1.3 GPU 2.15+2.7 129+2.7 208+2.7 349+2.7 400+2.7 CPU 80+240 9040+240 16200+240 20400+240 26300+240 Wall-clock speedup, BiFlow-B/2 vs. iTARFlow: (VAE excluded, see also Fig. 2) TPU 224× 293× 569× 697× GPU 60× 97× 162× 186× CPU 113× 203× 255× 329× Wall-clock speedup, BiFlow-B/2 vs. iTARFlow: (VAE included) TPU 42× 54× 105× 128× GPU 27× 43× 73× 83× CPU 29× 51× 65× 83× Table 3. Comparison between BiFlow and iTARFlow baseline. We report both generation quality (FID-50K) and inference cost per image. All wall-clock time measurements are reported as “generator + VAE decoding”. Compared to iTARFlow, BiFlow achieves one to two orders of magnitude faster sampling on TPU, GPU, and CPU, while attaining superior generation quality. (The VAE decoder contains 49M parameters and requires 308 Gflops.) Experiments show that our BiFlow-B/2 surpasses the exact inverse of the improved TARFlow-XL/2 baseline in generation quality. Remarkably, BiFlow requires only a single function evaluation (1-NFE), compared to 256×2 sequential decoding steps for the autoregressive inference of the exact analytic inverse — resulting in up to a 42× speedup for models of similar size on TPU. Why can a learned inverse outperform the exact inverse? Our reverse model Gϕ is trained to reconstruct real images directly, rather than to replicate synthetic samples produced by the exact inverse as in conventional distillation. This encourages its predictions to align more closely with the true data distribution. In addition, Gϕ is optimized end-to-end with the forward map fixed, learning to directly transform noise into clean data. This joint optimization can help the model to learn a stable and globally consistent mapping. Why is a learned inverse significantly faster than the exact inverse? From an algorithmic perspective, two key improvements reduce the computational cost of BiFlow. First, BiFlow eliminates the score-based denoising step required by the exact inverse of TARFlow, removing a major computational bottleneck. Second, we integrate CFG into the training stage, effectively halving the inference cost compared to applying CFG during sampling. Together, these two improvements reduce the flops by roughly 4×. From an architectural perspective, the autoregressive design of TARFlow imposes inherent limitations on parallelism during inference. Our bidirectional attention Transformer design allows for fully parallelized computation across the sequence dimension, which leads to significant speedups on modern accelerators. Notably, due to the efficiency of BiFlow, the VAE decoder has become a dominant computational overhead, which is outside the scope of this work. 7"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 8, "text": "Method\n# Params\nNFE\nFID(↓)\nIS(↑)\nAutoregressive Normalizing Flow\nTARFlow-XL/8@pix [65]\n1.3B\n⋆\n5.56\n-\nSTARFlow-XL/1 [21]\n1.4B\n⋆\n2.40\n-\nAutoregressive Normalizing Flow (our impl.)\niTARFlow-B/2\n120M\n⋆\n6.83\n226.2\niTARFlow-M/2\n296M\n⋆\n5.22\n255.5\niTARFlow-L/2\n448M\n⋆\n4.82\n254.8\niTARFlow-XL/2\n690M\n⋆\n4.54\n259.3\n1-NFE Normalizing Flow\nBiFlow-B/2 (Ours)\n133M\n1\n2.39\n303.0\nMethod\n# Params\nNFE\nFID(↓)\nIS(↑)\nGANs\nBigGAN-deep [3]\n112M\n1\n6.95\n202.6\nGigaGAN [26]\n569M\n1\n3.45\n225.5\nStyleGAN-XL [49]\n166M\n1\n2.30\n265.1\n1-NFE diffusion/flow matching from scratch\niCT-XL/2 [52]\n675M\n1\n34.24\n-\nShortcut-XL/2 [15]\n675M\n1\n10.60\n-\nMeanFlow-XL/2 [17]\n676M\n1\n3.43\n247.5\nTiM-XL/2 [60]\n664M\n1\n3.26\n210.3\nα-Flow-XL/2+ [67]\n676M\n1\n2.58\n-\niMF-XL/2 [18]\n610M\n1\n1.72\n282.0\n1-NFE diffusion/flow matching (distillation)\nπ-Flow-XL/2 [6]\n675M\n1\n2.85\n-\nDMF-XL/2+ [32]\n675M\n1\n2.16\n-\nFACM-XL/2 [43]\n675M\n1\n1.76\n290.0\nMethod\n# Params\nNFE\nFID(↓)\nIS(↑)\nautoregressive/masking\nMaskGIT [4]\n227M\n⋆\n6.18\n182.1\nRCG, conditional [34]\n512M\n⋆\n2.12\n267.7\nVAR-d30 [56]\n2.0B\n⋆\n1.92\n323.1\nMAR-H [35]\n943M\n⋆\n1.55\n303.7\nRAR-XXL [63]\n1.5B\n⋆\n1.48\n326.0\nxAR-H [44]\n1.1B\n⋆\n1.24\n301.6\nMulti-NFE diffusion/flow matching\nADM-G [9]\n554M\n250×2\n4.59\n-\nLDM-4-G [46]\n400M\n250×2\n3.60\n247.7\nDiT-XL/2 [42]\n675M\n250×2\n2.27\n278.2\nSiT-XL/2 [38]\n675M\n250×2\n2.06\n252.2\nJiT-G/16 [33]\n2B\n100×2\n1.82\n292.6\nSiT-XL/2 + REPA [64]\n675M\n250×2\n1.42\n305.7\nLightningDiT-XL/1 [62]\n675M\n250×2\n1.35\n295.3\nDDT-XL/2 [59]\n675M\n250×2\n1.26\n310.6\nDiTDH-XL + RAE [69]\n839M\n50×2\n1.13\n262.6\nTable 4. System-level comparison on ImageNet 256×256 class-conditional generation. All results are reported with CFG if applicable.\nLeft: Comparison with Normalizing Flow models. Middle: Other 1-NFE generative models, including GANs and diffusion/flow matching-\nbased models. Right: Other families of generative models. Our BiFlow model is trained for 350 epochs with perceptual distance. In all\ntables, ×2 indicates the use of CFG incurs double NFEs. ⋆: All AR-based methods, including AR Normalizing Flow (left) and other AR\nmodels (right), involve a large number of forward evaluations, yet each evaluation is on one or a very few tokens. For example, for standard\nleft-to-right order AR, the average NFE of the entire AR process is roughly 1 (or 2× w/ CFG), that is, K evaluations with a\n1\nK fraction of\ntokens each. In addition, TARFlow/iTARFlow has an extra NFE of 2 due to the score-based denoising post-processing. Results of [52] is collected\nfrom [70]; results of [65] is collected from [21]; TARFlow-XL/8@pix denotes TARFlow on pixel-space with patch size 8.\n5.4. Comparison with Prior Works\nIn Tab. 4, we provide system-level comparisons with\nprevious methods on class-conditional ImageNet 256×256\ngeneration. We categorize prior works into three groups:\nNormalizing Flows (Tab. 4, left), 1-NFE generative models\n(Tab. 4, middle), and other families of generative models\n(Tab. 4, right). All our models are trained to convergence.\nComparison with Normalizing Flows. Tab. 4 (left) com-\npares BiFlow with previous state-of-the-art Normalizing\nFlows models. Our BiFlow-B/2, with only 133 million pa-\nrameters, achieves an FID of 2.39 in a single function eval-\nuation (1-NFE), establishing a new state-of-the-art among\nNormalizing Flows. In contrast, STARFlow uses thousands\nof sequential decoding steps due to their autoregressive sam-\npling process. It yields a similar FID score with about 10×\nparameters and more than 400× inference wall-clock time\n(see Tab. 6 for details).\nMore broadly, BiFlow represents a significant advance-\nment in Normalizing Flows, demonstrating that direct and\nefficient generation can coexist with high fidelity.\nComparison with Other Generative Models. We compare\nBiFlow with other generative model families, especially\n1-NFE methods. As shown in Tab. 4, BiFlow offers an\nexcellent balance between generation quality and sampling\nefficiency. These results demonstrate that BiFlow achieves\nperformance on par with leading 1-NFE generative models.\n6. Conclusion\nThis work revisits one of the oldest, yet most principled,\nfoundations of generative modeling — Normalizing Flows —\nand redefines its boundaries. We challenge the conventional\nFigure 6. 1-NFE Generation Results. We show selected samples\ngenerated by our BiFlow-B/2 model with guidance scale 2.0 on\nImageNet 256×256. BiFlow achieves high-fidelity generation with\nonly a single function evaluation (1-NFE) from noise.\nwisdom that the reverse process must be the exact analytic\ninverse of the forward process, and demonstrate that the\nlong-held constraint is unnecessary. By introducing a learn-\nable reverse model, BiFlow pushes Normalizing Flows from\nanalytically invertible mappings to trainable bidirectional\nsystems, from autoregressive sampling to fully parallelized,\nefficient 1-NFE generation, and from an implicit genera-\ntive model towards a direct generative model. Experiments\ndemonstrate that BiFlow achieves competitive generation\nquality among Normalizing Flows, while delivering up to\ntwo orders of magnitude faster inference than its explicit\ninverse counterpart. We hope this work can serve as a step\ntoward rethinking and expanding the scope of Normaliz-\ning Flows, inspiring future research on direct, flexible, and\nefficient NF-based generation.\n8", "clean_text": "Method # Params NFE FID(↓) IS(↑) Autoregressive Normalizing Flow TARFlow-XL/8@pix [65] 1.3B ⋆ 5.56 STARFlow-XL/1 [21] 1.4B ⋆ 2.40 Autoregressive Normalizing Flow (our impl.) iTARFlow-B/2 120M ⋆ 6.83 226.2 iTARFlow-M/2 296M ⋆ 5.22 255.5 iTARFlow-L/2 448M ⋆ 4.82 254.8 iTARFlow-XL/2 690M ⋆ 4.54 259.3 1-NFE Normalizing Flow BiFlow-B/2 (Ours) 133M 1 2.39 303.0 Method # Params NFE FID(↓) IS(↑) GANs BigGAN-deep [3] 112M 1 6.95 202.6 GigaGAN [26] 569M 1 3.45 225.5 StyleGAN-XL [49] 166M 1 2.30 265.1 1-NFE diffusion/flow matching from scratch iCT-XL/2 [52] 675M 1 34.24 Shortcut-XL/2 [15] 675M 1 10.60 MeanFlow-XL/2 [17] 676M 1 3.43 247.5 TiM-XL/2 [60] 664M 1 3.26 210.3 α-Flow-XL/2+ [67] 676M 1 2.58 iMF-XL/2 [18] 610M 1 1.72 282.0 1-NFE diffusion/flow matching (distillation) π-Flow-XL/2 [6] 675M 1 2.85 DMF-XL/2+ [32] 675M 1 2.16 FACM-XL/2 [43] 675M 1 1.76 290.0 Method # Params NFE FID(↓) IS(↑) autoregressive/masking MaskGIT [4] 227M ⋆ 6.18 182.1 RCG, conditional [34] 512M ⋆ 2.12 267.7 VAR-d30 [56] 2.0B ⋆ 1.92 323.1 MAR-H [35] 943M ⋆ 1.55 303.7 RAR-XXL [63] 1.5B ⋆ 1.48 326.0 xAR-H [44] 1.1B ⋆ 1.24 301.6 Multi-NFE diffusion/flow matching ADM-G [9] 554M 250×2 4.59 LDM-4-G [46] 400M 250×2 3.60 247.7 DiT-XL/2 [42] 675M 250×2 2.27 278.2 SiT-XL/2 [38] 675M 250×2 2.06 252.2 JiT-G/16 [33] 2B 100×2 1.82 292.6 SiT-XL/2 + REPA [64] 675M 250×2 1.42 305.7 LightningDiT-XL/1 [62] 675M 250×2 1.35 295.3 DDT-XL/2 [59] 675M 250×2 1.26 310.6 DiTDH-XL + RAE [69] 839M 50×2 1.13 262.6 Table 4. System-level comparison on ImageNet 256×256 class-conditional generation. All results are reported with CFG if applicable. Left: Comparison with Normalizing Flow models. Middle: Other 1-NFE generative models, including GANs and diffusion/flow matchingbased models. Right: Other families of generative models. Our BiFlow model is trained for 350 epochs with perceptual distance. In all tables, ×2 indicates the use of CFG incurs double NFEs. ⋆: All AR-based methods, including AR Normalizing Flow (left) and other AR models (right), involve a large number of forward evaluations, yet each evaluation is on one or a very few tokens. For example, for standard left-to-right order AR, the average NFE of the entire AR process is roughly 1 (or 2× w/ CFG), that is, K evaluations with a 1 K fraction of tokens each. In addition, TARFlow/iTARFlow has an extra NFE of 2 due to the score-based denoising post-processing. Results of [52] is collected from [70]; results of [65] is collected from [21]; TARFlow-XL/8@pix denotes TARFlow on pixel-space with patch size 8. 5.4. Comparison with Prior Works In Tab. 4, we provide system-level comparisons with previous methods on class-conditional ImageNet 256×256 generation. We categorize prior works into three groups: Normalizing Flows (Tab. 4, left), 1-NFE generative models (Tab. 4, middle), and other families of generative models (Tab. 4, right). All our models are trained to convergence. Comparison with Normalizing Flows. Tab. 4 (left) compares BiFlow with previous state-of-the-art Normalizing Flows models. Our BiFlow-B/2, with only 133 million parameters, achieves an FID of 2.39 in a single function evaluation (1-NFE), establishing a new state-of-the-art among Normalizing Flows. In contrast, STARFlow uses thousands of sequential decoding steps due to their autoregressive sampling process. It yields a similar FID score with about 10× parameters and more than 400× inference wall-clock time (see Tab. 6 for details). More broadly, BiFlow represents a significant advancement in Normalizing Flows, demonstrating that direct and efficient generation can coexist with high fidelity. Comparison with Other Generative Models. We compare BiFlow with other generative model families, especially 1-NFE methods. As shown in Tab. 4, BiFlow offers an excellent balance between generation quality and sampling efficiency. These results demonstrate that BiFlow achieves performance on par with leading 1-NFE generative models. 6. Conclusion This work revisits one of the oldest, yet most principled, foundations of generative modeling — Normalizing Flows — and redefines its boundaries. We challenge the conventional Figure 6. 1-NFE Generation Results. We show selected samples generated by our BiFlow-B/2 model with guidance scale 2.0 on ImageNet 256×256. BiFlow achieves high-fidelity generation with only a single function evaluation (1-NFE) from noise. wisdom that the reverse process must be the exact analytic inverse of the forward process, and demonstrate that the long-held constraint is unnecessary. By introducing a learnable reverse model, BiFlow pushes Normalizing Flows from analytically invertible mappings to trainable bidirectional systems, from autoregressive sampling to fully parallelized, efficient 1-NFE generation, and from an implicit generative model towards a direct generative model. Experiments demonstrate that BiFlow achieves competitive generation quality among Normalizing Flows, while delivering up to two orders of magnitude faster inference than its explicit inverse counterpart. We hope this work can serve as a step toward rethinking and expanding the scope of Normalizing Flows, inspiring future research on direct, flexible, and efficient NF-based generation. 8"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 9, "text": "config\nBiFlow\nimproved TARFlow\nB/2\nB/2\nM/2\nL/2\nXL/2\n# Params (M)\n133\n120\n296\n448\n690\nblock\n9\n8\n10\n12\n15\nlayer\n8\n8\n9\n9\n9\nhidden dim\n384\n384\n512\n576\n640\nattn heads\n6\n6\n8\n9\n10\npatch size\n2\n2\nclass tokens\n8\n—\nguidance tokens\n4† / 1\n—\nepochs\n160† / 350\n960\n640\n640\n480\nbatch size\n256\n256\nlearning rate\n4e-4\n4e-4\n4e-4\n2e-4\n1e-4\nlr schedule\nconstant\nconstant\nlr warmup\n10 epochs\n10 epochs\noptimizer\nAdam\nAdam [28]\nAdam (β1, β2)\n(0.9, 0.95)\n(0.9, 0.95)\nweight decay\n0.0\n0.0\ndropout\n0.0\n0.0\nema decay\n0.9999\n0.9999\nlabel drop\n0.1\n0.1\nadaptive weight p\n1\n—\nwVGG\n1.0† / 0.8\n—\nwConvNeXt\n0.4† / 0.6\n—\nclip range c\n—\n1.0\n1.0\n3.0\n3.0\nnoise level σ\n—\n0.3\nTable 5. Configurations and training hyperparameters on Ima-\ngeNet 256×256. † indicates the setting in the ablation study.\nA. Implementation Details\nWe implement all experiments using the JAX frame-\nwork [2] on Google TPU hardware. All reported results are\nobtained on TPU v4, v5p, and v6e cores. The configurations\nand training hyperparameters for improved TARFlow and\nBiFlow are provided in Tab. 5. For the MSE-only ablation in\nSec. 5, we employ adaptive weighting with exponent p = 2;\nfor all other experiments we use p = 1 (see Appendix C.3\nfor detailed ablations).\nFID Evaluation. For generative evaluation, we compute\nthe Fr´echet Inception Distance (FID) [23] between 50,000\ngenerated images and training images, without applying any\ndata augmentation. We use the Inception-V3 model [54] pro-\nvided by StyleGAN3 [27], converted into a JAX-compatible\nimplementation. We sample 50 images per class for all 1000\nImageNet classes, following the protocol in [69].\nInference Cost Evaluation. In Fig. 2 and Tab. 3, we report\ninference cost across three hardware configurations: GPU,\nTPU, and CPU. For all metrics, we report the average per-\nimage runtime in seconds, averaged over multiple runs to\nensure stability. All measurements include the overhead of\nCFG and VAE decoding time when applicable. We also\nprovide a comparison with prior Normalizing Flow mod-\nels [65, 21] in Tab. 6. All autoregressive models utilize\nKV-cache to accelerate inference, and Gflops in Tab. 3 is\nestimated using JAX’s cost analysis function.\nMethod\n# Params\nTime (ms)\nSpeed\nVAE?\nTARFlow-XL/8@pix [65]\n1.3B\n1192\n1×\n✗\nSTARFlow-XL/1 [21]\n1.4B\n677+1.3\n1.76×\n✓\niTARFlow-B/2\n120M\n65+1.3\n18.0×\n✓\niTARFlow-M/2\n296M\n85+1.3\n13.8×\n✓\niTARFlow-L/2\n446M\n165+1.3\n7.17×\n✓\niTARFlow-XL/2\n675M\n202+1.3\n5.86×\n✓\nBiFlow-B/2 (Ours)\n133M\n0.29+1.3\n750×\n✓\n(a) TPU inference time comparison, benchmarked on 8 TPU v4 cores with\na pre-compiled JAX sampling function.\nMethod\n# Params\nTime (ms)\nSpeed\nVAE?\nTARFlow-XL/8@pix [65]\n1.3B\n3452\n1×\n✗\nSTARFlow-XL/1 [21]\n1.4B\n2193+2.7\n1.57×\n✓\niTARFlow-B/2\n120M\n129+2.7\n26.2×\n✓\niTARFlow-M/2\n296M\n208+2.7\n16.4×\n✓\niTARFlow-L/2\n446M\n349+2.7\n9.82×\n✓\niTARFlow-XL/2\n675M\n400+2.7\n8.57×\n✓\nBiFlow-B/2 (Ours)\n133M\n2.15+2.7\n712×\n✓\n(b) GPU inference time comparison, benchmarked on 1 NVIDIA H200 core\nwith PyTorch and torch.compile optimization if beneficial.\nMethod\n# Params\nTime (ms)\nSpeed\nVAE?\nTARFlow-XL/8@pix [65]\n1.3B\n512000\n1×\n✗\nSTARFlow-XL/1 [21]\n1.4B\n276700+240\n1.85×\n✓\niTARFlow-B/2\n120M\n9040+240\n55.2×\n✓\niTARFlow-M/2\n296M\n16200+240\n31.1×\n✓\niTARFlow-L/2\n446M\n20400+240\n24.8×\n✓\niTARFlow-XL/2\n675M\n26300+240\n19.3×\n✓\nBiFlow-B/2 (Ours)\n133M\n80+240\n1600×\n✓\n(c) CPU inference time comparison, benchmarked on 1 AMD EPYC 7B12\nnode with 120 physical CPU cores and 400GB RAM. We reuse the PyTorch\nimplementations with torch.compile optimization if beneficial.\nTable 6. Comparison of NF models’ inference wall-clock time\non TPU, GPU, and CPU. The wall-clock time is evaluated per\nimage on average in milliseconds. All models include the overhead\nof CFG at inference time, as well as the VAE decoding time when\napplicable. All autoregressive models utilize KV-cache to acceler-\nate inference. See Appendix A for further details.\nFor TPU wall-clock time, all models are evaluated using\na pre-compiled JAX sampling function on 8 TPU v4 cores.\nReported times exclude compilation overhead. We use a\nlocal device batch size of 10 for model inference, and 200\nfor VAE decoding.\nFor GPU wall-clock time, all models are re-implemented\nin PyTorch and evaluated on a single NVIDIA H200 GPU\nwith a batch size of 128. The VAE decoding time is obtained\nwith torch.compile optimization.\nFor CPU wall-clock time, we reuse the PyTorch imple-\nmentation on a single AMD EPYC 7B12 node (120 physical\nCPU cores and 400 GB RAM). We use a smaller batch size\nof 64 for most models; however, TARFlow and STARFlow\nare restricted to a batch size of 4 due to efficiency concerns.\nWe observe that batch size has a negligible impact on per-\nimage CPU inference time. All other experimental settings\nremain consistent with the GPU evaluation.\n9", "clean_text": "config BiFlow improved TARFlow B/2 B/2 M/2 L/2 XL/2 # Params (M) 133 120 296 448 690 block 9 8 10 12 15 layer 8 8 9 9 9 hidden dim 384 384 512 576 640 attn heads 6 6 8 9 10 patch size 2 2 class tokens 8 — guidance tokens 4† / 1 — epochs 160† / 350 960 640 640 480 batch size 256 256 learning rate 4e-4 4e-4 4e-4 2e-4 1e-4 lr schedule constant constant lr warmup 10 epochs 10 epochs optimizer Adam Adam [28] Adam (β1, β2) (0.9, 0.95) (0.9, 0.95) weight decay 0.0 0.0 dropout 0.0 0.0 ema decay 0.9999 0.9999 label drop 0.1 0.1 adaptive weight p 1 — wVGG 1.0† / 0.8 — wConvNeXt 0.4† / 0.6 — clip range c — 1.0 1.0 3.0 3.0 noise level σ — 0.3 Table 5. Configurations and training hyperparameters on ImageNet 256×256. † indicates the setting in the ablation study. A. Implementation Details We implement all experiments using the JAX framework [2] on Google TPU hardware. All reported results are obtained on TPU v4, v5p, and v6e cores. The configurations and training hyperparameters for improved TARFlow and BiFlow are provided in Tab. 5. For the MSE-only ablation in Sec. 5, we employ adaptive weighting with exponent p = 2; for all other experiments we use p = 1 (see Appendix C.3 for detailed ablations). FID Evaluation. For generative evaluation, we compute the Fr´echet Inception Distance (FID) [23] between 50,000 generated images and training images, without applying any data augmentation. We use the Inception-V3 model [54] provided by StyleGAN3 [27], converted into a JAX-compatible implementation. We sample 50 images per class for all 1000 ImageNet classes, following the protocol in [69]. Inference Cost Evaluation. In Fig. 2 and Tab. 3, we report inference cost across three hardware configurations: GPU, TPU, and CPU. For all metrics, we report the average perimage runtime in seconds, averaged over multiple runs to ensure stability. All measurements include the overhead of CFG and VAE decoding time when applicable. We also provide a comparison with prior Normalizing Flow models [65, 21] in Tab. 6. All autoregressive models utilize KV-cache to accelerate inference, and Gflops in Tab. 3 is estimated using JAX’s cost analysis function. Method # Params Time (ms) Speed VAE? TARFlow-XL/8@pix [65] 1.3B 1192 1× ✗ STARFlow-XL/1 [21] 1.4B 677+1.3 1.76× ✓ iTARFlow-B/2 120M 65+1.3 18.0× ✓ iTARFlow-M/2 296M 85+1.3 13.8× ✓ iTARFlow-L/2 446M 165+1.3 7.17× ✓ iTARFlow-XL/2 675M 202+1.3 5.86× ✓ BiFlow-B/2 (Ours) 133M 0.29+1.3 750× ✓ (a) TPU inference time comparison, benchmarked on 8 TPU v4 cores with a pre-compiled JAX sampling function. Method # Params Time (ms) Speed VAE? TARFlow-XL/8@pix [65] 1.3B 3452 1× ✗ STARFlow-XL/1 [21] 1.4B 2193+2.7 1.57× ✓ iTARFlow-B/2 120M 129+2.7 26.2× ✓ iTARFlow-M/2 296M 208+2.7 16.4× ✓ iTARFlow-L/2 446M 349+2.7 9.82× ✓ iTARFlow-XL/2 675M 400+2.7 8.57× ✓ BiFlow-B/2 (Ours) 133M 2.15+2.7 712× ✓ (b) GPU inference time comparison, benchmarked on 1 NVIDIA H200 core with PyTorch and torch.compile optimization if beneficial. Method # Params Time (ms) Speed VAE? TARFlow-XL/8@pix [65] 1.3B 512000 1× ✗ STARFlow-XL/1 [21] 1.4B 276700+240 1.85× ✓ iTARFlow-B/2 120M 9040+240 55.2× ✓ iTARFlow-M/2 296M 16200+240 31.1× ✓ iTARFlow-L/2 446M 20400+240 24.8× ✓ iTARFlow-XL/2 675M 26300+240 19.3× ✓ BiFlow-B/2 (Ours) 133M 80+240 1600× ✓ (c) CPU inference time comparison, benchmarked on 1 AMD EPYC 7B12 node with 120 physical CPU cores and 400GB RAM. We reuse the PyTorch implementations with torch.compile optimization if beneficial. Table 6. Comparison of NF models’ inference wall-clock time on TPU, GPU, and CPU. The wall-clock time is evaluated per image on average in milliseconds. All models include the overhead of CFG at inference time, as well as the VAE decoding time when applicable. All autoregressive models utilize KV-cache to accelerate inference. See Appendix A for further details. For TPU wall-clock time, all models are evaluated using a pre-compiled JAX sampling function on 8 TPU v4 cores. Reported times exclude compilation overhead. We use a local device batch size of 10 for model inference, and 200 for VAE decoding. For GPU wall-clock time, all models are re-implemented in PyTorch and evaluated on a single NVIDIA H200 GPU with a batch size of 128. The VAE decoding time is obtained with torch.compile optimization. For CPU wall-clock time, we reuse the PyTorch implementation on a single AMD EPYC 7B12 node (120 physical CPU cores and 400 GB RAM). We use a smaller batch size of 64 for most models; however, TARFlow and STARFlow are restricted to a batch size of 4 due to efficiency concerns. We observe that batch size has a negligible impact on perimage CPU inference time. All other experimental settings remain consistent with the GPU evaluation. 9"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 10, "text": "Algorithm 1 BiFlow: Training.\n# x: training batch, (N, H, W, C)\n# F: forward model (B blocks), frozen\n# G: reverse model (B + 1 blocks)\n# phi: projection heads\n# noise injection\ne = randn like(x)\nx_tilde = x + noise_level * e\n# get forward trajectory xs and prior z\nxs, z = F(x_tilde)\n# get reverse trajectory hs and\nreconstructed x’\nhs, x_prime = G(z)\n# project hidden into input space\nfor i in range(B):\nhs[i] = phi[i](hs[i])\n# compute loss\nloss_align = mse(xs, hs)\nloss_recon = metric(x, x_prime)\nloss = loss_align + loss_recon\nAlgorithm 2 BiFlow: 1-NFE Sampling.\ne = randn(x_shape)\n_, x = G(e)\nB. Method Details\nB.1. Pseudocode\nWe provide the pseudocode for training our BiFlow model\nwith hidden alignment in Alg. 1, as well as the 1-NFE sam-\npling procedure in Alg. 2.\nIn the algorithm, the forward model F produces the entire\nforward trajectory xs, i.e., ˜x, x1, x2, . . . , xB−1, along with\nthe prior z = xB. Similarly, the reverse model G outputs the\nsequence of intermediate hiddens states hs as reverse tra-\njectory: hB−1, hB−2, . . . , h0, along with the reconstructed\nclean input x prime.\nThe final loss function consists of alignment loss between\nforward and reverse hidden states in Eq. (2), and reconstruc-\ntion loss between the clean input x and reconstructed output\nx prime.\nB.2. BiFlow with Guidance\nTraining-time CFG. As discussed in Sec. 4.5, to enable\nguided sampling within a single forward pass (1-NFE), we\ndirectly train a guided reverse model Gcfg\nϕ defined as\nGi,cfg\nϕ\n(hi | c) = (1 + wi)Gi\nϕ(hi | c) −wiGi\nϕ(hi).\n0.0\n0.5\n1.0\n1.5\nCFG scale\n0\n10\n20\n30\n40\nFID-50K\ninference-time CFG (2-NFE)\ntraining-time CFG    (1-NFE)\nFigure 7. Comparison between training-time and inference-time\nCFG of BiFlow. Training-time CFG achieves similar or better\nperformance compared to inference-time CFG while requiring only\nhalf inference compute and retaining full post-hoc tuning flexibility.\n(Settings: BiFlow-B/2, 160 epochs, MSE-only baseline.)\nwhere wi is the guidance scale at block i. The unconditional\noutput of Gi,cfg\nϕ\nmatches that of the original Gi\nϕ. Therefore,\nthe unguided block output can be expressed as\nhi+1 =\nGi,cfg\nϕ\n(hi | c) + wiGi,cfg\nϕ\n(hi)\n1 + wi\n.\n(5)\nDuring training, we compute our hidden-alignment loss di-\nrectly on hi+1 from Eq. (5). At inference time, this for-\nmulation allows us to use Gi,cfg\nϕ\ndirectly, producing guided\nsamples with only a 1-NFE forward pass. We add stop gra-\ndient to the unconditional output to stabilize training.\nGuidance conditioning.\nTo retain the ability to adjust\nthe CFG scale at inference time, we explicitly condition\nthe reverse model on the guidance scale [39, 18], i.e.,\nGi,cfg\nϕ\n(hi | c, wi). During training, we sample wi from a\nuniform distribution U(0, wmax) and apply Eq. (5) to com-\npute the unguided output hi+1 for hidden alignment loss.\nWe compare this training-time CFG scheme with the\nmore conventional inference-time CFG in Fig. 7. Training-\ntime CFG achieves similar (even better) performance while\npreserving the 1-NFE efficiency and the flexibility to sweep\nCFG scales at inference time.\nB.3. Distance Metric\nAdaptive weighting. We adopt the adaptive loss reweighting\nstrategy from [16, 52, 17]. Given a prediction x and target y,\nthe adaptive-weighted distance is defined as:\nDp = sg(wp) · D(x, y),\nwp = (D(x, y) + c)−p,\nwhere c is a small constant and p ≥0 is adaptive weight.\nsg(·) denotes the stop-gradient operator. We apply adaptive\nweighting to all loss terms in our training objective.\nVGG feature. For the perceptual loss based on VGG fea-\ntures, we follow the LPIPS formulation [68]. Since our\nmodel operates in the latent space of a pre-trained VAE, we\ndecode the predicted latent x′ back into image space and\ncompute the LPIPS loss against the ground-truth image.\n10", "clean_text": "Algorithm 1 BiFlow: Training. # x: training batch, (N, H, W, C) # F: forward model (B blocks), frozen # G: reverse model (B + 1 blocks) # phi: projection heads # noise injection e = randn like(x) x_tilde = x + noise_level * e # get forward trajectory xs and prior z xs, z = F(x_tilde) # get reverse trajectory hs and reconstructed x’ hs, x_prime = G(z) # project hidden into input space for i in range(B): hs[i] = phi[i](hs[i]) # compute loss loss_align = mse(xs, hs) loss_recon = metric(x, x_prime) loss = loss_align + loss_recon Algorithm 2 BiFlow: 1-NFE Sampling. e = randn(x_shape) _, x = G(e) B. Method Details B.1. Pseudocode We provide the pseudocode for training our BiFlow model with hidden alignment in Alg. 1, as well as the 1-NFE sampling procedure in Alg. 2. In the algorithm, the forward model F produces the entire forward trajectory xs, i.e., ˜x, x1, x2, . . . , xB−1, along with the prior z = xB. Similarly, the reverse model G outputs the sequence of intermediate hiddens states hs as reverse trajectory: hB−1, hB−2, . . . , h0, along with the reconstructed clean input x prime. The final loss function consists of alignment loss between forward and reverse hidden states in Eq. (2), and reconstruction loss between the clean input x and reconstructed output x prime. B.2. BiFlow with Guidance Training-time CFG. As discussed in Sec. 4.5, to enable guided sampling within a single forward pass (1-NFE), we directly train a guided reverse model Gcfg ϕ defined as Gi,cfg ϕ (hi | c) = (1 + wi)Gi ϕ(hi | c) −wiGi ϕ(hi). 0.0 0.5 1.0 1.5 CFG scale 0 10 20 30 40 FID-50K inference-time CFG (2-NFE) training-time CFG (1-NFE) Figure 7. Comparison between training-time and inference-time CFG of BiFlow. Training-time CFG achieves similar or better performance compared to inference-time CFG while requiring only half inference compute and retaining full post-hoc tuning flexibility. (Settings: BiFlow-B/2, 160 epochs, MSE-only baseline.) where wi is the guidance scale at block i. The unconditional output of Gi,cfg ϕ matches that of the original Gi ϕ. Therefore, the unguided block output can be expressed as hi+1 = Gi,cfg ϕ (hi | c) + wiGi,cfg ϕ (hi) 1 + wi . (5) During training, we compute our hidden-alignment loss directly on hi+1 from Eq. (5). At inference time, this formulation allows us to use Gi,cfg ϕ directly, producing guided samples with only a 1-NFE forward pass. We add stop gradient to the unconditional output to stabilize training. Guidance conditioning. To retain the ability to adjust the CFG scale at inference time, we explicitly condition the reverse model on the guidance scale [39, 18], i.e., Gi,cfg ϕ (hi | c, wi). During training, we sample wi from a uniform distribution U(0, wmax) and apply Eq. (5) to compute the unguided output hi+1 for hidden alignment loss. We compare this training-time CFG scheme with the more conventional inference-time CFG in Fig. 7. Trainingtime CFG achieves similar (even better) performance while preserving the 1-NFE efficiency and the flexibility to sweep CFG scales at inference time. B.3. Distance Metric Adaptive weighting. We adopt the adaptive loss reweighting strategy from [16, 52, 17]. Given a prediction x and target y, the adaptive-weighted distance is defined as: Dp = sg(wp) · D(x, y), wp = (D(x, y) + c)−p, where c is a small constant and p ≥0 is adaptive weight. sg(·) denotes the stop-gradient operator. We apply adaptive weighting to all loss terms in our training objective. VGG feature. For the perceptual loss based on VGG features, we follow the LPIPS formulation [68]. Since our model operates in the latent space of a pre-trained VAE, we decode the predicted latent x′ back into image space and compute the LPIPS loss against the ground-truth image. 10"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 11, "text": "MSE\nLPIPS\nnaive distillation\n0.115\n0.331\nhidden distillation\n0.156\n0.392\nhidden alignment\n0.111\n0.321\nTable 7. Reverse learning methods: reconstruction fidelity. We\nreport MSE and LPIPS between the original sample x and the\nreconstructed sample x′ produced by the learned reverse model.\nAmong the three strategies for approximating the inverse transfor-\nmation, the hidden alignment method achieves the most accurate\nreconstruction. The corresponding FIDs are shown in Tab. 1 (Set-\ntings: BiFlow-B/2, 160 epochs, no CFG, adaptive-weighted MSE\nloss only. All three rows share the same forward model.)\nConvNeXt feature. In addition to VGG features, we in-\ncorporate ConvNeXt V2 [61] (ImageNet-22K pre-trained,\nbase-size) as a complementary perceptual feature extractor.\nSimilar to the LPIPS, both the reconstructed image x′ and\nthe ground-truth image x are passed through the ConvNeXt\nnetwork after VAE decoding. The perceptual distance is\ncomputed using the extracted features, excluding the final\nclassification head.\nUsage of loss terms. In the ablation studies in Sec. 5, we\nuse only the adaptively weighted MSE loss unless otherwise\nnoted. In Tab. 4, we combine all three loss terms:\nL(x) =\nX\ni\nLMSE(xi, φi(hi))\n+ wVGGLVGG(x, x′) + wConvNeXtLConvNeXt(x, x′),\nwhere wVGG and wConvNeXt are tunable hyperparameters. We\nobserve that the final performance is particularly sensitive to\nwConvNeXt. Concrete weights are specified in Appendix A.\nNormalized Trajectory. As described in Sec. 4.4, the re-\nverse model is trained to align with a normalized forward\ntrajectory. Specifically, we pre-compute the squared norm\n∥xi∥2 of each trajectory point and average it over the entire\ndataset. During reverse model training, the intermediate\ntrajectory points are divided by\np\nE[∥xi∥2], ensuring scale\nconsistency across different blocks.\nWe do not use normalized trajectories in any experiments\nexcept the one in Tab. 2c, as we observe no significant differ-\nence when combined with iTARFlow. Nonetheless, normal-\nized trajectories are worth noting for scenarios where one\nwishes to use a pre-trained NF model without clipping.\nC. Additional Experiments\nC.1. Learning to Approximate the Inverse\nIn Tab. 1, we compare the empirical performance of the\nthree reverse learning approaches introduced in Sec. 4. Here,\nwe further provide quantitative results on their reconstruction\nfidelity in Tab. 7. Specifically, we evaluate the reconstruction\ndistance D(x, x′) using MSE and LPIPS (VGG-based) as\nmetrics.\nwd\\w\n0.5\n0.6\n0.7\n0.5\n10.51\n9.45\n8.77\n0.6\n10.21\n9.21\n8.59\n0.7\n9.93\n8.99\n8.41\n3.5\n7.05\n6.87\n6.89\n4.0\n6.94\n6.81\n6.87\n4.5\n6.88\n6.79\n6.87\n5.0\n6.86\n6.80\n6.89\nTable 8. Separate guidance scale for the denoising block. BiFlow\neliminates the score-based denoising step in TARFlow by learning\na dedicated denoising block, jointly trained with other blocks. This\ndenoising block serves a different purpose from the rest NF reverse\nprocess. Using a separate, larger guidance scale for the denoising\nblock improves sample quality. (Settings: BiFlow-B/2, 160 epochs,\nadaptively-weighted MSE only, training-time CFG. FID w/o CFG:\n31.88.)\nWe observe that the proposed hidden-alignment strategy\nachieves the lowest regression loss across both metrics. This\nindicates that hidden alignment provides a more accurate\nmapping between x and x′, leading to a better-behaved re-\nverse learning process.\nC.2. BiFlow with Guidance\nAs discussed in Sec. 4.2, the additional denoising block\nin our reverse model functions as a dedicated denoiser, while\nthe preceding B blocks focus on inverting the forward sub-\ntransformations. This structure naturally motivates applying\nCFG differently across these two components. We empiri-\ncally validate this design choice in Tab. 8.\nFor training-time CFG, we use a shared guidance scale\nacross all blocks, sampling w from a simple uniform prior\nU[0, 0.5]. In ablation studies that use MSE loss only (Sec. 5),\nwe decouple the guidance scales for the inverse blocks and\nthe denoising block, since the optimal pair (w, wd) typically\nsatisfies wd ≫w. In this case, we sample w ∼U[0, 1] and\nwd ∼U[0, 8].\nC.3. Distance Metric\nIn Sec. 4.3, we discuss different choices of distance met-\nrics for training BiFlow. We ablate the choice of perceptual\ndistance terms in Tab. 9. First, we compare different fea-\nture extractors, including a ResNet-101 [22] pre-trained for\nclassification and a DINOv2-B model [40], as reported in\nTab. 9a. For ResNet-101, we extract features by remov-\ning the final MLP head, following the same procedure as\nConvNeXt. Among all tested feature extractors, ConvNeXt\nachieves the best empirical performance. We further evaluate\nthe combination of VGG and ConvNeXt features in Tab. 9b.\nThe results indicate that using both features together yields\nbetter FID scores than using either one individually.\nFurthermore, we study the effect of adaptive weighting\nin the MSE loss in Tab. 10. MSE with adaptive weighting\nconsistently outperforms the naive MSE loss.\n11", "clean_text": "MSE LPIPS naive distillation 0.115 0.331 hidden distillation 0.156 0.392 hidden alignment 0.111 0.321 Table 7. Reverse learning methods: reconstruction fidelity. We report MSE and LPIPS between the original sample x and the reconstructed sample x′ produced by the learned reverse model. Among the three strategies for approximating the inverse transformation, the hidden alignment method achieves the most accurate reconstruction. The corresponding FIDs are shown in Tab. 1 (Settings: BiFlow-B/2, 160 epochs, no CFG, adaptive-weighted MSE loss only. All three rows share the same forward model.) ConvNeXt feature. In addition to VGG features, we incorporate ConvNeXt V2 [61] (ImageNet-22K pre-trained, base-size) as a complementary perceptual feature extractor. Similar to the LPIPS, both the reconstructed image x′ and the ground-truth image x are passed through the ConvNeXt network after VAE decoding. The perceptual distance is computed using the extracted features, excluding the final classification head. Usage of loss terms. In the ablation studies in Sec. 5, we use only the adaptively weighted MSE loss unless otherwise noted. In Tab. 4, we combine all three loss terms: L(x) = X i LMSE(xi, φi(hi)) + wVGGLVGG(x, x′) + wConvNeXtLConvNeXt(x, x′), where wVGG and wConvNeXt are tunable hyperparameters. We observe that the final performance is particularly sensitive to wConvNeXt. Concrete weights are specified in Appendix A. Normalized Trajectory. As described in Sec. 4.4, the reverse model is trained to align with a normalized forward trajectory. Specifically, we pre-compute the squared norm ∥xi∥2 of each trajectory point and average it over the entire dataset. During reverse model training, the intermediate trajectory points are divided by p E[∥xi∥2], ensuring scale consistency across different blocks. We do not use normalized trajectories in any experiments except the one in Tab. 2c, as we observe no significant difference when combined with iTARFlow. Nonetheless, normalized trajectories are worth noting for scenarios where one wishes to use a pre-trained NF model without clipping. C. Additional Experiments C.1. Learning to Approximate the Inverse In Tab. 1, we compare the empirical performance of the three reverse learning approaches introduced in Sec. 4. Here, we further provide quantitative results on their reconstruction fidelity in Tab. 7. Specifically, we evaluate the reconstruction distance D(x, x′) using MSE and LPIPS (VGG-based) as metrics. wd\\w 0.5 0.6 0.7 0.5 10.51 9.45 8.77 0.6 10.21 9.21 8.59 0.7 9.93 8.99 8.41 3.5 7.05 6.87 6.89 4.0 6.94 6.81 6.87 4.5 6.88 6.79 6.87 5.0 6.86 6.80 6.89 Table 8. Separate guidance scale for the denoising block. BiFlow eliminates the score-based denoising step in TARFlow by learning a dedicated denoising block, jointly trained with other blocks. This denoising block serves a different purpose from the rest NF reverse process. Using a separate, larger guidance scale for the denoising block improves sample quality. (Settings: BiFlow-B/2, 160 epochs, adaptively-weighted MSE only, training-time CFG. FID w/o CFG: 31.88.) We observe that the proposed hidden-alignment strategy achieves the lowest regression loss across both metrics. This indicates that hidden alignment provides a more accurate mapping between x and x′, leading to a better-behaved reverse learning process. C.2. BiFlow with Guidance As discussed in Sec. 4.2, the additional denoising block in our reverse model functions as a dedicated denoiser, while the preceding B blocks focus on inverting the forward subtransformations. This structure naturally motivates applying CFG differently across these two components. We empirically validate this design choice in Tab. 8. For training-time CFG, we use a shared guidance scale across all blocks, sampling w from a simple uniform prior U[0, 0.5]. In ablation studies that use MSE loss only (Sec. 5), we decouple the guidance scales for the inverse blocks and the denoising block, since the optimal pair (w, wd) typically satisfies wd ≫w. In this case, we sample w ∼U[0, 1] and wd ∼U[0, 8]. C.3. Distance Metric In Sec. 4.3, we discuss different choices of distance metrics for training BiFlow. We ablate the choice of perceptual distance terms in Tab. 9. First, we compare different feature extractors, including a ResNet-101 [22] pre-trained for classification and a DINOv2-B model [40], as reported in Tab. 9a. For ResNet-101, we extract features by removing the final MLP head, following the same procedure as ConvNeXt. Among all tested feature extractors, ConvNeXt achieves the best empirical performance. We further evaluate the combination of VGG and ConvNeXt features in Tab. 9b. The results indicate that using both features together yields better FID scores than using either one individually. Furthermore, we study the effect of adaptive weighting in the MSE loss in Tab. 10. MSE with adaptive weighting consistently outperforms the naive MSE loss. 11"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 12, "text": "feature model\nFID, w/o CFG\nFID, w/ CFG\nnone\n31.88\n6.79\nVGG + ResNet\n9.69\n4.34\nVGG + DINO\n9.33\n4.36\nConvNeXt + DINO\n3.19\n3.19\nVGG + ConvNeXt\n2.46\n2.46\n(a) Feature model ablation.\nwVGG\nwConvNeXt\nFID, w/o CFG\nFID, w/ CFG\n0.0\n0.0\n31.88\n6.79\n1.0\n0.0\n16.97\n5.31\n0.0\n0.4\n2.62\n2.62\n1.0\n0.4\n2.46\n2.46\n(b) VGG and ConvNeXt weight ablation.\nTable 9. Ablation on perceptual loss for BiFlow-B/2. FID-50K\nwith/without CFG are reported. (Settings: BiFlow-B/2, 160 epochs,\ntraining-time CFG, weight for two perceptual losses are 1.0 and 0.4\nby default.)\nadaptive weight p\nFID, w/o CFG\nFID, w/ CFG\n0.0\n38.23\n7.49\n0.5\n34.74\n7.08\n1.0\n34.43\n6.70\n2.0\n31.88\n6.79\nTable 10. Ablation on adaptive weighting. Adaptive weighted\nMSE loss works better than naive MSE (p = 0.0). (Settings: BiFlow-\nB/2, 160 epochs, adaptive weighted MSE only, training-time CFG.)\nC.4. Improved TARFlow Norm Control\nTo mitigate the imbalance in loss magnitudes across dif-\nferent blocks of BiFlow, we introduce a simple yet effective\nmodification to the original TARFlow [65] in Sec. 4.4: clip-\nping the parameters µ and α within a fixed range. This\nadjustment stabilizes training and improves final FID perfor-\nmance.\nIn Fig. 8, we visualize the norms of intermediate trajec-\ntory states during training of the improved TARFlow. With-\nout clipping, the norms across blocks diverge sharply and\ncontinue to grow as training progresses. With clipping, the\nnorms remain stable and well-controlled within a reasonable\nrange. Such normalization substantially benefits the training\nof the reverse model in the downstream.\nC.5. Improved TARFlow CFG\nFor completeness, we also examine classifier-free guid-\nance (CFG) designs for TARFlow, although this component\nis orthogonal to our main contributions. In the original\nTARFlow [65], the reverse update rule at block i is\nzi\nt,cfg = zi+1\nt\n⊙exp(αi\nt,cfg) + µi\nt,cfg,\nwhere guidance is applied to the predicted parameters by\nαi\nt,cfg = (1 + wt) αi\nt(· | c) −wt αi\nt(·),\nµi\nt,cfg = (1 + wt) µi\nt(· | c) −wt µi\nt(·),\n0\n1\n2\n3\n4\n5\nTraining Steps (×106)\n0\n10\n20\n30\n40\n50\n60\nSquared L2 Norm\n||x1||2\n||x2||2\n||x3||2\n||x4||2\n||x5||2\n||x6||2\n||x7||2\n(a) Improved TARFlow w/o clipping.\n0\n1\n2\n3\n4\n5\nTraining Steps (×106)\n0.2\n0.4\n0.6\n0.8\nSquared L2 Norm\n||x1||2\n||x2||2\n||x3||2\n||x4||2\n||x5||2\n||x6||2\n||x7||2\n(b) Improved TARFlow w/ clipping.\nFigure 8.\nComparison of intermediate states’ norm during\nTARFlow training between training with and without clipping.\nLeft: without clipping, the norms at different blocks diverge signif-\nicantly, and continue to increase as training proceeds. Right: with\nclipping, the norms are well controlled within a reasonable range,\nstabilizing training and improving final FID scores.\nLinear\nConst\nµ, α\nz\nµ, α\nz\nOnline\n6.83(2.8)\n6.82(2.8)\n7.26(1.3)\n7.24(1.3)\nOffline\n22.14(1.2)\n22.03(1.2)\n18.23(1.0)\n18.11(1.0)\nTable 11. Improved TARFlow CFG ablation. Online guidance\nsubstantially outperforms the offline variants, whereas the choice\nof guidance schedule (linear vs. constant) and the level at which\nguidance is applied (µ, α vs. z) has only minor impact. Numbers\nin gray parentheses denote the corresponding optimal CFG scale.\n(Settings: improved TARFlow-B/2, FID w/o CFG: 44.46.)\nwith a linearly increasing guidance schedule wt =\nt\nT −1w\nalong the token dimension. Here, subscript t denotes the\ntoken dimension, superscript i denotes the block dimension,\nand (· | c) and (·) represent the conditional and uncondi-\ntional counterparts, respectively.\nFollowing prior CFG studies in diffusion models, we\ndecompose the design space into three orthogonal choices:\nSchedule. We can replace the original linearly increasing\nwt with a constant guidance scale: wt = w.\nSpace for applying guidance.\nParameter-space CFG\n(µ, α) vs. pixel-space CFG applied directly to z:\nzi\nt,cfg = (1 + wt)zi\nt(· | c) −wtzi\nt(·).\nWe denote these two settings by “µ, α” and “z”, respectively.\n12", "clean_text": "feature model FID, w/o CFG FID, w/ CFG none 31.88 6.79 VGG + ResNet 9.69 4.34 VGG + DINO 9.33 4.36 ConvNeXt + DINO 3.19 3.19 VGG + ConvNeXt 2.46 2.46 (a) Feature model ablation. wVGG wConvNeXt FID, w/o CFG FID, w/ CFG 0.0 0.0 31.88 6.79 1.0 0.0 16.97 5.31 0.0 0.4 2.62 2.62 1.0 0.4 2.46 2.46 (b) VGG and ConvNeXt weight ablation. Table 9. Ablation on perceptual loss for BiFlow-B/2. FID-50K with/without CFG are reported. (Settings: BiFlow-B/2, 160 epochs, training-time CFG, weight for two perceptual losses are 1.0 and 0.4 by default.) adaptive weight p FID, w/o CFG FID, w/ CFG 0.0 38.23 7.49 0.5 34.74 7.08 1.0 34.43 6.70 2.0 31.88 6.79 Table 10. Ablation on adaptive weighting. Adaptive weighted MSE loss works better than naive MSE (p = 0.0). (Settings: BiFlowB/2, 160 epochs, adaptive weighted MSE only, training-time CFG.) C.4. Improved TARFlow Norm Control To mitigate the imbalance in loss magnitudes across different blocks of BiFlow, we introduce a simple yet effective modification to the original TARFlow [65] in Sec. 4.4: clipping the parameters µ and α within a fixed range. This adjustment stabilizes training and improves final FID performance. In Fig. 8, we visualize the norms of intermediate trajectory states during training of the improved TARFlow. Without clipping, the norms across blocks diverge sharply and continue to grow as training progresses. With clipping, the norms remain stable and well-controlled within a reasonable range. Such normalization substantially benefits the training of the reverse model in the downstream. C.5. Improved TARFlow CFG For completeness, we also examine classifier-free guidance (CFG) designs for TARFlow, although this component is orthogonal to our main contributions. In the original TARFlow [65], the reverse update rule at block i is zi t,cfg = zi+1 t ⊙exp(αi t,cfg) + µi t,cfg, where guidance is applied to the predicted parameters by αi t,cfg = (1 + wt) αi t(· | c) −wt αi t(·), µi t,cfg = (1 + wt) µi t(· | c) −wt µi t(·), 0 1 2 3 4 5 Training Steps (×106) 0 10 20 30 40 50 60 Squared L2 Norm ||x1||2 ||x2||2 ||x3||2 ||x4||2 ||x5||2 ||x6||2 ||x7||2 (a) Improved TARFlow w/o clipping. 0 1 2 3 4 5 Training Steps (×106) 0.2 0.4 0.6 0.8 Squared L2 Norm ||x1||2 ||x2||2 ||x3||2 ||x4||2 ||x5||2 ||x6||2 ||x7||2 (b) Improved TARFlow w/ clipping. Figure 8. Comparison of intermediate states’ norm during TARFlow training between training with and without clipping. Left: without clipping, the norms at different blocks diverge significantly, and continue to increase as training proceeds. Right: with clipping, the norms are well controlled within a reasonable range, stabilizing training and improving final FID scores. Linear Const µ, α z µ, α z Online 6.83(2.8) 6.82(2.8) 7.26(1.3) 7.24(1.3) Offline 22.14(1.2) 22.03(1.2) 18.23(1.0) 18.11(1.0) Table 11. Improved TARFlow CFG ablation. Online guidance substantially outperforms the offline variants, whereas the choice of guidance schedule (linear vs. constant) and the level at which guidance is applied (µ, α vs. z) has only minor impact. Numbers in gray parentheses denote the corresponding optimal CFG scale. (Settings: improved TARFlow-B/2, FID w/o CFG: 44.46.) with a linearly increasing guidance schedule wt = t T −1w along the token dimension. Here, subscript t denotes the token dimension, superscript i denotes the block dimension, and (· | c) and (·) represent the conditional and unconditional counterparts, respectively. Following prior CFG studies in diffusion models, we decompose the design space into three orthogonal choices: Schedule. We can replace the original linearly increasing wt with a constant guidance scale: wt = w. Space for applying guidance. Parameter-space CFG (µ, α) vs. pixel-space CFG applied directly to z: zi t,cfg = (1 + wt)zi t(· | c) −wtzi t(·). We denote these two settings by “µ, α” and “z”, respectively. 12"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 13, "text": "Figure 9. Inpainting. BiFlow enables efficient image inpaint-\ning by leveraging its bidirectional mapping between images and\nnoise. By resampling the masked part of the noise, BiFlow can\nperform training-free inpainting on various image masks. Each\ntriplet contains ground-truth image (left), masked image (middle),\nand reconstructed image (right).\nOnline vs. Offline. We distinguish between online and\noffline CFG strategies. The online approach (TARFlow’s\npractice) applies guidance at each generation step; the offline\napproach generates the entire conditional and unconditional\nsequences independently and performs extrapolation only\nonce on the final outputs. The difference lies only in how\nguidance interacts with intermediate states.\nWhile both approaches have similar runtimes, Tab. 11\nshows that online CFG significantly outperforms the offline\nvariant. Regarding other hyperparameters, a linear schedule\noffers a slight advantage over a constant one, while applying\nguidance in parameter space versus pixel space yields similar\nperformance. Overall, TARFlow’s original CFG formulation\nis close to optimal.\nBased on these results, we use the original TARFlow CFG\nformulation (Online, Linear, Parameter-space) as our base-\nline. It is important to note that this CFG setting only affects\nthe inference quality of the forward model; the training of\nour BiFlow reverse model always relies on the unguided\nforward trajectory.\nD. Training-free Image Editing\nBiFlow naturally supports several training-free image\nediting applications by explicitly modeling a bidirectional\nmapping between the data and noise domains. We showcase\ntwo representative applications: inpainting and class editing.\nFor brevity, we omit the VAE encoder/decoder in the follow-\ning descriptions, as they always serve as pre-/post-processing\nsteps in our experiments.\nInpainting. The forward model Fθ encodes an image x\ninto noise z = Fθ(x). We empirically observe that localized\nperturbations in z predominantly affect corresponding spatial\nregions in the reconstructed image.\negyptian cat\n−→\nkit fox\nhen\n−→\nflamingo\ntennis ball\n−→\ngolden retriever\ndaisy\n−→\ncustard apple\nFigure 10. Class Editing. BiFlow constructs an explicit bidirec-\ntional mapping between images and noise. With this property,\nBiFlow is able to conduct training-free class editing by modifying\nonly the label condition in the forward and reverse process.\nBased on this property, BiFlow enables inpainting with\nan arbitrary binary mask M ∈{0, 1}H×W . Given a masked\nimage xmask = M ⊙x, we first map it to the noise domain\nusing the forward model: zmask = Fθ(xmask). We then\nresample the masked portion of the prior as\nz′ = M ⊙zmask + (1 −M) ⊙ϵ,\nϵ ∼N(0, I).\nFinally, the modified noise z′ is mapped back to the im-\nage domain by the reverse model Gϕ. This procedure fills\nthe masked region with content coherent with the context.\nRepresentative examples are shown in Fig. 9.\nClass Editing. The reverse model Gϕ allows us to generate\nimages from noise z under different class conditions. For\na fixed z, changing the class label c primarily modifies the\nclass-dependent appearance while largely preserving the\nglobal spatial structure.\nConcretely, given an image x with label c, we obtain its\nprior variable z = Fθ(x | c), and reconstruct it using a\ndifferent label c′, writing x′ = Gϕ(z | c′). As illustrated\nin Fig. 10, BiFlow effectively alters class-specific attributes\nwhile maintaining the overall structure, enabling intuitive\nclass editing without retraining.\nEfficiency. Both inpainting and class editing require only a\nsingle forward pass from data to noise and a single reverse\npass from noise to data, making BiFlow a lightweight and\nefficient tool for training-free image manipulation.\nE. Visualizations\nWe provide uncurated 1-NFE generation results of\nBiFlow-B/2 in Fig. 11 to Fig. 13.\nAcknowledgements. We greatly thank Google TPU Re-\nsearch Cloud (TRC) for granting us access to TPUs. Q. Sun,\nX. Wang, Z. Jiang, and H. Zhao are supported by the MIT\nUndergraduate Research Opportunities Program (UROP).\nWe thank Zhengyang Geng, Tianhong Li, and our other\ngroup members for their helpful discussions and feedback\non the draft.\n13", "clean_text": "Figure 9. Inpainting. BiFlow enables efficient image inpainting by leveraging its bidirectional mapping between images and noise. By resampling the masked part of the noise, BiFlow can perform training-free inpainting on various image masks. Each triplet contains ground-truth image (left), masked image (middle), and reconstructed image (right). Online vs. Offline. We distinguish between online and offline CFG strategies. The online approach (TARFlow’s practice) applies guidance at each generation step; the offline approach generates the entire conditional and unconditional sequences independently and performs extrapolation only once on the final outputs. The difference lies only in how guidance interacts with intermediate states. While both approaches have similar runtimes, Tab. 11 shows that online CFG significantly outperforms the offline variant. Regarding other hyperparameters, a linear schedule offers a slight advantage over a constant one, while applying guidance in parameter space versus pixel space yields similar performance. Overall, TARFlow’s original CFG formulation is close to optimal. Based on these results, we use the original TARFlow CFG formulation (Online, Linear, Parameter-space) as our baseline. It is important to note that this CFG setting only affects the inference quality of the forward model; the training of our BiFlow reverse model always relies on the unguided forward trajectory. D. Training-free Image Editing BiFlow naturally supports several training-free image editing applications by explicitly modeling a bidirectional mapping between the data and noise domains. We showcase two representative applications: inpainting and class editing. For brevity, we omit the VAE encoder/decoder in the following descriptions, as they always serve as pre-/post-processing steps in our experiments. Inpainting. The forward model Fθ encodes an image x into noise z = Fθ(x). We empirically observe that localized perturbations in z predominantly affect corresponding spatial regions in the reconstructed image. egyptian cat −→ kit fox hen −→ flamingo tennis ball −→ golden retriever daisy −→ custard apple Figure 10. Class Editing. BiFlow constructs an explicit bidirectional mapping between images and noise. With this property, BiFlow is able to conduct training-free class editing by modifying only the label condition in the forward and reverse process. Based on this property, BiFlow enables inpainting with an arbitrary binary mask M ∈{0, 1}H×W . Given a masked image xmask = M ⊙x, we first map it to the noise domain using the forward model: zmask = Fθ(xmask). We then resample the masked portion of the prior as z′ = M ⊙zmask + (1 −M) ⊙ϵ, ϵ ∼N(0, I). Finally, the modified noise z′ is mapped back to the image domain by the reverse model Gϕ. This procedure fills the masked region with content coherent with the context. Representative examples are shown in Fig. 9. Class Editing. The reverse model Gϕ allows us to generate images from noise z under different class conditions. For a fixed z, changing the class label c primarily modifies the class-dependent appearance while largely preserving the global spatial structure. Concretely, given an image x with label c, we obtain its prior variable z = Fθ(x | c), and reconstruct it using a different label c′, writing x′ = Gϕ(z | c′). As illustrated in Fig. 10, BiFlow effectively alters class-specific attributes while maintaining the overall structure, enabling intuitive class editing without retraining. Efficiency. Both inpainting and class editing require only a single forward pass from data to noise and a single reverse pass from noise to data, making BiFlow a lightweight and efficient tool for training-free image manipulation. E. Visualizations We provide uncurated 1-NFE generation results of BiFlow-B/2 in Fig. 11 to Fig. 13. Acknowledgements. We greatly thank Google TPU Research Cloud (TRC) for granting us access to TPUs. Q. Sun, X. Wang, Z. Jiang, and H. Zhao are supported by the MIT Undergraduate Research Opportunities Program (UROP). We thank Zhengyang Geng, Tianhong Li, and our other group members for their helpful discussions and feedback on the draft. 13"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 14, "text": "References\n[1] Michael S. Albergo and Eric Vanden-Eijnden. Building nor-\nmalizing flows with stochastic interpolants. In ICLR, 2023.\n[2] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James\nJohnson, Chris Leary, Dougal Maclaurin, George Necula,\nAdam Paszke, Jake VanderPlas, Skye Wanderman-Milne,\nand Qiao Zhang.\nJAX: composable transformations of\nPython+NumPy programs, 2018.\n[3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\nscale gan training for high fidelity natural image synthesis. In\nICLR, 2018.\n[4] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T.\nFreeman. Maskgit: Masked generative image transformer. In\nCVPR, 2022.\n[5] Huayu Chen, Kai Jiang, Kaiwen Zheng, Jianfei Chen, Hang\nSu, and Jun Zhu. Visual generation without guidance. In\nICML, 2025.\n[6] Hansheng Chen, Kai Zhang, Hao Tan, Leonidas Guibas,\nGordon Wetzstein, and Sai Bi. pi-flow: Policy-based few-\nstep generation via imitation distillation.\narXiv preprint\narXiv:2510.14974, 2025.\n[7] Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and\nDavid Duvenaud. Neural ordinary differential equations. In\nNeurIPS, 2018.\n[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\nFei-Fei. Imagenet: A large-scale hierarchical image database.\nIn CVPR, 2009.\n[9] Prafulla Dhariwal and Alex Nichol. Diffusion models beat\ngans on image synthesis. In NeurIPS, 2021.\n[10] Laurent Dinh, David Krueger, and Yoshua Bengio. Nice:\nNon-linear independent components estimation. In ICLR\nWorkshop, 2015.\n[11] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Den-\nsity estimation using real nvp. In ICLR, 2017.\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In ICLR, 2021.\n[13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim En-\ntezari, Jonas M¨uller, Harry Saini, Yam Levi, Dominik Lorenz,\nAxel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn,\nZion English, Kyle Lacey, Alex Goodwin, Yannik Marek,\nand Robin Rombach. Scaling rectified flow transformers for\nhigh-resolution image synthesis. In ICML, 2024.\n[14] Chris Finlay, J¨orn-Henrik Jacobsen, Levon Nurbekyan, and\nAdam M Oberman. How to train your neural ode: the world\nof jacobian and kinetic regularization. In ICML, 2020.\n[15] Kevin Frans, Danijar Hafner, Sergey Levine, and Pieter\nAbbeel. One step diffusion via shortcut models. In ICLR,\n2024.\n[16] Zhengyang Geng, Ashwini Pokle, Weijian Luo, Justin Lin,\nand J. Zico Kolter. Consistency models made easy. In ICLR,\n2024.\n[17] Zhengyang Geng, Mingyang Deng, Xingjian Bai, J. Zico\nKolter, and Kaiming He. Mean flows for one-step generative\nmodeling. In NeurIPS, 2025.\n[18] Zhengyang Geng, Yiyang Lu, Zongze Wu, Eli Shechtman,\nJ. Zico Kolter, and Kaiming He. Improved mean flows: On the\nchallenges of fastforward generative models. arXiv preprint\narXiv:2512.02012, 2025.\n[19] Arnab Ghosh, Harkirat Singh Behl, Emilien Dupont, Philip\nH. S. Torr, and Vinay Namboodiri. Steer: Simple temporal\nregularization for neural ode. In NeurIPS, 2020.\n[20] Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, Ilya\nSutskever, and David Duvenaud. Ffjord: Free-form continu-\nous dynamics for scalable reversible generative models. In\nICLR, 2018.\n[21] Jiatao Gu, Tianrong Chen, David Berthelot, Huangjie Zheng,\nYuyang Wang, Ruixiang Zhang, Laurent Dinh, Miguel An-\ngel Bautista, Josh Susskind, and Shuangfei Zhai. Starflow:\nScaling latent normalizing flows for high-resolution image\nsynthesis. In NeurIPS, 2025.\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR, 2016.\n[23] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-\nhard Nessler, and Sepp Hochreiter. Gans trained by a two\ntime-scale update rule converge to a local nash equilibrium.\nIn NeurIPS, 2017.\n[24] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. In NeurIPS Workshop, 2021.\n[25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In NeurIPS, 2020.\n[26] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli\nShechtman, Sylvain Paris, and Taesung Park. Scaling up gans\nfor text-to-image synthesis. In CVPR, 2023.\n[27] Tero Karras, Miika Aittala, Samuli Laine, Erik H¨ark¨onen,\nJanne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free\ngenerative adversarial networks. In NeurIPS, 2021.\n[28] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In ICLR, 2015.\n[29] Diederik P. Kingma and Prafulla Dhariwal. Glow: Generative\nflow with invertible 1x1 convolutions. In NeurIPS, 2018.\n[30] Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi\nChen, Ilya Sutskever, and Max Welling. Improved variational\ninference with inverse autoregressive flow. In NeurIPS, 2016.\n[31] Tuomas Kynk¨a¨anniemi, Miika Aittala, Tero Karras, Samuli\nLaine, Timo Aila, and Jaakko Lehtinen. Applying guidance\nin a limited interval improves sample and distribution quality\nin diffusion models. In NeurIPS, 2024.\n[32] Kyungmin Lee, Sihyun Yu, and Jinwoo Shin. Decoupled\nmeanflow: Turning flow models into flow maps for acceler-\nated sampling. arXiv preprint arXiv:2510.24474, 2025.\n[33] Tianhong Li and Kaiming He. Back to basics: Let denoising\ngenerative models denoise. arXiv preprint arXiv:2511.13720,\n2025.\n[34] Tianhong Li, Dina Katabi, and Kaiming He. Return of uncon-\nditional generation: A self-supervised representation genera-\ntion method. In NeurIPS, 2024.\n[35] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and\nKaiming He. Autoregressive image generation without vector\nquantization. In NeurIPS, 2024.\n14", "clean_text": "References [1] Michael S. Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In ICLR, 2023. [2] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. [3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. In ICLR, 2018. [4] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. Maskgit: Masked generative image transformer. In CVPR, 2022. [5] Huayu Chen, Kai Jiang, Kaiwen Zheng, Jianfei Chen, Hang Su, and Jun Zhu. Visual generation without guidance. In ICML, 2025. [6] Hansheng Chen, Kai Zhang, Hao Tan, Leonidas Guibas, Gordon Wetzstein, and Sai Bi. pi-flow: Policy-based fewstep generation via imitation distillation. arXiv preprint arXiv:2510.14974, 2025. [7] Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. In NeurIPS, 2018. [8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. [9] Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, 2021. [10] Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components estimation. In ICLR Workshop, 2015. [11] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. In ICLR, 2017. [12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. [13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M¨uller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. [14] Chris Finlay, J¨orn-Henrik Jacobsen, Levon Nurbekyan, and Adam M Oberman. How to train your neural ode: the world of jacobian and kinetic regularization. In ICML, 2020. [15] Kevin Frans, Danijar Hafner, Sergey Levine, and Pieter Abbeel. One step diffusion via shortcut models. In ICLR, 2024. [16] Zhengyang Geng, Ashwini Pokle, Weijian Luo, Justin Lin, and J. Zico Kolter. Consistency models made easy. In ICLR, 2024. [17] Zhengyang Geng, Mingyang Deng, Xingjian Bai, J. Zico Kolter, and Kaiming He. Mean flows for one-step generative modeling. In NeurIPS, 2025. [18] Zhengyang Geng, Yiyang Lu, Zongze Wu, Eli Shechtman, J. Zico Kolter, and Kaiming He. Improved mean flows: On the challenges of fastforward generative models. arXiv preprint arXiv:2512.02012, 2025. [19] Arnab Ghosh, Harkirat Singh Behl, Emilien Dupont, Philip H. S. Torr, and Vinay Namboodiri. Steer: Simple temporal regularization for neural ode. In NeurIPS, 2020. [20] Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord: Free-form continuous dynamics for scalable reversible generative models. In ICLR, 2018. [21] Jiatao Gu, Tianrong Chen, David Berthelot, Huangjie Zheng, Yuyang Wang, Ruixiang Zhang, Laurent Dinh, Miguel Angel Bautista, Josh Susskind, and Shuangfei Zhai. Starflow: Scaling latent normalizing flows for high-resolution image synthesis. In NeurIPS, 2025. [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. [23] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In NeurIPS, 2017. [24] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS Workshop, 2021. [25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. [26] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In CVPR, 2023. [27] Tero Karras, Miika Aittala, Samuli Laine, Erik H¨ark¨onen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. In NeurIPS, 2021. [28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. [29] Diederik P. Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In NeurIPS, 2018. [30] Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. In NeurIPS, 2016. [31] Tuomas Kynk¨a¨anniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in a limited interval improves sample and distribution quality in diffusion models. In NeurIPS, 2024. [32] Kyungmin Lee, Sihyun Yu, and Jinwoo Shin. Decoupled meanflow: Turning flow models into flow maps for accelerated sampling. arXiv preprint arXiv:2510.24474, 2025. [33] Tianhong Li and Kaiming He. Back to basics: Let denoising generative models denoise. arXiv preprint arXiv:2511.13720, 2025. [34] Tianhong Li, Dina Katabi, and Kaiming He. Return of unconditional generation: A self-supervised representation generation method. In NeurIPS, 2024. [35] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. In NeurIPS, 2024. 14"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 15, "text": "[36] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maxi-\nmilian Nickel, and Matt Le. Flow matching for generative\nmodeling. In ICLR, 2023.\n[37] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight\nand fast: Learning to generate and transfer data with rectified\nflow. In ICLR, 2023.\n[38] Nanye Ma, Mark Goldstein, Michael S. Albergo, Nicholas M.\nBoffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring\nflow and diffusion-based generative models with scalable\ninterpolant transformers. In ECCV, 2024.\n[39] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik P.\nKingma, Stefano Ermon, Jonathan Ho, and Tim Salimans.\nOn distillation of guided diffusion models. In CVPR, 2023.\n[40] Maxime Oquab, Timoth´ee Darcet, Th´eo Moutakanni, Huy Vo,\nMarc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel\nHaziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud\nAssran, Nicolas Ballas, Wojciech Galuba, Russell Howes,\nPo-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat,\nVasu Sharma, Gabriel Synnaeve, Hu Xu, Herv´e Jegou, Julien\nMairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski.\nDinov2: Learning robust visual features without supervision.\nTMLR, 2024.\n[41] George Papamakarios, Theo Pavlakou, and Iain Murray.\nMasked autoregressive flow for density estimation.\nIn\nNeurIPS, 2017.\n[42] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In ICCV, 2023.\n[43] Yansong Peng, Kai Zhu, Yu Liu, Pingyu Wu, Hebei Li, Xi-\naoyan Sun, and Feng Wu. Flow-anchored consistency models.\narXiv preprint arXiv:2507.03738, 2025.\n[44] Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan L Yuille,\nand Liang-Chieh Chen. Beyond next-token: Next-x prediction\nfor autoregressive visual generation. In ICCV, 2025.\n[45] Danilo Jimenez Rezende and Shakir Mohamed. Variational\ninference with normalizing flows. In ICML, 2015.\n[46] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¨orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In CVPR, 2022.\n[47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentation.\nIn MICCAI, 2015.\n[48] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\nCheung, Alec Radford, and Xi Chen. Improved techniques\nfor training gans. In NeurIPS, 2016.\n[49] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl:\nScaling stylegan to large diverse datasets. In SIGGRAPH,\n2022.\n[50] Karen Simonyan and Andrew Zisserman. Very deep convolu-\ntional networks for large-scale image recognition. In ICLR,\n2015.\n[51] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising\ndiffusion implicit models. In ICLR, 2020.\n[52] Yang Song and Prafulla Dhariwal. Improved techniques for\ntraining consistency models. In ICLR, 2023.\n[53] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo\nWen, and Yunfeng Liu. Roformer: Enhanced transformer\nwith rotary position embedding. Neurocomputing, 2024.\n[54] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the inception archi-\ntecture for computer vision. In CVPR, 2016.\n[55] Zhicong Tang, Jianmin Bao, Dong Chen, and Baining Guo.\nDiffusion models without classifier-free guidance.\narXiv\npreprint arXiv:2502.12154, 2025.\n[56] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Li-\nwei Wang. Visual autoregressive modeling: scalable image\ngeneration via next-scale prediction. In NeurIPS, 2024.\n[57] Jakub M. Tomczak and Max Welling.\nImproving varia-\ntional auto-encoders using householder flow. arXiv preprint\narXiv:1611.09630, 2016.\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017.\n[59] Shuai Wang, Zhi Tian, Weilin Huang, and Limin Wang.\nDdt:\nDecoupled diffusion transformer.\narXiv preprint\narXiv:2504.05741, 2025.\n[60] Zidong Wang, Yiyuan Zhang, Xiaoyu Yue, Xiangyu Yue,\nYangguang Li, Wanli Ouyang, and Lei Bai. Transition models:\nRethinking the generative learning objective. arXiv preprint\narXiv:2509.04394, 2025.\n[61] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei\nChen, Zhuang Liu, In So Kweon, and Saining Xie. Con-\nvnext v2: Co-designing and scaling convnets with masked\nautoencoders. In CVPR, 2023.\n[62] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruc-\ntion vs. generation: Taming optimization dilemma in latent\ndiffusion models. In CVPR, 2025.\n[63] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-\nChieh Chen. Randomized autoregressive visual generation.\nIn ICCV, 2025.\n[64] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong,\nJonathan Huang, Jinwoo Shin, and Saining Xie. Representa-\ntion alignment for generation: Training diffusion transformers\nis easier than you think. In ICLR, 2025.\n[65] Shuangfei Zhai, Ruixiang Zhang, Preetum Nakkiran, David\nBerthelot, Jiatao Gu, Huangjie Zheng, Tianrong Chen,\nMiguel Angel Bautista, Navdeep Jaitly, and Joshua Susskind.\nNormalizing flows are capable generative models. In ICML,\n2024.\n[66] Biao Zhang and Rico Sennrich. Root mean square layer\nnormalization. In NeurIPS, 2019.\n[67] Huijie Zhang, Aliaksandr Siarohin, Willi Menapace, Michael\nVasilkovsky, Sergey Tulyakov, Qing Qu, and Ivan Sko-\nrokhodov. Alphaflow: Understanding and improving mean-\nflow models. arXiv preprint arXiv:2510.20771, 2025.\n[68] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR, 2018.\n[69] Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining\nXie. Diffusion transformers with representation autoencoders.\narXiv preprint arXiv:2510.11690, 2025.\n[70] Linqi Zhou, Stefano Ermon, and Jiaming Song. Inductive\nmoment matching. In ICML, 2025.\n15", "clean_text": "[36] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In ICLR, 2023. [37] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR, 2023. [38] Nanye Ma, Mark Goldstein, Michael S. Albergo, Nicholas M. Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In ECCV, 2024. [39] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik P. Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In CVPR, 2023. [40] Maxime Oquab, Timoth´ee Darcet, Th´eo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv´e Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision. TMLR, 2024. [41] George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for density estimation. In NeurIPS, 2017. [42] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. [43] Yansong Peng, Kai Zhu, Yu Liu, Pingyu Wu, Hebei Li, Xiaoyan Sun, and Feng Wu. Flow-anchored consistency models. arXiv preprint arXiv:2507.03738, 2025. [44] Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan L Yuille, and Liang-Chieh Chen. Beyond next-token: Next-x prediction for autoregressive visual generation. In ICCV, 2025. [45] Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. In ICML, 2015. [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. [47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, 2015. [48] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In NeurIPS, 2016. [49] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In SIGGRAPH, 2022. [50] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. [51] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2020. [52] Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. In ICLR, 2023. [53] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 2024. [54] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In CVPR, 2016. [55] Zhicong Tang, Jianmin Bao, Dong Chen, and Baining Guo. Diffusion models without classifier-free guidance. arXiv preprint arXiv:2502.12154, 2025. [56] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: scalable image generation via next-scale prediction. In NeurIPS, 2024. [57] Jakub M. Tomczak and Max Welling. Improving variational auto-encoders using householder flow. arXiv preprint arXiv:1611.09630, 2016. [58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. [59] Shuai Wang, Zhi Tian, Weilin Huang, and Limin Wang. Ddt: Decoupled diffusion transformer. arXiv preprint arXiv:2504.05741, 2025. [60] Zidong Wang, Yiyuan Zhang, Xiaoyu Yue, Xiangyu Yue, Yangguang Li, Wanli Ouyang, and Lei Bai. Transition models: Rethinking the generative learning objective. arXiv preprint arXiv:2509.04394, 2025. [61] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Saining Xie. Convnext v2: Co-designing and scaling convnets with masked autoencoders. In CVPR, 2023. [62] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In CVPR, 2025. [63] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and LiangChieh Chen. Randomized autoregressive visual generation. In ICCV, 2025. [64] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In ICLR, 2025. [65] Shuangfei Zhai, Ruixiang Zhang, Preetum Nakkiran, David Berthelot, Jiatao Gu, Huangjie Zheng, Tianrong Chen, Miguel Angel Bautista, Navdeep Jaitly, and Joshua Susskind. Normalizing flows are capable generative models. In ICML, 2024. [66] Biao Zhang and Rico Sennrich. Root mean square layer normalization. In NeurIPS, 2019. [67] Huijie Zhang, Aliaksandr Siarohin, Willi Menapace, Michael Vasilkovsky, Sergey Tulyakov, Qing Qu, and Ivan Skorokhodov. Alphaflow: Understanding and improving meanflow models. arXiv preprint arXiv:2510.20771, 2025. [68] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, 2018. [69] Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining Xie. Diffusion transformers with representation autoencoders. arXiv preprint arXiv:2510.11690, 2025. [70] Linqi Zhou, Stefano Ermon, and Jiaming Song. Inductive moment matching. In ICML, 2025. 15"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 16, "text": "class 12: house finch, linnet, Carpodacus mexicanus\nclass 81: ptarmigan\nclass 207: golden retriever\nclass 279: Arctic fox, white fox, Alopex lagopus\nclass 309: bee\nclass 323: monarch, monarch butterfly, milkweed butterfly, Danaus plexippus\nclass 327: starfish, sea star\nclass 417: balloon\nFigure 11. Uncurated 1-NFE class-conditional generation samples of BiFlow-B/2 on ImageNet 256×256. CFG scale: 2.0\n16", "clean_text": "class 12: house finch, linnet, Carpodacus mexicanus class 81: ptarmigan class 207: golden retriever class 279: Arctic fox, white fox, Alopex lagopus class 309: bee class 323: monarch, monarch butterfly, milkweed butterfly, Danaus plexippus class 327: starfish, sea star class 417: balloon Figure 11. Uncurated 1-NFE class-conditional generation samples of BiFlow-B/2 on ImageNet 256×256. CFG scale: 2.0 16"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 17, "text": "class 425: barn\nclass 437: beacon, lighthouse, beacon light, pharos\nclass 449: boathouse\nclass 497: church, church building\nclass 538: dome\nclass 554: fireboat\nclass 562: fountain\nclass 616: knot\nFigure 12. Uncurated 1-NFE class-conditional generation samples of BiFlow-B/2 on ImageNet 256×256. CFG scale: 2.0\n17", "clean_text": "class 425: barn class 437: beacon, lighthouse, beacon light, pharos class 449: boathouse class 497: church, church building class 538: dome class 554: fireboat class 562: fountain class 616: knot Figure 12. Uncurated 1-NFE class-conditional generation samples of BiFlow-B/2 on ImageNet 256×256. CFG scale: 2.0 17"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 18, "text": "class 646: maze, labyrinth\nclass 649: megalith, megalithic structure\nclass 888: viaduct\nclass 952: fig\nclass 970: alp\nclass 973: coral reef\nclass 975: lakeside, lakeshore\nclass 985: daisy\nFigure 13. Uncurated 1-NFE class-conditional generation samples of BiFlow-B/2 on ImageNet 256×256. CFG scale: 2.0\n18", "clean_text": "class 646: maze, labyrinth class 649: megalith, megalithic structure class 888: viaduct class 952: fig class 970: alp class 973: coral reef class 975: lakeside, lakeshore class 985: daisy Figure 13. Uncurated 1-NFE class-conditional generation samples of BiFlow-B/2 on ImageNet 256×256. CFG scale: 2.0 18"}
