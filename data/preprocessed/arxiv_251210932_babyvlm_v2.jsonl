{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 1, "text": "BabyVLM-V2: Toward Developmentally Grounded Pretraining and\nBenchmarking of Vision Foundation Models\nShengao Wang*†, Wenqi Wang∗, Zecheng Wang∗, Max Whitton∗\nMichael Wakeham, Arjun Chandra, Joey Huang, Pengyue Zhu\nHelen Chen‡, David Li‡, Jeffrey Li‡, Shawn L. Li‡, Andrew Zagula‡, Amy Zhao‡, Andrew Zhu‡\nSayaka Nakamura2, Yuki Yamamoto2, Jerry Jun Yokono2\nAaron Mueller, Bryan A. Plummer, Kate Saenko, Venkatesh Saligrama, Boqing Gong\nBoston University,\n2Sony Group Corporation, {wsashawn,wqwang,vicwang0,maxwh,bgong}@bu.edu\nhttps://shawnking98.github.io/BabyVLM-v2/\nAbstract\nEarly children’s developmental trajectories set up a natu-\nral goal for sample-efficient pretraining of vision founda-\ntion models. We introduce BabyVLM-V2, a developmentally\ngrounded framework for infant-inspired vision-language\nmodeling that extensively improves upon BabyVLM-V1\nthrough a longitudinal, multifaceted pretraining set, a\nversatile model, and, most importantly, DevCV Tool-\nbox for cognitive evaluation.\nThe pretraining set maxi-\nmizes coverage while minimizing curation of a longitudinal,\ninfant-centric audiovisual corpus, yielding video-utterance,\nimage-utterance, and multi-turn conversational data that\nmirror infant experiences.\nDevCV Toolbox adapts all\nvision-related measures of the recently released NIH Baby\nToolbox® into a benchmark suite of ten multimodal tasks,\ncovering spatial reasoning, memory, and vocabulary un-\nderstanding aligned with early children’s capabilities. Ex-\nperimental results show that a compact model pretrained\nfrom scratch can achieve competitive performance on De-\nvCV Toolbox, outperforming GPT-4o on some tasks. We\nhope the principled, unified BabyVLM-V2 framework will\naccelerate research in developmentally plausible pretrain-\ning of vision foundation models.\n1. Introduction\nWe formalize our objective: Given a longitudinal, infant-\ncentric audiovisual sample of early children’s sensory expe-\nriences (e.g., Figure 1a), can we learn a foundation model\n*Equal contribution.\n†Project lead.\n‡Equal contribution; work done as interns at Boston University.\nFigure 1. BabyVLM-V2: An extensive, versatile, and develop-\nmentally plausible framework for research in vision foundation\nmodels. Its (a) pretraining set is diverse in format (video, image-\nutterance, and multiple turns), enabling (b) a flexible model. Its (c)\nbenchmark developmentally aligns with the pretraining set’s age\nspan by grounding on the newly released NIH Baby Toolbox®.\n(FM) that is as versatile and capable as the early children’s\nperception? As a further challenge, can we leverage princi-\nples of developmental psychology to create a benchmark as\nan initial step toward artificial developmental intelligence\n(ADI), in both what it is and how to achieve it within the\nconstraints of early children’s limited sensory intake? We\nconsider a resultant model and benchmark developmentally\nplausible if the training data and desired model performance\nclosely mirror those of early children.\nWe\nenvision\nthat\nour\nanswer\nto\nthis\nobjective,\nBabyVLM-V2, will have a threefold impact.\nFirst, by\nmaking the limited training data accessible to independent\nresearchers and friendly to university resources, we will\nbroaden research engagement in developing FMs [18, 59]\nin a time when the scaling law [23] causes research on FMs\n1\narXiv:2512.10932v1  [cs.CV]  11 Dec 2025", "clean_text": "BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models Shengao Wang*†, Wenqi Wang∗, Zecheng Wang∗, Max Whitton∗ Michael Wakeham, Arjun Chandra, Joey Huang, Pengyue Zhu Helen Chen‡, David Li‡, Jeffrey Li‡, Shawn L. Li‡, Andrew Zagula‡, Amy Zhao‡, Andrew Zhu‡ Sayaka Nakamura2, Yuki Yamamoto2, Jerry Jun Yokono2 Aaron Mueller, Bryan A. Plummer, Kate Saenko, Venkatesh Saligrama, Boqing Gong Boston University, 2Sony Group Corporation, {wsashawn,wqwang,vicwang0,maxwh,bgong}@bu.edu https://shawnking98.github.io/BabyVLM-v2/ Abstract Early children’s developmental trajectories set up a natural goal for sample-efficient pretraining of vision foundation models. We introduce BabyVLM-V2, a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, DevCV Toolbox for cognitive evaluation. The pretraining set maximizes coverage while minimizing curation of a longitudinal, infant-centric audiovisual corpus, yielding video-utterance, image-utterance, and multi-turn conversational data that mirror infant experiences. DevCV Toolbox adapts all vision-related measures of the recently released NIH Baby Toolbox® into a benchmark suite of ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children’s capabilities. Experimental results show that a compact model pretrained from scratch can achieve competitive performance on DevCV Toolbox, outperforming GPT-4o on some tasks. We hope the principled, unified BabyVLM-V2 framework will accelerate research in developmentally plausible pretraining of vision foundation models. 1. Introduction We formalize our objective: Given a longitudinal, infantcentric audiovisual sample of early children’s sensory experiences (e.g., Figure 1a), can we learn a foundation model *Equal contribution. †Project lead. ‡Equal contribution; work done as interns at Boston University. Figure 1. BabyVLM-V2: An extensive, versatile, and developmentally plausible framework for research in vision foundation models. Its (a) pretraining set is diverse in format (video, imageutterance, and multiple turns), enabling (b) a flexible model. Its (c) benchmark developmentally aligns with the pretraining set’s age span by grounding on the newly released NIH Baby Toolbox®. (FM) that is as versatile and capable as the early children’s perception? As a further challenge, can we leverage principles of developmental psychology to create a benchmark as an initial step toward artificial developmental intelligence (ADI), in both what it is and how to achieve it within the constraints of early children’s limited sensory intake? We consider a resultant model and benchmark developmentally plausible if the training data and desired model performance closely mirror those of early children. We envision that our answer to this objective, BabyVLM-V2, will have a threefold impact. First, by making the limited training data accessible to independent researchers and friendly to university resources, we will broaden research engagement in developing FMs [18, 59] in a time when the scaling law [23] causes research on FMs 1 arXiv:2512.10932v1 [cs.CV] 11 Dec 2025"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 2, "text": "Table 1. BabyVLM-V2 extensively extends BabyVLM-V1 [56].\nBabyVLM-V1\nBabyVLM-V2 (Ours)\nPretraining\n67k img-utterance\n768k img-utterance\n181k video-utterance\n63k interleaved\nInstruction\nNone\n150k examples\nBenchmarks\n4 tasks, intuitive\n10 tasks, grounded on\nNIH Baby Toolbox®\nVisual vocabulary,\ncaptioning\nVisual\nvocabulary,\ncounting,\nmemory,\nattention,\nspatial\nrea-\nsoning,\nlocalization,\nspatiotemporal\nreason-\ning, executive function\nModels\nInput: text, single\nimg\nInput: text, img, multi-\nimg, video, multi-turn\nOutput: logits\nOutput: language\nto be dominated by industry. Second, we envision that ADI\ncould advance studies in cognitive science and psychology\nby allowing scientists to read into early children’s minds in\nan unprecedented way. Lastly, we believe that the broad-\nened engagement in FMs will improve public understand-\ning, trust, and safe use of FMs and AI in general.\nPreviously, Wang et al. proposed BabyVLM-V1 [56], a\nscaffold for studying ADI from the lens of vision-language\nmodels (VLMs). It consists of 1) an image-text pretrain-\ning set extracted from SAYCam’s head-mounted camera\nrecordings from three children for approximately two hours\nper week from age 6 to 32 months [50], 2) four intuitive\nand developmentally inspired benchmark tasks, and 3) a\npublic codebase for pretraining and evaluation. BabyVLM-\nV1 pretrained a baseline VLM from scratch, whose perfor-\nmance, unfortunately, fell far behind the remarkable capa-\nbilities of early children [7, 34]. Similarly, Vong et al. [54]\ntrained a CLIP-style [44] contrastive model using SAYCam,\nbut with a narrower focus on word-referent mappings rather\nthan general perception. More related work is in Section 2.\nWhile BabyVLM-V1 sets up a basic framework, it lacks\ncrucial elements. Its pretraining set only leverages about\na third of SAYCam’s recordings, causing it to cover only a\ntiny portion of the total visual intake time of a three-year-old\nsince birth [38]. It does not support instruction tuning [64],\nwhich is crucial for a pretrained model to articulate its ca-\npabilities to user instructions. Importantly, its evaluation\nbenchmarks are not based on any established psychology\ntests. Finally, the models trained in BabyVLM-V1 have\nnear-zero open-set performance, and one has to postprocess\ntheir logits for evaluation.\nThis work extends BabyVLM-V1 to a comprehen-\nsive, extensive, and developmentally plausible framework,\nBabyVLM-V2 (see Figure 1), for studying the objective\nposed at the beginning of the paper. Table 1 contrasts the\ntwo frameworks in pretraining, instruction tuning, bench-\nmarks, and baseline models. Notably, we provide DevCV\nToolbox (see Figure 3), a benchmark of ten tasks designed\nusing the NIH Baby Toolbox® [11, 16], which was publicly\nreleased in February 2025 as a “universal assessment for\ndevelopmental and pediatric communities”. We make min-\nimal changes while adapting all of its vision-related mea-\nsures to DevCV Toolbox in order to maintain developmen-\ntal fidelity.\nInterestingly, the DevCV Toolbox tasks are naturally\ndiverse in format, desiring FMs to understand individual\nvideos and images, reason across multiple images, and\nsolve a task in multiple turns. To account for these require-\nments in the pretraining data, we compile video, image-\nutterance, and multi-turn data from the longitudinal, infant-\ncentric videos in SAYCam [50]. As in BabyVLM-V1, we\ninclude a minimal curation process to bring our pretraining\ndata as close to the children’s sensory intake as possible.\nWe validate BabyVLM-V2 through extensive experi-\nments and human performance surveys. A model trained\nfrom scratch within our BabyVLM-V2 framework outper-\nforms GPT-4o in math tasks, highlighting the potential of\ndevelopmentally grounded pretraining.\n2. Related work\nVision FMs refer to general-purpose models [3] often pre-\ntrained on massive visual data [4, 37, 46, 58]. They can\ntackle many vision tasks via a unified interface, such as\nCLIP [44], ALIGN [20], BLIP [26, 27], SAMs [24, 45],\nand vision LLMs [1, 6, 28, 40]. The development of these\npowerful models hinges critically on pretraining [3, 5, 10], a\nprocess that trains a model on a large, generic dataset before\ntuning it to any downstream tasks.\nSample-efficient pretraining. While FMs have been re-\nlying on the scaling law, sample-efficient pretraining has\ngained momentum recently in the language [59] and medi-\ncal [51] domains. To the best of our knowledge, BabyVLM-\nV1 was the first of this kind in vision, and we further their\neffort with a more comprehensive and extensive framework.\nCognitively plausible benchmarking. BabyVLM-V1 [56]\ndesigns four developmentally plausible tasks, which unfor-\ntunately lack grounding on established psychological tests.\nDevBench [52] and KIVA [62] draw inspiration from kid-\noriented tests, yet they are more age-advanced than our\npretraining data. Other cognitively plausible benchmarks\nhave a narrower focus, such as Zorro [19], LRS [25], In-\nfLevel [60], CoreCognition [29], and MEWL [21], and\nModelVsBaby [48]. Table 2 summarizes the differences.\nTools assessing neurodevelopment in children.\nOur\nbenchmark\ntasks\nare\ngrounded\non\nthe\nNIH\nBaby\nToolbox® [11], a standardized tool released in February\n2025 for assessing neurodevelopment in children. It is not\n2", "clean_text": "Table 1. BabyVLM-V2 extensively extends BabyVLM-V1 [56]. BabyVLM-V1 BabyVLM-V2 (Ours) Pretraining 67k img-utterance 768k img-utterance 181k video-utterance 63k interleaved Instruction None 150k examples Benchmarks 4 tasks, intuitive 10 tasks, grounded on NIH Baby Toolbox® Visual vocabulary, captioning Visual vocabulary, counting, memory, attention, spatial reasoning, localization, spatiotemporal reasoning, executive function Models Input: text, single img Input: text, img, multiimg, video, multi-turn Output: logits Output: language to be dominated by industry. Second, we envision that ADI could advance studies in cognitive science and psychology by allowing scientists to read into early children’s minds in an unprecedented way. Lastly, we believe that the broadened engagement in FMs will improve public understanding, trust, and safe use of FMs and AI in general. Previously, Wang et al. proposed BabyVLM-V1 [56], a scaffold for studying ADI from the lens of vision-language models (VLMs). It consists of 1) an image-text pretraining set extracted from SAYCam’s head-mounted camera recordings from three children for approximately two hours per week from age 6 to 32 months [50], 2) four intuitive and developmentally inspired benchmark tasks, and 3) a public codebase for pretraining and evaluation. BabyVLMV1 pretrained a baseline VLM from scratch, whose performance, unfortunately, fell far behind the remarkable capabilities of early children [7, 34]. Similarly, Vong et al. [54] trained a CLIP-style [44] contrastive model using SAYCam, but with a narrower focus on word-referent mappings rather than general perception. More related work is in Section 2. While BabyVLM-V1 sets up a basic framework, it lacks crucial elements. Its pretraining set only leverages about a third of SAYCam’s recordings, causing it to cover only a tiny portion of the total visual intake time of a three-year-old since birth [38]. It does not support instruction tuning [64], which is crucial for a pretrained model to articulate its capabilities to user instructions. Importantly, its evaluation benchmarks are not based on any established psychology tests. Finally, the models trained in BabyVLM-V1 have near-zero open-set performance, and one has to postprocess their logits for evaluation. This work extends BabyVLM-V1 to a comprehensive, extensive, and developmentally plausible framework, BabyVLM-V2 (see Figure 1), for studying the objective posed at the beginning of the paper. Table 1 contrasts the two frameworks in pretraining, instruction tuning, benchmarks, and baseline models. Notably, we provide DevCV Toolbox (see Figure 3), a benchmark of ten tasks designed using the NIH Baby Toolbox® [11, 16], which was publicly released in February 2025 as a “universal assessment for developmental and pediatric communities”. We make minimal changes while adapting all of its vision-related measures to DevCV Toolbox in order to maintain developmental fidelity. Interestingly, the DevCV Toolbox tasks are naturally diverse in format, desiring FMs to understand individual videos and images, reason across multiple images, and solve a task in multiple turns. To account for these requirements in the pretraining data, we compile video, imageutterance, and multi-turn data from the longitudinal, infantcentric videos in SAYCam [50]. As in BabyVLM-V1, we include a minimal curation process to bring our pretraining data as close to the children’s sensory intake as possible. We validate BabyVLM-V2 through extensive experiments and human performance surveys. A model trained from scratch within our BabyVLM-V2 framework outperforms GPT-4o in math tasks, highlighting the potential of developmentally grounded pretraining. 2. Related work Vision FMs refer to general-purpose models [3] often pretrained on massive visual data [4, 37, 46, 58]. They can tackle many vision tasks via a unified interface, such as CLIP [44], ALIGN [20], BLIP [26, 27], SAMs [24, 45], and vision LLMs [1, 6, 28, 40]. The development of these powerful models hinges critically on pretraining [3, 5, 10], a process that trains a model on a large, generic dataset before tuning it to any downstream tasks. Sample-efficient pretraining. While FMs have been relying on the scaling law, sample-efficient pretraining has gained momentum recently in the language [59] and medical [51] domains. To the best of our knowledge, BabyVLMV1 was the first of this kind in vision, and we further their effort with a more comprehensive and extensive framework. Cognitively plausible benchmarking. BabyVLM-V1 [56] designs four developmentally plausible tasks, which unfortunately lack grounding on established psychological tests. DevBench [52] and KIVA [62] draw inspiration from kidoriented tests, yet they are more age-advanced than our pretraining data. Other cognitively plausible benchmarks have a narrower focus, such as Zorro [19], LRS [25], InfLevel [60], CoreCognition [29], and MEWL [21], and ModelVsBaby [48]. Table 2 summarizes the differences. Tools assessing neurodevelopment in children. Our benchmark tasks are grounded on the NIH Baby Toolbox® [11], a standardized tool released in February 2025 for assessing neurodevelopment in children. It is not 2"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 3, "text": "Table 2. Comparison of existing developmentally inspired benchmarks.\nBenchmark\nDevelopmental\nTask Diversity\nMultimodal\nTrain\nVal\nTest\nIn-Domain\nOOD\nHuman Data\nModel\nDevBench [52]\n✓\n✓\n✓\n✗\n✗\n✓\n✗\n✓\n✓\n✗\nLabeled-S [54]\n✓\n✗\n✓\n✓\n✗\n✓\n✓\n✗\n✗\n✗\nModelVsBaby [48]\n✓\n✗\n✓\n✓\n✓\n✓\n✗\n✓\n✓\n✗\nMEWL [21]\n✗\n✓\n✓\n✓\n✓\n✓\n✓\n✗\n✓\n✗\nZorro [19]\n✓\n✗\n✗\n✓\n✗\n✓\n✓\n✗\n✗\n✓\nInfLevel [60]\n✓\n✗\n✓\n✗\n✗\n✓\n✗\n✓\n✓\n✗\nLRS [25]\n✓\n✓\n✗\n✗\n✗\n✓\n✗\n✓\n✗\n✗\nCoreCognition [29]\n✓\n✓\n✓\n✗\n✗\n✓\n✗\n✓\n✓\n✗\nBabyVLM [56]\n✗\n✓\n✓\n✓\n✗\n✓\n✓\n✗\n✗\n✓\nDevCV Toolbox (Ours)\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nonly more recent but also more comprehensive and normed\nthan alternatives, such as the Bayley Scales Of Infant and\nToddler Development [2], Mullen Scales of Early Learn-\ning [9], and Battelle Developmental Inventory [39]. Be-\nsides, its design for clinical use validates its credibility over\nthe psychological tests used in research settings.\n3. BabyVLM-V2\n3.1. Data source & the pretraining set\nWe describe SAYCam, the developmental data source, fol-\nlowed by our minimal process to curate the pretraining set.\nSAYCam:\nThe developmental plausibility of our work\nhinges on the use of a visual-audio-text corpus that faith-\nfully samples what early children have seen and heard by\na certain age, which requires the corpus to be 1) longitu-\ndinal and 2) infant-centric. To accomplish this, we use the\nSAYCam dataset [50], which is accessible to all nonprofit\ninstitutes, and will include BabyView [32] in future work.\nSAYCam contains egocentric recordings from three infants\n(left of Figure 1a) taken once every week from roughly 6\nto 32 months old.\nEach recording is approximately two\nhours, and the recordings total 478 hours (see bottom of\nFigure 2 for the recorded time vs. wake and sleep time [38]).\nNotably, the utterances found in SAYCam are mostly from\ncaregivers providing simple verbal instructions and descrip-\ntions to the infants (top of Figure 2). BabyView [32] is an\nongoing effort in the same spirit as SAYCam, but at a larger\nscale and with extra gyroscope/accelerometer sensors.\nData split & the pretraining set. To maximize our use\nof the SAYCam corpus, we designate all video clips con-\ntaining speech to the pretraining split, and evenly divide the\nremaining clips into validation and test splits. Their relative\nsizes are approximately 3:1:1, respectively. We then apply\nminimal processing to facilitate model pretraining while ob-\nserving the children’s sensory intake as much as possible.\nSpecifically, we transcribe all utterances, which are almost\nall from caregivers, using Azure Speech Recognition [36].\nWe then construct three types of pretraining data.\n• Video–utterance pairs. We segment the camera record-\nings into short clips based on transcript boundaries, with\neach clip corresponding to exactly one utterance.\nWe\nthen drop the video clips shorter than 0.5 seconds or\nwith a transcript confidence score below 0.3. Further, we\ncompute video-utterance similarities using X-CLIP [33]\nand only retain the video-utterance pairs with similarities\ngreater than 0.1. This process leaves approximately 181k\nvideo clips in our pretraining set, a total of 138 (out of\n478) hours. We pad 1 second to either side of the clips.\n• Image–utterance pairs. Following BabyVLM-V1, we\nsample at 1 FPS from the video-utterance pairs and com-\npute the CLIP similarity [44] between each frame and its\nutterance. Only frames with CLIP similarities > 0.2 are\nretained, resulting in 768k image-utterance pairs in total.\n• Interleaved text and images. We create sequences of\ninterleaved images and utterance from consecutive video\nsegments, aiming to enable downstream capabilities that\ninvolve conversations. For each video segment, we pair\nthe frame with the highest CLIP similarity with its as-\nsociated utterance and use a sliding window over the re-\nsulting image-utterance pairs to construct the interleaved\nsequences. We randomly choose a window size between\n4 and 8 and employ a stride of half the window size, re-\nsulting in 63k interleaved sequences.\nUnlike BabyVLM-V1’s image-utterance pairs, the mixing\nof three pretraining data formats prepares models for di-\nverse downstream tasks, which can involve videos, multiple\nor single images, and even multi-turn conversations.\n3.2. Pretraining & fine-tuning BabyLLaVA-V2\nUsing our pretraining split, we pretrain BabyLLaVA-V2,\nwhich uses a language model (LLaMA-1.1B [53, 63]) as\na versatile interface to probe various capabilities of a visual\nencoder (ViT-L-16 [8], 300M parameters). A lightweight\nMLP connector [30] projects visual features into the lan-\nguage space. This model architecture (Figure 1b) is the\nsame BabyVLM-V1’s BabyLLaVA-Llama. We pretrain the\nentire model from scratch using the three-stage pipeline de-\nscribed in Appendix A. Finally, we fine-tune the model us-\ning a small, curated instruction set consisting of the tasks as\nin DevCV Toolbox, which we describe next.\n3", "clean_text": "Table 2. Comparison of existing developmentally inspired benchmarks. Benchmark Developmental Task Diversity Multimodal Train Val Test In-Domain OOD Human Data Model DevBench [52] ✓ ✓ ✓ ✗ ✗ ✓ ✗ ✓ ✓ ✗ Labeled-S [54] ✓ ✗ ✓ ✓ ✗ ✓ ✓ ✗ ✗ ✗ ModelVsBaby [48] ✓ ✗ ✓ ✓ ✓ ✓ ✗ ✓ ✓ ✗ MEWL [21] ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✗ ✓ ✗ Zorro [19] ✓ ✗ ✗ ✓ ✗ ✓ ✓ ✗ ✗ ✓ InfLevel [60] ✓ ✗ ✓ ✗ ✗ ✓ ✗ ✓ ✓ ✗ LRS [25] ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✓ ✗ ✗ CoreCognition [29] ✓ ✓ ✓ ✗ ✗ ✓ ✗ ✓ ✓ ✗ BabyVLM [56] ✗ ✓ ✓ ✓ ✗ ✓ ✓ ✗ ✗ ✓ DevCV Toolbox (Ours) ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ only more recent but also more comprehensive and normed than alternatives, such as the Bayley Scales Of Infant and Toddler Development [2], Mullen Scales of Early Learning [9], and Battelle Developmental Inventory [39]. Besides, its design for clinical use validates its credibility over the psychological tests used in research settings. 3. BabyVLM-V2 3.1. Data source & the pretraining set We describe SAYCam, the developmental data source, followed by our minimal process to curate the pretraining set. SAYCam: The developmental plausibility of our work hinges on the use of a visual-audio-text corpus that faithfully samples what early children have seen and heard by a certain age, which requires the corpus to be 1) longitudinal and 2) infant-centric. To accomplish this, we use the SAYCam dataset [50], which is accessible to all nonprofit institutes, and will include BabyView [32] in future work. SAYCam contains egocentric recordings from three infants (left of Figure 1a) taken once every week from roughly 6 to 32 months old. Each recording is approximately two hours, and the recordings total 478 hours (see bottom of Figure 2 for the recorded time vs. wake and sleep time [38]). Notably, the utterances found in SAYCam are mostly from caregivers providing simple verbal instructions and descriptions to the infants (top of Figure 2). BabyView [32] is an ongoing effort in the same spirit as SAYCam, but at a larger scale and with extra gyroscope/accelerometer sensors. Data split & the pretraining set. To maximize our use of the SAYCam corpus, we designate all video clips containing speech to the pretraining split, and evenly divide the remaining clips into validation and test splits. Their relative sizes are approximately 3:1:1, respectively. We then apply minimal processing to facilitate model pretraining while observing the children’s sensory intake as much as possible. Specifically, we transcribe all utterances, which are almost all from caregivers, using Azure Speech Recognition [36]. We then construct three types of pretraining data. • Video–utterance pairs. We segment the camera recordings into short clips based on transcript boundaries, with each clip corresponding to exactly one utterance. We then drop the video clips shorter than 0.5 seconds or with a transcript confidence score below 0.3. Further, we compute video-utterance similarities using X-CLIP [33] and only retain the video-utterance pairs with similarities greater than 0.1. This process leaves approximately 181k video clips in our pretraining set, a total of 138 (out of 478) hours. We pad 1 second to either side of the clips. • Image–utterance pairs. Following BabyVLM-V1, we sample at 1 FPS from the video-utterance pairs and compute the CLIP similarity [44] between each frame and its utterance. Only frames with CLIP similarities > 0.2 are retained, resulting in 768k image-utterance pairs in total. • Interleaved text and images. We create sequences of interleaved images and utterance from consecutive video segments, aiming to enable downstream capabilities that involve conversations. For each video segment, we pair the frame with the highest CLIP similarity with its associated utterance and use a sliding window over the resulting image-utterance pairs to construct the interleaved sequences. We randomly choose a window size between 4 and 8 and employ a stride of half the window size, resulting in 63k interleaved sequences. Unlike BabyVLM-V1’s image-utterance pairs, the mixing of three pretraining data formats prepares models for diverse downstream tasks, which can involve videos, multiple or single images, and even multi-turn conversations. 3.2. Pretraining & fine-tuning BabyLLaVA-V2 Using our pretraining split, we pretrain BabyLLaVA-V2, which uses a language model (LLaMA-1.1B [53, 63]) as a versatile interface to probe various capabilities of a visual encoder (ViT-L-16 [8], 300M parameters). A lightweight MLP connector [30] projects visual features into the language space. This model architecture (Figure 1b) is the same BabyVLM-V1’s BabyLLaVA-Llama. We pretrain the entire model from scratch using the three-stage pipeline described in Appendix A. Finally, we fine-tune the model using a small, curated instruction set consisting of the tasks as in DevCV Toolbox, which we describe next. 3"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 4, "text": "Time: 15:22\nUtterance: It’s really \ngood with yogurt.\nTime: 17:03\nUtterance: Want to \ntry a little piece of \nthis?\nTime: 18:46\nUtterance: How \nabout another \nblueberry?\nTime: 18:59\nUtterance: Oh that’s \na very sweet one.\nTime: 00:40\nUtterance: You won’t?\nTime: 00:45\nUtterance: Which one \nare you choosing?\nTime: 00:48\nUtterance: Hard, huh?\nTime: 00:50\nUtterance: Don’t use \nyour hand.\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\nmonth\nSleep\nWake\nRecorded\nFigure 2. Top: Video frames and utterances recorded from the infants’ view. Bottom: Recorded wake time vs. wake/sleep time for the ages\nof 6 months to 32 months in SAYCam [50].\n3.3. Age-appropriate benchmarking\nOur objective with BabyVLM-V2 is to design benchmark\ntasks that test age-appropriate visual skills given our pre-\ntraining data’s age span. However, we acknowledge that de-\nvelopmental benchmarking is an ongoing and rapidly evolv-\ning field of research. Early children’s growth rates vary\nsignificantly, and among psychologists and cognitive scien-\ntists, substantial conceptual and methodological disagree-\nments exist regarding the notion of developmental intelli-\ngence and how to properly probe, measure, and benchmark\nit [16]. How can we properly define ADI, then, given the in-\nconsistent measurement techniques in human developmen-\ntal research? To answer this, we consult with two experi-\nenced psychologists specializing in development and learn-\ning. Numerous meetings with them led us to the timely NIH\nBaby Toolbox®, over which we ground the design of our\nbenchmark, DevCV Toolbox.\n3.3.1. Background: NIH Baby Toolbox®\nIn February 2025, a multi-institutional team solicited by the\nNIH publicly released the NIH Baby Toolbox®, envision-\ning it as a standardized evaluation of neurodevelopmental\nintelligence in infants [15]. The NIH Baby Toolbox® di-\nvides developmental function into three domains: Cogni-\ntion, Motor, and Social-Emotional, where the Cognition\ndomain includes the subdomains of Language, Executive\nFunction/Memory, and Math, each consisting of some num-\nber of specific tests, known in the toolbox as measures. See\nTable 3 for a summary of these measures and Appendix B\nfor technical details.\n3.3.2. DevCV Toolbox\nIn this section, we develop a computer vision counterpart,\ncalled task for clarity, for every vision-related measure in\nthe NIH Baby Toolbox®, leading to ten tasks in our DevCV\nToolbox, which are summarized in Table 3 and illustrated\nin Figure 3.\nThe need to adapt measures to tasks. Unlike the prac-\ntice in computer vision, most of the measures originally\nfound in the NIH Baby Toolbox® 1) have only a couple of\ntest examples and 2) are human-oriented but not accessi-\nble to AI models. Additionally, the cartoon stimuli in NIH\nBaby Toolbox® are out-of-domain from our pretraining set,\npreventing their direct use. Hence, we adapt the measures\nto computer vision tasks by standardizing their format and\nequipping each task with thousands of naturalistic examples\n(see Table 3), separated into instruction and test sets accord-\ning to the split defined in the pretraining stage.\nWe construct the tasks using SAYCam to ensure that\nthe examples are in the same domain as the pretraining\ndata, thereby focusing the benchmarking on the models’\nin-domain cognitive capabilities. To provide an additional\ntool to evaluate models’ generalizability, we also compile\nan out-of-domain test set using Ego4D [14] with the same\ntechniques. Below, we detail the construction of Picture Vo-\ncabulary as a representative example, and briefly describe\nthe rest. See Appendix B for more details on the construc-\ntion of DevCV Toolbox.\nPicture vocabulary (≥25 months): The top right of Fig-\nure 3 shows the original picture vocabulary (PV) measure\nfound in the NIH Baby Toolbox®, which assesses the Recep-\ntive Language of children aged 25 months and older. Partic-\nipants are presented with four clipart images on an iPad, and\nan audio prompt instructs them to touch the named image.\nWe adapt PV to DevCV Toolbox using the pipeline in\nFigure 4, to replace the clipart in the NIH Baby Toolbox®\nmanual with objects and actions detected from SAYCam\nvideo frames. Concretely, we sample frames at 1FPS, la-\nbel all objects and actions present using manual transcripts\nand GPT-4o, and then crop out regions for each label us-\ning Grounding-DINO [31]. Low quality crops and labels\nbeyond the child-oriented MAB-CDI vocabulary [35] are\nremoved. Each PV example (e.g., the top left of Figure 3)\nconsists of a language prompt, a target image corresponding\n4", "clean_text": "Time: 15:22 Utterance: It’s really good with yogurt. Time: 17:03 Utterance: Want to try a little piece of this? Time: 18:46 Utterance: How about another blueberry? Time: 18:59 Utterance: Oh that’s a very sweet one. Time: 00:40 Utterance: You won’t? Time: 00:45 Utterance: Which one are you choosing? Time: 00:48 Utterance: Hard, huh? Time: 00:50 Utterance: Don’t use your hand. 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 month Sleep Wake Recorded Figure 2. Top: Video frames and utterances recorded from the infants’ view. Bottom: Recorded wake time vs. wake/sleep time for the ages of 6 months to 32 months in SAYCam [50]. 3.3. Age-appropriate benchmarking Our objective with BabyVLM-V2 is to design benchmark tasks that test age-appropriate visual skills given our pretraining data’s age span. However, we acknowledge that developmental benchmarking is an ongoing and rapidly evolving field of research. Early children’s growth rates vary significantly, and among psychologists and cognitive scientists, substantial conceptual and methodological disagreements exist regarding the notion of developmental intelligence and how to properly probe, measure, and benchmark it [16]. How can we properly define ADI, then, given the inconsistent measurement techniques in human developmental research? To answer this, we consult with two experienced psychologists specializing in development and learning. Numerous meetings with them led us to the timely NIH Baby Toolbox®, over which we ground the design of our benchmark, DevCV Toolbox. 3.3.1. Background: NIH Baby Toolbox® In February 2025, a multi-institutional team solicited by the NIH publicly released the NIH Baby Toolbox®, envisioning it as a standardized evaluation of neurodevelopmental intelligence in infants [15]. The NIH Baby Toolbox® divides developmental function into three domains: Cognition, Motor, and Social-Emotional, where the Cognition domain includes the subdomains of Language, Executive Function/Memory, and Math, each consisting of some number of specific tests, known in the toolbox as measures. See Table 3 for a summary of these measures and Appendix B for technical details. 3.3.2. DevCV Toolbox In this section, we develop a computer vision counterpart, called task for clarity, for every vision-related measure in the NIH Baby Toolbox®, leading to ten tasks in our DevCV Toolbox, which are summarized in Table 3 and illustrated in Figure 3. The need to adapt measures to tasks. Unlike the practice in computer vision, most of the measures originally found in the NIH Baby Toolbox® 1) have only a couple of test examples and 2) are human-oriented but not accessible to AI models. Additionally, the cartoon stimuli in NIH Baby Toolbox® are out-of-domain from our pretraining set, preventing their direct use. Hence, we adapt the measures to computer vision tasks by standardizing their format and equipping each task with thousands of naturalistic examples (see Table 3), separated into instruction and test sets according to the split defined in the pretraining stage. We construct the tasks using SAYCam to ensure that the examples are in the same domain as the pretraining data, thereby focusing the benchmarking on the models’ in-domain cognitive capabilities. To provide an additional tool to evaluate models’ generalizability, we also compile an out-of-domain test set using Ego4D [14] with the same techniques. Below, we detail the construction of Picture Vocabulary as a representative example, and briefly describe the rest. See Appendix B for more details on the construction of DevCV Toolbox. Picture vocabulary (≥25 months): The top right of Figure 3 shows the original picture vocabulary (PV) measure found in the NIH Baby Toolbox®, which assesses the Receptive Language of children aged 25 months and older. Participants are presented with four clipart images on an iPad, and an audio prompt instructs them to touch the named image. We adapt PV to DevCV Toolbox using the pipeline in Figure 4, to replace the clipart in the NIH Baby Toolbox® manual with objects and actions detected from SAYCam video frames. Concretely, we sample frames at 1FPS, label all objects and actions present using manual transcripts and GPT-4o, and then crop out regions for each label using Grounding-DINO [31]. Low quality crops and labels beyond the child-oriented MAB-CDI vocabulary [35] are removed. Each PV example (e.g., the top left of Figure 3) consists of a language prompt, a target image corresponding 4"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 5, "text": "Figure 3. DevCV Toolbox tasks and their corresponding NIH Baby Toolbox® measures\nto the prompt, and three distractor images, and we construct\nthe examples in a round-robin manner for diversity. The\ntarget and distractor images are related either semantically\nor phonologically in NIH Baby Toolbox®; therefore, we de-\nrive a distractor distribution over phonology and semantics\nfrom the toolbox and then sample distractor images accord-\ningly. We manually screen the process to ensure quality and\ndiversity. Appendix B presents more details.\nOther tasks. We describe the other tasks in DevCV Tool-\nbox briefly. Construction details are in Appendix B.\n1. Looking while listening (6–24 months) shows infants\ntwo clipart objects, and plays an audio prompt describ-\ning one of them. Eye tracking is used to detect the partic-\nipant’s response. We replace clipart with natural objects\nfrom SAYCam, and eye tracking with multiple choice.\n2. Localization / Mullen visual receptive language #19\n5", "clean_text": "Figure 3. DevCV Toolbox tasks and their corresponding NIH Baby Toolbox® measures to the prompt, and three distractor images, and we construct the examples in a round-robin manner for diversity. The target and distractor images are related either semantically or phonologically in NIH Baby Toolbox®; therefore, we derive a distractor distribution over phonology and semantics from the toolbox and then sample distractor images accordingly. We manually screen the process to ensure quality and diversity. Appendix B presents more details. Other tasks. We describe the other tasks in DevCV Toolbox briefly. Construction details are in Appendix B. 1. Looking while listening (6–24 months) shows infants two clipart objects, and plays an audio prompt describing one of them. Eye tracking is used to detect the participant’s response. We replace clipart with natural objects from SAYCam, and eye tracking with multiple choice. 2. Localization / Mullen visual receptive language #19 5"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 6, "text": "Table 3. DevCV Toolbox tasks and their corresponding NIH Baby Toolbox® measures (EF/M stands for Executive Function/Memory).\nDevCV Toolbox tasks\n#Instruct/#Test\nModel Input\nNIH Baby Toolbox® measures\nMonths\nSubdomain\nLooking While Listening\n0/1.2k\n2 images\nLooking While Listening\n6-24\nLanguage\nPicture Vocabulary\n63.9k/1.2k\n4 images\nPicture Vocabulary\n25+\nLanguage\nLocalization\n12.3k/2.1k\n1 image\nMullen Receptive Language #19\n1-42\nLanguage\nLeft/Right\n12.3k/2.3k\n4 images\nMullen Visual Reception #29\n1-42\nEF/M\nSpatial Details\n11.8k/1.2k\n4 image\nMullen Visual Reception #20\n1-42\nEF/M\nVisual Delayed Response\n5.2k/0.9k\n5-8 images\nVisual Delayed Response\n22-42\nEF/M\nMemory\n10.0k/0.5k\n29 images\nDelayed Memory\n22-42\nEF/M\nWho Has More (synthetic)\n11.2k/1.8k\n2 images\nWho Has More\n25-42\nMath\nWho Has More (naturalistic)\n6.9k/2.2k\n2 images\nWho Has More\n25-42\nMath\nSubitizing (synthetic)\n0/1.9k\n3 images\nSubitizing\n25-42\nMath\nSubitizing (naturalistic)\n0/0.2k\n3 images\nSubitizing\n25-42\nMath\nObject Counting\n13.7k/3.0k\n1 image\nObject Counting\n25-42\nMath\nFigure 4. Pipeline to adapt the picture vocabulary measure in NIH Baby Toolbox® to DevCV Toolbox.\n(1–42 months) tests an infant’s ability to point at\nsketched objects as they are named. We task a model\nwith localizing an object in a natural video frame.\n3. Left/Right / Mullen visual reception #29 (1–42\nmonths) measures an infant’s attention to detail by in-\nstructing them to match objects by orientation.\n4. Spatial details / Mullen visual reception #29 (1–42\nmonths) measures attention to detail in identical objects\namong distractors of the same type.\n5. Visual delayed response (22–42 months) shows infants\na creature moving behind one of two occluders, and after\na short pause, instructs them to tap the target occluder.\nWe use video clips with prominent objects moving out\nof the field of view.\n6. (Delayed) memory (22–42 months) involves multiple\nturns, each presenting a pair of animals. Participants\nare asked to “feed” the new animal appearing for the\nfirst time, and they receive corrective feedback during\nthe early rounds.\n7. Who has more (25–42 months) shows two images with\nthe same shape in different quantities and asks which im-\nage has more. We replace the shape with natural objects\nas one sub-task, and use entire natural video frames for\nthe other sub-task.\n8. Subitizing (25–42 months) refers to the rapid identifica-\ntion of the number of items in a small set. An infant sees\none to four identical shapes for one second, and then an\naudio prompt requests the count.\n9. Object counting (25–42 months) evaluates a child’s\nability to count up to 12 colored shapes on a screen.\nDuring evaluation, we employ accuracy as the metric.\nThese tasks cover all cognitive measures in NIH Baby\nToolbox® except the non-visual MacArthur-Bates lan-\nguage (9–30 months, 7–18 months), familiarization (6–21\nmonths), verbal counting (25–42 months), and verbal arith-\nmetic (37–42 months). Adult performance data on these\ntasks confirms the validity of our DevCV Toolbox (see Hu-\nman performance in Tables 4 and Appendix C for details).\nIn future work, we hope to complete a survey of children’s\nperformance.\n4. Experiments\nWe\ndesign\nexperiments\nabout\nthe\nkey\nelements\nof\nBabyVLM-V2 framework, aiming to validate the quality of\nthe DevCV Toolbox, as well as illustrate the effectiveness of\nour training data and training recipe. Meanwhile, the exper-\n6", "clean_text": "Table 3. DevCV Toolbox tasks and their corresponding NIH Baby Toolbox® measures (EF/M stands for Executive Function/Memory). DevCV Toolbox tasks #Instruct/#Test Model Input NIH Baby Toolbox® measures Months Subdomain Looking While Listening 0/1.2k 2 images Looking While Listening 6-24 Language Picture Vocabulary 63.9k/1.2k 4 images Picture Vocabulary 25+ Language Localization 12.3k/2.1k 1 image Mullen Receptive Language #19 1-42 Language Left/Right 12.3k/2.3k 4 images Mullen Visual Reception #29 1-42 EF/M Spatial Details 11.8k/1.2k 4 image Mullen Visual Reception #20 1-42 EF/M Visual Delayed Response 5.2k/0.9k 5-8 images Visual Delayed Response 22-42 EF/M Memory 10.0k/0.5k 29 images Delayed Memory 22-42 EF/M Who Has More (synthetic) 11.2k/1.8k 2 images Who Has More 25-42 Math Who Has More (naturalistic) 6.9k/2.2k 2 images Who Has More 25-42 Math Subitizing (synthetic) 0/1.9k 3 images Subitizing 25-42 Math Subitizing (naturalistic) 0/0.2k 3 images Subitizing 25-42 Math Object Counting 13.7k/3.0k 1 image Object Counting 25-42 Math Figure 4. Pipeline to adapt the picture vocabulary measure in NIH Baby Toolbox® to DevCV Toolbox. (1–42 months) tests an infant’s ability to point at sketched objects as they are named. We task a model with localizing an object in a natural video frame. 3. Left/Right / Mullen visual reception #29 (1–42 months) measures an infant’s attention to detail by instructing them to match objects by orientation. 4. Spatial details / Mullen visual reception #29 (1–42 months) measures attention to detail in identical objects among distractors of the same type. 5. Visual delayed response (22–42 months) shows infants a creature moving behind one of two occluders, and after a short pause, instructs them to tap the target occluder. We use video clips with prominent objects moving out of the field of view. 6. (Delayed) memory (22–42 months) involves multiple turns, each presenting a pair of animals. Participants are asked to “feed” the new animal appearing for the first time, and they receive corrective feedback during the early rounds. 7. Who has more (25–42 months) shows two images with the same shape in different quantities and asks which image has more. We replace the shape with natural objects as one sub-task, and use entire natural video frames for the other sub-task. 8. Subitizing (25–42 months) refers to the rapid identification of the number of items in a small set. An infant sees one to four identical shapes for one second, and then an audio prompt requests the count. 9. Object counting (25–42 months) evaluates a child’s ability to count up to 12 colored shapes on a screen. During evaluation, we employ accuracy as the metric. These tasks cover all cognitive measures in NIH Baby Toolbox® except the non-visual MacArthur-Bates language (9–30 months, 7–18 months), familiarization (6–21 months), verbal counting (25–42 months), and verbal arithmetic (37–42 months). Adult performance data on these tasks confirms the validity of our DevCV Toolbox (see Human performance in Tables 4 and Appendix C for details). In future work, we hope to complete a survey of children’s performance. 4. Experiments We design experiments about the key elements of BabyVLM-V2 framework, aiming to validate the quality of the DevCV Toolbox, as well as illustrate the effectiveness of our training data and training recipe. Meanwhile, the exper6"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 7, "text": "Figure 5.\nTask-specific supervised finetuning of LLaVA-\nOneVision-7B and Qwen2.5-VL-7B.\niments position our BabyLLaVA-V2 in context across three\ncognitive subdomains and ten tasks. Note that we exclude\ntwo tasks, Subitizing and Looking While Listening, from\nthe majority of the experiments to test our models’ general-\nization on unseen tasks near the end. Implementation details\nare in Appendix D.\n4.1. Examining DevCV Toolbox\nOverall quality. We validate the quality of DevCV Tool-\nbox by conducting human surveys, detailed in Appendix C.\nAs shown in Table 4, the human volunteers recruited in our\nhome institute achieved near-perfect accuracy on the the\nexecutive functioning/memory subdomain (Spatial Details,\nMemory, Visual Delayed Response) and the math tasks of\nObject Counting and Who Has More. Their accuracy on\nLocalization is slightly low (87.3%), and a follow-up re-\nvealed that it could improve when the volunteers were in-\nstructed to spend more time on the task.\nDifferentiating capability. Table 4 also demonstrates that,\nbetween Human performance and Random guess,\nthere is a sufficiently big room for differentiating various\nmodels. Indeed, the proprietary GPT and Gemini models\nare on the upper end, while our BabyLLaVA-V2 and the\nopen-source models of about the same size as ours are on\nthe lower end, indicating that the tasks in DevCV Toolbox\nare challenging but solvable.\nDevelopmental fidelity. DevCV Toolbox should develop-\nmentally align with the pretraining data’s age span (6–32\nmonths).\nHence, we are in the process of performing a\nlarge-scale children survey about DevCV Toolbox using the\nChildren Helping Science platform [49], though this survey\nwill take a couple of years per our estimation.\n4.2. Validating the instruction tuning dataset\nInstruction tuning addresses the mismatch between pre-\ntraining and downstream tasks, steering models towards the\ndownstream. To validate the effectiveness of our instruction\ntuning data, we use it to supervise the fine-tuning of three\nmodels under two strategies. Figure 5 fine-tunes LLaVA-\nOneVision-7B and Qwen2.5-VL-7B on each task separately\n(see Appendix A for the experiment setup). The consistent\nand relatively big gains from the fine-tuning are highlighted\nin the red top bars, signifying that the instruction data can\neffectively guide the models to the downstream tasks in De-\nvCV Toolbox.\nFurthermore, we experiment with the second fine-tuning\nstrategy that combines the instruction data into a single set.\nTable 5 contrasts it against the first strategy, fine-tuning a\nmodel for each task separately, over our BabyLLaVA-V2.\nThe results show that the overall difference between the\ntwo strategies is marginal. The results on most tasks de-\ncrease under the mixed-tuning setting, which produces a\nsingle unified model rather than multiple per-task models,\nbut some tasks, such as Memory and Spatial Details, can ac-\ntually benefit from the mixed fine-tuning, implying knowl-\nedge transfer or regularization from other tasks.\n4.3. Ablating the pretraining data\nThe speech transcripts in our pretraining set could be noisy\nbecause the naturalistic child-directed utterances are often\nmisaligned with the children’s visual intake. We study their\nimpact on the pretrained models by replacing the transcripts\nwith video captions generated by GPT-4o (see Appendix D\nfor how we prompt GPT-4o). We train BabyLLaVA-V2-\nsynthetic on this altered pretraining dataset and present the\nresults in Table 6. Overall, the synthetic captions improve\nperformance, especially on tasks that demand semantic rea-\nsoning (Picture Vocabulary) and a long attention window\n(Memory).\nHowever, the gains are modest, suggesting\nthat our minimally curated pretraining set already provides\nstrong supervision. In future work, novel pretraining algo-\nrithms can likely mine stronger supervision from this or-\nganic pretraining set.\n4.4. Inspecting BabyLLaVA-V2\nOur BabyLLaVA-V2’s overall performance in Table 4 is en-\ncouraging, on par with the open-source models whose size\nis about the same as ours. Of course, one could argue that\nthose models are not fine-tuned under the BabyVLM-V2\nframework, but they are probably trained on much larger\ndatasets than ours.\nTo further stretch BabyLLaVA-V2, we study its gener-\nalization along two axes: 1) out-of-domain generalization\nand 2) performance over previously unseen tasks.\nOut-of-domain generalization. We have created a sibling\nof DevCV Toolbox by replacing SAYCam with Ego4D.\nBoth are about egocentric videos, but Ego4D is from the\nperspective of grown-ups.\nBabyLLaVA-V2’s overall ac-\ncuracy on this sibling benchmark is 41.1% (vs. 31.8%\nof random guess), significantly lower than its in-domain\nperformance (55.2%) on DevCV Toolbox.\nWe conclude\nthat BabyLLaVA-V2 can generalize beyond its training do-\nmain to some degree, but it is far from human infants’ re-\nmarkable generalization capabilities. Appendix D further\n7", "clean_text": "Figure 5. Task-specific supervised finetuning of LLaVAOneVision-7B and Qwen2.5-VL-7B. iments position our BabyLLaVA-V2 in context across three cognitive subdomains and ten tasks. Note that we exclude two tasks, Subitizing and Looking While Listening, from the majority of the experiments to test our models’ generalization on unseen tasks near the end. Implementation details are in Appendix D. 4.1. Examining DevCV Toolbox Overall quality. We validate the quality of DevCV Toolbox by conducting human surveys, detailed in Appendix C. As shown in Table 4, the human volunteers recruited in our home institute achieved near-perfect accuracy on the the executive functioning/memory subdomain (Spatial Details, Memory, Visual Delayed Response) and the math tasks of Object Counting and Who Has More. Their accuracy on Localization is slightly low (87.3%), and a follow-up revealed that it could improve when the volunteers were instructed to spend more time on the task. Differentiating capability. Table 4 also demonstrates that, between Human performance and Random guess, there is a sufficiently big room for differentiating various models. Indeed, the proprietary GPT and Gemini models are on the upper end, while our BabyLLaVA-V2 and the open-source models of about the same size as ours are on the lower end, indicating that the tasks in DevCV Toolbox are challenging but solvable. Developmental fidelity. DevCV Toolbox should developmentally align with the pretraining data’s age span (6–32 months). Hence, we are in the process of performing a large-scale children survey about DevCV Toolbox using the Children Helping Science platform [49], though this survey will take a couple of years per our estimation. 4.2. Validating the instruction tuning dataset Instruction tuning addresses the mismatch between pretraining and downstream tasks, steering models towards the downstream. To validate the effectiveness of our instruction tuning data, we use it to supervise the fine-tuning of three models under two strategies. Figure 5 fine-tunes LLaVAOneVision-7B and Qwen2.5-VL-7B on each task separately (see Appendix A for the experiment setup). The consistent and relatively big gains from the fine-tuning are highlighted in the red top bars, signifying that the instruction data can effectively guide the models to the downstream tasks in DevCV Toolbox. Furthermore, we experiment with the second fine-tuning strategy that combines the instruction data into a single set. Table 5 contrasts it against the first strategy, fine-tuning a model for each task separately, over our BabyLLaVA-V2. The results show that the overall difference between the two strategies is marginal. The results on most tasks decrease under the mixed-tuning setting, which produces a single unified model rather than multiple per-task models, but some tasks, such as Memory and Spatial Details, can actually benefit from the mixed fine-tuning, implying knowledge transfer or regularization from other tasks. 4.3. Ablating the pretraining data The speech transcripts in our pretraining set could be noisy because the naturalistic child-directed utterances are often misaligned with the children’s visual intake. We study their impact on the pretrained models by replacing the transcripts with video captions generated by GPT-4o (see Appendix D for how we prompt GPT-4o). We train BabyLLaVA-V2synthetic on this altered pretraining dataset and present the results in Table 6. Overall, the synthetic captions improve performance, especially on tasks that demand semantic reasoning (Picture Vocabulary) and a long attention window (Memory). However, the gains are modest, suggesting that our minimally curated pretraining set already provides strong supervision. In future work, novel pretraining algorithms can likely mine stronger supervision from this organic pretraining set. 4.4. Inspecting BabyLLaVA-V2 Our BabyLLaVA-V2’s overall performance in Table 4 is encouraging, on par with the open-source models whose size is about the same as ours. Of course, one could argue that those models are not fine-tuned under the BabyVLM-V2 framework, but they are probably trained on much larger datasets than ours. To further stretch BabyLLaVA-V2, we study its generalization along two axes: 1) out-of-domain generalization and 2) performance over previously unseen tasks. Out-of-domain generalization. We have created a sibling of DevCV Toolbox by replacing SAYCam with Ego4D. Both are about egocentric videos, but Ego4D is from the perspective of grown-ups. BabyLLaVA-V2’s overall accuracy on this sibling benchmark is 41.1% (vs. 31.8% of random guess), significantly lower than its in-domain performance (55.2%) on DevCV Toolbox. We conclude that BabyLLaVA-V2 can generalize beyond its training domain to some degree, but it is far from human infants’ remarkable generalization capabilities. Appendix D further 7"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 8, "text": "Table 4. Performance comparison of different models on DevCV Toolbox (in-domain). Different background colors denote different\nmodel families. We report accuracy (%) for all tasks; the higher, the better.\nModel\nOverall\nCount\nLeftRight\nSpatial\nPV\nMemory\nLocalization\nVisual Delay Response\nWho Has More\nbinary\nmulti-exact\nmulti-adjacent\nsynthetic\nnaturalistic\nUpper bound\nHuman performance\n93.0\n99.1\n94.5\n100\n91.8\n97.9\n87.3\n98.2\n63.6\n95.5\n98.2\n96.4\nProprietary models\nGemini-2.5-flash\n72.7\n71.1\n34.9\n73.8\n91.2\n96.9\n84.8\n75.9\n42.4\n70.3\n87.5\n70.7\nGPT-4o\n74.6\n39.0\n89.8\n92.6\n93.7\n99.7\n81.7\n64.2\n29.3\n62.9\n87.9\n79.3\nGemini-2.5-pro\n82.5\n77.2\n68.8\n90.5\n93.8\n97.8\n88.8\n86.9\n54.0\n87.7\n90.6\n71.7\nGPT-5\n87.6\n69.1\n96.0\n94.5\n95.0\n99.9\n85.2\n95.1\n62.9\n90.1\n88.9\n86.6\nOpen-source models\nLLaVA-OneVision-0.5B\n33.2\n43.5\n33.7\n28.7\n23.5\n24.0\n12.3\n58.9\n7.31\n49.2\n37.3\n46.2\nInternVL3.5-1B\n37.2\n27.9\n32.2\n34.6\n34.4\n25.8\n44.8\n64.1\n11.6\n36.8\n47.8\n49.1\nQwen2.5-VL-3B\n47.0\n29.2\n33.7\n40.0\n71.7\n36.5\n85.8\n66.7\n17.0\n32.7\n51.7\n52.3\nBaby models (Ours)\nBabyLLaVA-V2\n55.2\n44.6\n42.3\n91.3\n27.4\n75.3\n38.8\n57.6\n33.1\n45.6\n98.4\n52.8\nLower bound\nRandom guess\n31.8\n8.33\n33.3\n33.3\n25.0\n25.0\n25.0\n50.0\n12.5\n37.5\n50.0\n50.0\nTable 5. Two supervised fine-tuning strategies. BabyLLaVA-V2-separate denotes models fine-tuned on each task’s instruction dataset\nseparately, and BabyLLaVA-V2-mixed is a single model fine-tuned on the mixed instruction set.\nModel\nOverall\nCount\nLeftRight\nSpatial\nPV\nMemory\nLocalization\nVisual Delay Response\nWho Has More\nbinary\nmulti-exact\nmulti-adjacent\nsynthetic\nnaturalistic\nBabyLLaVA-V2-separate\n56.0\n45.2\n42.5\n87.1\n28.4\n70.7\n43.3\n55.7\n37.0\n49.9\n98.6\n56.4\nBabyLLaVA-V2-mixed\n55.2\n44.6\n42.3\n91.3\n27.4\n75.3\n38.8\n57.6\n33.1\n45.6\n98.4\n52.8\nFigure 6. GPT-4o and our model’s counting performance by dif-\nferent object numbers.\ntests BabyLLaVA-V2’s out-of-domain generalization on the\noriginal NIH Baby Toolbox®.\nUnseen tasks. We have excluded Looking While Listen-\ning and Subitizing from the instruction tuning, which are\nthus unseen by BabyLLaVA-V2. While the two tasks are\nin spirit similar to Picture Vocabulary and Object Counting,\nrespectively, BabyLLaVA-V2 yields near-random-guess re-\nsults on them. We will address this issue in future work by\nimproving the instruction tuning algorithm.\n4.5. Intriguing findings\nFinally, we draw some intriguing “byproduct” findings from\nTable 4, which can improve our understanding of the pro-\nprietary GPT and Gemini models.\nGPT models struggle to count. Object Counting requires\na model to count objects in an image (between 1 and 12),\nand GPT-4o can hardly count beyond 5 (see Figure 6).\nBabyLLaVA-V2 can match or outperform GPT-4o on\nsome cognitive tasks. On Spatial Details and Who Has\nMore, BabyLLaVA-V2 is on par with the four latest GPT\nand Gemini models. Moreover, it even outperforms GPT-\n4o on the math tasks of Object Counting and Who Has\nMore. Figure 6 shows that BabyLLaVA-V2 counts better\nthan GPT-4o given six or more objects.\nGPT vs. Gemini. In general, the proprietary models give\nrise to similar results on DevCV Toolbox. However, when\nwe zoom into the individual tasks, GPT-5 is significantly\nbetter than the rest on Spatial Details, while Gemini models\nare better at Object Counting than the GPT models.\n5. Conclusion\nWe introduced BabyVLM-V2, a framework that fea-\ntures a developmentally plausible pretraining set derived\nfrom the longitudinal SAYCam corpus, a compact VLM\n(BabyLLaVA-V2) trained from scratch, and comprehen-\nsive developmental benchmarks (DevCV Toolbox).\nDe-\nvCV Toolbox adapts all vision-related measures from the\nnewly published NIH Baby Toolbox®. It contains ten mea-\nsures spanning three subdomains (language, executive func-\ntion/memory, and math) and requires a flexible model inter-\nface that can process image, video, and multi-turn dialogue.\nWe demonstrate the potential of developmentally plausi-\nble vision FMs through extensive experiments on our pre-\ntraining and instruction tuning datasets, and we confirm the\nquality of DevCV Toolbox through extensive benchmark-\ning with proprietary and open-source models. This frame-\nwork will serve as a principled platform to broaden research\n8", "clean_text": "Table 4. Performance comparison of different models on DevCV Toolbox (in-domain). Different background colors denote different model families. We report accuracy (%) for all tasks; the higher, the better. Model Overall Count LeftRight Spatial PV Memory Localization Visual Delay Response Who Has More binary multi-exact multi-adjacent synthetic naturalistic Upper bound Human performance 93.0 99.1 94.5 100 91.8 97.9 87.3 98.2 63.6 95.5 98.2 96.4 Proprietary models Gemini-2.5-flash 72.7 71.1 34.9 73.8 91.2 96.9 84.8 75.9 42.4 70.3 87.5 70.7 GPT-4o 74.6 39.0 89.8 92.6 93.7 99.7 81.7 64.2 29.3 62.9 87.9 79.3 Gemini-2.5-pro 82.5 77.2 68.8 90.5 93.8 97.8 88.8 86.9 54.0 87.7 90.6 71.7 GPT-5 87.6 69.1 96.0 94.5 95.0 99.9 85.2 95.1 62.9 90.1 88.9 86.6 Open-source models LLaVA-OneVision-0.5B 33.2 43.5 33.7 28.7 23.5 24.0 12.3 58.9 7.31 49.2 37.3 46.2 InternVL3.5-1B 37.2 27.9 32.2 34.6 34.4 25.8 44.8 64.1 11.6 36.8 47.8 49.1 Qwen2.5-VL-3B 47.0 29.2 33.7 40.0 71.7 36.5 85.8 66.7 17.0 32.7 51.7 52.3 Baby models (Ours) BabyLLaVA-V2 55.2 44.6 42.3 91.3 27.4 75.3 38.8 57.6 33.1 45.6 98.4 52.8 Lower bound Random guess 31.8 8.33 33.3 33.3 25.0 25.0 25.0 50.0 12.5 37.5 50.0 50.0 Table 5. Two supervised fine-tuning strategies. BabyLLaVA-V2-separate denotes models fine-tuned on each task’s instruction dataset separately, and BabyLLaVA-V2-mixed is a single model fine-tuned on the mixed instruction set. Model Overall Count LeftRight Spatial PV Memory Localization Visual Delay Response Who Has More binary multi-exact multi-adjacent synthetic naturalistic BabyLLaVA-V2-separate 56.0 45.2 42.5 87.1 28.4 70.7 43.3 55.7 37.0 49.9 98.6 56.4 BabyLLaVA-V2-mixed 55.2 44.6 42.3 91.3 27.4 75.3 38.8 57.6 33.1 45.6 98.4 52.8 Figure 6. GPT-4o and our model’s counting performance by different object numbers. tests BabyLLaVA-V2’s out-of-domain generalization on the original NIH Baby Toolbox®. Unseen tasks. We have excluded Looking While Listening and Subitizing from the instruction tuning, which are thus unseen by BabyLLaVA-V2. While the two tasks are in spirit similar to Picture Vocabulary and Object Counting, respectively, BabyLLaVA-V2 yields near-random-guess results on them. We will address this issue in future work by improving the instruction tuning algorithm. 4.5. Intriguing findings Finally, we draw some intriguing “byproduct” findings from Table 4, which can improve our understanding of the proprietary GPT and Gemini models. GPT models struggle to count. Object Counting requires a model to count objects in an image (between 1 and 12), and GPT-4o can hardly count beyond 5 (see Figure 6). BabyLLaVA-V2 can match or outperform GPT-4o on some cognitive tasks. On Spatial Details and Who Has More, BabyLLaVA-V2 is on par with the four latest GPT and Gemini models. Moreover, it even outperforms GPT4o on the math tasks of Object Counting and Who Has More. Figure 6 shows that BabyLLaVA-V2 counts better than GPT-4o given six or more objects. GPT vs. Gemini. In general, the proprietary models give rise to similar results on DevCV Toolbox. However, when we zoom into the individual tasks, GPT-5 is significantly better than the rest on Spatial Details, while Gemini models are better at Object Counting than the GPT models. 5. Conclusion We introduced BabyVLM-V2, a framework that features a developmentally plausible pretraining set derived from the longitudinal SAYCam corpus, a compact VLM (BabyLLaVA-V2) trained from scratch, and comprehensive developmental benchmarks (DevCV Toolbox). DevCV Toolbox adapts all vision-related measures from the newly published NIH Baby Toolbox®. It contains ten measures spanning three subdomains (language, executive function/memory, and math) and requires a flexible model interface that can process image, video, and multi-turn dialogue. We demonstrate the potential of developmentally plausible vision FMs through extensive experiments on our pretraining and instruction tuning datasets, and we confirm the quality of DevCV Toolbox through extensive benchmarking with proprietary and open-source models. This framework will serve as a principled platform to broaden research 8"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 9, "text": "Table 6. Two language sources for pretraining. BabyLLaVA-V2-original is pretrained on our pretraining set whose language is mainly\ncaregivers’ speech transcripts, while BabyLLaVA-V2-synthetic is pretrained on synthetic utterances generated by GPT-4o.\nModel\nOverall\nCount\nLeftRight\nSpatial\nPV\nMemory\nLocalization\nVisual Delay Response\nWho Has More\nbinary\nmulti-exact\nmulti-adjacent\nsynthetic\nnaturalistic\nBabyLLaVA-V2-original\n55.2\n44.6\n42.3\n91.3\n27.4\n75.3\n38.8\n57.6\n33.1\n45.6\n98.4\n52.8\nBabyLLaVA-V2-synthetic\n57.4\n46.7\n35.3\n92.0\n30.7\n87.7\n36.9\n57.8\n38.1\n49.0\n99.2\n57.6\nengagement in vision FMs and accelerate progress toward\ndevelopmentally plausible learning.\nAcknowledgements\nSpecial thanks to Chen Yu, Jessica Sullivan, and Michel C.\nFrank for their wholehearted support and feedback through-\nout the project!\nReferences\n[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin\nGe, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun\nTang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhao-\nhai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu,\nYiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng,\nHang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin.\nQwen2.5-VL Technical Report, 2025.\narXiv:2502.13923\n[cs]. 2\n[2] Palanikumar Balasundaram and Indirapriya Darshini Avu-\nlakunta. Bayley Scales Of Infant and Toddler Development.\nIn StatPearls. StatPearls Publishing, Treasure Island (FL),\n2025. 3\n[3] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Alt-\nman, Simran Arora, Sydney von Arx, Michael S. Bernstein,\nJeannette Bohg, Antoine Bosselut, and Emma Brunskill et\nal. On the Opportunities and Risks of Foundation Models,\n2022. arXiv:2108.07258 [cs]. 2\n[4] Mathilde Caron, Alireza Fathi, Cordelia Schmid, and Ahmet\nIscen.\nWeb-scale visual entity recognition: an llm-driven\ndata approach.\nIn Proceedings of the 38th International\nConference on Neural Information Processing Systems, Red\nHook, NY, USA, 2024. Curran Associates Inc. 2\n[5] Fei-Long Chen, Du-Zhen Zhang, Ming-Lun Han, Xiu-Yi\nChen, Jing Shi, Shuang Xu, and Bo Xu. VLP: A Survey\non Vision-language Pre-training. Machine Intelligence Re-\nsearch, 20(1):38–56, 2023. 2\n[6] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice\nPasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blis-\ntein, Ori Ram, Dan Zhang, and Evan Rosen et al. Gemini\n2.5: Pushing the Frontier with Advanced Reasoning, Multi-\nmodality, Long Context, and Next Generation Agentic Capa-\nbilities, 2025. arXiv:2507.06261 [cs]. 2\n[7] Gil Diesendruck and Paul Bloom. How specific is the shape\nbias? Child Development, 74(1):168–178, 2003. 2\n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In International Conference on Learning Representa-\ntions, 2021. 3, 13\n[9] Ron Dumont, John O. Willis, Kathleen Viezel, and Jamie\nZibulsky. Mullen Scales of Early Learning, AGS Edition,\n1995. In Encyclopedia of Special Education. John Wiley &\nSons, Ltd, 2014. 3\n[10] Dumitru Erhan, Aaron Courville, Yoshua Bengio, and Pas-\ncal Vincent.\nWhy Does Unsupervised Pre-training Help\nDeep Learning?\nIn Proceedings of the Thirteenth Inter-\nnational Conference on Artificial Intelligence and Statistics,\npages 201–208. JMLR Workshop and Conference Proceed-\nings, 2010. ISSN: 1938-7228. 2\n[11] Richard Gershon, Miriam A. Novack, and Aaron J. Kaat.\nThe NIH Infant and Toddler Toolbox: A new standardized\ntool for assessing neurodevelopment in children ages 1–42\nmonths. Child Development, 95(6):2252–2254, 2024. 2, 15\n[12] Richard C. Gershon, Molly V. Wagster, Hugh C. Hendrie,\nNathan A. Fox, Karon F. Cook, and Cindy J. Nowinski. Nih\ntoolbox for assessment of neurological and behavioral func-\ntion. Neurology, 80(11_supplement_3):S2–S6, 2013. 15\n[13] Kristen\nGrauman,\nAndrew\nWestbury,\nEugene\nByrne,\nZachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson\nHamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d:\nAround the world in 3,000 hours of egocentric video.\nIn\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 18995–19012, 2022. 24\n[14] Kristen\nGrauman,\nAndrew\nWestbury,\nEugene\nByrne,\nZachary Chavis, Antonino Furnari, Rohit Girdhar, Jack-\nson Hamburger, Hao Jiang, Miao Liu, and Xingyu Liu et\nal. Ego4d: Around the world in 3,000 hours of egocentric\nvideo. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 18995–\n19012, 2022. 4\n[15] Y. Catherine Han, Courtney K. Blackwell, Elizabeth M.\nDworak, Rachel M. Flynn, Maxwell A. Mansolf, Miriam A.\nNovack, Sarah Pila, and Aaron J. Kaat. NIH Baby Toolbox®\nTechnical Manual. Northwestern University, Evanston, IL,\nversion 1.1 edition, 2025. 4, 15, 16\n[16] Y. Catherine Han, Elizabeth M. Dworak, Maxwell Mansolf,\nHubert Adam, Lihua Yao, Miriam A. Novack, Sarah Pila,\nRachel M. Flynn, Amanda M. Flagg, Vitali Ustsinovich, Kay\nSavio, Greg J. Byrne, Richard C. Gershon, and Aaron J.\nKaat.\nNIH Baby Toolbox® methodology and norms de-\nvelopment. Infant Behavior and Development, 80:102117,\n2025. 2, 4\n[17] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de\nLas Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan\nClark, Tom Hennigan, Eric Noland, Katie Millican, George\nvan den Driessche, Bogdan Damoc, Aurelia Guy, Simon\n9", "clean_text": "Table 6. Two language sources for pretraining. BabyLLaVA-V2-original is pretrained on our pretraining set whose language is mainly caregivers’ speech transcripts, while BabyLLaVA-V2-synthetic is pretrained on synthetic utterances generated by GPT-4o. Model Overall Count LeftRight Spatial PV Memory Localization Visual Delay Response Who Has More binary multi-exact multi-adjacent synthetic naturalistic BabyLLaVA-V2-original 55.2 44.6 42.3 91.3 27.4 75.3 38.8 57.6 33.1 45.6 98.4 52.8 BabyLLaVA-V2-synthetic 57.4 46.7 35.3 92.0 30.7 87.7 36.9 57.8 38.1 49.0 99.2 57.6 engagement in vision FMs and accelerate progress toward developmentally plausible learning. Acknowledgements Special thanks to Chen Yu, Jessica Sullivan, and Michel C. Frank for their wholehearted support and feedback throughout the project! References [1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-VL Technical Report, 2025. arXiv:2502.13923 [cs]. 2 [2] Palanikumar Balasundaram and Indirapriya Darshini Avulakunta. Bayley Scales Of Infant and Toddler Development. In StatPearls. StatPearls Publishing, Treasure Island (FL), 2025. 3 [3] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, and Emma Brunskill et al. On the Opportunities and Risks of Foundation Models, 2022. arXiv:2108.07258 [cs]. 2 [4] Mathilde Caron, Alireza Fathi, Cordelia Schmid, and Ahmet Iscen. Web-scale visual entity recognition: an llm-driven data approach. In Proceedings of the 38th International Conference on Neural Information Processing Systems, Red Hook, NY, USA, 2024. Curran Associates Inc. 2 [5] Fei-Long Chen, Du-Zhen Zhang, Ming-Lun Han, Xiu-Yi Chen, Jing Shi, Shuang Xu, and Bo Xu. VLP: A Survey on Vision-language Pre-training. Machine Intelligence Research, 20(1):38–56, 2023. 2 [6] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, and Evan Rosen et al. Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities, 2025. arXiv:2507.06261 [cs]. 2 [7] Gil Diesendruck and Paul Bloom. How specific is the shape bias? Child Development, 74(1):168–178, 2003. 2 [8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. 3, 13 [9] Ron Dumont, John O. Willis, Kathleen Viezel, and Jamie Zibulsky. Mullen Scales of Early Learning, AGS Edition, 1995. In Encyclopedia of Special Education. John Wiley & Sons, Ltd, 2014. 3 [10] Dumitru Erhan, Aaron Courville, Yoshua Bengio, and Pascal Vincent. Why Does Unsupervised Pre-training Help Deep Learning? In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 201–208. JMLR Workshop and Conference Proceedings, 2010. ISSN: 1938-7228. 2 [11] Richard Gershon, Miriam A. Novack, and Aaron J. Kaat. The NIH Infant and Toddler Toolbox: A new standardized tool for assessing neurodevelopment in children ages 1–42 months. Child Development, 95(6):2252–2254, 2024. 2, 15 [12] Richard C. Gershon, Molly V. Wagster, Hugh C. Hendrie, Nathan A. Fox, Karon F. Cook, and Cindy J. Nowinski. Nih toolbox for assessment of neurological and behavioral function. Neurology, 80(11_supplement_3):S2–S6, 2013. 15 [13] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 18995–19012, 2022. 24 [14] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, and Xingyu Liu et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 18995– 19012, 2022. 4 [15] Y. Catherine Han, Courtney K. Blackwell, Elizabeth M. Dworak, Rachel M. Flynn, Maxwell A. Mansolf, Miriam A. Novack, Sarah Pila, and Aaron J. Kaat. NIH Baby Toolbox® Technical Manual. Northwestern University, Evanston, IL, version 1.1 edition, 2025. 4, 15, 16 [16] Y. Catherine Han, Elizabeth M. Dworak, Maxwell Mansolf, Hubert Adam, Lihua Yao, Miriam A. Novack, Sarah Pila, Rachel M. Flynn, Amanda M. Flagg, Vitali Ustsinovich, Kay Savio, Greg J. Byrne, Richard C. Gershon, and Aaron J. Kaat. NIH Baby Toolbox® methodology and norms development. Infant Behavior and Development, 80:102117, 2025. 2, 4 [17] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon 9"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 10, "text": "Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol\nVinyals, and Laurent Sifre. Training compute-optimal large\nlanguage models, 2022. 25\n[18] Michael Y. Hu, Aaron Mueller, Candace Ross, Adina\nWilliams, Tal Linzen, Chengxu Zhuang, Ryan Cotterell,\nLeshem Choshen, Alex Warstadt, and Ethan Gotlieb Wilcox.\nFindings of the Second BabyLM Challenge:\nSample-\nEfficient Pretraining on Developmentally Plausible Corpora,\n2024. arXiv:2412.05149 [cs] version: 1. 1\n[19] Philip A. Huebner, Elior Sulem, Fisher Cynthia, and Dan\nRoth. BabyBERTa: Learning More Grammar With Small-\nScale Child-Directed Language. In Proceedings of the 25th\nConference on Computational Natural Language Learning,\npages 624–646, Online, 2021. Association for Computa-\ntional Linguistics. 2, 3\n[20] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom\nDuerig. Scaling Up Visual and Vision-Language Representa-\ntion Learning With Noisy Text Supervision. In Proceedings\nof the 38th International Conference on Machine Learning,\npages 4904–4916. PMLR, 2021. ISSN: 2640-3498. 2\n[21] Guangyuan Jiang, Manjie Xu, Shiji Xin, Wei Liang, Yujia\nPeng, Chi Zhang, and Yixin Zhu. MEWL: Few-shot mul-\ntimodal word learning with referential uncertainty. In Pro-\nceedings of the 40th International Conference on Machine\nLearning, pages 15144–15169. PMLR, 2023. ISSN: 2640-\n3498. 2, 3\n[22] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for\nneural language models, 2020. 25\n[23] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. Scaling Laws for\nNeural Language Models, 2020. arXiv:2001.08361 [cs]. 1\n[24] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and\nRoss Girshick. Segment anything. In 2023 IEEE/CVF In-\nternational Conference on Computer Vision (ICCV), pages\n3992–4003, 2023. 2\n[25] Eliza Kosoy, Emily Rose Reagan, Leslie Lai, Alison Gop-\nnik, and Danielle Krettek Cobb.\nComparing machines\nand children: Using developmental psychology experiments\nto assess the strengths and weaknesses of LaMDA re-\nsponses.\nSSRN Electronic Journal, 2024.\nAvailable at\nSSRN: https://ssrn.com/abstract=4696693 or\nhttp://dx.doi.org/10.2139/ssrn.4696693. 2,\n3\n[26] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBLIP: Bootstrapping Language-Image Pre-training for Uni-\nfied Vision-Language Understanding and Generation.\nIn\nProceedings of the 39th International Conference on Ma-\nchine Learning, pages 12888–12900. PMLR, 2022. ISSN:\n2640-3498. 2\n[27] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBLIP-2: Bootstrapping Language-Image Pre-training with\nFrozen Image Encoders and Large Language Models.\nIn\nProceedings of the 40th International Conference on Ma-\nchine Learning, pages 19730–19742. PMLR, 2023. ISSN:\n2640-3498. 2\n[28] Songtao Li and Hao Tang. Multimodal Alignment and Fu-\nsion: A Survey, 2025. arXiv:2411.17040 [cs] version: 2. 2\n[29] Yijiang Li, Qingying Gao, Tianwei Zhao, Bingyang Wang,\nHaoran Sun, Haiyun Lyu, Robert D. Hawkins, Nuno Vas-\nconcelos, Tal Golan, Dezhi Luo, and Hokin Deng.\nCore\nknowledge deficits in multi-modal language models.\nIn\nForty-second International Conference on Machine Learn-\ning, 2025. 2, 3\n[30] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. In Proceedings of the 37th Inter-\nnational Conference on Neural Information Processing Sys-\ntems, Red Hook, NY, USA, 2023. Curran Associates Inc. 3,\n13\n[31] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang,\nHang Su, Jun Zhu, and Lei Zhang. Grounding DINO: Mar-\nrying DINO with Grounded Pre-training for Open-Set Ob-\nject Detection. In Computer Vision – ECCV 2024: 18th Eu-\nropean Conference, Milan, Italy, September 29–October 4,\n2024, Proceedings, Part XLVII, pages 38–55, Berlin, Hei-\ndelberg, 2024. Springer-Verlag. 4, 14\n[32] Bria Long, Robert Z. Sparks, Violet Xiang, Stefan Sto-\njanov, Zi Yin, Grace E. Keene, Alvin W. M. Tan, Steven Y.\nFeng, Chengxu Zhuang, Virginia A. Marchman, Daniel L. K.\nYamins, and Michael C. Frank.\nThe BabyView dataset:\nHigh-resolution egocentric videos of infants’ and young chil-\ndren’s everyday experiences, 2025. arXiv:2406.10447 [cs].\n3\n[33] Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Ming Yan, Ji Zhang,\nand Rongrong Ji.\nX-clip: End-to-end multi-grained con-\ntrastive learning for video-text retrieval. In Proceedings of\nthe 30th ACM International Conference on Multimedia, page\n638–647, New York, NY, USA, 2022. Association for Com-\nputing Machinery. 3\n[34] Maya Malaviya,\nIlia Sucholutsky,\nKerem Oktar,\nand\n{Thomas L.} Griffiths. Can humans do less-than-one-shot\nlearning? pages 997–1003, 2022. Publisher Copyright: ©\n2022 The Author(s). This work is licensed under a Creative\nCommons Attribution 4.0 International License (CC BY);\n44th Annual Meeting of the Cognitive Science Society: Cog-\nnitive Diversity, CogSci 2022 ; Conference date: 27-07-2022\nThrough 30-07-2022. 2\n[35] Virginia A. Marchman and Philip S. Dale. The MacArthur-\nBates Communicative Development Inventories:\nupdates\nfrom the CDI Advisory Board. Frontiers in Psychology, 14,\n2023. Publisher: Frontiers. 4, 14, 15\n[36] Microsoft. Azure AI Speech | Microsoft Azure. [Accessed\n13-11-2025] https://azure.microsoft.com/en-\nus/products/ai-services/ai-speech. 3\n[37] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,\nMakarand\nTapaswi,\nIvan\nLaptev,\nand\nJosef\nSivic.\nHowTo100M:\nLearning\na\nText-Video\nEmbedding\nby\nWatching Hundred Million Narrated Video Clips. In 2019\n10", "clean_text": "Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022. 25 [18] Michael Y. Hu, Aaron Mueller, Candace Ross, Adina Williams, Tal Linzen, Chengxu Zhuang, Ryan Cotterell, Leshem Choshen, Alex Warstadt, and Ethan Gotlieb Wilcox. Findings of the Second BabyLM Challenge: SampleEfficient Pretraining on Developmentally Plausible Corpora, 2024. arXiv:2412.05149 [cs] version: 1. 1 [19] Philip A. Huebner, Elior Sulem, Fisher Cynthia, and Dan Roth. BabyBERTa: Learning More Grammar With SmallScale Child-Directed Language. In Proceedings of the 25th Conference on Computational Natural Language Learning, pages 624–646, Online, 2021. Association for Computational Linguistics. 2, 3 [20] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. In Proceedings of the 38th International Conference on Machine Learning, pages 4904–4916. PMLR, 2021. ISSN: 2640-3498. 2 [21] Guangyuan Jiang, Manjie Xu, Shiji Xin, Wei Liang, Yujia Peng, Chi Zhang, and Yixin Zhu. MEWL: Few-shot multimodal word learning with referential uncertainty. In Proceedings of the 40th International Conference on Machine Learning, pages 15144–15169. PMLR, 2023. ISSN: 26403498. 2, 3 [22] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. 25 [23] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling Laws for Neural Language Models, 2020. arXiv:2001.08361 [cs]. 1 [24] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 3992–4003, 2023. 2 [25] Eliza Kosoy, Emily Rose Reagan, Leslie Lai, Alison Gopnik, and Danielle Krettek Cobb. Comparing machines and children: Using developmental psychology experiments to assess the strengths and weaknesses of LaMDA responses. SSRN Electronic Journal, 2024. Available at SSRN: https://ssrn.com/abstract=4696693 or http://dx.doi.org/10.2139/ssrn.4696693. 2, 3 [26] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. In Proceedings of the 39th International Conference on Machine Learning, pages 12888–12900. PMLR, 2022. ISSN: 2640-3498. 2 [27] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. In Proceedings of the 40th International Conference on Machine Learning, pages 19730–19742. PMLR, 2023. ISSN: 2640-3498. 2 [28] Songtao Li and Hao Tang. Multimodal Alignment and Fusion: A Survey, 2025. arXiv:2411.17040 [cs] version: 2. 2 [29] Yijiang Li, Qingying Gao, Tianwei Zhao, Bingyang Wang, Haoran Sun, Haiyun Lyu, Robert D. Hawkins, Nuno Vasconcelos, Tal Golan, Dezhi Luo, and Hokin Deng. Core knowledge deficits in multi-modal language models. In Forty-second International Conference on Machine Learning, 2025. 2, 3 [30] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Proceedings of the 37th International Conference on Neural Information Processing Systems, Red Hook, NY, USA, 2023. Curran Associates Inc. 3, 13 [31] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, and Lei Zhang. Grounding DINO: Marrying DINO with Grounded Pre-training for Open-Set Object Detection. In Computer Vision – ECCV 2024: 18th European Conference, Milan, Italy, September 29–October 4, 2024, Proceedings, Part XLVII, pages 38–55, Berlin, Heidelberg, 2024. Springer-Verlag. 4, 14 [32] Bria Long, Robert Z. Sparks, Violet Xiang, Stefan Stojanov, Zi Yin, Grace E. Keene, Alvin W. M. Tan, Steven Y. Feng, Chengxu Zhuang, Virginia A. Marchman, Daniel L. K. Yamins, and Michael C. Frank. The BabyView dataset: High-resolution egocentric videos of infants’ and young children’s everyday experiences, 2025. arXiv:2406.10447 [cs]. 3 [33] Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Ming Yan, Ji Zhang, and Rongrong Ji. X-clip: End-to-end multi-grained contrastive learning for video-text retrieval. In Proceedings of the 30th ACM International Conference on Multimedia, page 638–647, New York, NY, USA, 2022. Association for Computing Machinery. 3 [34] Maya Malaviya, Ilia Sucholutsky, Kerem Oktar, and {Thomas L.} Griffiths. Can humans do less-than-one-shot learning? pages 997–1003, 2022. Publisher Copyright: © 2022 The Author(s). This work is licensed under a Creative Commons Attribution 4.0 International License (CC BY); 44th Annual Meeting of the Cognitive Science Society: Cognitive Diversity, CogSci 2022 ; Conference date: 27-07-2022 Through 30-07-2022. 2 [35] Virginia A. Marchman and Philip S. Dale. The MacArthurBates Communicative Development Inventories: updates from the CDI Advisory Board. Frontiers in Psychology, 14, 2023. Publisher: Frontiers. 4, 14, 15 [36] Microsoft. Azure AI Speech | Microsoft Azure. [Accessed 13-11-2025] https://azure.microsoft.com/enus/products/ai-services/ai-speech. 3 [37] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips. In 2019 10"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 11, "text": "IEEE/CVF International Conference on Computer Vision\n(ICCV), pages 2630–2640, 2019. ISSN: 2380-7504. 2\n[38] National Heart, Lung, and Blood Institute. How Sleep Works\n- How Much Sleep Is Enough? NHLBI, National Institutes\nof Health (NIH) website, 2022. 2, 3\n[39] Jean. Newborg and Riverside Publishing Company. Battelle\ndevelopmental inventory.\nBDI-2, 2005.\nEdition: 2nd ed.\nPlace: Itasca, Ill Publisher: Riverside Pub. 3\n[40] OpenAI, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam\nPerelman, Aditya Ramesh, Aidan Clark, A. J. Ostrow, Akila\nWelihinda, Alan Hayes, and Alec Radford et al. GPT-4o\nSystem Card, 2024. arXiv:2410.21276 [cs]. 2\n[41] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mah-\nmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell\nHowes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael\nRabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Je-\ngou, Julien Mairal, Patrick Labatut, Armand Joulin, and Pi-\notr Bojanowski. DINOv2: Learning Robust Visual Features\nwithout Supervision, 2024. arXiv:2304.07193 [cs]. 13\n[42] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. Improving Language Understanding by Genera-\ntive Pre-Training. . 13\n[43] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language Models are Unsuper-\nvised Multitask Learners. . 13\n[44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. Learning Transferable Visual\nModels From Natural Language Supervision. In Proceedings\nof the 38th International Conference on Machine Learning,\npages 8748–8763. PMLR, 2021. ISSN: 2640-3498. 2, 3, 15\n[45] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang\nHu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman\nRädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junt-\ning Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-\nYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feicht-\nenhofer. SAM 2: Segment anything in images and videos. In\nThe Thirteenth International Conference on Learning Rep-\nresentations, 2025. 2\n[46] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, Patrick Schramowski, Srivatsa Kundurthy, Katherine\nCrowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia\nJitsev. LAION-5B: an open large-scale dataset for training\nnext generation image-text models. In Proceedings of the\n36th International Conference on Neural Information Pro-\ncessing Systems, pages 25278–25294, Red Hook, NY, USA,\n2022. Curran Associates Inc. 2\n[47] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neu-\nral machine translation of rare words with subword units.\nIn Proceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Papers),\npages 1715–1725, Berlin, Germany, 2016. Association for\nComputational Linguistics. 13\n[48] Saber Sheybani, Sahaj Singh Maini, Aravind Dendukuri, Zo-\nran Tiganj, and Linda B. Smith. ModelVsBaby: a Develop-\nmentally Motivated Benchmark of Out-of-Distribution Ob-\nject Recognition, 2024. 2, 3\n[49] Melissa Kline Struhl, Laura Schulz, and Mark Sheskin et al.\nChildren Helping Science. [Accessed 13-11-2025] https:\n//childrenhelpingscience.com/. 7, 24\n[50] Jessica Sullivan, Michelle Mei, Andrew Perfors, Erica Woj-\ncik, and Michael C. Frank. SAYCam: A Large, Longitudinal\nAudiovisual Dataset Recorded From the Infant’s Perspective.\nOpen Mind, 5:20–29, 2021. 2, 3, 4\n[51] Yuqi Sun, Weimin Tan, Zhuoyao Gu, Ruian He, Siyuan\nChen, Miao Pang, and Bo Yan. A data-efficient strategy for\nbuilding high-performing medical foundation models. Na-\nture Biomedical Engineering, 9(4):539–551, 2025.\nPub-\nlisher: Nature Publishing Group. 2\n[52] Alvin Wei Ming Tan, Chunhua Yu, Bria Lorelle Long, Wan-\njing Anya Ma, Tonya Murray, Rebecca D. Silverman, Ja-\nson D Yeatman, and Michael Frank. Devbench: A multi-\nmodal developmental benchmark for language learning. In\nThe Thirty-eight Conference on Neural Information Process-\ning Systems Datasets and Benchmarks Track, 2024. 2, 3\n[53] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste\nRozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien\nRodriguez, Armand Joulin, Edouard Grave, and Guillaume\nLample. LLaMA: Open and Efficient Foundation Language\nModels, 2023. arXiv:2302.13971 [cs]. 3, 13\n[54] Wai Keen Vong, Wentao Wang, A. Emin Orhan, and Bren-\nden M. Lake. Grounded language acquisition through the\neyes and ears of a single child. Science (New York, N.Y.),\n383(6682):504–511, 2024. 2, 3\n[55] Ao Wang, Lihao Liu, Hui Chen, Zijia Lin, Jungong Han, and\nGuiguang Ding. Yoloe: Real-time seeing anything, 2025. 20\n[56] Shengao Wang, Arjun Chandra, Aoming Liu, Venkatesh\nSaligrama, and Boqing Gong. Babyvlm: Data-efficient pre-\ntraining of vlms inspired by infant learning. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), pages 1380–1390, 2025. 2, 3, 13\n[57] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang,\nand Ming Zhou. Minilm: deep self-attention distillation for\ntask-agnostic compression of pre-trained transformers.\nIn\nProceedings of the 34th International Conference on Neural\nInformation Processing Systems, Red Hook, NY, USA, 2020.\nCurran Associates Inc. 15\n[58] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu,\nXin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui\nWang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, and\nYu Qiao. Internvid: A large-scale video-text dataset for mul-\ntimodal understanding and generation. In The Twelfth Inter-\nnational Conference on Learning Representations, 2024. 2\n[59] Alex Warstadt, Aaron Mueller, Leshem Choshen, Ethan\nWilcox, Chengxu Zhuang, Juan Ciro, Rafael Mosquera,\nBhargavi Paranjabe, Adina Williams, Tal Linzen, and Ryan\nCotterell.\nFindings of the BabyLM Challenge: Sample-\nEfficient Pretraining on Developmentally Plausible Corpora.\nIn Proceedings of the BabyLM Challenge at the 27th Confer-\nence on Computational Natural Language Learning, pages\n11", "clean_text": "IEEE/CVF International Conference on Computer Vision (ICCV), pages 2630–2640, 2019. ISSN: 2380-7504. 2 [38] National Heart, Lung, and Blood Institute. How Sleep Works - How Much Sleep Is Enough? NHLBI, National Institutes of Health (NIH) website, 2022. 2, 3 [39] Jean. Newborg and Riverside Publishing Company. Battelle developmental inventory. BDI-2, 2005. Edition: 2nd ed. Place: Itasca, Ill Publisher: Riverside Pub. 3 [40] OpenAI, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, A. J. Ostrow, Akila Welihinda, Alan Hayes, and Alec Radford et al. GPT-4o System Card, 2024. arXiv:2410.21276 [cs]. 2 [41] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning Robust Visual Features without Supervision, 2024. arXiv:2304.07193 [cs]. 13 [42] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Understanding by Generative Pre-Training. . 13 [43] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language Models are Unsupervised Multitask Learners. . 13 [44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision. In Proceedings of the 38th International Conference on Machine Learning, pages 8748–8763. PMLR, 2021. ISSN: 2640-3498. 2, 3, 15 [45] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. SAM 2: Segment anything in images and videos. In The Thirteenth International Conference on Learning Representations, 2025. 2 [46] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: an open large-scale dataset for training next generation image-text models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, pages 25278–25294, Red Hook, NY, USA, 2022. Curran Associates Inc. 2 [47] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–1725, Berlin, Germany, 2016. Association for Computational Linguistics. 13 [48] Saber Sheybani, Sahaj Singh Maini, Aravind Dendukuri, Zoran Tiganj, and Linda B. Smith. ModelVsBaby: a Developmentally Motivated Benchmark of Out-of-Distribution Object Recognition, 2024. 2, 3 [49] Melissa Kline Struhl, Laura Schulz, and Mark Sheskin et al. Children Helping Science. [Accessed 13-11-2025] https: //childrenhelpingscience.com/. 7, 24 [50] Jessica Sullivan, Michelle Mei, Andrew Perfors, Erica Wojcik, and Michael C. Frank. SAYCam: A Large, Longitudinal Audiovisual Dataset Recorded From the Infant’s Perspective. Open Mind, 5:20–29, 2021. 2, 3, 4 [51] Yuqi Sun, Weimin Tan, Zhuoyao Gu, Ruian He, Siyuan Chen, Miao Pang, and Bo Yan. A data-efficient strategy for building high-performing medical foundation models. Nature Biomedical Engineering, 9(4):539–551, 2025. Publisher: Nature Publishing Group. 2 [52] Alvin Wei Ming Tan, Chunhua Yu, Bria Lorelle Long, Wanjing Anya Ma, Tonya Murray, Rebecca D. Silverman, Jason D Yeatman, and Michael Frank. Devbench: A multimodal developmental benchmark for language learning. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. 2, 3 [53] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and Efficient Foundation Language Models, 2023. arXiv:2302.13971 [cs]. 3, 13 [54] Wai Keen Vong, Wentao Wang, A. Emin Orhan, and Brenden M. Lake. Grounded language acquisition through the eyes and ears of a single child. Science (New York, N.Y.), 383(6682):504–511, 2024. 2, 3 [55] Ao Wang, Lihao Liu, Hui Chen, Zijia Lin, Jungong Han, and Guiguang Ding. Yoloe: Real-time seeing anything, 2025. 20 [56] Shengao Wang, Arjun Chandra, Aoming Liu, Venkatesh Saligrama, and Boqing Gong. Babyvlm: Data-efficient pretraining of vlms inspired by infant learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1380–1390, 2025. 2, 3, 13 [57] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: deep self-attention distillation for task-agnostic compression of pre-trained transformers. In Proceedings of the 34th International Conference on Neural Information Processing Systems, Red Hook, NY, USA, 2020. Curran Associates Inc. 15 [58] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, and Yu Qiao. Internvid: A large-scale video-text dataset for multimodal understanding and generation. In The Twelfth International Conference on Learning Representations, 2024. 2 [59] Alex Warstadt, Aaron Mueller, Leshem Choshen, Ethan Wilcox, Chengxu Zhuang, Juan Ciro, Rafael Mosquera, Bhargavi Paranjabe, Adina Williams, Tal Linzen, and Ryan Cotterell. Findings of the BabyLM Challenge: SampleEfficient Pretraining on Developmentally Plausible Corpora. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning, pages 11"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 12, "text": "1–34, Singapore, 2023. Association for Computational Lin-\nguistics. 1, 2\n[60] Luca Weihs, Amanda Yuile, Renée Baillargeon, Cynthia\nFisher, Gary Marcus, Roozbeh Mottaghi, and Aniruddha\nKembhavi.\nBenchmarking Progress to Infant-Level Phys-\nical Reasoning in AI. Transactions on Machine Learning\nResearch, 2022. 2, 3\n[61] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1492–1500,\n2017. 13\n[62] Eunice Yiu, Maan Qraitem, Anisa Noor Majhi, Charlie\nWong, Yutong Bai, Shiry Ginosar, Alison Gopnik, and Kate\nSaenko.\nKiVA: Kid-inspired visual analogies for testing\nlarge multimodal models. In The Thirteenth International\nConference on Learning Representations, 2025. 2\n[63] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu.\nTinyLlama: An Open-Source Small Language Model, 2024.\narXiv:2401.02385 [cs]. 3, 13\n[64] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xi-\naofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang,\nFei Wu, and Guoyin Wang.\nInstruction Tuning for Large\nLanguage Models: A Survey, 2025. arXiv:2308.10792 [cs].\n2\n[65] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Fucheng\nWeng, Zehuan Yuan, Ping Luo, Wenyu Liu, and Xinggang\nWang. Bytetrack: Multi-object tracking by associating every\ndetection box. 2022. 20\n12", "clean_text": "1–34, Singapore, 2023. Association for Computational Linguistics. 1, 2 [60] Luca Weihs, Amanda Yuile, Renée Baillargeon, Cynthia Fisher, Gary Marcus, Roozbeh Mottaghi, and Aniruddha Kembhavi. Benchmarking Progress to Infant-Level Physical Reasoning in AI. Transactions on Machine Learning Research, 2022. 2, 3 [61] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1492–1500, 2017. 13 [62] Eunice Yiu, Maan Qraitem, Anisa Noor Majhi, Charlie Wong, Yutong Bai, Shiry Ginosar, Alison Gopnik, and Kate Saenko. KiVA: Kid-inspired visual analogies for testing large multimodal models. In The Thirteenth International Conference on Learning Representations, 2025. 2 [63] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. TinyLlama: An Open-Source Small Language Model, 2024. arXiv:2401.02385 [cs]. 3, 13 [64] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, and Guoyin Wang. Instruction Tuning for Large Language Models: A Survey, 2025. arXiv:2308.10792 [cs]. 2 [65] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Fucheng Weng, Zehuan Yuan, Ping Luo, Wenyu Liu, and Xinggang Wang. Bytetrack: Multi-object tracking by associating every detection box. 2022. 20 12"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 13, "text": "BabyVLM-V2: Toward Developmentally Grounded Pretraining and\nBenchmarking of Vision Foundation Models\nSupplementary Material\nA. Model training\nA.1. BabyLLaVA-V2 architecture\nWe build upon the original BabyLLaVA-Llama model in-\ntroduced in BabyVLM-V1 [56], by giving it the capabil-\nity to process multiple images as input and conduct multi-\nturn visual–linguistic interactions. The model architecture\nconsists of a compact language backbone, a visual encoder,\nand a lightweight multilayer perceptron (MLP) connector\nthat projects visual features into the language space. Un-\nlike BabyVLM-V1, which also experimented with smaller\nbackbones (GPT-2 [43] + ResNeXt-50 [61]), we only adopt\nthe larger variant composed of a LLaMA-1.1B [53, 63]\nlanguage model and a ViT-L-16 [8] visual encoder (300M\nparams). We find that the smaller variant often struggles to\ncomplete complex downstream tasks such as memory, pri-\nmarily due to its limited model capacity, whereas the larger\nconfiguration achieves a better balance between develop-\nmental plausibility and expressive capability.\nA.2. BabyLLaVA-V2 training paradigm\nWe train the entire model from scratch using a four-stage\npipeline, as summarized in Table 7.\nStage 0: Unimodal Training. In the first stage, the lan-\nguage and vision backbones are trained independently to\nacquire the basic representational abilities for each modal-\nity. The language backbone is trained on all transcribed\nutterances using a standard autoregressive loss [42].\nIts\ntokenizer is initialized via Byte-Pair Encoding (BPE) [47]\ntrained on the same corpus, with a fixed vocabulary size of\n6000. The vision backbone is trained using a DINOv2 [41]\nobjective on SAYCam frames. We do not apply any filtering\nduring this stage—except restricting samples to the training\nsplit—since the filtering procedures are primarily designed\nto enforce image–utterance alignment, which is irrelevant\nto unimodal representation learning.\nStage 1: Feature Alignment. This stage corresponds to\nPhase 1 training in LLaVA [30]. Both the vision and lan-\nguage backbones are frozen, and only the MLP connector\nis optimized using an autoregressive loss. The objective is\nto align visual features with the language embedding space,\neffectively bridging the two modalities. To maintain train-\ning stability, we use only the image–utterance subset of the\npretraining data in this stage, postponing exposure to multi-\nimage inputs until later phases.\n1The training dataset, the model checkpoints, the training scripts and\nthe evaluation samples will be released to the public in the near future.\nStage 2: Joint Pretraining. In this stage, the vision back-\nbone remains frozen, while the MLP connector and lan-\nguage backbone are trained jointly on the full mixed-format\npretraining dataset, as described in Section 3.1. This allows\nthe model to learn multimodal grounding over diverse input\nstructures.\nStage 3: Instruction Fine-tuning. Finally, we fine-tune the\nmodel using the mixed instruction dataset, which is a com-\nbination of all the instruction samples mentioned in Table 3.\nThis step enables the model to perform various downstream\ntasks through natural-language prompts. The vision back-\nbone, MLP connector and language backbone are all up-\ndated to learn instruction-following behavior and context-\ndependent reasoning. We apply two different learning rates\nfor different modules in this stage: the learning rate of the\nvision backbone is 1e-5, while that of the MLP connector\nand language backbone is 5e-5.\nMain hyperparameters of all 4 stages are summarized in\nTable 7. All experiments are conducted on four NVIDIA\nA6000 GPUs with 48 GB of VRAM each. Language back-\nbone training completes in less than one hour, while the vi-\nsion backbone completes in 4 days. Next, training the MLP\nconnector requires approximately five hours. Joint pretrain-\ning on the mixed-format dataset takes roughly 34 hours to\nconverge. Finally, instruction tuning takes 60 hours.\nA.3. Open-source model fine-tuning\nWe conduct LoRA finetuning experiments on two open-\nsource models, LLaVA-OneVision-7B and Qwen2.5-VL-\n7B, to evaluate the effectiveness of our instruction-\nfinetuning dataset. Each task is finetuned separately. We\nset the LoRA rank to 64, use a scaling factor of 64, and\napply a dropout rate of 0.05. Training is performed for 5\nepochs with a global batch size of 128, a learning rate of\n1e-4, a weight decay of 0.1, a warmup ratio of 0.03, and a\ncosine learning-rate schedule.\nB. Developmentally aligned benchmarks\nIn Appendix B, we adopt the following organization: Sub-\nsection B.1 describes general implementation details that\nare shared by several tasks, including details on the vocab-\nulary used in DevCV Toolbox, acquisition of SAYCam an-\nnotations, acquisition of Ego4d annotations, and important\ndistinction between SAYCam and Ego4d. Then, each sub-\nsection between 2 and 11 describes how these annotations\nare used to construct one task in DevCV Toolbox each, and\nare each broken up into Original Toolbox Task, Adaptation,\n13", "clean_text": "BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models Supplementary Material A. Model training A.1. BabyLLaVA-V2 architecture We build upon the original BabyLLaVA-Llama model introduced in BabyVLM-V1 [56], by giving it the capability to process multiple images as input and conduct multiturn visual–linguistic interactions. The model architecture consists of a compact language backbone, a visual encoder, and a lightweight multilayer perceptron (MLP) connector that projects visual features into the language space. Unlike BabyVLM-V1, which also experimented with smaller backbones (GPT-2 [43] + ResNeXt-50 [61]), we only adopt the larger variant composed of a LLaMA-1.1B [53, 63] language model and a ViT-L-16 [8] visual encoder (300M params). We find that the smaller variant often struggles to complete complex downstream tasks such as memory, primarily due to its limited model capacity, whereas the larger configuration achieves a better balance between developmental plausibility and expressive capability. A.2. BabyLLaVA-V2 training paradigm We train the entire model from scratch using a four-stage pipeline, as summarized in Table 7. Stage 0: Unimodal Training. In the first stage, the language and vision backbones are trained independently to acquire the basic representational abilities for each modality. The language backbone is trained on all transcribed utterances using a standard autoregressive loss [42]. Its tokenizer is initialized via Byte-Pair Encoding (BPE) [47] trained on the same corpus, with a fixed vocabulary size of 6000. The vision backbone is trained using a DINOv2 [41] objective on SAYCam frames. We do not apply any filtering during this stage—except restricting samples to the training split—since the filtering procedures are primarily designed to enforce image–utterance alignment, which is irrelevant to unimodal representation learning. Stage 1: Feature Alignment. This stage corresponds to Phase 1 training in LLaVA [30]. Both the vision and language backbones are frozen, and only the MLP connector is optimized using an autoregressive loss. The objective is to align visual features with the language embedding space, effectively bridging the two modalities. To maintain training stability, we use only the image–utterance subset of the pretraining data in this stage, postponing exposure to multiimage inputs until later phases. 1The training dataset, the model checkpoints, the training scripts and the evaluation samples will be released to the public in the near future. Stage 2: Joint Pretraining. In this stage, the vision backbone remains frozen, while the MLP connector and language backbone are trained jointly on the full mixed-format pretraining dataset, as described in Section 3.1. This allows the model to learn multimodal grounding over diverse input structures. Stage 3: Instruction Fine-tuning. Finally, we fine-tune the model using the mixed instruction dataset, which is a combination of all the instruction samples mentioned in Table 3. This step enables the model to perform various downstream tasks through natural-language prompts. The vision backbone, MLP connector and language backbone are all updated to learn instruction-following behavior and contextdependent reasoning. We apply two different learning rates for different modules in this stage: the learning rate of the vision backbone is 1e-5, while that of the MLP connector and language backbone is 5e-5. Main hyperparameters of all 4 stages are summarized in Table 7. All experiments are conducted on four NVIDIA A6000 GPUs with 48 GB of VRAM each. Language backbone training completes in less than one hour, while the vision backbone completes in 4 days. Next, training the MLP connector requires approximately five hours. Joint pretraining on the mixed-format dataset takes roughly 34 hours to converge. Finally, instruction tuning takes 60 hours. A.3. Open-source model fine-tuning We conduct LoRA finetuning experiments on two opensource models, LLaVA-OneVision-7B and Qwen2.5-VL7B, to evaluate the effectiveness of our instructionfinetuning dataset. Each task is finetuned separately. We set the LoRA rank to 64, use a scaling factor of 64, and apply a dropout rate of 0.05. Training is performed for 5 epochs with a global batch size of 128, a learning rate of 1e-4, a weight decay of 0.1, a warmup ratio of 0.03, and a cosine learning-rate schedule. B. Developmentally aligned benchmarks In Appendix B, we adopt the following organization: Subsection B.1 describes general implementation details that are shared by several tasks, including details on the vocabulary used in DevCV Toolbox, acquisition of SAYCam annotations, acquisition of Ego4d annotations, and important distinction between SAYCam and Ego4d. Then, each subsection between 2 and 11 describes how these annotations are used to construct one task in DevCV Toolbox each, and are each broken up into Original Toolbox Task, Adaptation, 13"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 14, "text": "Table 7. Training stage specification of BabyLLaVA-V2. Note that for stage 3, different modules have different learning rate, as\nmentioned in Section A.2.\nStage\nTrained modules\nFrozen modules\nDataset\nLoss\nLearning rate\nEpoch\nGlobal batch size\n0-language\nLanguage backbone\nN/A\n283k utterance only\nAutoregressive\n2e-4\n10\n16\n0-vision\nVision backbone\nN/A\n1085k image only\nDINOv2\n1e-4\n100\n64\n1\nMLP connector\nLanguage backbone\n+ vision backbone\n768k image-utterance\nAutoregressive\n3e-3\n5\n128\n2\nMLP connector\n+ language backbone\nVision backbone\n768k image-utterance\n+ 181k video-utterance\n+ 63k multi-turn\nAutoregressive\n2e-4\n5\n128\n3\nMLP connector\n+ language backbone\n+ vision backbone\nNone\n150k instruction finetune\nAutoregressive\n5e-5 | 1e-5\n5\n128\nData Collection, and Example Prompt. Some of these also\ninclude information on Evaluation or Data Composition.\nB.1. Data collection procedures common to all tasks\nVocabulary filtering\nTo ensure that all benchmarks in this work focus on\ndevelopmentally appropriate vocabulary, we draw on the\nMacArthur–Bates Communicative Development Invento-\nries (MAB–CDI): Words and Gestures [35]. The MAB–CDI\nis a standardized instrument assessing early vocabulary\ncomprehension and production in infants and toddlers, cov-\nering familiar words across core semantic categories (e.g.,\nanimals, foods, body parts, actions).\nBecause it is widely regarded as a gold-standard\nreference for early lexical development, we restrict our\nbenchmark vocabulary to words that appear in—or are\nclosely aligned with—those in the MAB–CDI. Accord-\ningly, during visual concept mining from SAYCam and\nEgo4D, we retain only crops whose labels fall within this\ndevelopmentally grounded lexical domain, ensuring that\nevery keyword used across tasks reflects concepts young\nchildren could plausibly understand.\nSAYCam annotations\nTo support all SAYCam-based benchmarks in this work,\nwe build the following unified preprocessing pipeline that\nextracts high-quality image crops for every object and\naction concept appearing in the corpus.\nThis pipeline\nis reused (with task-specific modifications described in\nthe corresponding benchmark sections) across tasks and\nprovides consistent visual grounding for all downstream\ndatasets.\n• Frame-level detection and indexing: We first sample\nSAYCam videos at 1 FPS and run an open-vocabulary de-\ntector (Grounding–DINO [31]) using the GPT-annotated\nlabels associated with each frame as the open set. Let\nS denote the set of all such SAYCam labels. For each\nlabel s ∈S, we construct an index Index(s) that maps\ns to all frames in which it is detected, together with its\nproportionally buffered bounding boxes and GPT-derived\nblurriness scores. This Index(s) structure serves as the\nmaster lookup table for retrieving visual instances of any\nconcept.\n• Normalizing label variants: Raw SAYCam labels s ∈S\noften include plural forms, paraphrases, or compositional\ndescriptions. To ensure consistent visual grounding, we\ncluster lexically or semantically equivalent labels into\nsmall groups based on lexical similarity, plural equiva-\nlence, and phrase containment heuristics. Each label s is\nassigned to its cluster M(s). This allows us to treat vari-\nants of s such as “shoes”, “a shoe”, or “pair of shoes” as\na single underlying concept by retrieving visual instances\nfrom {Index(s′)|s′ ∈M(s)}.\n• Quality filtering: Because SAYCam contains natural-\nistic video frames from children’s head-cam footage,\nmany detections are of low-quality due to motion\nblur, wrong/irrelevant detector predictions, small/partial\nbounding boxes. Therefore, we score each detection re-\nsult using four broad signals: (1) detector confidence, (2)\nCLIP image–text alignment, (3) crop size, and (4) spatial\nclarity (e.g., centeredness). These signals are normalized\nper-concept and combined into a single quality measure.\nWe also employ additional light-weight adjustments to\nensure that within M(s), rare labels are not overwhelmed\nby frequent ones and that exact label matches are pre-\nferred over looser variants.\n• Ensuring lexical and visual diversity: To avoid select-\ning many near-duplicate frames of the same scene, we\napply simple diversity controls. We first ensure that dif-\nferent lexical variants of a concept are represented, and\nthen enforce a minimal temporal spacing between cho-\nsen frames. From this diversified pool, we keep only a\nsmall number (≤10) of final crops per concept, prioritiz-\ning clarity and representativeness.\n• Final output: The result is a compact, high-quality set\nof image crops for every object or action concept in SAY-\nCam. These curated crops act as the visual foundation for\nthe majority of benchmarks built from SAYCam in this\npaper. They guarantee concept fidelity, diversity of visual\n14", "clean_text": "Table 7. Training stage specification of BabyLLaVA-V2. Note that for stage 3, different modules have different learning rate, as mentioned in Section A.2. Stage Trained modules Frozen modules Dataset Loss Learning rate Epoch Global batch size 0-language Language backbone N/A 283k utterance only Autoregressive 2e-4 10 16 0-vision Vision backbone N/A 1085k image only DINOv2 1e-4 100 64 1 MLP connector Language backbone + vision backbone 768k image-utterance Autoregressive 3e-3 5 128 2 MLP connector + language backbone Vision backbone 768k image-utterance + 181k video-utterance + 63k multi-turn Autoregressive 2e-4 5 128 3 MLP connector + language backbone + vision backbone None 150k instruction finetune Autoregressive 5e-5 | 1e-5 5 128 Data Collection, and Example Prompt. Some of these also include information on Evaluation or Data Composition. B.1. Data collection procedures common to all tasks Vocabulary filtering To ensure that all benchmarks in this work focus on developmentally appropriate vocabulary, we draw on the MacArthur–Bates Communicative Development Inventories (MAB–CDI): Words and Gestures [35]. The MAB–CDI is a standardized instrument assessing early vocabulary comprehension and production in infants and toddlers, covering familiar words across core semantic categories (e.g., animals, foods, body parts, actions). Because it is widely regarded as a gold-standard reference for early lexical development, we restrict our benchmark vocabulary to words that appear in—or are closely aligned with—those in the MAB–CDI. Accordingly, during visual concept mining from SAYCam and Ego4D, we retain only crops whose labels fall within this developmentally grounded lexical domain, ensuring that every keyword used across tasks reflects concepts young children could plausibly understand. SAYCam annotations To support all SAYCam-based benchmarks in this work, we build the following unified preprocessing pipeline that extracts high-quality image crops for every object and action concept appearing in the corpus. This pipeline is reused (with task-specific modifications described in the corresponding benchmark sections) across tasks and provides consistent visual grounding for all downstream datasets. • Frame-level detection and indexing: We first sample SAYCam videos at 1 FPS and run an open-vocabulary detector (Grounding–DINO [31]) using the GPT-annotated labels associated with each frame as the open set. Let S denote the set of all such SAYCam labels. For each label s ∈S, we construct an index Index(s) that maps s to all frames in which it is detected, together with its proportionally buffered bounding boxes and GPT-derived blurriness scores. This Index(s) structure serves as the master lookup table for retrieving visual instances of any concept. • Normalizing label variants: Raw SAYCam labels s ∈S often include plural forms, paraphrases, or compositional descriptions. To ensure consistent visual grounding, we cluster lexically or semantically equivalent labels into small groups based on lexical similarity, plural equivalence, and phrase containment heuristics. Each label s is assigned to its cluster M(s). This allows us to treat variants of s such as “shoes”, “a shoe”, or “pair of shoes” as a single underlying concept by retrieving visual instances from {Index(s′)|s′ ∈M(s)}. • Quality filtering: Because SAYCam contains naturalistic video frames from children’s head-cam footage, many detections are of low-quality due to motion blur, wrong/irrelevant detector predictions, small/partial bounding boxes. Therefore, we score each detection result using four broad signals: (1) detector confidence, (2) CLIP image–text alignment, (3) crop size, and (4) spatial clarity (e.g., centeredness). These signals are normalized per-concept and combined into a single quality measure. We also employ additional light-weight adjustments to ensure that within M(s), rare labels are not overwhelmed by frequent ones and that exact label matches are preferred over looser variants. • Ensuring lexical and visual diversity: To avoid selecting many near-duplicate frames of the same scene, we apply simple diversity controls. We first ensure that different lexical variants of a concept are represented, and then enforce a minimal temporal spacing between chosen frames. From this diversified pool, we keep only a small number (≤10) of final crops per concept, prioritizing clarity and representativeness. • Final output: The result is a compact, high-quality set of image crops for every object or action concept in SAYCam. These curated crops act as the visual foundation for the majority of benchmarks built from SAYCam in this paper. They guarantee concept fidelity, diversity of visual 14"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 15, "text": "Table 8. Comparison between SAYCam and Ego4d. Object\nSize is reported in terms of the average % of the frame’s area filled.\nData Source\nparticipants\nNumber of Pixels\nObject Size\nSAYCam\ninfants\n307k (fixed)\n57%\nEgo4d\nadults\nover 2M (average)\n4%\ncontexts, and consistent quality standards across tasks.\nEgo4D Annotations\nFor Ego4D, we do not perform any heavy data clean-\ning or processing due to the native, high-quality annota-\ntions of the dataset.\nFor most of our benchmarks, we\nuse image data from the egotracks split, which con-\ntains densely annotated egocentric video tracks. In addi-\ntion, Picture Vocabulary (see Section B.2) also draws image\ncrops from fho_lta—a subset of Ego4D focused on fu-\nture hand–object interactions—providing additional object-\ncentric visual diversity.\nOverall differences between SAYCam and Ego4d\nHere, we analyze the differences between SAYCam and\nEgo4d which result in BabyLLaVA-V2’s very poor gener-\nalization to Ego4d. Specifically, SAYCam was filmed by 3\nbabies across 4 homes, while Ego4d was filmed by 923 par-\nticipants across 74 sites. There’s also a significant domain\nshift in the size of the frames and the sizes of the objects\nrelative to the frames. See Table 8 for a summary.\nIn addition, although all of the Ego4d examples con-\nstructed in DevCV Toolbox are directly based on objects\nlisted in SAYCam’s vocabulary, their backgrounds may still\ninclude objects that BabyLLaVA-V2 never saw in its train-\ning, and thus detract from its’ overall understanding of the\nscene. For example, we might construct an example from\nEgo4d that asks about the location of a hand, and although\nBabyLLaVA-V2 saw examples of hand during training,\nthe frame is full of other objects to which BabyLLaVA-V2\ncan attribute no meaning.\nIn such a case, context clues\nlearned by BabyLLaVA-V2 about where gloves are usually\nfound relative to their scene, such as at the end of an arm\nor holding onto a known object, are lost, and performance\ndrops correspondingly. Further, we conjecture that this lack\nof generalization stems from not only the explicit action\ncategories included in Ego4d that a baby would never have\nseen (like fixing a car or performing a laboratory exper-\niment), but also from the inherently wider field of view\ncaptured adult demonstrators relative to babies.\nFurther,\nwe argue that even if Ego4d had been filmed of the same\nlocations and actions as SAYCam, we would still observe\na domain shift caused solely because the demonstrators\nare adults, perceiving the world from a higher point of\nview than babies. This point reinforces the uniqueness of\nthe baby domain in the space of egocentric computer vision.\nB.2. Picture Vocabulary\nOriginal Toolbox Task\nOur\ntask\nis\ndirectly\nadapted\nfrom\nthe\nNIH\nBaby\nToolbox® Picture Vocabulary Test (PVT), which evaluates\na participant’s receptive vocabulary by presenting a spoken\ntarget word alongside four images (one correct, three dis-\ntractors) [11]. The goal is to touch the picture matching\nthe target word. Distractors in the original PVT are de-\nsigned to be plausible but incorrect, typically encompassing\ncoarse-categorical, fine-categorical, or phonological simi-\nlarity.\nWhile the full PVT, taken directly from the NIH\nToolbox® [12, 15], includes 373 examples, we identify 52\nexamples intended for early childhood receptive vocabulary\nevaluation through combining all-MiniLM-L6-v2 em-\nbedding similarity [57] comparison to vocabulary in [35]\nand manual inspection.\nWhile the Baby Toolbox PVT uses an IRT-based\ncomputer-adaptive score that converts response patterns\ninto age-normed ability estimates [15], our adaptation sim-\nplifies this to straightforward 4-way accuracy since all items\nin the benchmark are evaluated rather than adaptively se-\nlected.\nOur adaptation preserves the original developmental\nintent while replacing controlled illustrations with natural-\nistic egocentric visual inputs (SAYCam/Ego4D), providing\na grounded benchmark for modeling baby-level vocabulary\ncomprehension in realistic developmental environments.\nAdaptation\nTo adapt the original PVT design to naturalistic corpora,\nwe first map MAB-CDI words r ∈R to corpus vocab-\nularies S: GPT-annotated labels for SAYCam and native\nobjects/actions labels for Ego4D. This produces a set of vi-\nsually grounded targets Gr ⊂S for each CDI anchor r,\nforming a one-to-many mapping r→Gr.\nWe then analyze the 52 baby-level NIH PVT items to\nquantify the original distractor structure. We define three\ncategories: fine-categorical, coarse-categorical, and phono-\nlogical. We manually annotate every NIH distractor to one\nor more of these types accordingly. Note that the original\nPVT includes unrelated distractors and we exclude those\ngiven the difficulty in controlling the quality of unrelated\ndistractors in naturalistic imagery. We obtain the unnormal-\nized distractor-type weights:\nwcoarse = 0.5643,\nwfine = 0.1472,\nwphon = 0.0321.\nUsing these proportions, we construct corpus-specific dis-\ntractor pools from the entire corpus S (because the model\nis only required to identify the correct target concept, not to\ncorrectly recognize or label the distractors):\n• Fine-categorical: We use similarity scoring based on\nCLIP text embeddings [44]. For SAYCam, candidates\n15", "clean_text": "Table 8. Comparison between SAYCam and Ego4d. Object Size is reported in terms of the average % of the frame’s area filled. Data Source participants Number of Pixels Object Size SAYCam infants 307k (fixed) 57% Ego4d adults over 2M (average) 4% contexts, and consistent quality standards across tasks. Ego4D Annotations For Ego4D, we do not perform any heavy data cleaning or processing due to the native, high-quality annotations of the dataset. For most of our benchmarks, we use image data from the egotracks split, which contains densely annotated egocentric video tracks. In addition, Picture Vocabulary (see Section B.2) also draws image crops from fho_lta—a subset of Ego4D focused on future hand–object interactions—providing additional objectcentric visual diversity. Overall differences between SAYCam and Ego4d Here, we analyze the differences between SAYCam and Ego4d which result in BabyLLaVA-V2’s very poor generalization to Ego4d. Specifically, SAYCam was filmed by 3 babies across 4 homes, while Ego4d was filmed by 923 participants across 74 sites. There’s also a significant domain shift in the size of the frames and the sizes of the objects relative to the frames. See Table 8 for a summary. In addition, although all of the Ego4d examples constructed in DevCV Toolbox are directly based on objects listed in SAYCam’s vocabulary, their backgrounds may still include objects that BabyLLaVA-V2 never saw in its training, and thus detract from its’ overall understanding of the scene. For example, we might construct an example from Ego4d that asks about the location of a hand, and although BabyLLaVA-V2 saw examples of hand during training, the frame is full of other objects to which BabyLLaVA-V2 can attribute no meaning. In such a case, context clues learned by BabyLLaVA-V2 about where gloves are usually found relative to their scene, such as at the end of an arm or holding onto a known object, are lost, and performance drops correspondingly. Further, we conjecture that this lack of generalization stems from not only the explicit action categories included in Ego4d that a baby would never have seen (like fixing a car or performing a laboratory experiment), but also from the inherently wider field of view captured adult demonstrators relative to babies. Further, we argue that even if Ego4d had been filmed of the same locations and actions as SAYCam, we would still observe a domain shift caused solely because the demonstrators are adults, perceiving the world from a higher point of view than babies. This point reinforces the uniqueness of the baby domain in the space of egocentric computer vision. B.2. Picture Vocabulary Original Toolbox Task Our task is directly adapted from the NIH Baby Toolbox® Picture Vocabulary Test (PVT), which evaluates a participant’s receptive vocabulary by presenting a spoken target word alongside four images (one correct, three distractors) [11]. The goal is to touch the picture matching the target word. Distractors in the original PVT are designed to be plausible but incorrect, typically encompassing coarse-categorical, fine-categorical, or phonological similarity. While the full PVT, taken directly from the NIH Toolbox® [12, 15], includes 373 examples, we identify 52 examples intended for early childhood receptive vocabulary evaluation through combining all-MiniLM-L6-v2 embedding similarity [57] comparison to vocabulary in [35] and manual inspection. While the Baby Toolbox PVT uses an IRT-based computer-adaptive score that converts response patterns into age-normed ability estimates [15], our adaptation simplifies this to straightforward 4-way accuracy since all items in the benchmark are evaluated rather than adaptively selected. Our adaptation preserves the original developmental intent while replacing controlled illustrations with naturalistic egocentric visual inputs (SAYCam/Ego4D), providing a grounded benchmark for modeling baby-level vocabulary comprehension in realistic developmental environments. Adaptation To adapt the original PVT design to naturalistic corpora, we first map MAB-CDI words r ∈R to corpus vocabularies S: GPT-annotated labels for SAYCam and native objects/actions labels for Ego4D. This produces a set of visually grounded targets Gr ⊂S for each CDI anchor r, forming a one-to-many mapping r→Gr. We then analyze the 52 baby-level NIH PVT items to quantify the original distractor structure. We define three categories: fine-categorical, coarse-categorical, and phonological. We manually annotate every NIH distractor to one or more of these types accordingly. Note that the original PVT includes unrelated distractors and we exclude those given the difficulty in controlling the quality of unrelated distractors in naturalistic imagery. We obtain the unnormalized distractor-type weights: wcoarse = 0.5643, wfine = 0.1472, wphon = 0.0321. Using these proportions, we construct corpus-specific distractor pools from the entire corpus S (because the model is only required to identify the correct target concept, not to correctly recognize or label the distractors): • Fine-categorical: We use similarity scoring based on CLIP text embeddings [44]. For SAYCam, candidates 15"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 16, "text": "above a similarity threshold of 0.7 is considered belong-\ning to the same fine-grained category while for Ego4D\nwe use a quantile band [0.997, 0.99973] to also filter out\noverly similar and thus indistinguishable words.\n• Coarse-categorical: We use Kmeans clustering based on\nCLIP text embeddings (SAYCam: K = 100; Ego4D:\nK = 150).\n• Phonological: We use Soundex-based string similarity\nfor both datasets.\nFor each CDI anchor r, we select a ground-truth label\ng ∈Gr and sample three distinct distractors from these\npools using the weights.\nFor SAYCam examples, we\nperform a final round of manual screening to filter out the\ninfeasible examples, while Ego4D examples are filtered\nwith a hybrid procedure combining Gemini2.5-flash\nchecks with a lightweight manual review.\nData Collection\nTo produce high-quality 4-way visual choices, we collect\nimage crops corresponding to every target and distractor la-\nbel.\nFor SAYCam, because Picture Vocabulary requires ex-\ntremely precise, semantically clear images, we modify the\nfully automated pipeline in Section B.1 with the following\nchanges:\n1. Candidates come directly from Index(g) where g ∈Gr\ngiven anchor r.\n2. Human annotators manually filter irrelevant, ambiguous,\nor blurry crops and refine bounding boxes, replacing au-\ntomated quality scores.\nThis process yields a compact, high-precision crop inven-\ntory used for all SAYCam examples.\nFor Ego4D, for the objects, we use the bounding boxes\nfrom the visual_crop field of the EgoTracks bench-\nmark, applying a deterministic buffer (1.2× + 8px mar-\ngin) and requiring a post-buffer normalized area > 0.03.\nFor actions, we use the fho_lta benchmark which con-\ntains abundant action annotations. As there are no explicit\nbounding box annotations, we sample frames from the mid-\ndle 25% of each action frame interval and apply minimal\ncenter-biased cropping to maintain clarity. For each label,\nwe keep 10 candidates while preserving diversity and vi-\nsual fidelity, and we apply a Gemini2.5-flash pass to\neliminate unusable crops.\nDataset composition.\nAs shown in Figure 7, We obtain\n1181 SAYCam examples, covering 344 unique GT labels,\n1311 unique distrator labels, and 1660 unique crops. Due\nto manual filtering, its distractor distribution only loosely\nfollows NIH proportions (shown in Figure 8). Similarly,\nwe obtain 346 Ego4D examples over 124 unique GT labels,\n343 unique distractor labels, and 633 unique images (shown\nin Figure 7) with the corresponding distractor distribution\nFigure 7.\nLabel/Image crop uniqueness comparison between\nPicture Vocabulary Test in NIH Baby Toolbox®, SAYCam, and\nEgo4D.\nFigure 8. Distractor type composition for the Picture Vocabulary\nTest in NIH Baby Toolbox®, SAYCam, and Ego4D. The origi-\nnal PVT contains multi-type overlaps, while our sampling assigns\neach distractor a single type even though some satisfy multiple\ncues. Ego4D uses unrelated distractors only as a rare fallback.\nshown in Figure 8.\nExample Prompt\nEach finalized example is a prompt embedded with 4 image\nchoices for which the following is an example:\n\" Touch the image of ’foot’ (A)\n<image> (B) <image> (C) <image>\n(D) <image> \"\nThe model needs to output one of A, B, C, or D to be\nevaluated.\nB.3. Looking While Listening\nOriginal Toolbox Task\nThe Looking while listening test (LwL) from NIH Baby\nToolbox®aims to evaluate comprehension for object label-\ning and receptive language [15]. The infant is shown two\nclipart images which is followed by an audio prompt de-\nscribing one of them. Eye tracking is used to detect whether\nthe participant is looking at the ground-truth image. Similar\nto PVT, we simplify the original metric to accuracy only.\nAdaptation\nTo adapt LwL to our benchmark in SAYCam, We replace\nclipart with naturalistic image crops from SAYCam, and\neye tracking with multiple choice, similar to Picture Vocab-\nulary.\n16", "clean_text": "above a similarity threshold of 0.7 is considered belonging to the same fine-grained category while for Ego4D we use a quantile band [0.997, 0.99973] to also filter out overly similar and thus indistinguishable words. • Coarse-categorical: We use Kmeans clustering based on CLIP text embeddings (SAYCam: K = 100; Ego4D: K = 150). • Phonological: We use Soundex-based string similarity for both datasets. For each CDI anchor r, we select a ground-truth label g ∈Gr and sample three distinct distractors from these pools using the weights. For SAYCam examples, we perform a final round of manual screening to filter out the infeasible examples, while Ego4D examples are filtered with a hybrid procedure combining Gemini2.5-flash checks with a lightweight manual review. Data Collection To produce high-quality 4-way visual choices, we collect image crops corresponding to every target and distractor label. For SAYCam, because Picture Vocabulary requires extremely precise, semantically clear images, we modify the fully automated pipeline in Section B.1 with the following changes: 1. Candidates come directly from Index(g) where g ∈Gr given anchor r. 2. Human annotators manually filter irrelevant, ambiguous, or blurry crops and refine bounding boxes, replacing automated quality scores. This process yields a compact, high-precision crop inventory used for all SAYCam examples. For Ego4D, for the objects, we use the bounding boxes from the visual_crop field of the EgoTracks benchmark, applying a deterministic buffer (1.2× + 8px margin) and requiring a post-buffer normalized area > 0.03. For actions, we use the fho_lta benchmark which contains abundant action annotations. As there are no explicit bounding box annotations, we sample frames from the middle 25% of each action frame interval and apply minimal center-biased cropping to maintain clarity. For each label, we keep 10 candidates while preserving diversity and visual fidelity, and we apply a Gemini2.5-flash pass to eliminate unusable crops. Dataset composition. As shown in Figure 7, We obtain 1181 SAYCam examples, covering 344 unique GT labels, 1311 unique distrator labels, and 1660 unique crops. Due to manual filtering, its distractor distribution only loosely follows NIH proportions (shown in Figure 8). Similarly, we obtain 346 Ego4D examples over 124 unique GT labels, 343 unique distractor labels, and 633 unique images (shown in Figure 7) with the corresponding distractor distribution Figure 7. Label/Image crop uniqueness comparison between Picture Vocabulary Test in NIH Baby Toolbox®, SAYCam, and Ego4D. Figure 8. Distractor type composition for the Picture Vocabulary Test in NIH Baby Toolbox®, SAYCam, and Ego4D. The original PVT contains multi-type overlaps, while our sampling assigns each distractor a single type even though some satisfy multiple cues. Ego4D uses unrelated distractors only as a rare fallback. shown in Figure 8. Example Prompt Each finalized example is a prompt embedded with 4 image choices for which the following is an example: \" Touch the image of ’foot’ (A) <image> (B) <image> (C) <image> (D) <image> \" The model needs to output one of A, B, C, or D to be evaluated. B.3. Looking While Listening Original Toolbox Task The Looking while listening test (LwL) from NIH Baby Toolbox®aims to evaluate comprehension for object labeling and receptive language [15]. The infant is shown two clipart images which is followed by an audio prompt describing one of them. Eye tracking is used to detect whether the participant is looking at the ground-truth image. Similar to PVT, we simplify the original metric to accuracy only. Adaptation To adapt LwL to our benchmark in SAYCam, We replace clipart with naturalistic image crops from SAYCam, and eye tracking with multiple choice, similar to Picture Vocabulary. 16"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 17, "text": "Data collection\nExamples for Looking While Listening are taken directly\nfrom Picture Vocabulary examples.\nExample Prompt\nEach finalized example is a prompt embedded with 2 image\nchoices for which the following is an example:\n\" Touch the image of ’foot’\n(A) <image> (B) <image>\"\nThe model needs to output one of A or B to be evaluated.\nB.4. Localization\nOriginal Toolbox Task\nMuch like Picture Vocabulary,\nthe Mullen Receptive\nLanguage test #19 tests infants on their ability to point at\nsketched target objects as they are named, avoiding con-\nfusing them with the distractor objects. Specifically, after\ngesturing to a group of sketched objects, the psychologist\nasks: Look at these. Where is the cat? If the child points in\nthe direction of the cat, they pass the test.\nAdaptation\nLocalization makes a significant modification to the origi-\nnal NIH Baby Toolbox® measure- In DevCV Toolbox, we\nfind it meaningful to test pointing to objects in their nat-\nuralistic environments, namely, we treat the objects natu-\nrally occurring in the background of the frame as distrac-\ntors rather than inserting unrelated objects. Additionally,\nbecause if is infeasible to ask a model to ’point’, the answer\nchoices are always top left, top right, bottom left, bottom\nright.\nAgain, the objects in this task are real objects from\nSAYCam and Ego4d rather than the sketches used in\nthe NIH Baby Toolbox®, and just like in the NIH Baby\nToolbox®, the prompt is the full frame and the name of the\nobject to be localized.\nData collection\nThe examples for both SAYCam and Ego4d are generated\nusing the centers of the bounding boxes annotated in B.1\nand Ego4d’s egotracks, respectively.\nTo avoid including test examples where a bounding box\nstretches across two answers ambiguously (for example,\nan object in the bottom middle that could reasonably be\ncalled either bottom left or bottom right), we 1) crop each\nframe so that its closest corner is flush with the edges of\nthe object’s bounding box, and 2) enforce a maximum\nbounding box area of 1/4 of the frame’s area (see Figure 9),\nwhich filters out 5.2k of the 7.3k possible test examples.\nIn practice, we find that both of these steps are needed to\nensure fair, reasonably unambiguous examples. We enforce\nno minimum confidence in the SAYCam object annotations\nFigure 9\nand use all object names generated in B.1.\nExample Prompt\nEach finalized example is a prompt embedded with one im-\nage and the same four choices, for which the following is\nan example:\n\"<image>\nPoint at the cup.\nIs it in (A)\nthe top left of the image, (B)\nthe top right, (C) the bottom\nleft, or (D) the bottom right?\"\nThe model needs to output one of top left, top\nright, bottom left, or bottom right to be evalu-\nated.\nB.5. Left/Right\nOriginal Toolbox Task\nLeft/Right is adapted directly from Mullen Visual Recep-\ntion test #29, in which a psychologist shows a child an ob-\nject, then instructs the child to match it with the identical\none. If the child correctly points to the identical object,\n17", "clean_text": "Data collection Examples for Looking While Listening are taken directly from Picture Vocabulary examples. Example Prompt Each finalized example is a prompt embedded with 2 image choices for which the following is an example: \" Touch the image of ’foot’ (A) <image> (B) <image>\" The model needs to output one of A or B to be evaluated. B.4. Localization Original Toolbox Task Much like Picture Vocabulary, the Mullen Receptive Language test #19 tests infants on their ability to point at sketched target objects as they are named, avoiding confusing them with the distractor objects. Specifically, after gesturing to a group of sketched objects, the psychologist asks: Look at these. Where is the cat? If the child points in the direction of the cat, they pass the test. Adaptation Localization makes a significant modification to the original NIH Baby Toolbox® measure- In DevCV Toolbox, we find it meaningful to test pointing to objects in their naturalistic environments, namely, we treat the objects naturally occurring in the background of the frame as distractors rather than inserting unrelated objects. Additionally, because if is infeasible to ask a model to ’point’, the answer choices are always top left, top right, bottom left, bottom right. Again, the objects in this task are real objects from SAYCam and Ego4d rather than the sketches used in the NIH Baby Toolbox®, and just like in the NIH Baby Toolbox®, the prompt is the full frame and the name of the object to be localized. Data collection The examples for both SAYCam and Ego4d are generated using the centers of the bounding boxes annotated in B.1 and Ego4d’s egotracks, respectively. To avoid including test examples where a bounding box stretches across two answers ambiguously (for example, an object in the bottom middle that could reasonably be called either bottom left or bottom right), we 1) crop each frame so that its closest corner is flush with the edges of the object’s bounding box, and 2) enforce a maximum bounding box area of 1/4 of the frame’s area (see Figure 9), which filters out 5.2k of the 7.3k possible test examples. In practice, we find that both of these steps are needed to ensure fair, reasonably unambiguous examples. We enforce no minimum confidence in the SAYCam object annotations Figure 9 and use all object names generated in B.1. Example Prompt Each finalized example is a prompt embedded with one image and the same four choices, for which the following is an example: \"<image> Point at the cup. Is it in (A) the top left of the image, (B) the top right, (C) the bottom left, or (D) the bottom right?\" The model needs to output one of top left, top right, bottom left, or bottom right to be evaluated. B.5. Left/Right Original Toolbox Task Left/Right is adapted directly from Mullen Visual Reception test #29, in which a psychologist shows a child an object, then instructs the child to match it with the identical one. If the child correctly points to the identical object, 17"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 18, "text": "Figure 10\navoiding confusing it with its own mirror image, they pass\nthe test.\nAdaptation\nThe only modification made while adapting VR test #29 to\nDevCV Toolbox is replacing the clipart objects with real\nobjects from SAYCam and Ego4d. In DevCV Toolbox, the\nbasic format is preserved: a prompt image, followed by a\ncorrect answer and two distractor choices in some random\norder, are presented to the model. The target image is a\nduplicate of the prompt, and the incorrect answers are the\nmirror image of the target.\nSome examples in Left/Right are harder than others; we\nconjecture that difficulty in this task can result from either\n1) naturally symmetric objects, or 2) low resolution objects\n(see Figure 10). Naturally symmetric objects are difficult\nbecause they require an encoding of fine-grained details.\nHowever, low resolution objects are difficult because even\nthough there might be some spatial clues to discriminate\nthe target from its mirror image, if have models can’t\nascribe any semantic meaning to the image, they won’t\nencode any semantic meaning to its details. By filtering out\nsmall bounding boxes, we aim to remove the examples that\nare difficult solely due to low resolution.\nData collection\nFor the SAYCam variant, use the object names and bound-\ning boxes generated in B.1. We enforce no minimum or\nmaximum object size, and for the val and test splits, we en-\nforce a minimum confidence in the bounding box of .85. In\nboth variants, all object crops are zero-padded to (640, 480).\nFor the Ego4d variant, we use object names and bound-\ning boxes from the published Ego4d egotracks annotations\nand include only objects that belong to the vocabulary\ndefined in B.1. To remove examples with poor resolution,\nwe require either a minimum bounding box height or width\nof one fifth of the frame, which filters out about half of the\notherwise qualifying examples.\nExample Prompt\nEach finalized example is a prompt embedded with 1 image\nprompt and 3 image choices for which the following is an\nexample:\n\"<image>\nWhich of the following is the\nsame as this?\n(A) <image> (B)\n<image>, or (C) <image>?\"\nThe model needs to output one of A, B, or C to be evaluated.\nB.6. Spatial Details\nOriginal Toolbox Task\nSimilarly, Mullen Visual Reception test #25 also tests\nunderstanding of details in images. In this test, the child\nis presented with a sketch of a tulip, and the psychologist\nasks: See this flower. Find one just like this. Look for it\nhere, while tracing their finger along a page filled with\nsketches of a tulip, a sunflower, a clover, and a daisy. The\nchild is allowed to refer back to the tulip while choosing\ntheir answer. If the child points to the tulip, they pass the\ntest.\nAdaptation\nAgain, the objects in our benchmark are real, cropped ob-\njects from SAYCam and Ego4d rather than clipart, and they\ncome from more categories than just flower.\nAddition-\nally, because the models cannot \"point\" to the choices, the\nchoices are passed as separate images and the correct an-\nswer is the index (A, B, or C) of the matching image.\nOur final modification to the original measure is that\nto make it more difficult for a computer, we present the\nanswer choices in their naturalistic backgrounds rather than\ncropped as in the NIH Baby Toolbox®. In practice, we find\nthe final modification necessary to make Spatial Details\nrequire a fine-grained understanding of detail, as matching\nidentical images is trivial even for a small vision model.\nData collection\nTo construct examples from both SAYCam and Ego4d, we\nmatch objects with the label, but require that they come\nfrom different videos. The labels for each come from B.1\nand egotracks, respectively. Note that the same object can\nappear multiple times within an example- for instance, the\nsame chair, captured in two separate videos, can show up\nas two of the choices. In such cases, the model is forced to\nrely on spatial details such as orientation, perspective, and\nlighting, to match identical occurrences.\nTo ensure quality, we enforce a minimum object con-\nfidence of .92 in the SAYCam annotations.\nTo increase\ndifficulty, we also require that objects have an area of less\nthan half of the frame’s area.\n18", "clean_text": "Figure 10 avoiding confusing it with its own mirror image, they pass the test. Adaptation The only modification made while adapting VR test #29 to DevCV Toolbox is replacing the clipart objects with real objects from SAYCam and Ego4d. In DevCV Toolbox, the basic format is preserved: a prompt image, followed by a correct answer and two distractor choices in some random order, are presented to the model. The target image is a duplicate of the prompt, and the incorrect answers are the mirror image of the target. Some examples in Left/Right are harder than others; we conjecture that difficulty in this task can result from either 1) naturally symmetric objects, or 2) low resolution objects (see Figure 10). Naturally symmetric objects are difficult because they require an encoding of fine-grained details. However, low resolution objects are difficult because even though there might be some spatial clues to discriminate the target from its mirror image, if have models can’t ascribe any semantic meaning to the image, they won’t encode any semantic meaning to its details. By filtering out small bounding boxes, we aim to remove the examples that are difficult solely due to low resolution. Data collection For the SAYCam variant, use the object names and bounding boxes generated in B.1. We enforce no minimum or maximum object size, and for the val and test splits, we enforce a minimum confidence in the bounding box of .85. In both variants, all object crops are zero-padded to (640, 480). For the Ego4d variant, we use object names and bounding boxes from the published Ego4d egotracks annotations and include only objects that belong to the vocabulary defined in B.1. To remove examples with poor resolution, we require either a minimum bounding box height or width of one fifth of the frame, which filters out about half of the otherwise qualifying examples. Example Prompt Each finalized example is a prompt embedded with 1 image prompt and 3 image choices for which the following is an example: \"<image> Which of the following is the same as this? (A) <image> (B) <image>, or (C) <image>?\" The model needs to output one of A, B, or C to be evaluated. B.6. Spatial Details Original Toolbox Task Similarly, Mullen Visual Reception test #25 also tests understanding of details in images. In this test, the child is presented with a sketch of a tulip, and the psychologist asks: See this flower. Find one just like this. Look for it here, while tracing their finger along a page filled with sketches of a tulip, a sunflower, a clover, and a daisy. The child is allowed to refer back to the tulip while choosing their answer. If the child points to the tulip, they pass the test. Adaptation Again, the objects in our benchmark are real, cropped objects from SAYCam and Ego4d rather than clipart, and they come from more categories than just flower. Additionally, because the models cannot \"point\" to the choices, the choices are passed as separate images and the correct answer is the index (A, B, or C) of the matching image. Our final modification to the original measure is that to make it more difficult for a computer, we present the answer choices in their naturalistic backgrounds rather than cropped as in the NIH Baby Toolbox®. In practice, we find the final modification necessary to make Spatial Details require a fine-grained understanding of detail, as matching identical images is trivial even for a small vision model. Data collection To construct examples from both SAYCam and Ego4d, we match objects with the label, but require that they come from different videos. The labels for each come from B.1 and egotracks, respectively. Note that the same object can appear multiple times within an example- for instance, the same chair, captured in two separate videos, can show up as two of the choices. In such cases, the model is forced to rely on spatial details such as orientation, perspective, and lighting, to match identical occurrences. To ensure quality, we enforce a minimum object confidence of .92 in the SAYCam annotations. To increase difficulty, we also require that objects have an area of less than half of the frame’s area. 18"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 19, "text": "Figure 11. Comparison between sources of occlusion. Left: object\nocclusion from a static camera and moving object. Right: object\nocclusion from a moving camera and static object. Each panel\nshows a top-down view of the scene along with the corresponding\nprojected 2D video depicting the occlusion event.\nExample Prompt\nEach finalized example is a prompt embedded with 1 image\nprompt and 3 image choices for which the following is an\nexample:\n\"<image>\nWhich of the following is the\nsame as this?\n(A) <image> (B)\n<image>, or (C) <image>?\"\nThe model needs to output one of A, B, or C to be evaluated.\nB.7. Visual Delayed Response\nOriginal Toolbox Task\nInspired by Visual Delayed Response in the NIH Baby\nToolbox, we introduce an evaluation task designed to\nassess the spatiotemporal reasoning capabilities of vision-\nlanguage models. More specifically, our task focuses on\nobject tracking and spatial localization over time, requiring\nmodels to process multi-frame/video input to infer spatial\ntrajectory and disappearance of a designated object.\nAdaptation\nIn the original NIH Baby Toolbox task, a cartoon creature\nis placed in a frame with grey walls to its left and right. The\ncreature moves from the center of the frame to behind either\nwall, and the child must identify which wall the creature hid\nbehind. (See Figure 12)\nTranslating this task to real-world videos is challenging,\nas the synthetic examples from the toolbox portray an un-\nrealistically ideal scenario. Each toolbox example depicts\na moving object observed from a static camera perspective,\nwith simplified backgrounds and perfectly smooth motion\ntrajectories. Such controlled scenarios are rare in real-world\nfootage, especially in egocentric videos captured from a\ntoddler’s perspective.\nTo address this challenge we exploit the frequent head\nmovements captured in SAYCam footage, together with the\nFigure 12. Example of Visual Delayed Response task, taken di-\nrectly from the NIH Baby Toolbox.\nfact that many objects in real-world scenes are largely sta-\ntionary. By inverting the source of 2D object motion from a\nstatic camera with moving objects to a moving camera with\nstationary objects (see Figure 11), we are able to expand the\ndataset by over an order of magnitude.\nFormally,\nthe model is provided a video V\n=\n{f1, f2, ...fT } and designated key object k. The video de-\npicts the key object k moving within the field of view and\neventually exiting the visible frame at time t∗≤T. The\nmodel’s objective is to predict the exit region r ∈R, where\nR denotes the set of possible frame boundaries through\nwhich the object may leave.\nWe define two variants of this task, which differ in the\nset of selectable exit regions R provided to the model:\n• Multi-choice setting: Rm = {left, right, top, bottom,\ntop-left, top-right, bottom-left, bottom-right}\n• Binary setting: Rb = {correct, opposite}, Rb ⊆Rm\nThe multi-choice variant provides a comprehensive set\nof possible exit regions, where the model is given eight re-\ngions as selectable options. The binary variant is a sim-\nplified version of the task, where the model only chooses\nbetween two options: the correct exit region or the region\ndirectly opposite to it.\nThe overall task can be summarized as a mapping\nfV DR(V, k) →r, where r ∈R. Here, fV DR represents\nthe function that, given a video V and designated key\nobject k, predicts the exit region r ∈R through which the\nobject leaves the frame.\nData Collection\nSAYCam. Collecting examples for the SAYCam variant\nof Visual Delayed Response can be split into 3 stages: fil-\n19", "clean_text": "Figure 11. Comparison between sources of occlusion. Left: object occlusion from a static camera and moving object. Right: object occlusion from a moving camera and static object. Each panel shows a top-down view of the scene along with the corresponding projected 2D video depicting the occlusion event. Example Prompt Each finalized example is a prompt embedded with 1 image prompt and 3 image choices for which the following is an example: \"<image> Which of the following is the same as this? (A) <image> (B) <image>, or (C) <image>?\" The model needs to output one of A, B, or C to be evaluated. B.7. Visual Delayed Response Original Toolbox Task Inspired by Visual Delayed Response in the NIH Baby Toolbox, we introduce an evaluation task designed to assess the spatiotemporal reasoning capabilities of visionlanguage models. More specifically, our task focuses on object tracking and spatial localization over time, requiring models to process multi-frame/video input to infer spatial trajectory and disappearance of a designated object. Adaptation In the original NIH Baby Toolbox task, a cartoon creature is placed in a frame with grey walls to its left and right. The creature moves from the center of the frame to behind either wall, and the child must identify which wall the creature hid behind. (See Figure 12) Translating this task to real-world videos is challenging, as the synthetic examples from the toolbox portray an unrealistically ideal scenario. Each toolbox example depicts a moving object observed from a static camera perspective, with simplified backgrounds and perfectly smooth motion trajectories. Such controlled scenarios are rare in real-world footage, especially in egocentric videos captured from a toddler’s perspective. To address this challenge we exploit the frequent head movements captured in SAYCam footage, together with the Figure 12. Example of Visual Delayed Response task, taken directly from the NIH Baby Toolbox. fact that many objects in real-world scenes are largely stationary. By inverting the source of 2D object motion from a static camera with moving objects to a moving camera with stationary objects (see Figure 11), we are able to expand the dataset by over an order of magnitude. Formally, the model is provided a video V = {f1, f2, ...fT } and designated key object k. The video depicts the key object k moving within the field of view and eventually exiting the visible frame at time t∗≤T. The model’s objective is to predict the exit region r ∈R, where R denotes the set of possible frame boundaries through which the object may leave. We define two variants of this task, which differ in the set of selectable exit regions R provided to the model: • Multi-choice setting: Rm = {left, right, top, bottom, top-left, top-right, bottom-left, bottom-right} • Binary setting: Rb = {correct, opposite}, Rb ⊆Rm The multi-choice variant provides a comprehensive set of possible exit regions, where the model is given eight regions as selectable options. The binary variant is a simplified version of the task, where the model only chooses between two options: the correct exit region or the region directly opposite to it. The overall task can be summarized as a mapping fV DR(V, k) →r, where r ∈R. Here, fV DR represents the function that, given a video V and designated key object k, predicts the exit region r ∈R through which the object leaves the frame. Data Collection SAYCam. Collecting examples for the SAYCam variant of Visual Delayed Response can be split into 3 stages: fil19"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 20, "text": "tering with GPT annotations, filtering with object tracking,\nand manual labeling.\nStage 1. We first use the 1 FPS annotations provided by\nGPT in B.1, where each frame is labeled with a \"key object\"\nand \"objects\" attribute. The \"key object\" denotes a singular\nobject being attended to in a particular frame (if any), and\n\"object\" denotes a list of all visible objects within the frame\nof view. We do an initial filtering for candidate clips by\nusing a sliding window over the 1 FPS frames of each long-\nrange video. For a clip to pass the filter, the first half of\nframes in the window must have the same \"key object\", k.\nIn addition, the second half of frames must not have k listed\nas a \"key object\" or be present in the \"objects\" list. From\nthe 422990 initial clips, 17443 are passed as candidate clips\nto the next stage.\nStage 2. We then perform open-set object detection [55]\nover the 1 FPS frames sampled from each candidate clip,\nwhere the only object class to be detected is the \"key object\"\nitself. An object tracking algorithm [65] is also used to track\nthe \"key object\" over the full fps video. The clips are filtered\naccording to the object tracks, where each track must satisfy\nall of the following:\n–\nStart within the middle 70% of the frame\n–\nAppear in at least 10 consecutive frames\n–\nDisappear for at least 10 consecutive frames before the\nfull clip ends\nTo help account for errors in the object detection/tracking,\nwe purposefully loosen the filters and add additional mea-\nsures for sporadic/false detections. From the 17443 initial\nclips, 3908 are passed as candidate clips to the next stage.\nStage 3. The final stage involves manually reviewing and\nhand-labeling each candidate example from the previous\nstage. We label not only for the ground truth exit direc-\ntion, but also for a variety of annotations related to overall\nquality of the clip. In total, we annotate for camera motion,\nscene visibility, camera stability, occlusion, exit direction,\nand presence of multiple objects. A breakdown for each is\nprovided as follows:\n–\nOcclusion: {Fully Occluded, Partially Occluded, Re-\nmains in View}\n–\nCamera Motion: {Static, Moving}\n–\nDirection of Exit: [Up, Down, Left, Right]\n–\nScene Visibility: {Excellent, Good, Fair, Poor}\n–\nCamera Stability: {Very Stable, Stable, Shaky, Very\nShaky}\n–\nMultiple Objects: {True, False}\nWe then filter for valid high-quality clips according to\nthe following criteria:\n–\nObject must become fully occluded\n–\nDirection of exit cannot be contradicting (both left &\nright, or both up & down)\n–\nScene visibility better than \"Poor\"\n–\nCamera stability better than \"Very Shaky\"\nFigure 13. Visualization of evaluation methods for Visual Delayed\nResponse task. Left: Binary evaluation for the binary setting,\nwhere there is only a correct and opposite incorrect option. Right:\nExact and Adjacent evaluation for the multi-region setting, where\nthe correct region for Exact is defined by only the green region,\nand the correct region for Adjacent is defined by both the green\nand yellow regions.\nFrom the 3908 initial clips, 2380 are passed as final clips\nfor the dataset.\nEgo4D. Data collection for Ego4D follows a very sim-\nilar structure to the SAYCam process, with the addition of\ntracked object annotations being already provided by the\nEgo4D dataset. We use a sliding window over each long-\nrange video’s object tracks and filter for all of the following:\n–\nObject is present in first half of window and disappears\nin second half\n–\nObject bounding box ≥40000 pixels ( 13% of screen)\n–\nStarts within the middle 50% frame\nEach clip is also manually reviewed/labelled according\nto the same procedure as SAYCam data collection\nMulti-frame versions.\nSince the average clip can\nrange from 100-150 frames, we manually create multi-\nframe counterparts to each example.\nMore specifically,\nwe look to obtain 1 representative object frame and 3-9\nlinearly sampled frames that best showcase the object\nmotion/disappearance in a given clip. To do this, we first\nfind the specific frame for three different fields: full object\nframe, start occlusion frame, and end occlusion frame. The\nfull object frame is always used as the first frame in the\nmulti-frame sequence, and shows the key object in clear\nview.\nThe start/end occlusion frames mark the interval\nwith which the key object becomes occluded. A random\nnumber of frames (3-9) are linearly sampled along this in-\nterval to complete the multi-frame sequence for a given clip.\nEvaluation\nEvaluation is performed over three separate variants: Ex-\nact and Adjacent in the multi-choice setting, and Binary\nin the binary setting (see Figure 13). Accuracy is used as\nthe metric for evaluation, defined as the fraction of predic-\ntions considered correct across all trials for a given variant.\nIn the multi-choice setting:\n20", "clean_text": "tering with GPT annotations, filtering with object tracking, and manual labeling. Stage 1. We first use the 1 FPS annotations provided by GPT in B.1, where each frame is labeled with a \"key object\" and \"objects\" attribute. The \"key object\" denotes a singular object being attended to in a particular frame (if any), and \"object\" denotes a list of all visible objects within the frame of view. We do an initial filtering for candidate clips by using a sliding window over the 1 FPS frames of each longrange video. For a clip to pass the filter, the first half of frames in the window must have the same \"key object\", k. In addition, the second half of frames must not have k listed as a \"key object\" or be present in the \"objects\" list. From the 422990 initial clips, 17443 are passed as candidate clips to the next stage. Stage 2. We then perform open-set object detection [55] over the 1 FPS frames sampled from each candidate clip, where the only object class to be detected is the \"key object\" itself. An object tracking algorithm [65] is also used to track the \"key object\" over the full fps video. The clips are filtered according to the object tracks, where each track must satisfy all of the following: – Start within the middle 70% of the frame – Appear in at least 10 consecutive frames – Disappear for at least 10 consecutive frames before the full clip ends To help account for errors in the object detection/tracking, we purposefully loosen the filters and add additional measures for sporadic/false detections. From the 17443 initial clips, 3908 are passed as candidate clips to the next stage. Stage 3. The final stage involves manually reviewing and hand-labeling each candidate example from the previous stage. We label not only for the ground truth exit direction, but also for a variety of annotations related to overall quality of the clip. In total, we annotate for camera motion, scene visibility, camera stability, occlusion, exit direction, and presence of multiple objects. A breakdown for each is provided as follows: – Occlusion: {Fully Occluded, Partially Occluded, Remains in View} – Camera Motion: {Static, Moving} – Direction of Exit: [Up, Down, Left, Right] – Scene Visibility: {Excellent, Good, Fair, Poor} – Camera Stability: {Very Stable, Stable, Shaky, Very Shaky} – Multiple Objects: {True, False} We then filter for valid high-quality clips according to the following criteria: – Object must become fully occluded – Direction of exit cannot be contradicting (both left & right, or both up & down) – Scene visibility better than \"Poor\" – Camera stability better than \"Very Shaky\" Figure 13. Visualization of evaluation methods for Visual Delayed Response task. Left: Binary evaluation for the binary setting, where there is only a correct and opposite incorrect option. Right: Exact and Adjacent evaluation for the multi-region setting, where the correct region for Exact is defined by only the green region, and the correct region for Adjacent is defined by both the green and yellow regions. From the 3908 initial clips, 2380 are passed as final clips for the dataset. Ego4D. Data collection for Ego4D follows a very similar structure to the SAYCam process, with the addition of tracked object annotations being already provided by the Ego4D dataset. We use a sliding window over each longrange video’s object tracks and filter for all of the following: – Object is present in first half of window and disappears in second half – Object bounding box ≥40000 pixels ( 13% of screen) – Starts within the middle 50% frame Each clip is also manually reviewed/labelled according to the same procedure as SAYCam data collection Multi-frame versions. Since the average clip can range from 100-150 frames, we manually create multiframe counterparts to each example. More specifically, we look to obtain 1 representative object frame and 3-9 linearly sampled frames that best showcase the object motion/disappearance in a given clip. To do this, we first find the specific frame for three different fields: full object frame, start occlusion frame, and end occlusion frame. The full object frame is always used as the first frame in the multi-frame sequence, and shows the key object in clear view. The start/end occlusion frames mark the interval with which the key object becomes occluded. A random number of frames (3-9) are linearly sampled along this interval to complete the multi-frame sequence for a given clip. Evaluation Evaluation is performed over three separate variants: Exact and Adjacent in the multi-choice setting, and Binary in the binary setting (see Figure 13). Accuracy is used as the metric for evaluation, defined as the fraction of predictions considered correct across all trials for a given variant. In the multi-choice setting: 20"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 21, "text": "• Exact: Only the labelled ground truth region is counted\nas correct.\n• Adjacent: Both the labelled ground truth region and its\ntwo adjacent regions are counted as correct. This helps\naccount for small ambiguities in the ground truth label.\nIn the binary setting:\n• Binary: A prediction is correct if it matches the \"correct\"\nregion rather than the \"opposite\" region.\nExample Prompt\nEach finalized example includes a series of <image> tags\nor singular <video> tag, followed by the prompt. To be\nproperly evaluated, the model must output exactly one op-\ntion from the choices given in the prompt.\nExample from binary setting with multi-frame input:\n\"<image><image><image><image>\ndoes the bottle leave the frame\nthrough the right side of the\nframe or the left side of\nthe frame?\nrespond ONLY with\n’right’ or ’left’.\"\nExample from multi-choice setting with video input:\n\"<video>\nwhich part of the frame do the\ntoys leave from?\nrespond ONLY\nwith one of:\n’top’, ’bottom’,\n’left’, ’right’, ’top right’,\n’top left’, ’bottom right’, or\n’bottom left’.\"\nB.8. Memory\nOriginal Toolbox Task\nThe Memory task in the NIH Toolbox is designed to mea-\nsure how well toddlers (22–42 months old) learn and re-\nmember new information using a touchscreen.\nChildren\nplay a short game where they “feed” hungry cartoon ani-\nmals by touching them on the screen. The test is divided\ninto the learning phase and the test phase.\n• Learning phase: children see pairs of animals and are\ntold to touch the new animal—the one they have not fed\nbefore. They complete 10 trials and receive feedback so\nthey can learn the rules and memorize the animals seen\nin this phase.\n• Testing phase: children again see pairs of animals and\ntold to touch the new animal, where each old animal from\nthe learning phase appears twice, each time paired with a\ndifferent new animal. They complete 20 trials and receive\nno feedback so correct responses reflect their memory for\nanimals in learning phase.\nThe animals were selected based on how many 24-month\nold infants were familiar with them according to data from\nthe MB-CDI Wordbank. Performance is scored based on\nwhether the child touches the correct animal in the testing\nphase, along with optional reaction time measures to show\nhow quickly they respond.\nAdaptation\nTo simplify the problem and enlarge the potential dataset\nsize, we define the set of word labels used in the learning\nphase as\nWlearn = {w1, w2, . . . , wk},\nwhere each wi corresponds to an image xi ∈Xlearn. These\nimage–label pairs (xi, wi) serve as the stimuli to be memo-\nrized during the learning phase. We further sample 2k ad-\nditional word labels for the testing phase,\nWtest = {wk+1, wk+2, . . . , w3k},\neach associated with a novel image xj ∈Xtest.\nAt each round t, the Vision–Language Model (VLM) re-\nceives an input consisting of two images and a text prompt:\nIt = {xpt, xqt, Pt},\nwhere xpt, xqt are the image inputs and Pt is the corre-\nsponding prompt.\n• Learning phase: The learning phase contains k rounds:\nIlearn\nt\n=\n(\n{x1, P1},\nt = 1,\n{xt−1, xt, Pt},\n2 ≤t ≤k,\nwhere the two images in the second case are presented in\nrandom order. This setup enables the model to incremen-\ntally associate visual concepts across consecutive rounds\nwithin a single context window.\n• Testing phase: The testing phase consists of 2k rounds,\neach comparing a learned stimulus with a new one:\nItest\nt\n= {xi(t), xj(t), P test\nt\n},\nxi(t) ∈Xlearn, xj(t) ∈Xtest.\nHere, xi(t) is a previously seen image and xj(t) a novel\none. The model must identify which image corresponds\nto the new concept described in P test\nt\n.\nEvaluation\nEach learned concept wi ∈Wlearn is paired with two distinct\nnew concepts:\n(wi, wa(i)), (wi, wb(i)),\na(i), b(i) ∈{k+1, . . . , 3k},\na(i) ̸= b(i),\n(1)\nforming two dyads per old stimulus and a total of 2k dyads\nin the testing phase. To mitigate the influence of random\nguessing, an old stimulus wi is considered successfully re-\nmembered only if both of its dyads are answered correctly:\nri =\n(\n1,\nif both dyads for wi are correct,\n0,\notherwise.\n21", "clean_text": "• Exact: Only the labelled ground truth region is counted as correct. • Adjacent: Both the labelled ground truth region and its two adjacent regions are counted as correct. This helps account for small ambiguities in the ground truth label. In the binary setting: • Binary: A prediction is correct if it matches the \"correct\" region rather than the \"opposite\" region. Example Prompt Each finalized example includes a series of <image> tags or singular <video> tag, followed by the prompt. To be properly evaluated, the model must output exactly one option from the choices given in the prompt. Example from binary setting with multi-frame input: \"<image><image><image><image> does the bottle leave the frame through the right side of the frame or the left side of the frame? respond ONLY with ’right’ or ’left’.\" Example from multi-choice setting with video input: \"<video> which part of the frame do the toys leave from? respond ONLY with one of: ’top’, ’bottom’, ’left’, ’right’, ’top right’, ’top left’, ’bottom right’, or ’bottom left’.\" B.8. Memory Original Toolbox Task The Memory task in the NIH Toolbox is designed to measure how well toddlers (22–42 months old) learn and remember new information using a touchscreen. Children play a short game where they “feed” hungry cartoon animals by touching them on the screen. The test is divided into the learning phase and the test phase. • Learning phase: children see pairs of animals and are told to touch the new animal—the one they have not fed before. They complete 10 trials and receive feedback so they can learn the rules and memorize the animals seen in this phase. • Testing phase: children again see pairs of animals and told to touch the new animal, where each old animal from the learning phase appears twice, each time paired with a different new animal. They complete 20 trials and receive no feedback so correct responses reflect their memory for animals in learning phase. The animals were selected based on how many 24-month old infants were familiar with them according to data from the MB-CDI Wordbank. Performance is scored based on whether the child touches the correct animal in the testing phase, along with optional reaction time measures to show how quickly they respond. Adaptation To simplify the problem and enlarge the potential dataset size, we define the set of word labels used in the learning phase as Wlearn = {w1, w2, . . . , wk}, where each wi corresponds to an image xi ∈Xlearn. These image–label pairs (xi, wi) serve as the stimuli to be memorized during the learning phase. We further sample 2k additional word labels for the testing phase, Wtest = {wk+1, wk+2, . . . , w3k}, each associated with a novel image xj ∈Xtest. At each round t, the Vision–Language Model (VLM) receives an input consisting of two images and a text prompt: It = {xpt, xqt, Pt}, where xpt, xqt are the image inputs and Pt is the corresponding prompt. • Learning phase: The learning phase contains k rounds: Ilearn t = ( {x1, P1}, t = 1, {xt−1, xt, Pt}, 2 ≤t ≤k, where the two images in the second case are presented in random order. This setup enables the model to incrementally associate visual concepts across consecutive rounds within a single context window. • Testing phase: The testing phase consists of 2k rounds, each comparing a learned stimulus with a new one: Itest t = {xi(t), xj(t), P test t }, xi(t) ∈Xlearn, xj(t) ∈Xtest. Here, xi(t) is a previously seen image and xj(t) a novel one. The model must identify which image corresponds to the new concept described in P test t . Evaluation Each learned concept wi ∈Wlearn is paired with two distinct new concepts: (wi, wa(i)), (wi, wb(i)), a(i), b(i) ∈{k+1, . . . , 3k}, a(i) ̸= b(i), (1) forming two dyads per old stimulus and a total of 2k dyads in the testing phase. To mitigate the influence of random guessing, an old stimulus wi is considered successfully remembered only if both of its dyads are answered correctly: ri = ( 1, if both dyads for wi are correct, 0, otherwise. 21"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 22, "text": "Figure 14. A sample of our memory task adaptation. We use the MAB-CDI words detected in SAYCam as the images to be memorized.\nThe overall memory accuracy is then computed as\nAccmem = 1\nk\nk\nX\ni=1\nri.\nIn all experiments, we set k = 5, resulting in a total of\n3k = 15 distinct word–image pairs. This design preserves\nthe spirit of the original Toolbox while adapting the\nprocedure to the VLM’s limited context window.\nWhen\ndesigning the evaluation metric, we follow the structure\nof the original Toolbox with appropriate simplifications.\nSpecifically, we remove the original intermediate 6–8\nmin delay settings between the learning and testing\nphases in our benchmark design.\nFuture extensions\nmay incorporate external memory mechanisms such as\nRetrieval-Augmented Generation (RAG), or introduce\nirrelevant contexts between the two phases to simulate\nreal-world temporal gaps. In this work, however, we focus\nexclusively on assessing the model’s in-context retrieval\nability.\nData collection\nFor the scalability of the memory task, we expanded the\nimage set from the cartoon animals in the original Toolbox\nto the objects in the SAYCam dataset, which also ensures\nthat the items are familiar to children. We used a combi-\nnation of annotation-based search scripts and automated\nvision models, including CLIP for object–text similarity\nand SAM for object segmentation as shown in B.1, to find\nand isolate frames where these objects appeared clearly.\nManual screening was also done after auto-filtering. This\nprocess allowed us to gather real-world visual examples\nof common objects seen by young children, supporting\nthe creation of new learning and memory trials for our\nbenchmark.\nThe visual objects collected from SAYCam\ndataset will serve as our stimuli in the memory task.\nExample Prompt\nEach finalized example is a list of prompts each embedded\nwith 2 image choices, for which the following is an exam-\nple:\n\"Let’s try more.\nTouch the new image.\n(A) <image> or (B) <image>.\"\nThe model needs to output one of A or B to be evaluated.\nB.9. Who Has More\nOriginal Toolbox Task\nIn the NIH Baby Toolbox®, the Who Has More Measure is\npoised as a simple narrative: there are two animals; each\nof them is pictured with some number of the same object.\nWhich animal has more?\nAdaptation\nIn DevCV Toolbox, we remove the narrative aspect and\nreplace the clipart objects with naturalistic SAYCam\nand Ego4d objects.\nIn the Naturalistic adaptation, the\nobjects are not necessarily identical and appear in their\nnaturalistic backgrounds;\nin the Synthetic adaptation,\nthe objects are perfectly identical, cropped, and pasted\nonto black backgrounds in matching layouts. The model\nis prompted to identify whether the first or second has more.\nData collection\nIn the synthetic variants, to pick the two quantities to com-\npare, we first sample a number between one and ten. Then,\nfrom the numbers remaining that are lower than the first\none, we sample the second quantity. We do this to ensure\n22", "clean_text": "Figure 14. A sample of our memory task adaptation. We use the MAB-CDI words detected in SAYCam as the images to be memorized. The overall memory accuracy is then computed as Accmem = 1 k k X i=1 ri. In all experiments, we set k = 5, resulting in a total of 3k = 15 distinct word–image pairs. This design preserves the spirit of the original Toolbox while adapting the procedure to the VLM’s limited context window. When designing the evaluation metric, we follow the structure of the original Toolbox with appropriate simplifications. Specifically, we remove the original intermediate 6–8 min delay settings between the learning and testing phases in our benchmark design. Future extensions may incorporate external memory mechanisms such as Retrieval-Augmented Generation (RAG), or introduce irrelevant contexts between the two phases to simulate real-world temporal gaps. In this work, however, we focus exclusively on assessing the model’s in-context retrieval ability. Data collection For the scalability of the memory task, we expanded the image set from the cartoon animals in the original Toolbox to the objects in the SAYCam dataset, which also ensures that the items are familiar to children. We used a combination of annotation-based search scripts and automated vision models, including CLIP for object–text similarity and SAM for object segmentation as shown in B.1, to find and isolate frames where these objects appeared clearly. Manual screening was also done after auto-filtering. This process allowed us to gather real-world visual examples of common objects seen by young children, supporting the creation of new learning and memory trials for our benchmark. The visual objects collected from SAYCam dataset will serve as our stimuli in the memory task. Example Prompt Each finalized example is a list of prompts each embedded with 2 image choices, for which the following is an example: \"Let’s try more. Touch the new image. (A) <image> or (B) <image>.\" The model needs to output one of A or B to be evaluated. B.9. Who Has More Original Toolbox Task In the NIH Baby Toolbox®, the Who Has More Measure is poised as a simple narrative: there are two animals; each of them is pictured with some number of the same object. Which animal has more? Adaptation In DevCV Toolbox, we remove the narrative aspect and replace the clipart objects with naturalistic SAYCam and Ego4d objects. In the Naturalistic adaptation, the objects are not necessarily identical and appear in their naturalistic backgrounds; in the Synthetic adaptation, the objects are perfectly identical, cropped, and pasted onto black backgrounds in matching layouts. The model is prompted to identify whether the first or second has more. Data collection In the synthetic variants, to pick the two quantities to compare, we first sample a number between one and ten. Then, from the numbers remaining that are lower than the first one, we sample the second quantity. We do this to ensure 22"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 23, "text": "a balanced distribution in the differences in numbers being\ncompared for each answer. The objects being compared\ncome from the annotations in B.1 and egotracks for SAY-\nCam and Ego4d, respectively.\nFor the test sets in the naturalistic adaptations, each ex-\nample is hand annotated by two separate human experts to\ncross-validate annotation quality. Specifically, the first hu-\nman expert labels video frames with an object type and the\nnumber of that object. Next, for each the frames that the\nfirst annotator labeled, the second annotator labels the num-\nber of the named object in each, without access to the first’s\nannotation.\nWith both labels for each frame, we construct an exam-\nple for every pair of frames of with objects of the same type\nfor which both annotators would have arrived at the same\nanswer answer as to which has more had they based their\ndecision solely on their count annotation. As an example,\nsay the first annotator labels frame A as having 5 cups, and\nframe B as having 6 cups. If the second annotator labels 5\ncups in frame A and 7 cups in frame B, we construct a Who\nHas More example from frames A and B (despite the anno-\ntators giving frame B two different labels) because 5<7 and\n6<7. However, if the second annotator instead labeled frame\nB as having 5 cups, we do not construct a Who Has More\nexample from frames A and B, because the two annotators\nwould have given different answers for such an example.\nIn constructing Who Has More, we observe that some\nobjects occur in multiples more than others, and each object\nfollows a unique (and usually nonuniform) distribution of\nquantity- for example, the number of hands visible in a\nframe is usually one or two and rarely another number,\nwhile an object like books could reasonably be seen in any\nquantity between one and ten. Additionally, we observe\nthat given the differences in settings and scene perspective,\nthe distributions of object types as well as quantity per\nobject is inherently different for SAYCam and Ego4d.\nExample Prompt\nEach finalized example is a prompt embedded with 2 image\nchoices for which the following is an example:\nWhich of the following has more\nof shoe?\n(A) <image>, or (B)\n<image>?\"\nThe model needs to output one of A or B to be evaluated.\nB.10. Subitizing\nOriginal Toolbox Task\nIn the NIH Baby Toolbox® , the infant sees one to four\ncolored dots for only one second, then an audio prompt\nrequests the number of dots. Importantly, the dots are not\nshown for long enough to be counted one at a time- Subitize\nis intended to measure the ability to quickly identify small\nquantities, without counting.\nAdaptation\nTo construct Subitizing in DevCV Toolbox, we paste\nobjects onto random locations on black frames, in random\nquantities between one and four.\nTo simulate the \"one\nsecond flash\", we insert empty frames before and after the\nframe including the objects.\nData collection\nIn the SAYCam variant, the objects being pasted come from\nframes cropped by the bounding boxes obtained in Section\nB.1, subjected to a minimum confidence of .95.\nIn the\nEgo4d variant, the bounding boxes come from egotracks,\nand only objects in the MAB-CDI vocabulary are included.\nExample Prompt\nEach finalized example is a prompt embedded with 1 blank\nframe, 1 image prompt, and 1 blank frame for which the\nfollowing is an example:\n<image> <image> <image>\nHow many of apple did you see?\nAnswer with 1, 2, 3, or 4.\"\nThe model needs to output one of 1, 2, 3, or 4 to be evalu-\nated.\nB.11. Object Counting\nOriginal Toolbox Task\nIn the NIH Baby Toolbox®, infants are shown some number\nof an object on a screen, and asked to count them. Unlike\nthe Subitize measure, there is no time limit- participants\nhave time to count each item individually.\nAdaptation\nIn DevCV Toolbox, the examples are constructed in the\nsame way as the Subitizingexamples, except the quantities\nare between one and twelve, and there are no blank frames\ncorresponding with the lack of a time limit.\nData collection\nThe data collection for Object Countingis the same as for\nSubitizing.\nExample Prompt\nEach finalized example is a prompt embedded with 1 image\nprompt, for which the following is an example:\n<image>\nHow many of chair did you see?\nAnswer with a number 1-12.\"\nThe model needs to output a number between 1 and 12 to\nbe evaluated.\n23", "clean_text": "a balanced distribution in the differences in numbers being compared for each answer. The objects being compared come from the annotations in B.1 and egotracks for SAYCam and Ego4d, respectively. For the test sets in the naturalistic adaptations, each example is hand annotated by two separate human experts to cross-validate annotation quality. Specifically, the first human expert labels video frames with an object type and the number of that object. Next, for each the frames that the first annotator labeled, the second annotator labels the number of the named object in each, without access to the first’s annotation. With both labels for each frame, we construct an example for every pair of frames of with objects of the same type for which both annotators would have arrived at the same answer answer as to which has more had they based their decision solely on their count annotation. As an example, say the first annotator labels frame A as having 5 cups, and frame B as having 6 cups. If the second annotator labels 5 cups in frame A and 7 cups in frame B, we construct a Who Has More example from frames A and B (despite the annotators giving frame B two different labels) because 5<7 and 6<7. However, if the second annotator instead labeled frame B as having 5 cups, we do not construct a Who Has More example from frames A and B, because the two annotators would have given different answers for such an example. In constructing Who Has More, we observe that some objects occur in multiples more than others, and each object follows a unique (and usually nonuniform) distribution of quantity- for example, the number of hands visible in a frame is usually one or two and rarely another number, while an object like books could reasonably be seen in any quantity between one and ten. Additionally, we observe that given the differences in settings and scene perspective, the distributions of object types as well as quantity per object is inherently different for SAYCam and Ego4d. Example Prompt Each finalized example is a prompt embedded with 2 image choices for which the following is an example: Which of the following has more of shoe? (A) <image>, or (B) <image>?\" The model needs to output one of A or B to be evaluated. B.10. Subitizing Original Toolbox Task In the NIH Baby Toolbox® , the infant sees one to four colored dots for only one second, then an audio prompt requests the number of dots. Importantly, the dots are not shown for long enough to be counted one at a time- Subitize is intended to measure the ability to quickly identify small quantities, without counting. Adaptation To construct Subitizing in DevCV Toolbox, we paste objects onto random locations on black frames, in random quantities between one and four. To simulate the \"one second flash\", we insert empty frames before and after the frame including the objects. Data collection In the SAYCam variant, the objects being pasted come from frames cropped by the bounding boxes obtained in Section B.1, subjected to a minimum confidence of .95. In the Ego4d variant, the bounding boxes come from egotracks, and only objects in the MAB-CDI vocabulary are included. Example Prompt Each finalized example is a prompt embedded with 1 blank frame, 1 image prompt, and 1 blank frame for which the following is an example: <image> <image> <image> How many of apple did you see? Answer with 1, 2, 3, or 4.\" The model needs to output one of 1, 2, 3, or 4 to be evaluated. B.11. Object Counting Original Toolbox Task In the NIH Baby Toolbox®, infants are shown some number of an object on a screen, and asked to count them. Unlike the Subitize measure, there is no time limit- participants have time to count each item individually. Adaptation In DevCV Toolbox, the examples are constructed in the same way as the Subitizingexamples, except the quantities are between one and twelve, and there are no blank frames corresponding with the lack of a time limit. Data collection The data collection for Object Countingis the same as for Subitizing. Example Prompt Each finalized example is a prompt embedded with 1 image prompt, for which the following is an example: <image> How many of chair did you see? Answer with a number 1-12.\" The model needs to output a number between 1 and 12 to be evaluated. 23"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 24, "text": "C. Human survey\nC.1. Small-scale human adult test\nTo confirm the validity of DevCV Toolbox, we collect\nsmall-scale adult performance data on eight of the ten tasks.\nWe omit Looking While Listening and Subitizing as their\nexamples are directly taken from Picture Vocabulary and\nObject Counting, respectively. In total, we have data from\nn=11 adult participants, each completing 10 trials per task\nfor the SAYCam variants of Picture Vocabulary, Localiza-\ntion, Left/Right, Spatial Details, Visual Delayed Response,\nand Object Counting, and 5 trials per task for the SAYCam\nvariants of naturalistic Who Has More and synthetic Who\nHas More, and as well as the Ego4d variants of all tasks\nother than Memory. Participants completed 30 consecu-\ntive rounds of each Memory variant, requiring a maximum\nmemory of 29 distinct images.\nResults for each task can be found in the Human per-\nformance rows of Tables 4 and 9. In summary, our partici-\npants achieved an average accuracy of 93.0 on all SAYCam\ntasks and 93.5 on all Ego4d tasks, for both of which they\nfar outperform any model. From this, we conclude 1) De-\nvCV Toolbox is a valid discriminator of vision FMs with\nadult performance as a strong upper bound, and 2) the SAY-\nCam and Ego4d variants have roughly similar complexity\nand ambiguity for humans.\nC.2. Children Helping Science tests\nTo further examine the developmental fidelity of DevCV\nToolbox, an IRB review process is currently underway to\nextend this survey to a large scale children survey, where\nwe plan to collect response data for each task from children\nof the ages recommended for the corresponding NIH Baby\nToolbox® measure.\nTo this end, we collaborated with expert psychologists\nto develop child-friendly web interfaces for selected tasks\nand prepared them for deployment on the online develop-\nmental research platform Children Helping Science (CHS)\n[49]. CHS is a widely used, home-based platform through\nwhich families can participate in browser-based develop-\nmental studies run by researchers worldwide. By adapting\nour SAYCam-based tasks (PV, VDR and Memory) to CHS,\nwe aim to collect performance from young children under\nconditions analogous to the NIH Baby Toolbox®. At the\ntime of writing, the studies are under review and not yet\nlive. We show two examples of our task UI design in Fig-\nures15 and 16.\nTaking PV as an example (Figure 15), to approximate\nthe modality of the original NIH Baby Toolbox®task, which\nrelies on audio-visual interaction with spoken prompts and\nobserved child responses, we design an audio&video test\npage to verify that instructions and target words can be de-\nlivered clearly via audio and that the child’s webcam setup\nis functioning for basic participation monitoring. The in-\nstruction page provides caregiver-friendly guidance in both\ntext and spoken form. Finally, the trial pages present each\nexample in a clean 2×2 grid of four large image options,\npaired with an audio prompt of the target word, optimizing\nengagement and accessibility for infants and toddlers while\nstaying faithful to the original task format.\nFollowing the PV setup, VDR also has an initial audio\n& video test page, along with an instruction page to provide\ncontext of the experiment to the caregiver. The trial page for\nthis task (see Figure 16) displays the object that should be\ntracked, along with the video clip itself and two selectable\narrows to submit an answer. Since MP4 with interactive dis-\nplay is not yet supported on the website, a GIF is created in\nits place. The beginning 5 seconds of the GIF show the first\nframe with a countdown, then the clip is played as normal\nand followed with another 5 second buffer to show that the\nvideo has ended. To help the caregiver and child understand\nthe experiment, an interactive demo is played as the first 3\ntrials to showcase how each one should be properly done.\nD. Additional experiments & details\nD.1. Out-Of-Domain evaluation\nTo test BabyLLaVA-V2’s capability of generalizing to un-\nseen data domain, we further evaluate it on a set of out-of-\ndomain (OOD) tasks that share the same structure as the\nin-domain benchmarks but differ in their visual domains.\nWe consider two OOD settings: (1) Ego4D-based tasks\nuse egocentric videos from the Ego4D dataset [13], which\nremain first-person and naturalistic but introduce distinct\nenvironments and contexts. (2) BabyToolbox-based tasks\ncorrespond directly to standardized developmental psychol-\nogy and clinical assessments, where the visual stimuli are\nabstract, non-egocentric cartoon images. The detailed test\nresults are reflected in Table 9 and Table 10.\nD.2. Importance of the pretraining stage\nTo evaluate the contribution of the pretraining stage, we\ncompare two variants of BabyLLaVA-V2: (1) the full model\ntrained with Stage 0–2 pretraining before instruction tuning,\nand (2) a randomly initialized model that skips pretraining\nand is trained only with Stage 3. For both variants, we fine-\ntune using different fractions of the instruction dataset and\nevaluate each model on in-domain tasks.\nAs shown in Figure 17, the pretrained model consistently\noutperforms the non-pretrained variant across all data frac-\ntions. The gap is especially pronounced when the instruc-\ntion data is limited, demonstrating that pretraining provides\na strong and sample-efficient initialization for downstream\ninstruction tuning.\nAs the instruction data fraction in-\ncreases, both models improve, reflecting a clear scaling-like\ntrend qualitatively consistent with observations in large-\n24", "clean_text": "C. Human survey C.1. Small-scale human adult test To confirm the validity of DevCV Toolbox, we collect small-scale adult performance data on eight of the ten tasks. We omit Looking While Listening and Subitizing as their examples are directly taken from Picture Vocabulary and Object Counting, respectively. In total, we have data from n=11 adult participants, each completing 10 trials per task for the SAYCam variants of Picture Vocabulary, Localization, Left/Right, Spatial Details, Visual Delayed Response, and Object Counting, and 5 trials per task for the SAYCam variants of naturalistic Who Has More and synthetic Who Has More, and as well as the Ego4d variants of all tasks other than Memory. Participants completed 30 consecutive rounds of each Memory variant, requiring a maximum memory of 29 distinct images. Results for each task can be found in the Human performance rows of Tables 4 and 9. In summary, our participants achieved an average accuracy of 93.0 on all SAYCam tasks and 93.5 on all Ego4d tasks, for both of which they far outperform any model. From this, we conclude 1) DevCV Toolbox is a valid discriminator of vision FMs with adult performance as a strong upper bound, and 2) the SAYCam and Ego4d variants have roughly similar complexity and ambiguity for humans. C.2. Children Helping Science tests To further examine the developmental fidelity of DevCV Toolbox, an IRB review process is currently underway to extend this survey to a large scale children survey, where we plan to collect response data for each task from children of the ages recommended for the corresponding NIH Baby Toolbox® measure. To this end, we collaborated with expert psychologists to develop child-friendly web interfaces for selected tasks and prepared them for deployment on the online developmental research platform Children Helping Science (CHS) [49]. CHS is a widely used, home-based platform through which families can participate in browser-based developmental studies run by researchers worldwide. By adapting our SAYCam-based tasks (PV, VDR and Memory) to CHS, we aim to collect performance from young children under conditions analogous to the NIH Baby Toolbox®. At the time of writing, the studies are under review and not yet live. We show two examples of our task UI design in Figures15 and 16. Taking PV as an example (Figure 15), to approximate the modality of the original NIH Baby Toolbox®task, which relies on audio-visual interaction with spoken prompts and observed child responses, we design an audio&video test page to verify that instructions and target words can be delivered clearly via audio and that the child’s webcam setup is functioning for basic participation monitoring. The instruction page provides caregiver-friendly guidance in both text and spoken form. Finally, the trial pages present each example in a clean 2×2 grid of four large image options, paired with an audio prompt of the target word, optimizing engagement and accessibility for infants and toddlers while staying faithful to the original task format. Following the PV setup, VDR also has an initial audio & video test page, along with an instruction page to provide context of the experiment to the caregiver. The trial page for this task (see Figure 16) displays the object that should be tracked, along with the video clip itself and two selectable arrows to submit an answer. Since MP4 with interactive display is not yet supported on the website, a GIF is created in its place. The beginning 5 seconds of the GIF show the first frame with a countdown, then the clip is played as normal and followed with another 5 second buffer to show that the video has ended. To help the caregiver and child understand the experiment, an interactive demo is played as the first 3 trials to showcase how each one should be properly done. D. Additional experiments & details D.1. Out-Of-Domain evaluation To test BabyLLaVA-V2’s capability of generalizing to unseen data domain, we further evaluate it on a set of out-ofdomain (OOD) tasks that share the same structure as the in-domain benchmarks but differ in their visual domains. We consider two OOD settings: (1) Ego4D-based tasks use egocentric videos from the Ego4D dataset [13], which remain first-person and naturalistic but introduce distinct environments and contexts. (2) BabyToolbox-based tasks correspond directly to standardized developmental psychology and clinical assessments, where the visual stimuli are abstract, non-egocentric cartoon images. The detailed test results are reflected in Table 9 and Table 10. D.2. Importance of the pretraining stage To evaluate the contribution of the pretraining stage, we compare two variants of BabyLLaVA-V2: (1) the full model trained with Stage 0–2 pretraining before instruction tuning, and (2) a randomly initialized model that skips pretraining and is trained only with Stage 3. For both variants, we finetune using different fractions of the instruction dataset and evaluate each model on in-domain tasks. As shown in Figure 17, the pretrained model consistently outperforms the non-pretrained variant across all data fractions. The gap is especially pronounced when the instruction data is limited, demonstrating that pretraining provides a strong and sample-efficient initialization for downstream instruction tuning. As the instruction data fraction increases, both models improve, reflecting a clear scaling-like trend qualitatively consistent with observations in large24"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 25, "text": "Figure 15. User interface design for our CHS-adapted Picture Vocabulary task.\nFigure 16. User Interface design for the trial page of Visual Delayed Response task on CHS.\nFigure 17. DevCV Toolbox overall performance on different in-\nstruction tuning data fraction.\nscale model studies [17, 22].\nThis suggests that data-\ndependent performance gains also exist in compact, devel-\nopmentally inspired models, while pretraining remains a\ncrucial component for achieving data-efficient learning.\nD.3. Synthetic caption generation\nWe study the impact of noisy visual-alignment in the natu-\nralistic child-directed utterances transcribed in the pretrain-\ning dataset by replacing them with video captions gener-\nated by GPT-4o. To encourage diversity in the generated\ncaptions and ensure they remain close to the style of the\noriginal dataset, we include 10 randomly sampled transcrip-\ntions in each prompt. The transcriptions are sampled from\na pool of the 1,000 highest confidence transcriptions in the\noriginal dataset that contain at least one noun and more than\nthree words. These heuristic filters help ensure that the sam-\npled transcriptions contain stylistic information rather than\nsimple phrases that are common in the dataset like \"wow\"\nor \"let’s go\". The pool of 1,000 transcriptions are manually\nscreened to remove uninformative transcriptions that passed\nthe filtering step. The full prompt to GPT-4o is shown in\nFigure 18 and an example of a generated caption is shown\nin Figure 19.\nFigure 18. Full prompt for pretraining data ablation\n25", "clean_text": "Figure 15. User interface design for our CHS-adapted Picture Vocabulary task. Figure 16. User Interface design for the trial page of Visual Delayed Response task on CHS. Figure 17. DevCV Toolbox overall performance on different instruction tuning data fraction. scale model studies [17, 22]. This suggests that datadependent performance gains also exist in compact, developmentally inspired models, while pretraining remains a crucial component for achieving data-efficient learning. D.3. Synthetic caption generation We study the impact of noisy visual-alignment in the naturalistic child-directed utterances transcribed in the pretraining dataset by replacing them with video captions generated by GPT-4o. To encourage diversity in the generated captions and ensure they remain close to the style of the original dataset, we include 10 randomly sampled transcriptions in each prompt. The transcriptions are sampled from a pool of the 1,000 highest confidence transcriptions in the original dataset that contain at least one noun and more than three words. These heuristic filters help ensure that the sampled transcriptions contain stylistic information rather than simple phrases that are common in the dataset like \"wow\" or \"let’s go\". The pool of 1,000 transcriptions are manually screened to remove uninformative transcriptions that passed the filtering step. The full prompt to GPT-4o is shown in Figure 18 and an example of a generated caption is shown in Figure 19. Figure 18. Full prompt for pretraining data ablation 25"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 26, "text": "Table 9. Performance comparison across models on DevCV Toolbox out-of-domain tasks (Ego4D). Different background colors\ndenote different model families. We report accuracy (%) for all tasks.\nModel\nOverall\nCount\nLeftRight\nSpatial\nPV\nMemory\nLocalization\nVisual Delay Response\nWho Has More\nbinary\nmulti-exact\nmulti-adjacent\nsynthetic\nnaturalistic\nUpper Bound\nHuman performance\n93.5\n96.4\n98.2\n96.4\n96.4\n98.8\n90.9\n100\n58.2\n100\n100\n92.7\nProprietary models\nGPT-4o\n67.6\n62.1\n45.1\n94.7\n85.3\n100\n80.4\n45.5\n13.2\n48.3\n84.3\n84.6\nGPT-5\n86.7\n77.5\n88.0\n96.8\n91.9\n100\n88.7\n94.4\n50.3\n82.6\n94.6\n88.5\nGemini-2.5-flash\n77.7\n72.9\n49.6\n86.7\n92.5\n99.2\n88.4\n80.6\n37.1\n70.2\n97.8\n80.1\nGemini-2.5-pro\n88.2\n81.9\n88.0\n94.8\n91.9\n100\n90.2\n91.3\n50.3\n87.9\n96.5\n97.8\nOpen-source models\nLLaVA-OneVision-0.5B\n39.4\n43.9\n32.6\n33.3\n27.7\n22.6\n21.6\n73.0\n15.2\n67.7\n46.8\n49.4\nInternVL3.5-1B\n43.7\n34.7\n34.0\n34.1\n33.8\n24.9\n60.7\n73.9\n16.9\n68.5\n49.0\n49.9\nQwen2.5-VL-3B\n48.1\n35.7\n32.6\n44.1\n41.9\n25.7\n86.7\n79.8\n28.9\n51.1\n50.2\n53.4\nBaby models (Ours)\nBabyLLaVA-V2\n41.1\n33.9\n32.9\n42.4\n29.8\n40.7\n30.0\n55.3\n17.7\n37.1\n86.0\n45.8\nLower Bound\nRandom guess\n31.8\n8.33\n33.3\n33.3\n25.0\n25.0\n25.0\n50.0\n12.5\n37.5\n50.0\n50.0\nTable 10. Performance on NIH Baby Toolbox out-of-domain\ntasks. We report the #correct/#total for all tasks.\nModel\nWho Has More\nCount\nMullen Visual Reception\nBabyLLaVA-V2\n13/24\n2/6\n3/12\nFigure 19. Example of caption generated by GPT-4o\nTable 11. Comparison between Gemini-2.5-flash performance\nwith different prompting strategies\nPrompt Type\nCount\nLeftRight\nstandard\n69\n55\none-shot\n66\n82\nalternate prompt 1\n55\n54\nalternate prompt 2\n67\n56\nD.4. Prompting Experiment\nFinally, we complete a prompting experiment to show\nthe stability of DevCV Toolbox examples with respect to\ncommercial models, the results of which are shown in Table\n11.\nWe select Left/Right and Object Counting for this\nexperiment, as we found that commercial models had the\nlowest and most variable performance on these. For both\ntasks, 100 examples are randomly selected and presented\nto Gemini-2.5-flash with a standard prompt, a one-shot\nprompt, and two variations of the standard prompt, called\nalternate prompt 1 and alternate prompt 2. The standard\nprompt is the one used in all other experiments, and the one\nshot-prompt is a prompt that includes one other example,\nwith its correct answer, prepended to the standard prompt.\nFor Object Counting, alternate prompt 1 does not\ngive the object’s name to be counted, e.g.\n\"<image>\nHow many objects do you see?\",\nwhich\nwe\nsee drops performance, which is intuitive because large\nmodels thrive on context, in this case the name of the\nobject to be counted.\nAlternate prompt 2 gives more\ndetail,\ne.g.\n<image> count the flora very\nclosely, starting from one.\nKeep track\nof which ones have already been counted\nand what number you’ve counted to thus\nfar.\nThen, report how many flora you\ncounted.\".\nUnsurprisingly, alternate prompt 2 does\nnot improve performance, showing that 1) the standard\nprompt was sufficient and 2) Gemini-2.5-flash has capable\ninstruction-following capabilities. For Object Counting, we\nfind that a one-shot prompt does not boost performance.\nFor\nLeft/Right,\nthe\nstandard\nprompt\ngives\neach\nimage\ntoken\ninterleaved\nwith\ntheir\nanswer\nlabels,\ne.g.\n\"<image> Which of the following\n26", "clean_text": "Table 9. Performance comparison across models on DevCV Toolbox out-of-domain tasks (Ego4D). Different background colors denote different model families. We report accuracy (%) for all tasks. Model Overall Count LeftRight Spatial PV Memory Localization Visual Delay Response Who Has More binary multi-exact multi-adjacent synthetic naturalistic Upper Bound Human performance 93.5 96.4 98.2 96.4 96.4 98.8 90.9 100 58.2 100 100 92.7 Proprietary models GPT-4o 67.6 62.1 45.1 94.7 85.3 100 80.4 45.5 13.2 48.3 84.3 84.6 GPT-5 86.7 77.5 88.0 96.8 91.9 100 88.7 94.4 50.3 82.6 94.6 88.5 Gemini-2.5-flash 77.7 72.9 49.6 86.7 92.5 99.2 88.4 80.6 37.1 70.2 97.8 80.1 Gemini-2.5-pro 88.2 81.9 88.0 94.8 91.9 100 90.2 91.3 50.3 87.9 96.5 97.8 Open-source models LLaVA-OneVision-0.5B 39.4 43.9 32.6 33.3 27.7 22.6 21.6 73.0 15.2 67.7 46.8 49.4 InternVL3.5-1B 43.7 34.7 34.0 34.1 33.8 24.9 60.7 73.9 16.9 68.5 49.0 49.9 Qwen2.5-VL-3B 48.1 35.7 32.6 44.1 41.9 25.7 86.7 79.8 28.9 51.1 50.2 53.4 Baby models (Ours) BabyLLaVA-V2 41.1 33.9 32.9 42.4 29.8 40.7 30.0 55.3 17.7 37.1 86.0 45.8 Lower Bound Random guess 31.8 8.33 33.3 33.3 25.0 25.0 25.0 50.0 12.5 37.5 50.0 50.0 Table 10. Performance on NIH Baby Toolbox out-of-domain tasks. We report the #correct/#total for all tasks. Model Who Has More Count Mullen Visual Reception BabyLLaVA-V2 13/24 2/6 3/12 Figure 19. Example of caption generated by GPT-4o Table 11. Comparison between Gemini-2.5-flash performance with different prompting strategies Prompt Type Count LeftRight standard 69 55 one-shot 66 82 alternate prompt 1 55 54 alternate prompt 2 67 56 D.4. Prompting Experiment Finally, we complete a prompting experiment to show the stability of DevCV Toolbox examples with respect to commercial models, the results of which are shown in Table 11. We select Left/Right and Object Counting for this experiment, as we found that commercial models had the lowest and most variable performance on these. For both tasks, 100 examples are randomly selected and presented to Gemini-2.5-flash with a standard prompt, a one-shot prompt, and two variations of the standard prompt, called alternate prompt 1 and alternate prompt 2. The standard prompt is the one used in all other experiments, and the one shot-prompt is a prompt that includes one other example, with its correct answer, prepended to the standard prompt. For Object Counting, alternate prompt 1 does not give the object’s name to be counted, e.g. \"<image> How many objects do you see?\", which we see drops performance, which is intuitive because large models thrive on context, in this case the name of the object to be counted. Alternate prompt 2 gives more detail, e.g. <image> count the flora very closely, starting from one. Keep track of which ones have already been counted and what number you’ve counted to thus far. Then, report how many flora you counted.\". Unsurprisingly, alternate prompt 2 does not improve performance, showing that 1) the standard prompt was sufficient and 2) Gemini-2.5-flash has capable instruction-following capabilities. For Object Counting, we find that a one-shot prompt does not boost performance. For Left/Right, the standard prompt gives each image token interleaved with their answer labels, e.g. \"<image> Which of the following 26"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 27, "text": "is the same as this?\n(A) <image>\n(B) <image> (C) <image>\".\nIn\nalternate\nprompt\n1,\nwe\nundo\nthis\ninterleaving,\nresulting\nin\n\"<image><image><image><image> Which\nof the following is the same as the\nfirst one?\n(A) the second one, (B) the\nthird one, or (C) the fourth one?\".\nIn\nalternate prompt 2, we interleave even more, by giving\nsome descriptive text before the prompt image,\ne.g.\n\"Here is an image:\n<image>.\nWhich of\nthe following is the same as it?\n(A)\n<image>, (B) <image>, or (C) <image>?\".\nIntuitively we expect alternate prompt 2 to be the easiest,\nalternate prompt 1 to be the hardest, and the standard\nprompt to fall in between.\nHowever, we find that none\nof these prompts elicits significantly different perfor-\nmance, however, the one-shot prompt significantly boosts\nperformance.\nThese two findings show the robustness\nof Gemini-2.5-flash, and the complexity of Left/Right,\nrespectively.\n27", "clean_text": "is the same as this? (A) <image> (B) <image> (C) <image>\". In alternate prompt 1, we undo this interleaving, resulting in \"<image><image><image><image> Which of the following is the same as the first one? (A) the second one, (B) the third one, or (C) the fourth one?\". In alternate prompt 2, we interleave even more, by giving some descriptive text before the prompt image, e.g. \"Here is an image: <image>. Which of the following is the same as it? (A) <image>, (B) <image>, or (C) <image>?\". Intuitively we expect alternate prompt 2 to be the easiest, alternate prompt 1 to be the hardest, and the standard prompt to fall in between. However, we find that none of these prompts elicits significantly different performance, however, the one-shot prompt significantly boosts performance. These two findings show the robustness of Gemini-2.5-flash, and the complexity of Left/Right, respectively. 27"}
