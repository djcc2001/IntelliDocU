{"pdf_id": "arxiv_251210954_group_diffusion", "page": 1, "text": "Group Diffusion: Enhancing Image Generation\nby Unlocking Cross-Sample Collaboration\nSicheng Mo1 Thao Nguyen2 Richard Zhang3 Nick Kolkin3 Siddharth Srinivasan Iyer3\nEli Shechtman3 Krishna Kumar Singh3 Yong Jae Lee3 Bolei Zhou1 Yuheng Li3\n1University of California, Los Angeles\n2University of Wisconsin‚ÄìMadison\n3Adobe Research\nhttps://sichengmo.github.io/GroupDiff/\nAbstract\nIn this work, we explore an untapped signal in diffusion\nmodel inference. While all previous methods generate im-\nages independently at inference, we instead ask if samples\ncan be generated collaboratively. We propose Group Diffu-\nsion, unlocking the attention mechanism to be shared across\nimages, rather than limited to just the patches within an im-\nage. This enables images to be jointly denoised at inference\ntime, learning both intra and inter-image correspondence.\nWe observe a clear scaling effect ‚Äì larger group sizes yield\nstronger cross-sample attention and better generation qual-\nity.\nFurthermore, we introduce a qualitative measure to\ncapture this behavior and show that its strength closely cor-\nrelates with FID. Built on standard diffusion transformers,\nour GroupDiff achieves up to 32.2% FID improvement on\nImageNet-256√ó256. Our work reveals cross-sample infer-\nence as an effective, previously unexplored mechanism for\ngenerative modeling.\n1. Introduction\n‚ÄúAlone we can do so little;\ntogether we can do so much.‚Äù\nHelen Keller\nDuring generative model training, network weights are\noptimized using batches of images to learn an underlying\nimage distribution [6, 12, 16, 22, 36, 44]. However, at in-\nference time, images are typically generated independently.\nWhile patches within an image can interact to produce a co-\nherent output, patches across different images are processed\nseparately. This raises an intriguing, unexplored question ‚Äì\ncan images and patches across a batch collaborate to en-\nhance generation quality collectively?\nFollowing our inquiry, we introduce Group Diffusion,\nwhich jointly denoises a group of samples with the same\nExample \ngeneration\nCo-generations \nwithin group\nGroup Diffusion\n(Ours)\n1\nFID: 3.50\nFID: 2.92\nFID: 2.42\nFID: 2.14\nIncreasing \ngroup size\n2\n4\n8\nIndividual Diffusion\n(baseline)\nFigure 1. In standard diffusion (top row), samples are generated\nindependently. Our GroupDiff uses cross-sample attention, en-\nabling samples within a batch to collaborate on a generation. We\nshow selected examples of class-conditional ImageNet generation,\nusing group sizes {1, 2, 4, 8}. We find that the average generation\nquality improves with larger group size.\nconditioning. This is enabled using bidirectional attention\nacross samples. During training, we construct each group\nby querying semantically or visually similar samples from\nthe training dataset, allowing the attention mechanism to\nsee all patches from within the group. Then, at test time,\nwe generate images in a batch, allowing images within the\nbatch to aid one another in the diffusion process.\nWe observe a clear scaling effect, where increasing the\ngroup size strengthens cross-sample attention and consis-\ntently improves generation quality, as illustrated in Figure 1.\nWe further analyze the attention patterns across images. As\nshown in Figure 2, the group-wise denoising enables each\npatch to attend to others within the group, allowing the\n1\narXiv:2512.10954v1  [cs.CV]  11 Dec 2025", "clean_text": "Group Diffusion: Enhancing Image Generation by Unlocking Cross-Sample Collaboration Sicheng Mo1 Thao Nguyen2 Richard Zhang3 Nick Kolkin3 Siddharth Srinivasan Iyer3 Eli Shechtman3 Krishna Kumar Singh3 Yong Jae Lee3 Bolei Zhou1 Yuheng Li3 1University of California, Los Angeles 2University of Wisconsin‚ÄìMadison 3Adobe Research https://sichengmo.github.io/GroupDiff/ Abstract In this work, we explore an untapped signal in diffusion model inference. While all previous methods generate images independently at inference, we instead ask if samples can be generated collaboratively. We propose Group Diffusion, unlocking the attention mechanism to be shared across images, rather than limited to just the patches within an image. This enables images to be jointly denoised at inference time, learning both intra and inter-image correspondence. We observe a clear scaling effect ‚Äì larger group sizes yield stronger cross-sample attention and better generation quality. Furthermore, we introduce a qualitative measure to capture this behavior and show that its strength closely correlates with FID. Built on standard diffusion transformers, our GroupDiff achieves up to 32.2% FID improvement on ImageNet-256√ó256. Our work reveals cross-sample inference as an effective, previously unexplored mechanism for generative modeling. 1. Introduction ‚ÄúAlone we can do so little; together we can do so much.‚Äù Helen Keller During generative model training, network weights are optimized using batches of images to learn an underlying image distribution [6, 12, 16, 22, 36, 44]. However, at inference time, images are typically generated independently. While patches within an image can interact to produce a coherent output, patches across different images are processed separately. This raises an intriguing, unexplored question ‚Äì can images and patches across a batch collaborate to enhance generation quality collectively? Following our inquiry, we introduce Group Diffusion, which jointly denoises a group of samples with the same Example generation Co-generations within group Group Diffusion (Ours) 1 FID: 3.50 FID: 2.92 FID: 2.42 FID: 2.14 Increasing group size 2 4 8 Individual Diffusion (baseline) Figure 1. In standard diffusion (top row), samples are generated independently. Our GroupDiff uses cross-sample attention, enabling samples within a batch to collaborate on a generation. We show selected examples of class-conditional ImageNet generation, using group sizes {1, 2, 4, 8}. We find that the average generation quality improves with larger group size. conditioning. This is enabled using bidirectional attention across samples. During training, we construct each group by querying semantically or visually similar samples from the training dataset, allowing the attention mechanism to see all patches from within the group. Then, at test time, we generate images in a batch, allowing images within the batch to aid one another in the diffusion process. We observe a clear scaling effect, where increasing the group size strengthens cross-sample attention and consistently improves generation quality, as illustrated in Figure 1. We further analyze the attention patterns across images. As shown in Figure 2, the group-wise denoising enables each patch to attend to others within the group, allowing the 1 arXiv:2512.10954v1 [cs.CV] 11 Dec 2025"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 2, "text": "Class 555: fire engine, fire truck\nClass 980: volcano\nClass 88: macaw\nClass 208: golden retriever\nFigure 2. Attention map visualization. We show the attention map, using the query point starred on the left, across samples (group size\n4), from the second layer. The star refers to the anchor patch. High attention score patches are denoted in red. During the generation\nprocess, each image patch is encouraged to attend to similar patches from other images, which enhances the generation quality.\nmodel to learn both intra and inter-image correspondence.\nInterestingly, we show that generation quality is largely de-\ntermined by how attention is distributed across samples,\nwith the model assigning higher weights to semantically\nrelevant samples that exert a stronger influence on the fi-\nnal output. We additionally identify a qualitative measure\nof cross-sample attention whose strength correlates closely\nwith generation quality, providing deeper insight into how\ngroup-wise interaction governs the generation process.\nWe summarize our contribution as follows:\n(1) We\npresent GroupDiff, a simple yet effective framework that\njointly denoise a group of samples with the same condi-\ntion rather than individual images, enabling cross-sample\ninteraction through attention.\n(2) A systematic study on\nGroupDiff training and inference behavior, offering insights\nfor better leveraging inter-sample correspondence in image\ngeneration. (3) Our framework improves generation qual-\nity and flexibility over traditional systems; e.g., integrat-\ning GroupDiff with SiT yields 20.9% and 32.2% better FID\nwhen trained from scratch and resumed from a pre-trained\ncheckpoint, respectively.\n2. Related Work\nDiffusion models. Powered by their ability to model com-\nplex distributions via iterative denoising, diffusion mod-\nels have become the leading paradigm for high-fidelity im-\nage [9, 16, 42, 44, 45], video [17, 24, 32, 37, 58, 66, 71]\nand multi-modal concept [34, 47, 72] generation.\nBe-\nsides relying solely on the diffusion objective, recent litera-\nture [26, 62, 67, 70] explores the alignment between gener-\native modeling and representation learning. REPA [67] ac-\ncelerates diffusion model training by aligning its represen-\ntation with the pretrained SSL models. REPA-E [26] further\nleverages the pretrained model‚Äôs knowledge with additional\nlearnable parameters from the latent encoder.\nMeanwhile, another line of work addresses this potential\nlimitation from the pre-trained vision encoder by aligning\ncross-layer features to each other (SRA [20]) or explicitly\napplying SSL object function on generative model represen-\ntation (Dispersive Loss [59]). In contrast, GroupDiff learns\na stronger representation implicitly by allowing group at-\ntention to learn both inter and intra-image correspondence.\nThis novel approach offers a fresh perspective on integrat-\ning diffusion modeling with representation learning.\nSemantic correspondence in diffusion models. Semantic\ncorrespondence maps semantically related regions across\nimages, enabling alignment despite changes in appearance\nor pose. In addition to its state-of-the-art generation ca-\npability, a large-scale pre-trained text-to-image diffusion\nmodel [9, 43, 45] naturally captures such semantic cor-\nrespondence robustly, which unlocks promising applica-\ntions in classification [27] and segmentation [51, 54, 63]\nwith such features. Meanwhile, a line of works [30, 69]\nextract high-quality representation from the denoiser by\nadding different levels of noise and enabling robust cross-\nimage point matching. Follow-up work leverages the global\nlevel dense semantic correspondence for image-to-image\ntranslation [8, 29, 33, 35, 56] method without additional\ntraining. Furthermore, there is another line of work that\ngoes beyond single-image generation to multi-view genera-\ntion [18], style-controlled group generation [48], and video\ngeneration [21], by modeling inter-image correspondence\nwith mutual attention. Different from the aforementioned\nliterature, our method explicitly leverages cross-sample re-\nlationships to enhance individual sample‚Äôs quality by jointly\ndenoising all images within a group together, instead of im-\nplicitly learning it from individual samples.\nUnified transformer models.\nTransformer models [57]\nhave unified domain-specific architecture design across lan-\nguage, vision, and audio. It first showcased its strong capa-\nbility on encoder-decoder and later decoder-only language\nmodels in the language domain. ViT [7] proposed to con-\n2", "clean_text": "Class 555: fire engine, fire truck Class 980: volcano Class 88: macaw Class 208: golden retriever Figure 2. Attention map visualization. We show the attention map, using the query point starred on the left, across samples (group size 4), from the second layer. The star refers to the anchor patch. High attention score patches are denoted in red. During the generation process, each image patch is encouraged to attend to similar patches from other images, which enhances the generation quality. model to learn both intra and inter-image correspondence. Interestingly, we show that generation quality is largely determined by how attention is distributed across samples, with the model assigning higher weights to semantically relevant samples that exert a stronger influence on the final output. We additionally identify a qualitative measure of cross-sample attention whose strength correlates closely with generation quality, providing deeper insight into how group-wise interaction governs the generation process. We summarize our contribution as follows: (1) We present GroupDiff, a simple yet effective framework that jointly denoise a group of samples with the same condition rather than individual images, enabling cross-sample interaction through attention. (2) A systematic study on GroupDiff training and inference behavior, offering insights for better leveraging inter-sample correspondence in image generation. (3) Our framework improves generation quality and flexibility over traditional systems; e.g., integrating GroupDiff with SiT yields 20.9% and 32.2% better FID when trained from scratch and resumed from a pre-trained checkpoint, respectively. 2. Related Work Diffusion models. Powered by their ability to model complex distributions via iterative denoising, diffusion models have become the leading paradigm for high-fidelity image [9, 16, 42, 44, 45], video [17, 24, 32, 37, 58, 66, 71] and multi-modal concept [34, 47, 72] generation. Besides relying solely on the diffusion objective, recent literature [26, 62, 67, 70] explores the alignment between generative modeling and representation learning. REPA [67] accelerates diffusion model training by aligning its representation with the pretrained SSL models. REPA-E [26] further leverages the pretrained model‚Äôs knowledge with additional learnable parameters from the latent encoder. Meanwhile, another line of work addresses this potential limitation from the pre-trained vision encoder by aligning cross-layer features to each other (SRA [20]) or explicitly applying SSL object function on generative model representation (Dispersive Loss [59]). In contrast, GroupDiff learns a stronger representation implicitly by allowing group attention to learn both inter and intra-image correspondence. This novel approach offers a fresh perspective on integrating diffusion modeling with representation learning. Semantic correspondence in diffusion models. Semantic correspondence maps semantically related regions across images, enabling alignment despite changes in appearance or pose. In addition to its state-of-the-art generation capability, a large-scale pre-trained text-to-image diffusion model [9, 43, 45] naturally captures such semantic correspondence robustly, which unlocks promising applications in classification [27] and segmentation [51, 54, 63] with such features. Meanwhile, a line of works [30, 69] extract high-quality representation from the denoiser by adding different levels of noise and enabling robust crossimage point matching. Follow-up work leverages the global level dense semantic correspondence for image-to-image translation [8, 29, 33, 35, 56] method without additional training. Furthermore, there is another line of work that goes beyond single-image generation to multi-view generation [18], style-controlled group generation [48], and video generation [21], by modeling inter-image correspondence with mutual attention. Different from the aforementioned literature, our method explicitly leverages cross-sample relationships to enhance individual sample‚Äôs quality by jointly denoising all images within a group together, instead of implicitly learning it from individual samples. Unified transformer models. Transformer models [57] have unified domain-specific architecture design across language, vision, and audio. It first showcased its strong capability on encoder-decoder and later decoder-only language models in the language domain. ViT [7] proposed to con2"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 3, "text": "Individual\ndiffusion\n(baseline)\nGroup\nDiffusion\n(ours)\nReshape\nMulti-Head \nSelf-Attention\nPointwise \nFeedforward\nùëê\nInput\ntokens\nConditioning\npatches\nbatch\nReshape\nGroup Attention Operation\nMLP\nOutput\ntokens\nFigure 3. Approach. (Left) Previous approaches generate images independently. We explore Group Diffusion, which allows a set of\nimages to collaborate together during inference time. (Right) Group attention can be implemented simply by reshaping the tokens within\na batch, before and after the attention operation.\nvert images to a series of smaller patches to adapt the trans-\nformer model to the vision field and find its remarkable\nscaling capabilities under increasing data, training compute,\nand data. In the image generative model field, Diffusion\nTransformer [40] firstly verified the outstanding scalability\nof such an architecture, and a similar model design has been\nfurther extended to video diffusion models in [2, 24, 58].\nMoreover, multi-modal models [34, 47, 53, 72] with unified\ntransformer again verified the generaizability of such archi-\ntecture. GroupDiff benefits from the flexibility of the uni-\nfied transformer model design by adding multi-image gen-\neration capability to the image generative model.\n3. Group Diffusion\n3.1. Preliminary\nDiffusion\nmodels\ngradually\nreverse\nthe\nprocess\nof\nadding noise to an image, starting from a noise vec-\ntor xT and progressively generating less noisy samples\nxT ‚àí1, xT ‚àí2, ..., x0 with learned denoising function eŒ∏.\nThe training objective aims to minimize the difference\nbetween the predicted and true noise. Specifically, for each\ntime step t, the objective is to solve the following denoising\nproblem on the image data x:\nLDM = Ex,œµ‚àºN(0,I),t\n\u0002\n‚à•œµ ‚àíeŒ∏(xt; t, c)‚à•2\n2\n\u0003\n,\n(1)\nwhere xt is the noisy image at time step t, uniformly sam-\npled from {1, . . . , T}, and eŒ∏(xt, t, c) is the denoising func-\ntion that predicts the noise added to xt conditioned on the\ntime step t and context c (often a text prompt or class label).\nClassifier-free diffusion guidance [15] enables control-\nling the trade-off between sample quality and diversity in\ndiffusion models. It shifts pŒ∏(c|xt) to assign a higher likeli-\nhood to the condition c without additional classifier. This is\nimplemented by training the diffusion model for both con-\nditional and unconditional denoising and combining the two\nscore estimates at inference time. Specifically, at inference\ntime, the modified score estimate ÀúeŒ∏(xt, c) is extrapolated\nin the direction towards the conditional eŒ∏(xt, c) and away\nfrom the unconditional eŒ∏(xt, ‚àÖ).\nÀúeŒ∏(xt; t, c) = eŒ∏(xt; t, c) + s ¬∑\n\u0000eŒ∏(xt; t, c) ‚àíeŒ∏(x;t, ‚àÖ)\n\u0001\n(2)\n3.2. Approach\nAt the core of our method is the idea of generating multi-\nple images together, so each sample can enhance its gener-\nation by selectively learning from other samples, as illus-\ntrated in Figure 2. In our GroupDiff, we construct a group\nwith related image data, thus allowing the diffusion model\nto learn a better representation that can be aided by other\nsamples. At test time, we generate multiple images, condi-\ntioned on the same conditioning c, a setup that aligns well\nwith modern applications, where users typically expect sev-\neral outputs under the same condition. We follow best prac-\ntices, adopting the Diffusion Transformer (DiT [40]) model\narchitecture, which uses an attention mechanism between\npatches within an image. We simply modify the attention\nby concatenating the group of image patches together, so\nthat each patch can take other samples into consideration.\nTo ensure that the diffusion model can recognize different\nimage samples, we add the same learnable sample embed-\nding to all patches from a given image. We formally define\nthe GroupDiff method as follows.\nQuery method. Our hypothesis for GroupDiff is that im-\nages in the same group are related either semantically or\nvisually, and can be used to aid in the denoising process.\n3", "clean_text": "Individual diffusion (baseline) Group Diffusion (ours) Reshape Multi-Head Self-Attention Pointwise Feedforward ùëê Input tokens Conditioning patches batch Reshape Group Attention Operation MLP Output tokens Figure 3. Approach. (Left) Previous approaches generate images independently. We explore Group Diffusion, which allows a set of images to collaborate together during inference time. (Right) Group attention can be implemented simply by reshaping the tokens within a batch, before and after the attention operation. vert images to a series of smaller patches to adapt the transformer model to the vision field and find its remarkable scaling capabilities under increasing data, training compute, and data. In the image generative model field, Diffusion Transformer [40] firstly verified the outstanding scalability of such an architecture, and a similar model design has been further extended to video diffusion models in [2, 24, 58]. Moreover, multi-modal models [34, 47, 53, 72] with unified transformer again verified the generaizability of such architecture. GroupDiff benefits from the flexibility of the unified transformer model design by adding multi-image generation capability to the image generative model. 3. Group Diffusion 3.1. Preliminary Diffusion models gradually reverse the process of adding noise to an image, starting from a noise vector xT and progressively generating less noisy samples xT ‚àí1, xT ‚àí2, ..., x0 with learned denoising function eŒ∏. The training objective aims to minimize the difference between the predicted and true noise. Specifically, for each time step t, the objective is to solve the following denoising problem on the image data x: LDM = Ex,œµ‚àºN(0,I),t \u0002 ‚à•œµ ‚àíeŒ∏(xt; t, c)‚à•2 2 \u0003 , (1) where xt is the noisy image at time step t, uniformly sampled from {1, . . . , T}, and eŒ∏(xt, t, c) is the denoising function that predicts the noise added to xt conditioned on the time step t and context c (often a text prompt or class label). Classifier-free diffusion guidance [15] enables controlling the trade-off between sample quality and diversity in diffusion models. It shifts pŒ∏(c|xt) to assign a higher likelihood to the condition c without additional classifier. This is implemented by training the diffusion model for both conditional and unconditional denoising and combining the two score estimates at inference time. Specifically, at inference time, the modified score estimate ÀúeŒ∏(xt, c) is extrapolated in the direction towards the conditional eŒ∏(xt, c) and away from the unconditional eŒ∏(xt, ‚àÖ). ÀúeŒ∏(xt; t, c) = eŒ∏(xt; t, c) + s ¬∑ \u0000eŒ∏(xt; t, c) ‚àíeŒ∏(x;t, ‚àÖ) \u0001 (2) 3.2. Approach At the core of our method is the idea of generating multiple images together, so each sample can enhance its generation by selectively learning from other samples, as illustrated in Figure 2. In our GroupDiff, we construct a group with related image data, thus allowing the diffusion model to learn a better representation that can be aided by other samples. At test time, we generate multiple images, conditioned on the same conditioning c, a setup that aligns well with modern applications, where users typically expect several outputs under the same condition. We follow best practices, adopting the Diffusion Transformer (DiT [40]) model architecture, which uses an attention mechanism between patches within an image. We simply modify the attention by concatenating the group of image patches together, so that each patch can take other samples into consideration. To ensure that the diffusion model can recognize different image samples, we add the same learnable sample embedding to all patches from a given image. We formally define the GroupDiff method as follows. Query method. Our hypothesis for GroupDiff is that images in the same group are related either semantically or visually, and can be used to aid in the denoising process. 3"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 4, "text": "Thus, we must construct sets of images that are related dur-\ning training time. Given the image x ‚ààRH√óW √ó3 and the\nentire image dataset D, we define the query function q(x)\nas the following:\nq(x; D; œÑimg) = {xi ‚ààD | sim(x, xi) ‚â•œÑimg} ,\n(3)\nwhere sim(¬∑) returns the image similarity between two im-\nages, and œÑimg is a similarity threshold. In practice, we com-\npute the sim(¬∑) by cosine similarity between image embed-\ndings from pre-trained models like CLIP [41] or DINO [38].\nGroupDiff training. At each training step, we first con-\nstruct a group of related images X ‚ààRN√óH√óW √ó3, includ-\ning the original image x, by randomly sampling N ‚àí1 im-\nages from the images returned by query function q(x; D; œÑ).\nWe use threshold œÑimg = 0.7 in our experiments, which\nretrieves a sufficient number of related samples. For such\nimage group, we first extract their latent with a pre-trained\nVAE from Stable Diffusion [45]. To obtain the noisy la-\ntent, we sample the timestep independently for each sam-\nple but ensure that the variance of the timestep within each\ngroup is under the threshold of timestep variation œÉtv. To\ncompute the group attention, we first extract the hidden\nstates h from the input X, and then reshape them from\nRN√óL√óC ‚ÜíR1√ó(NL)√óC, where L is the image patch se-\nquence length and C is the channel. After the Attention(¬∑)\noperation, we reshape the hidden states back.\nIn particular, GroupDiff enables generating multiple\nsamples in a group by using LGroup as the loss function as\nfollows:\nLGroup = EX,E‚àºN(0,I),t\n\" N\nX\ni=1\n‚à•œµi ‚àíeŒ∏(X; t, c)i‚à•2\n2\n#\n, (4)\nwhere c is the condition and t is the denoising timestep.\nGroupDiff inference. GroupDiff enables generating N de-\npendent images following the condition c together at the in-\nference time, instead of N independent image as in previous\nsystems [40]. At each timestep, the denoiser predicts two\nscores: conditional and unconditional. We introduce two\nvariations of our method, GroupDiff-f and GroupDiff-l,\nby flexibly deciding whether to predict the conditional score\nwith group attention or not. For GroupDiff-f, we obtain both\nscores from group attention and apply the CFG guidance to\ncombine those scores as follows:\nÀúeŒ∏(Xt; t, c) = eŒ∏(Xt; t, c)\n+ s ¬∑ (eŒ∏(Xt; t, c) ‚àíeŒ∏(Xt; t, ‚àÖ)).\n(5)\nFor GroupDiff-l, only the unconditional score is predicted\nfrom group attention. In this case, we obtain ÀúeŒ∏ as follows:\nÀúeŒ∏(Xt; t, c) = {eŒ∏(Xi\nt; t, c)}n\ni=1\n+ s ¬∑ ({eŒ∏(Xi\nt; t, c)}n\ni=1 ‚àíeŒ∏(Xt; t, ‚àÖ)),\n(6)\nwhere Xi\nt is the ith element in group X.\nBy convention, only 10% of the data is used to train the\nunconditional model for generation with CFG [15]. Since\nGroupDiff-l applies the large group size only to this uncon-\nditional model, the remaining 90% is trained with a group\nsize of one. Thus, most of the training remains identical\nto standard diffusion, making GroupDiff-l computationally\nlightweight compared to GroupDiff-f and close to baseline\nsystems [31, 40].\nEmpirically, we find that GroupDiff-\nl strikes a good balance between generation quality and\ncomputational cost.\nThroughout the paper, we refer to\nGroupDiff as GroupDiff-l unless otherwise specified.\n4. Experiments\nWe now analyze our proposed GroupDiff, beginning with\nthe introduction of the experiment setup and a series of ab-\nlation studies on the group settings, followed by observa-\ntions of the intriguing property and behavior of GroupDiff.\nLastly, we benchmark with previous leading systems.\n4.1. Setup\nImplementation Details. We strictly follow the DiT [40]\nand SiT [31] model architecture/configuration and data pro-\ncess.\nWe train the GroupDiff with AdamW optimizer,\na constant learning rate of 1 √ó 10‚àí4, and weight decay\n0.01 on A100 GPUs.\nSampling is performed using the\nSDE Euler-Maruyama sampler and the iDDPM [36] sam-\npler with NFE = 250 when SiT [31] and DiT [40] are se-\nlected as the baseline model, respectively. We consistently\nuse a global batch size of 256 when adjusting the group size\nto ensure a fair comparison across variations and baseline\nmethods. Additional implementation details and baseline\nintroduction are provided in the Supplementary.\nDatasets and metrics. Following DiT [40], we conduct\nexperiments on ImageNet [5] and use a pretrained Stable\nDiffusion VAE with a compression ratio of 8 to encode each\n256 √ó 256 image into a compressed vector x ‚ààR32√ó32√ó4.\nAnd we report the FID [14], Inception Score [46], Precision\nand Recall [25] for measuring the generation quality.\n4.2. Main Properties\nAs shown in Table 1, we discover that GroupDiff consis-\ntently provides a substantially improved generation perfor-\nmance across various design choices, achieving a much bet-\nter FID score than the vanilla model. Below, we provide a\ndetailed analysis of the impact of each component.\nGroup model. The leading diffusion systems usually ben-\nefit from Classifier-Free Guidance [15], which takes the\njoint effect with the conditional model and unconditional\nmodel. In practice, those two models usually share most\nmodel weights besides the condition embedding. We begin\n4", "clean_text": "Thus, we must construct sets of images that are related during training time. Given the image x ‚ààRH√óW √ó3 and the entire image dataset D, we define the query function q(x) as the following: q(x; D; œÑimg) = {xi ‚ààD | sim(x, xi) ‚â•œÑimg} , (3) where sim(¬∑) returns the image similarity between two images, and œÑimg is a similarity threshold. In practice, we compute the sim(¬∑) by cosine similarity between image embeddings from pre-trained models like CLIP [41] or DINO [38]. GroupDiff training. At each training step, we first construct a group of related images X ‚ààRN√óH√óW √ó3, including the original image x, by randomly sampling N ‚àí1 images from the images returned by query function q(x; D; œÑ). We use threshold œÑimg = 0.7 in our experiments, which retrieves a sufficient number of related samples. For such image group, we first extract their latent with a pre-trained VAE from Stable Diffusion [45]. To obtain the noisy latent, we sample the timestep independently for each sample but ensure that the variance of the timestep within each group is under the threshold of timestep variation œÉtv. To compute the group attention, we first extract the hidden states h from the input X, and then reshape them from RN√óL√óC ‚ÜíR1√ó(NL)√óC, where L is the image patch sequence length and C is the channel. After the Attention(¬∑) operation, we reshape the hidden states back. In particular, GroupDiff enables generating multiple samples in a group by using LGroup as the loss function as follows: LGroup = EX,E‚àºN(0,I),t \" N X i=1 ‚à•œµi ‚àíeŒ∏(X; t, c)i‚à•2 2 # , (4) where c is the condition and t is the denoising timestep. GroupDiff inference. GroupDiff enables generating N dependent images following the condition c together at the inference time, instead of N independent image as in previous systems [40]. At each timestep, the denoiser predicts two scores: conditional and unconditional. We introduce two variations of our method, GroupDiff-f and GroupDiff-l, by flexibly deciding whether to predict the conditional score with group attention or not. For GroupDiff-f, we obtain both scores from group attention and apply the CFG guidance to combine those scores as follows: ÀúeŒ∏(Xt; t, c) = eŒ∏(Xt; t, c) + s ¬∑ (eŒ∏(Xt; t, c) ‚àíeŒ∏(Xt; t, ‚àÖ)). (5) For GroupDiff-l, only the unconditional score is predicted from group attention. In this case, we obtain ÀúeŒ∏ as follows: ÀúeŒ∏(Xt; t, c) = {eŒ∏(Xi t; t, c)}n i=1 + s ¬∑ ({eŒ∏(Xi t; t, c)}n i=1 ‚àíeŒ∏(Xt; t, ‚àÖ)), (6) where Xi t is the ith element in group X. By convention, only 10% of the data is used to train the unconditional model for generation with CFG [15]. Since GroupDiff-l applies the large group size only to this unconditional model, the remaining 90% is trained with a group size of one. Thus, most of the training remains identical to standard diffusion, making GroupDiff-l computationally lightweight compared to GroupDiff-f and close to baseline systems [31, 40]. Empirically, we find that GroupDiffl strikes a good balance between generation quality and computational cost. Throughout the paper, we refer to GroupDiff as GroupDiff-l unless otherwise specified. 4. Experiments We now analyze our proposed GroupDiff, beginning with the introduction of the experiment setup and a series of ablation studies on the group settings, followed by observations of the intriguing property and behavior of GroupDiff. Lastly, we benchmark with previous leading systems. 4.1. Setup Implementation Details. We strictly follow the DiT [40] and SiT [31] model architecture/configuration and data process. We train the GroupDiff with AdamW optimizer, a constant learning rate of 1 √ó 10‚àí4, and weight decay 0.01 on A100 GPUs. Sampling is performed using the SDE Euler-Maruyama sampler and the iDDPM [36] sampler with NFE = 250 when SiT [31] and DiT [40] are selected as the baseline model, respectively. We consistently use a global batch size of 256 when adjusting the group size to ensure a fair comparison across variations and baseline methods. Additional implementation details and baseline introduction are provided in the Supplementary. Datasets and metrics. Following DiT [40], we conduct experiments on ImageNet [5] and use a pretrained Stable Diffusion VAE with a compression ratio of 8 to encode each 256 √ó 256 image into a compressed vector x ‚ààR32√ó32√ó4. And we report the FID [14], Inception Score [46], Precision and Recall [25] for measuring the generation quality. 4.2. Main Properties As shown in Table 1, we discover that GroupDiff consistently provides a substantially improved generation performance across various design choices, achieving a much better FID score than the vanilla model. Below, we provide a detailed analysis of the impact of each component. Group model. The leading diffusion systems usually benefit from Classifier-Free Guidance [15], which takes the joint effect with the conditional model and unconditional model. In practice, those two models usually share most model weights besides the condition embedding. We begin 4"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 5, "text": "GroupDiff Settings\nw/o CFG\nw/ CFG\nCross-Sample\nLinear Prob\nIter\nModel\nQuery Method\nNoise Var.\nFID ‚Üì\nFID ‚Üì\ncfg-scale\nAttn. Score ‚Üë\nAcc. ‚Üë\n800K\nC = 1, UC = 1\n-\n0\n14.38\n3.50\n1.5\n-\n49.48\n800K\nC = 4, UC = 4\nClass\n0\n14.27\n3.08\n1.6\n-\n62.15\n800K\nC = 1, UC = 4\nClass\n0\n13.22\n2.81\n1.6\n-\n64.44\n800K\nC = 1, UC = 2\nCLIP-L\n0\n13.47\n2.92\n1.5\n0.00%\n55.33\n800K\nC = 1, UC = 4\nCLIP-L\n0\n13.93\n2.42\n2.0\n19.95%\n58.83\n800K\nC = 1, UC = 8\nCLIP-L\n0\n13.08\n2.14\n2.2\n51.13%\n67.93\n800K\nC = 1, UC = 16\nCLIP-L\n0\n13.84\n1.86\n2.5\n56.47%\n72.91\n800K\nC = 1, UC = 4\nRandom\n0\n13.28\n3.57\n1.5\n23.17%\n-\n800K\nC = 1, UC = 4\nClass\n0\n13.22\n2.81\n1.6\n22.51%\n64.44\n800K\nC = 1, UC = 4\nCLIP-B\n0\n13.47\n2.51\n1.9\n19.20%\n61.14\n800K\nC = 1, UC = 4\nCLIP-L\n0\n13.93\n2.42\n2.0\n19.95%\n58.83\n800K\nC = 1, UC = 4\nSigLIP\n0\n13.83\n2.45\n2.0\n19.98%\n63.32\n800K\nC = 1, UC = 4\nDINOv2-B\n0\n14.40\n2.51\n1.9\n18.45%\n63.32\n800K\nC = 1, UC = 4\nDINOv2-L\n0\n13.35\n2.51\n1.9\n22.85%\n59.16\n800K\nC = 1, UC = 4\nI-JEPA\n0\n13.08\n2.44\n1.8\n18.50%\n60.50\n800K\nC = 1, UC = 4\nCLIP-L\n0\n13.93\n2.42\n2.0\n19.95%\n58.83\n800K\nC = 1, UC = 4\nCLIP-L\n20\n13.50\n2.42\n2.0\n21.37%\n60.48\n800K\nC = 1, UC = 4\nCLIP-L\n50\n12.81\n2.34\n2.0\n26.23%\n68.91\n800K\nC = 1, UC = 4\nCLIP-L\n100\n12.78\n2.32\n1.9\n23.33%\n62.82\n800K\nC = 1, UC = 4\nCLIP-L\n150\n13.70\n2.25\n2.0\n24.31%\n63.74\n800K\nC = 1, UC = 4\nCLIP-L\n200\n13.26\n2.32\n1.8\n24.46%\n60.03\nTable 1. Component-wise analysis on ImageNet 256 √ó 256 with DiT-XL/2 [40] trained for 800K iterations. All metrics except accuracy\n(Acc.) are measured with the iDDPM [36] sampler with NFE= 250. For generation results with Classifier-Free Guidance, we search for\nthe optimal guidance scale using an interval of 0.1 and report the one with the optimal FID score. ‚Üëand ‚Üìindicate whether higher or lower\nvalues are better, respectively. C and UC referring to the conditional model and unconditional model,respectively.\nthe ablation by analyzing the model behavior when apply-\ning the Group Attention operation on one or both models.\nIn this analysis, we use the ImageNet [5] class label as\nthe query method to build each group. We observe that\nGroupDiff consistently outperforms the individual diffusion\nbaseline. Notably, when only running the UC model in the\nGroupDiff mode, our system further achieves higher gener-\nation quality when both the CFG is disabled or enabled, re-\nflected by lower FID. Under this setting, we observe that the\ncondition model‚Äôs generation capability has also improved\nwhen we train only the unconditional model with group at-\ntention. We hypothesize that the stronger representation in\nthe UC model implicitly enhances the C model via weight\nsharing. In later experiments, we set C=1, UC=N as the\ndefault choice to balance training and inference.\nGroup size. We also study the impact of group size in\nGroupDiff.\nLarger groups generally yield better genera-\ntion results, as reflected by consistent improvements in FID\nand feature quality. We hypothesize that larger groups of-\nfer greater flexibility for finding better patch-level matches,\nthereby enhancing generation and internal representations.\nDetailed pattern analysis is provided in Sec. 4.3. In the fol-\nlowing experiments, we choose 4 as the group size for fair\ncomparison with baseline methods.\nGroup construction method. We then investigate the im-\nClass Label\nCLIP-L\nDINOv2-L\nQuery Image\nFigure 4.\nComparison of group candidates from different\nquery methods. The difference in pretraining settings lead each\nquery method to form distinct groups. We show nearest samples\nfrom the ImageNet [5] training split, with the class label row show-\ning random same-class samples.\npact of different group construction methods, including ran-\ndom sampling, class-based grouping, and similarity-based\nretrieval via pre-trained vision encoders.\nQuantitatively,\nsimilarity-based grouping yields the best generation quality,\nfollowed by class-based grouping, while random sampling\nperforms the worst (on par with the baseline). This indicates\nthat group attention does not degrade the baseline diffusion\nmodel‚Äôs performance, even without any bells and whistles.\nMeanwhile, we hypothesize that image similarity within a\n5", "clean_text": "GroupDiff Settings w/o CFG w/ CFG Cross-Sample Linear Prob Iter Model Query Method Noise Var. FID ‚Üì FID ‚Üì cfg-scale Attn. Score ‚Üë Acc. ‚Üë 800K C = 1, UC = 1 0 14.38 3.50 1.5 49.48 800K C = 4, UC = 4 Class 0 14.27 3.08 1.6 62.15 800K C = 1, UC = 4 Class 0 13.22 2.81 1.6 64.44 800K C = 1, UC = 2 CLIP-L 0 13.47 2.92 1.5 0.00% 55.33 800K C = 1, UC = 4 CLIP-L 0 13.93 2.42 2.0 19.95% 58.83 800K C = 1, UC = 8 CLIP-L 0 13.08 2.14 2.2 51.13% 67.93 800K C = 1, UC = 16 CLIP-L 0 13.84 1.86 2.5 56.47% 72.91 800K C = 1, UC = 4 Random 0 13.28 3.57 1.5 23.17% 800K C = 1, UC = 4 Class 0 13.22 2.81 1.6 22.51% 64.44 800K C = 1, UC = 4 CLIP-B 0 13.47 2.51 1.9 19.20% 61.14 800K C = 1, UC = 4 CLIP-L 0 13.93 2.42 2.0 19.95% 58.83 800K C = 1, UC = 4 SigLIP 0 13.83 2.45 2.0 19.98% 63.32 800K C = 1, UC = 4 DINOv2-B 0 14.40 2.51 1.9 18.45% 63.32 800K C = 1, UC = 4 DINOv2-L 0 13.35 2.51 1.9 22.85% 59.16 800K C = 1, UC = 4 I-JEPA 0 13.08 2.44 1.8 18.50% 60.50 800K C = 1, UC = 4 CLIP-L 0 13.93 2.42 2.0 19.95% 58.83 800K C = 1, UC = 4 CLIP-L 20 13.50 2.42 2.0 21.37% 60.48 800K C = 1, UC = 4 CLIP-L 50 12.81 2.34 2.0 26.23% 68.91 800K C = 1, UC = 4 CLIP-L 100 12.78 2.32 1.9 23.33% 62.82 800K C = 1, UC = 4 CLIP-L 150 13.70 2.25 2.0 24.31% 63.74 800K C = 1, UC = 4 CLIP-L 200 13.26 2.32 1.8 24.46% 60.03 Table 1. Component-wise analysis on ImageNet 256 √ó 256 with DiT-XL/2 [40] trained for 800K iterations. All metrics except accuracy (Acc.) are measured with the iDDPM [36] sampler with NFE= 250. For generation results with Classifier-Free Guidance, we search for the optimal guidance scale using an interval of 0.1 and report the one with the optimal FID score. ‚Üëand ‚Üìindicate whether higher or lower values are better, respectively. C and UC referring to the conditional model and unconditional model,respectively. the ablation by analyzing the model behavior when applying the Group Attention operation on one or both models. In this analysis, we use the ImageNet [5] class label as the query method to build each group. We observe that GroupDiff consistently outperforms the individual diffusion baseline. Notably, when only running the UC model in the GroupDiff mode, our system further achieves higher generation quality when both the CFG is disabled or enabled, reflected by lower FID. Under this setting, we observe that the condition model‚Äôs generation capability has also improved when we train only the unconditional model with group attention. We hypothesize that the stronger representation in the UC model implicitly enhances the C model via weight sharing. In later experiments, we set C=1, UC=N as the default choice to balance training and inference. Group size. We also study the impact of group size in GroupDiff. Larger groups generally yield better generation results, as reflected by consistent improvements in FID and feature quality. We hypothesize that larger groups offer greater flexibility for finding better patch-level matches, thereby enhancing generation and internal representations. Detailed pattern analysis is provided in Sec. 4.3. In the following experiments, we choose 4 as the group size for fair comparison with baseline methods. Group construction method. We then investigate the imClass Label CLIP-L DINOv2-L Query Image Figure 4. Comparison of group candidates from different query methods. The difference in pretraining settings lead each query method to form distinct groups. We show nearest samples from the ImageNet [5] training split, with the class label row showing random same-class samples. pact of different group construction methods, including random sampling, class-based grouping, and similarity-based retrieval via pre-trained vision encoders. Quantitatively, similarity-based grouping yields the best generation quality, followed by class-based grouping, while random sampling performs the worst (on par with the baseline). This indicates that group attention does not degrade the baseline diffusion model‚Äôs performance, even without any bells and whistles. Meanwhile, we hypothesize that image similarity within a 5"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 6, "text": "FID\nCross-Sample Score (%)\nCross Sample Attention Weight\nLayer Index\nFigure 5. Cross-Sample Attention in GroupDiff. (a) FID vs Cross-Sample Score (left). Our GroupDiff shows a strong correlation\n(0.95) between cross-attention to other samples and generation quality. (b) Cross-Sample Attention Visualization (right).\ngroup is crucial for strengthening cross-sample interaction.\nRandom groups often contain unrelated samples and thus\nlack meaningful mutual information, whereas similarity-\nbased retrieval retrieves semantically coherent images, re-\nducing the FID (with CFG) from 3.57 to around 2.4.\nInterestingly, Figure 4 shows that different pre-trained\nencoders form visually distinct groups. For instance, CLIP-\nL [41] tends to cluster semantically similar samples, while\nDINOv2-B [38] captures alternative aspects of visual sim-\nilarity. Nevertheless, their resulting generation quality re-\nmains comparable, suggesting that the benefit primarily\narises from semantic consistency rather than the specific en-\ncoder style. Overall, GroupDiff demonstrates strong flex-\nibility and generalization, showing that the quality of the\npre-trained encoders does not limit its performance.\nGroup noise-level variation. Lastly, we explore the ef-\nfect of introducing noise-level variance within each group.\nInstead of applying the same noise level to the entire group,\nwe restrict the noise levels of the other samples to differ\nfrom that of the first sample by up to a specified range, e.g.\n50 or 200. Prior works [4, 65] verified that adding different\nlevel of noise could be an effective augmentation method for\nimproving representations learning and generation quality.\nIn our setting, we hypothesize that noisier samples benefit\nfrom cleaner ones within the same group, further encourag-\ning cross-sample attention. We find that setting the noise-\nlevel variation in the range of 50 to 200 yields the best per-\nformance, improving both FID and linear probe accuracy\nwhile strengthening cross-sample attention.\n4.3. GroupDiff Generation Pattern Analysis\nAfter validating the effectiveness of different group settings,\nwe now analyze why and how GroupDiff improves genera-\ntion quality and investigate its unique generation patterns.\nCross-Sample Attention. To understand why GroupDiff\nimproves generation, we examine how cross-sample inter-\naction influences the diffusion process.\nAt the core of\nGroupDiff is cross-sample attention, enabling each patch\n0.2\n0.4\n0.8\n0.6\nFigure 6.\nControlling GroupDiff denoising steps.\nWe show\ngenerated sample examples when GroupDiff is turned off after\ndifferent denoising stages.\nStable quality after denoising with\nGroupDiff at early steps.\nto establish intra-image and inter-image correspondence\nacross the group. Figure 2 shows that a patch corresponding\nto a ‚Äúdog‚Äôs ear‚Äù attends to both the same region of its own\ninstance and to similar ‚Äúear‚Äù regions in other dog images.\nTo quantitatively measure the cross-sample attention, we\ndefine the image-level self-attention as attention assigned to\nits own patches, and cross-attention as attention assigned to\npatches from other images in the group. Formally, let image\nxi contain patch indices Ii. For a query patch q ‚ààIi and\nany key patch k, let the attention weight be Œ±qk. We define\nthe image-level cross-attention weight for image xi as\nP xi\ncross =\n\b\npxi‚Üíxj \f\f j Ã∏= i\n\t\n,\nwhere pxi‚Üíxj =\nX\nq‚ààIi\nX\nk‚ààIj\nŒ±qk.\n(7)\nFurthermore, we introduce the mean cross-attention\nscore and the max cross-attention score of image xi by tak-\n6", "clean_text": "FID Cross-Sample Score (%) Cross Sample Attention Weight Layer Index Figure 5. Cross-Sample Attention in GroupDiff. (a) FID vs Cross-Sample Score (left). Our GroupDiff shows a strong correlation (0.95) between cross-attention to other samples and generation quality. (b) Cross-Sample Attention Visualization (right). group is crucial for strengthening cross-sample interaction. Random groups often contain unrelated samples and thus lack meaningful mutual information, whereas similaritybased retrieval retrieves semantically coherent images, reducing the FID (with CFG) from 3.57 to around 2.4. Interestingly, Figure 4 shows that different pre-trained encoders form visually distinct groups. For instance, CLIPL [41] tends to cluster semantically similar samples, while DINOv2-B [38] captures alternative aspects of visual similarity. Nevertheless, their resulting generation quality remains comparable, suggesting that the benefit primarily arises from semantic consistency rather than the specific encoder style. Overall, GroupDiff demonstrates strong flexibility and generalization, showing that the quality of the pre-trained encoders does not limit its performance. Group noise-level variation. Lastly, we explore the effect of introducing noise-level variance within each group. Instead of applying the same noise level to the entire group, we restrict the noise levels of the other samples to differ from that of the first sample by up to a specified range, e.g. 50 or 200. Prior works [4, 65] verified that adding different level of noise could be an effective augmentation method for improving representations learning and generation quality. In our setting, we hypothesize that noisier samples benefit from cleaner ones within the same group, further encouraging cross-sample attention. We find that setting the noiselevel variation in the range of 50 to 200 yields the best performance, improving both FID and linear probe accuracy while strengthening cross-sample attention. 4.3. GroupDiff Generation Pattern Analysis After validating the effectiveness of different group settings, we now analyze why and how GroupDiff improves generation quality and investigate its unique generation patterns. Cross-Sample Attention. To understand why GroupDiff improves generation, we examine how cross-sample interaction influences the diffusion process. At the core of GroupDiff is cross-sample attention, enabling each patch 0.2 0.4 0.8 0.6 Figure 6. Controlling GroupDiff denoising steps. We show generated sample examples when GroupDiff is turned off after different denoising stages. Stable quality after denoising with GroupDiff at early steps. to establish intra-image and inter-image correspondence across the group. Figure 2 shows that a patch corresponding to a ‚Äúdog‚Äôs ear‚Äù attends to both the same region of its own instance and to similar ‚Äúear‚Äù regions in other dog images. To quantitatively measure the cross-sample attention, we define the image-level self-attention as attention assigned to its own patches, and cross-attention as attention assigned to patches from other images in the group. Formally, let image xi contain patch indices Ii. For a query patch q ‚ààIi and any key patch k, let the attention weight be Œ±qk. We define the image-level cross-attention weight for image xi as P xi cross = \b pxi‚Üíxj j Ã∏= i , where pxi‚Üíxj = X q‚ààIi X k‚ààIj Œ±qk. (7) Furthermore, we introduce the mean cross-attention score and the max cross-attention score of image xi by tak6"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 7, "text": "ing the mean and maximum over P xi\ncross:\nP xi\ncross-mean = mean(P xi\ncross) ,\nP xi\ncross-max = max(P xi\ncross) .\nAttention over denoising steps. To further quantify this\neffect, we measure cross-sample attention across different\ndenoising steps using the image-level cross-attention score,\nP xi\ncross. For each image, we compute its mean and maxi-\nmum cross-attention scores, P xi\ncross-mean and P xi\ncross-max, and\naverage these statistics over all images in the group. As\nshown in Figure 5 (right), both the mean and maximum\ncross-attention scores gradually decrease as the noise level\nreduces, indicating that inter-sample information exchange\nis most active at the early stages of denoising when global\nstructure and semantics are being formed.\nTo validate this observation, we conduct an intervention\nexperiment by turning off GroupDiff after a certain num-\nber of denoising steps and continuing the process using the\nbaseline DiT model. As illustrated in Figure 6, disabling\nGroupDiff in the middle or late stages yields little qual-\nity degradation, confirming our aforementioned hypothesis.\nTable 2 shows that GroupDiff could be faster without de-\ngraded quality by only applying group attention in the early\nand middle stages.\nAttention over denoiser layers.\nWe also exam-\nine the layer-wise distribution of cross-sample attention.\nGroupDiff shows stronger cross-sample attention in the\nearly and final layers, suggesting that it uses other samples\nto form global context and later refine details. Table 3 shows\nthat early layers are essential, while late layers have much\nless impact on GroupDiff. These results indicate GroupDiff\nstrengthens cross-sample interaction in the early timesteps\nand shallow layers, leading to improved generation quality.\nMethods\nFID-10K\nBaseline\n4.21\nw/t 0.0-0.2\n4.04\nw/t 0.0-0.4\n3.92\nw/t 0.0-0.6\n4.63\nTable 2.\nAblation on\ngroup attention timestep.\nMethods\nFID-10K\nBaseline\n4.21\nw/o layer 1-9\n294.38\nw/o layer 10-19\n5.49\nw/o layer 20-27\n4.49\nTable 3.\nAblation on\ngroup attention layers.\nCross-sample attention score.\nUnder a setting that en-\ncourages cross-image attention, we hypothesize two possi-\nble operating modes: (i) an evenly distributed mode, where\nan image spreads attention across all others, and (ii) a\nneighbor-focused mode, where it primarily attends to its\nmost similar counterpart. We focus on the latter behavior\nand quantify its strength using an image-level cross-sample\nattention score defined as\nScross = Pcross-max ‚àíPcross-mean\nPcross-max\n,\n(8)\nwhere Pcross-max and Pcross-mean denote the maximum and\nmean cross-sample attention from one image to the others\nReference\nReplace 2nd\nReplace 3rd\nReplace 4th\nLower attention weights to the first sample  \nFigure 7. Controlling conditions. The reference group uses class\n89, and in each row, one sample‚Äôs (red) condition is changed to\nclass 360.\nin the group. Intuitively, this score measures how strongly\nthe attention distribution concentrates on the most similar\nimage, normalized by the overall attention magnitude. A\nscore close to 0 indicates a uniform, distributed attention\npattern, while a score close to 1 reflects a highly peaked,\nneighbor-focused attention on a single image.\nBy varying the query method, noise range, and group\nsize across GroupDiff variants, we compare their cross-\nsample attention scores with their FID. We observe a strong\ncorrelation (r = 0.95; Fig. 5 left), showing that more\nneighbor-focused cross-sample attention leads to higher\ngeneration quality. Upon closer inspection, several distinct\nclusters emerge in the plot, primarily corresponding to dif-\nferent group sizes. We find that increasing the group size ef-\nfectively encourages stronger cross-sample attention behav-\nior, further improving generation quality. Moreover, even\nwithin each cluster, higher cross-sample attention scores\nstill correlate with lower FID, showing that this interaction\nreliably reflects generation quality.\nCross-condition generation. To further validate the role of\ncross-sample attention, we conduct a controlled experiment\nby replacing one image in the group with a sample from a\ndifferent class while keeping the latent variables fixed. We\nfirst generate a group of reference images and rank them\nby their cross-attention weights to the first sample, pxi‚Üíx1.\nThen, we gradually replace the condition of one sample\nwith another class during the entire denoising process and\nshow the results in Figure 7. We observed that the genera-\ntion of the reference (green box) image is highly sensitive\nto which sample is replaced. When we replace a sample\nthat originally receives high attention weights, the refer-\nence image changes significantly. In contrast, replacing a\nlow-attention sample results in almost no visual difference.\n7", "clean_text": "ing the mean and maximum over P xi cross: P xi cross-mean = mean(P xi cross) , P xi cross-max = max(P xi cross) . Attention over denoising steps. To further quantify this effect, we measure cross-sample attention across different denoising steps using the image-level cross-attention score, P xi cross. For each image, we compute its mean and maximum cross-attention scores, P xi cross-mean and P xi cross-max, and average these statistics over all images in the group. As shown in Figure 5 (right), both the mean and maximum cross-attention scores gradually decrease as the noise level reduces, indicating that inter-sample information exchange is most active at the early stages of denoising when global structure and semantics are being formed. To validate this observation, we conduct an intervention experiment by turning off GroupDiff after a certain number of denoising steps and continuing the process using the baseline DiT model. As illustrated in Figure 6, disabling GroupDiff in the middle or late stages yields little quality degradation, confirming our aforementioned hypothesis. Table 2 shows that GroupDiff could be faster without degraded quality by only applying group attention in the early and middle stages. Attention over denoiser layers. We also examine the layer-wise distribution of cross-sample attention. GroupDiff shows stronger cross-sample attention in the early and final layers, suggesting that it uses other samples to form global context and later refine details. Table 3 shows that early layers are essential, while late layers have much less impact on GroupDiff. These results indicate GroupDiff strengthens cross-sample interaction in the early timesteps and shallow layers, leading to improved generation quality. Methods FID-10K Baseline 4.21 w/t 0.0-0.2 4.04 w/t 0.0-0.4 3.92 w/t 0.0-0.6 4.63 Table 2. Ablation on group attention timestep. Methods FID-10K Baseline 4.21 w/o layer 1-9 294.38 w/o layer 10-19 5.49 w/o layer 20-27 4.49 Table 3. Ablation on group attention layers. Cross-sample attention score. Under a setting that encourages cross-image attention, we hypothesize two possible operating modes: (i) an evenly distributed mode, where an image spreads attention across all others, and (ii) a neighbor-focused mode, where it primarily attends to its most similar counterpart. We focus on the latter behavior and quantify its strength using an image-level cross-sample attention score defined as Scross = Pcross-max ‚àíPcross-mean Pcross-max , (8) where Pcross-max and Pcross-mean denote the maximum and mean cross-sample attention from one image to the others Reference Replace 2nd Replace 3rd Replace 4th Lower attention weights to the first sample Figure 7. Controlling conditions. The reference group uses class 89, and in each row, one sample‚Äôs (red) condition is changed to class 360. in the group. Intuitively, this score measures how strongly the attention distribution concentrates on the most similar image, normalized by the overall attention magnitude. A score close to 0 indicates a uniform, distributed attention pattern, while a score close to 1 reflects a highly peaked, neighbor-focused attention on a single image. By varying the query method, noise range, and group size across GroupDiff variants, we compare their crosssample attention scores with their FID. We observe a strong correlation (r = 0.95; Fig. 5 left), showing that more neighbor-focused cross-sample attention leads to higher generation quality. Upon closer inspection, several distinct clusters emerge in the plot, primarily corresponding to different group sizes. We find that increasing the group size effectively encourages stronger cross-sample attention behavior, further improving generation quality. Moreover, even within each cluster, higher cross-sample attention scores still correlate with lower FID, showing that this interaction reliably reflects generation quality. Cross-condition generation. To further validate the role of cross-sample attention, we conduct a controlled experiment by replacing one image in the group with a sample from a different class while keeping the latent variables fixed. We first generate a group of reference images and rank them by their cross-attention weights to the first sample, pxi‚Üíx1. Then, we gradually replace the condition of one sample with another class during the entire denoising process and show the results in Figure 7. We observed that the generation of the reference (green box) image is highly sensitive to which sample is replaced. When we replace a sample that originally receives high attention weights, the reference image changes significantly. In contrast, replacing a low-attention sample results in almost no visual difference. 7"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 8, "text": "Figure 8. Qualitative Results. Examples of class-conditional generation on ImageNet 256√ó256 using GroupDiff-4 with SiT-XL/2.\nMethod\nEpoch\nw/ CFG\nFID\nIS\nPre.\nRec.\nWith semantic feature distillation\nDDT-XL [61]\n800\n1.26\n310.6\n0.79\n0.65\nSiT-XL/2 + REPA-E [26]\n800\n1.26\n314.9\n0.79\n0.66\nSiT-XL/2 + REPA [67]\n800\n1.42\n305.7\n0.80\n0.64\n+ our GroupDiff-4*\n800 + 100\n1.14\n315.3\n0.77\n0.66\nWithout semantic feature distillation\nADM [6]\n400\n4.59\n186.7\n0.82\n0.52\nVDM++ [23]\n-\n2.12\n267.7\n-\n-\nLDM-4 [43]\n1400\n3.60\n247.7\n0.87\n0.48\nMDTv2-XL/2 [11]\n900\n1.58\n314.7\n0.79\n0.65\nVAR-d30 [55]\n350\n1.92\n323.1\n0.82\n0.59\nLlamaGen-3B [50]\n-\n2.18\n263.3\n0.81\n0.58\nRandAR-XXL [39]\n300\n2.15\n322.0\n0.79\n0.62\nMaskDiT [70]\n1600\n2.28\n276.6\n0.89\n0.61\nDiT-XL/2 [40]\n1400\n2.27\n278.2\n0.83\n0.57\n+ our GroupDiff-4\n800\n1.66\n279.4\n0.83\n0.57\n+ our GroupDiff-4‚àó\n1400 + 100\n1.55\n285.4\n0.80\n0.63\nSiT-XL/2 [31]\n1400\n2.06\n270.3\n0.82\n0.59\n+ SRA [20]\n800\n1.58\n311.4\n0.80\n0.63\n+ Dispersive Loss [59]\n1200\n1.97\n-\n-\n-\n+ our GroupDiff-4\n800\n1.63\n283.2\n0.81\n0.64\n+ our GroupDiff-4‚àó\n1400 + 100\n1.40\n290.7\n0.79\n0.64\nTable 4. System-Level performance comparison on ImageNet\n256 √ó 256. Our GroupDiff enables the DiT/SiT model to achieve\nstate-of-the-art performance both with/without semantic feature\ndistillation.\n‚àó: continue training from pre-trained checkpoint for\nan additional 100 epochs.\nThis indicates that cross-sample attention controls the inter-\nimage correspondence within the group, with high-attention\nsamples contributing more to the final generation, consis-\ntent with our earlier observations of cross-sample attention\npatterns. Furthermore, we believe this property points to a\npromising future direction. When the group size is suffi-\nciently large, the generation process of GroupDiff could be\nextended to handle diverse or cross-conditioned inputs, en-\nabling more flexible inter-image correspondence within the\ngeneration process.\n4.4. Benchmarking with Previous Systems\nWe compare against leading generative systems in Table 4.\nFor this experiment, we train GroupDiff in two settings:\nfrom scratch and from the pre-trained weights, denoted as\nGroupDiff-4 and GroupDiff-4‚àó, respectively. When train-\ning from scratch, GroupDiff improves DiT-XL/2 with 29%\nlower FID and SiT-XL/2 with 30% lower FID while only\nusing 57% of original training iterations.\nFor the second setting, we only use the Lgroup as the\ntraining objective, no matter if other objectives, e.g. Lrepa\nfrom REPA [67], exist in the previous stages.\nNotably,\nGroupDiff-4 with DiT-XL/2 achieves an FID of 1.55 (from\n2.27) and GroupDiff-4 with SiT-XL/2 further improves to\n1.40 (from 2.06) with only 100 additional training epochs,\noutperforming all other state-of-the-art methods when no\nsemantic feature distillation has been applied. Moreover,\nwhen using pre-trained weights from the semantic feature\ndistillation method, GroupDiff again obtains a significant\nimprovement, achieving an FID of 1.14 (down from 1.42).\nQualitative samples are provided in Figure 8.\n5. Discussion and Conclusion\nLimitations.\nWhile GroupDiff demonstrates strong im-\nprovements in generation quality, its increased training cost\nremains a challenge. When the group size is n, GroupDiff-\nf and GroupDiff-l require approximately (n ‚àí1)√ó and\n(0.1n)√ó longer training time in every iteration, and (n ‚àí\n1)√ó and 0.5(n ‚àí1)√ó longer inference time, respectively.\nNevertheless, (a) this design opens a new avenue for explor-\ning the trade-off between computational cost and generation\nquality, and (b) a high-quality model can serve as a teacher\nto distill faster and lighter students. We leave the study for\na more efficient method for future exploration.\nConclusion.\nWe introduce Group Diffusion, a simple\nyet effective framework that reshapes diffusion training\ninto a group-wise denoising process. By enabling cross-\nsample attention among related instances, the model im-\nplicitly learns relational structures that enhance represen-\ntation quality and generation fidelity. Experiments on Ima-\ngeNet demonstrate consistent FID improvements across ar-\nchitectures with minimal computational overhead. Beyond\nboosting performance, Group Diffusion provides a new lens\nconnecting representation learning and generative model-\ning, suggesting that cross-sample interactions can serve as\nan implicit form of supervision for stronger and more gen-\neralizable diffusion models.\n8", "clean_text": "Figure 8. Qualitative Results. Examples of class-conditional generation on ImageNet 256√ó256 using GroupDiff-4 with SiT-XL/2. Method Epoch w/ CFG FID IS Pre. Rec. With semantic feature distillation DDT-XL [61] 800 1.26 310.6 0.79 0.65 SiT-XL/2 + REPA-E [26] 800 1.26 314.9 0.79 0.66 SiT-XL/2 + REPA [67] 800 1.42 305.7 0.80 0.64 + our GroupDiff-4* 800 + 100 1.14 315.3 0.77 0.66 Without semantic feature distillation ADM [6] 400 4.59 186.7 0.82 0.52 VDM++ [23] 2.12 267.7 LDM-4 [43] 1400 3.60 247.7 0.87 0.48 MDTv2-XL/2 [11] 900 1.58 314.7 0.79 0.65 VAR-d30 [55] 350 1.92 323.1 0.82 0.59 LlamaGen-3B [50] 2.18 263.3 0.81 0.58 RandAR-XXL [39] 300 2.15 322.0 0.79 0.62 MaskDiT [70] 1600 2.28 276.6 0.89 0.61 DiT-XL/2 [40] 1400 2.27 278.2 0.83 0.57 + our GroupDiff-4 800 1.66 279.4 0.83 0.57 + our GroupDiff-4‚àó 1400 + 100 1.55 285.4 0.80 0.63 SiT-XL/2 [31] 1400 2.06 270.3 0.82 0.59 + SRA [20] 800 1.58 311.4 0.80 0.63 + Dispersive Loss [59] 1200 1.97 + our GroupDiff-4 800 1.63 283.2 0.81 0.64 + our GroupDiff-4‚àó 1400 + 100 1.40 290.7 0.79 0.64 Table 4. System-Level performance comparison on ImageNet 256 √ó 256. Our GroupDiff enables the DiT/SiT model to achieve state-of-the-art performance both with/without semantic feature distillation. ‚àó: continue training from pre-trained checkpoint for an additional 100 epochs. This indicates that cross-sample attention controls the interimage correspondence within the group, with high-attention samples contributing more to the final generation, consistent with our earlier observations of cross-sample attention patterns. Furthermore, we believe this property points to a promising future direction. When the group size is sufficiently large, the generation process of GroupDiff could be extended to handle diverse or cross-conditioned inputs, enabling more flexible inter-image correspondence within the generation process. 4.4. Benchmarking with Previous Systems We compare against leading generative systems in Table 4. For this experiment, we train GroupDiff in two settings: from scratch and from the pre-trained weights, denoted as GroupDiff-4 and GroupDiff-4‚àó, respectively. When training from scratch, GroupDiff improves DiT-XL/2 with 29% lower FID and SiT-XL/2 with 30% lower FID while only using 57% of original training iterations. For the second setting, we only use the Lgroup as the training objective, no matter if other objectives, e.g. Lrepa from REPA [67], exist in the previous stages. Notably, GroupDiff-4 with DiT-XL/2 achieves an FID of 1.55 (from 2.27) and GroupDiff-4 with SiT-XL/2 further improves to 1.40 (from 2.06) with only 100 additional training epochs, outperforming all other state-of-the-art methods when no semantic feature distillation has been applied. Moreover, when using pre-trained weights from the semantic feature distillation method, GroupDiff again obtains a significant improvement, achieving an FID of 1.14 (down from 1.42). Qualitative samples are provided in Figure 8. 5. Discussion and Conclusion Limitations. While GroupDiff demonstrates strong improvements in generation quality, its increased training cost remains a challenge. When the group size is n, GroupDifff and GroupDiff-l require approximately (n ‚àí1)√ó and (0.1n)√ó longer training time in every iteration, and (n ‚àí 1)√ó and 0.5(n ‚àí1)√ó longer inference time, respectively. Nevertheless, (a) this design opens a new avenue for exploring the trade-off between computational cost and generation quality, and (b) a high-quality model can serve as a teacher to distill faster and lighter students. We leave the study for a more efficient method for future exploration. Conclusion. We introduce Group Diffusion, a simple yet effective framework that reshapes diffusion training into a group-wise denoising process. By enabling crosssample attention among related instances, the model implicitly learns relational structures that enhance representation quality and generation fidelity. Experiments on ImageNet demonstrate consistent FID improvements across architectures with minimal computational overhead. Beyond boosting performance, Group Diffusion provides a new lens connecting representation learning and generative modeling, suggesting that cross-sample interactions can serve as an implicit form of supervision for stronger and more generalizable diffusion models. 8"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 9, "text": "References\n[1] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li,\nHang Su, and Jun Zhu. All are worth words: A vit backbone\nfor diffusion models. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n22669‚Äì22679, 2023. 21\n[2] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue,\nYufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luh-\nman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya\nRamesh.\nVideo generation models as world simulators.\n2024. 3\n[3] Shoufa Chen, Chongjian Ge, Shilong Zhang, Peize Sun, and\nPing Luo.\nPixelflow: Pixel-space generative models with\nflow. arXiv preprint arXiv:2504.07963, 2025. 13\n[4] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In International conference on ma-\nchine learning, pages 1597‚Äì1607. PmLR, 2020. 6\n[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei.\nImagenet: A large-scale hierarchical im-\nage database. In Computer Vision and Pattern Recognition,\n2009. CVPR 2009. IEEE Conference on, pages 248‚Äì255.\nIEEE, 2009. 4, 5\n[6] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780‚Äì8794, 2021. 1, 8, 12, 13\n[7] Alexey Dosovitskiy.\nAn image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 2\n[8] Dave Epstein, Allan Jabri, Ben Poole, Alexei Efros, and\nAleksander Holynski. Diffusion self-guidance for control-\nlable image generation.\nAdvances in Neural Information\nProcessing Systems, 36:16222‚Äì16239, 2023. 2\n[9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim\nEntezari, Jonas M¬®uller, Harry Saini, Yam Levi, Dominik\nLorenz, Axel Sauer, Frederic Boesel, et al. Scaling recti-\nfied flow transformers for high-resolution image synthesis.\nIn Forty-first international conference on machine learning,\n2024. 2, 21\n[10] Wan-Cyuan Fan, Yen-Chun Chen, DongDong Chen, Yu\nCheng, Lu Yuan, and Yu-Chiang Frank Wang. Frido: Fea-\nture pyramid diffusion for complex scene image synthesis.\nIn Proceedings of the AAAI conference on artificial intelli-\ngence, pages 579‚Äì587, 2023. 21\n[11] Shanghua\nGao,\nPan\nZhou,\nMing-Ming\nCheng,\nand\nShuicheng Yan. Mdtv2: Masked diffusion transformer is a\nstrong image synthesizer. arXiv preprint arXiv:2303.14389,\n2023. 8, 12\n[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial networks. Commu-\nnications of the ACM, 63(11):139‚Äì144, 2020. 1\n[13] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo\nZhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-\ntor quantized diffusion model for text-to-image synthesis. In\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 10696‚Äì10706, 2022. 21\n[14] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017. 4, 12\n[15] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 3, 4\n[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840‚Äì6851, 2020. 1, 2\n[17] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu,\nand Jie Tang.\nCogvideo:\nLarge-scale pretraining for\ntext-to-video generation via transformers.\narXiv preprint\narXiv:2205.15868, 2022. 2\n[18] Zehuan Huang, Yuan-Chen Guo, Haoran Wang, Ran Yi,\nLizhuang Ma, Yan-Pei Cao, and Lu Sheng.\nMv-adapter:\nMulti-view consistent image generation made easy.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 16377‚Äì16387, 2025. 2\n[19] Allan Jabri, David Fleet, and Ting Chen.\nScalable adap-\ntive computation for iterative generation.\narXiv preprint\narXiv:2212.11972, 2022. 13\n[20] Dengyang Jiang, Mengmeng Wang, Liuzhuozheng Li, Lei\nZhang, Haoyu Wang, Wei Wei, Guang Dai, Yanning Zhang,\nand Jingdong Wang. No other representation component is\nneeded: Diffusion transformers can provide representation\nguidance by themselves. arXiv preprint arXiv:2505.02831,\n2025. 2, 8, 12\n[21] Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James M\nRehg, and Pinar Yanardag. Rave: Randomized noise shuf-\nfling for fast and consistent video editing with diffusion mod-\nels. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 6507‚Äì6516,\n2024. 2\n[22] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks,\n2019. 1\n[23] Diederik Kingma and Ruiqi Gao.\nUnderstanding diffu-\nsion objectives as the elbo with simple data augmentation.\nAdvances in Neural Information Processing Systems, 36:\n65484‚Äì65516, 2023. 8\n[24] W Kong, Q Tian, Z Zhang, R Min, Z Dai, J Zhou, J Xiong,\nX Li, B Wu, J Zhang, et al. Hunyuanvideo: A systematic\nframework for large video generative models, 2025. URL\nhttps://arxiv. org/abs/2412.03603. 2, 3\n[25] Tuomas Kynk¬®a¬®anniemi, Tero Karras, Samuli Laine, Jaakko\nLehtinen, and Timo Aila. Improved precision and recall met-\nric for assessing generative models. Advances in neural in-\nformation processing systems, 32, 2019. 4, 12\n[26] Xingjian Leng, Jaskirat Singh, Yunzhong Hou, Zhenchang\nXing, Saining Xie, and Liang Zheng. Repa-e: Unlocking\nvae for end-to-end tuning with latent diffusion transformers.\narXiv preprint arXiv:2504.10483, 2025. 2, 8, 12\n[27] Alexander C Li, Mihir Prabhudesai, Shivam Duggal, Ellis\nBrown, and Deepak Pathak. Your diffusion model is secretly\na zero-shot classifier. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 2206‚Äì2217,\n2023. 2\n9", "clean_text": "References [1] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: A vit backbone for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 22669‚Äì22679, 2023. 21 [2] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 3 [3] Shoufa Chen, Chongjian Ge, Shilong Zhang, Peize Sun, and Ping Luo. Pixelflow: Pixel-space generative models with flow. arXiv preprint arXiv:2504.07963, 2025. 13 [4] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 1597‚Äì1607. PmLR, 2020. 6 [5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248‚Äì255. IEEE, 2009. 4, 5 [6] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780‚Äì8794, 2021. 1, 8, 12, 13 [7] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 2 [8] Dave Epstein, Allan Jabri, Ben Poole, Alexei Efros, and Aleksander Holynski. Diffusion self-guidance for controllable image generation. Advances in Neural Information Processing Systems, 36:16222‚Äì16239, 2023. 2 [9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M¬®uller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 2, 21 [10] Wan-Cyuan Fan, Yen-Chun Chen, DongDong Chen, Yu Cheng, Lu Yuan, and Yu-Chiang Frank Wang. Frido: Feature pyramid diffusion for complex scene image synthesis. In Proceedings of the AAAI conference on artificial intelligence, pages 579‚Äì587, 2023. 21 [11] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Mdtv2: Masked diffusion transformer is a strong image synthesizer. arXiv preprint arXiv:2303.14389, 2023. 8, 12 [12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139‚Äì144, 2020. 1 [13] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10696‚Äì10706, 2022. 21 [14] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017. 4, 12 [15] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 3, 4 [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840‚Äì6851, 2020. 1, 2 [17] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 2 [18] Zehuan Huang, Yuan-Chen Guo, Haoran Wang, Ran Yi, Lizhuang Ma, Yan-Pei Cao, and Lu Sheng. Mv-adapter: Multi-view consistent image generation made easy. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16377‚Äì16387, 2025. 2 [19] Allan Jabri, David Fleet, and Ting Chen. Scalable adaptive computation for iterative generation. arXiv preprint arXiv:2212.11972, 2022. 13 [20] Dengyang Jiang, Mengmeng Wang, Liuzhuozheng Li, Lei Zhang, Haoyu Wang, Wei Wei, Guang Dai, Yanning Zhang, and Jingdong Wang. No other representation component is needed: Diffusion transformers can provide representation guidance by themselves. arXiv preprint arXiv:2505.02831, 2025. 2, 8, 12 [21] Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James M Rehg, and Pinar Yanardag. Rave: Randomized noise shuffling for fast and consistent video editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6507‚Äì6516, 2024. 2 [22] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks, 2019. 1 [23] Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives as the elbo with simple data augmentation. Advances in Neural Information Processing Systems, 36: 65484‚Äì65516, 2023. 8 [24] W Kong, Q Tian, Z Zhang, R Min, Z Dai, J Zhou, J Xiong, X Li, B Wu, J Zhang, et al. Hunyuanvideo: A systematic framework for large video generative models, 2025. URL https://arxiv. org/abs/2412.03603. 2, 3 [25] Tuomas Kynk¬®a¬®anniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in neural information processing systems, 32, 2019. 4, 12 [26] Xingjian Leng, Jaskirat Singh, Yunzhong Hou, Zhenchang Xing, Saining Xie, and Liang Zheng. Repa-e: Unlocking vae for end-to-end tuning with latent diffusion transformers. arXiv preprint arXiv:2504.10483, 2025. 2, 8, 12 [27] Alexander C Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, and Deepak Pathak. Your diffusion model is secretly a zero-shot classifier. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2206‚Äì2217, 2023. 2 9"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 10, "text": "[28] Tianhong Li and Kaiming He.\nBack to basics:\nLet\ndenoising generative models denoise.\narXiv preprint\narXiv:2511.13720, 2025. 13\n[29] Kuan Heng Lin, Sicheng Mo, Ben Klingher, Fangzhou\nMu, and Bolei Zhou.\nCtrl-x: Controlling structure and\nappearance for text-to-image generation without guidance.\nAdvances in Neural Information Processing Systems, 37:\n128911‚Äì128939, 2024. 2\n[30] Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holyn-\nski, and Trevor Darrell. Diffusion hyperfeatures: Search-\ning through time and space for semantic correspondence.\nAdvances in Neural Information Processing Systems, 36:\n47500‚Äì47510, 2023. 2\n[31] Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M\nBoffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Explor-\ning flow and diffusion-based generative models with scalable\ninterpolant transformers. In European Conference on Com-\nputer Vision, pages 23‚Äì40. Springer, 2024. 4, 8, 12\n[32] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Zi-\nwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte:\nLatent diffusion transformer for video generation.\narXiv\npreprint arXiv:2401.03048, 2024. 2\n[33] Sicheng Mo, Fangzhou Mu, Kuan Heng Lin, Yanli Liu,\nBochen Guan, Yin Li, and Bolei Zhou.\nFreecontrol:\nTraining-free spatial control of any text-to-image diffusion\nmodel with any condition. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 7465‚Äì7475, 2024. 2\n[34] Sicheng Mo, Thao Nguyen, Xun Huang, Siddharth Srini-\nvasan Iyer, Yijun Li, Yuchen Liu, Abhishek Tandon, Eli\nShechtman, Krishna Kumar Singh, Yong Jae Lee, et al. X-\nfusion: Introducing new modality to frozen large language\nmodels. arXiv preprint arXiv:2504.20996, 2025. 2, 3\n[35] Thao Nguyen, Yuheng Li, Utkarsh Ojha, and Yong Jae\nLee. Visual instruction inversion: Image editing via image\nprompting. Advances in Neural Information Processing Sys-\ntems, 36:9598‚Äì9613, 2023. 2\n[36] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models.\nIn International\nconference on machine learning, pages 8162‚Äì8171. PMLR,\n2021. 1, 4, 5\n[37] OpenAI.\nSora: Creating video from text ‚Äî openai.com.\nhttps://openai.com/index/sora/, 2025.\n[Ac-\ncessed 30-04-2025]. 2\n[38] Maxime Oquab,\nTimoth¬¥ee Darcet,\nTh¬¥eo Moutakanni,\nHuy Q. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernan-\ndez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby,\nMahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russ\nHowes, Po-Yao (Bernie) Huang, Shang-Wen Li, Ishan Misra,\nMichael G. Rabbat, Vasu Sharma, Gabriel Synnaeve, Huijiao\nXu, Herv¬¥e J¬¥egou, Julien Mairal, Patrick Labatut, Armand\nJoulin, and Piotr Bojanowski. Dinov2: Learning robust vi-\nsual features without supervision. ArXiv, abs/2304.07193,\n2023. 4, 6\n[39] Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao\nTan, Kai Zhang, William T Freeman, and Yu-Xiong Wang.\nRandar: Decoder-only autoregressive visual generation in\nrandom orders. In Proceedings of the Computer Vision and\nPattern Recognition Conference, pages 45‚Äì55, 2025. 8, 12\n[40] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision, pages 4195‚Äì4205,\n2023. 3, 4, 5, 8, 12, 21\n[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. arXiv preprint arXiv:2103.00020, 2021. 4, 6\n[42] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 2\n[43] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick\nEsser, and Bj¬®orn Ommer. High-resolution image synthesis\nwith latent diffusion models. CVPR, 2022. 2, 8, 12\n[44] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¬®orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684‚Äì10695, 2022. 1, 2\n[45] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¬®orn Ommer. High-resolution image syn-\nthesis with latent diffusion models, 2022. 2, 4\n[46] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\nCheung, Alec Radford, and Xi Chen. Improved techniques\nfor training gans. Advances in neural information processing\nsystems, 29, 2016. 4, 12\n[47] Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang,\nXi Victoria Lin, Luke Zettlemoyer, and Lili Yu. Llamafu-\nsion: Adapting pretrained language models for multimodal\ngeneration. arXiv preprint arXiv:2412.15188, 2024. 2, 3\n[48] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro\nChin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang,\nGlenn Entis, Yuanzhen Li, et al. Styledrop: Text-to-image\ngeneration in any style. arXiv preprint arXiv:2306.00983,\n2023. 2\n[49] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue\nPeng, Ping Luo, and Zehuan Yuan. Autoregressive model\nbeats diffusion: Llama for scalable image generation. arXiv\npreprint arXiv:2406.06525, 2024. 12\n[50] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue\nPeng, Ping Luo, and Zehuan Yuan. Autoregressive model\nbeats diffusion: Llama for scalable image generation. arXiv\npreprint arXiv:2406.06525, 2024. 8\n[51] Raphael Tang, Linqing Liu, Akshat Pandey, Zhiying Jiang,\nGefei Yang, Karun Kumar, Pontus Stenetorp, Jimmy Lin,\nand Ferhan Ture. What the daam: Interpreting stable diffu-\nsion using cross attention. arXiv preprint arXiv:2210.04885,\n2022. 2\n[52] Ming Tao, Hao Tang, Fei Wu, Xiao-Yuan Jing, Bing-Kun\nBao, and Changsheng Xu.\nDf-gan: A simple and effec-\ntive baseline for text-to-image synthesis. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 16515‚Äì16525, 2022. 21\n10", "clean_text": "[28] Tianhong Li and Kaiming He. Back to basics: Let denoising generative models denoise. arXiv preprint arXiv:2511.13720, 2025. 13 [29] Kuan Heng Lin, Sicheng Mo, Ben Klingher, Fangzhou Mu, and Bolei Zhou. Ctrl-x: Controlling structure and appearance for text-to-image generation without guidance. Advances in Neural Information Processing Systems, 37: 128911‚Äì128939, 2024. 2 [30] Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski, and Trevor Darrell. Diffusion hyperfeatures: Searching through time and space for semantic correspondence. Advances in Neural Information Processing Systems, 36: 47500‚Äì47510, 2023. 2 [31] Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pages 23‚Äì40. Springer, 2024. 4, 8, 12 [32] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 2 [33] Sicheng Mo, Fangzhou Mu, Kuan Heng Lin, Yanli Liu, Bochen Guan, Yin Li, and Bolei Zhou. Freecontrol: Training-free spatial control of any text-to-image diffusion model with any condition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7465‚Äì7475, 2024. 2 [34] Sicheng Mo, Thao Nguyen, Xun Huang, Siddharth Srinivasan Iyer, Yijun Li, Yuchen Liu, Abhishek Tandon, Eli Shechtman, Krishna Kumar Singh, Yong Jae Lee, et al. Xfusion: Introducing new modality to frozen large language models. arXiv preprint arXiv:2504.20996, 2025. 2, 3 [35] Thao Nguyen, Yuheng Li, Utkarsh Ojha, and Yong Jae Lee. Visual instruction inversion: Image editing via image prompting. Advances in Neural Information Processing Systems, 36:9598‚Äì9613, 2023. 2 [36] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 8162‚Äì8171. PMLR, 2021. 1, 4, 5 [37] OpenAI. Sora: Creating video from text ‚Äî openai.com. https://openai.com/index/sora/, 2025. [Accessed 30-04-2025]. 2 [38] Maxime Oquab, Timoth¬¥ee Darcet, Th¬¥eo Moutakanni, Huy Q. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russ Howes, Po-Yao (Bernie) Huang, Shang-Wen Li, Ishan Misra, Michael G. Rabbat, Vasu Sharma, Gabriel Synnaeve, Huijiao Xu, Herv¬¥e J¬¥egou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision. ArXiv, abs/2304.07193, 2023. 4, 6 [39] Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao Tan, Kai Zhang, William T Freeman, and Yu-Xiong Wang. Randar: Decoder-only autoregressive visual generation in random orders. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 45‚Äì55, 2025. 8, 12 [40] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 4195‚Äì4205, 2023. 3, 4, 5, 8, 12, 21 [41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021. 4, 6 [42] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 2 [43] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Bj¬®orn Ommer. High-resolution image synthesis with latent diffusion models. CVPR, 2022. 2, 8, 12 [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¬®orn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684‚Äì10695, 2022. 1, 2 [45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¬®orn Ommer. High-resolution image synthesis with latent diffusion models, 2022. 2, 4 [46] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. 4, 12 [47] Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, and Lili Yu. Llamafusion: Adapting pretrained language models for multimodal generation. arXiv preprint arXiv:2412.15188, 2024. 2, 3 [48] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang, Glenn Entis, Yuanzhen Li, et al. Styledrop: Text-to-image generation in any style. arXiv preprint arXiv:2306.00983, 2023. 2 [49] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 12 [50] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 8 [51] Raphael Tang, Linqing Liu, Akshat Pandey, Zhiying Jiang, Gefei Yang, Karun Kumar, Pontus Stenetorp, Jimmy Lin, and Ferhan Ture. What the daam: Interpreting stable diffusion using cross attention. arXiv preprint arXiv:2210.04885, 2022. 2 [52] Ming Tao, Hao Tang, Fei Wu, Xiao-Yuan Jing, Bing-Kun Bao, and Changsheng Xu. Df-gan: A simple and effective baseline for text-to-image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16515‚Äì16525, 2022. 21 10"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 11, "text": "[53] Chameleon Team. Chameleon: Mixed-modal early-fusion\nfoundation models. arXiv preprint arXiv:2405.09818, 2024.\n3\n[54] Junjiao Tian, Lavisha Aggarwal, Andrea Colaco, Zsolt Kira,\nand Mar Gonzalez-Franco. Diffuse attend and segment: Un-\nsupervised zero-shot segmentation using stable diffusion. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 3554‚Äì3563, 2024. 2\n[55] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Li-\nwei Wang. Visual autoregressive modeling: Scalable image\ngeneration via next-scale prediction. Advances in neural in-\nformation processing systems, 37:84839‚Äì84865, 2024. 8, 12\n[56] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali\nDekel.\nPlug-and-play diffusion features for text-driven\nimage-to-image translation. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 1921‚Äì1930, 2023. 2\n[57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 2\n[58] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao,\nChen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao\nYang, et al. Wan: Open and advanced large-scale video gen-\nerative models. arXiv preprint arXiv:2503.20314, 2025. 2,\n3\n[59] Runqian Wang and Kaiming He. Diffuse and disperse: Im-\nage generation with representation regularization.\narXiv\npreprint arXiv:2506.09027, 2025. 2, 8, 12\n[60] Shuai Wang, Ziteng Gao, Chenhui Zhu, Weilin Huang, and\nLimin Wang. Pixnerd: Pixel neural field diffusion. arXiv\npreprint arXiv:2507.23268, 2025. 13\n[61] Shuai Wang, Zhi Tian, Weilin Huang, and Limin Wang.\nDdt:\nDecoupled diffusion transformer.\narXiv preprint\narXiv:2504.05741, 2025. 8, 12\n[62] Ge Wu, Shen Zhang, Ruijing Shi, Shanghua Gao, Zhenyuan\nChen, Lei Wang, Zhaowei Chen, Hongcheng Gao, Yao Tang,\nJian Yang, et al. Representation entanglement for genera-\ntion: Training diffusion transformers is much easier than you\nthink. arXiv preprint arXiv:2507.01467, 2025. 2\n[63] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiao-\nlong Wang, and Shalini De Mello. Open-vocabulary panop-\ntic segmentation with text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 2955‚Äì2966, 2023. 2\n[64] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang,\nZhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-\ngrained text to image generation with attentional generative\nadversarial networks. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 1316‚Äì\n1324, 2018. 21\n[65] Jiawei Yang, Tianhong Li, Lijie Fan, Yonglong Tian, and\nYue Wang. Latent denoising makes good visual tokenizers.\narXiv preprint arXiv:2507.15856, 2025. 6\n[66] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu\nHuang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-\nhan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video\ndiffusion models with an expert transformer. arXiv preprint\narXiv:2408.06072, 2024. 2\n[67] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon\nJeong, Jonathan Huang, Jinwoo Shin, and Saining Xie.\nRepresentation alignment for generation:\nTraining dif-\nfusion transformers is easier than you think.\nArXiv,\nabs/2410.06940, 2024. 2, 8, 12\n[68] Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and\nYinfei Yang. Cross-modal contrastive learning for text-to-\nimage generation.\nIn Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n833‚Äì842, 2021. 21\n[69] Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Pola-\nnia Cabrera, Varun Jampani, Deqing Sun, and Ming-Hsuan\nYang. A tale of two features: Stable diffusion complements\ndino for zero-shot semantic correspondence.\nAdvances in\nNeural Information Processing Systems, 36:45533‚Äì45547,\n2023. 2\n[70] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima\nAnandkumar. Fast training of diffusion models with masked\ntransformers. arXiv preprint arXiv:2306.09305, 2023. 2, 8,\n12\n[71] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen,\nShenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang\nYou. Open-sora: Democratizing efficient video production\nfor all. arXiv preprint arXiv:2412.20404, 2024. 2\n[72] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala,\nMichihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe\nMa, Luke Zettlemoyer, and Omer Levy. Transfusion: Pre-\ndict the next token and diffuse images with one multi-modal\nmodel. 2024. 2, 3\n[73] Mingyuan\nZhou,\nHuangjie\nZheng,\nZhendong\nWang,\nMingzhang Yin, and Hai Huang. Score identity distillation:\nExponentially fast distillation of pretrained diffusion models\nfor one-step generation. In Forty-first International Confer-\nence on Machine Learning, 2024. 13\n[74] Y Zhou, R Zhang, C Chen, C Li, C Tensmeyer, T Yu,\nJ Gu, J Xu, and T Sun.\nLafite: Towards language-free\ntraining for text-to-image generation. arxiv. arXiv preprint\narXiv:2111.13792, 3, 2022. 21\n[75] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. Dm-gan:\nDynamic memory generative adversarial networks for text-\nto-image synthesis. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n5802‚Äì5810, 2019. 21\n11", "clean_text": "[53] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 3 [54] Junjiao Tian, Lavisha Aggarwal, Andrea Colaco, Zsolt Kira, and Mar Gonzalez-Franco. Diffuse attend and segment: Unsupervised zero-shot segmentation using stable diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3554‚Äì3563, 2024. 2 [55] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:84839‚Äì84865, 2024. 8, 12 [56] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1921‚Äì1930, 2023. 2 [57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 2 [58] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 3 [59] Runqian Wang and Kaiming He. Diffuse and disperse: Image generation with representation regularization. arXiv preprint arXiv:2506.09027, 2025. 2, 8, 12 [60] Shuai Wang, Ziteng Gao, Chenhui Zhu, Weilin Huang, and Limin Wang. Pixnerd: Pixel neural field diffusion. arXiv preprint arXiv:2507.23268, 2025. 13 [61] Shuai Wang, Zhi Tian, Weilin Huang, and Limin Wang. Ddt: Decoupled diffusion transformer. arXiv preprint arXiv:2504.05741, 2025. 8, 12 [62] Ge Wu, Shen Zhang, Ruijing Shi, Shanghua Gao, Zhenyuan Chen, Lei Wang, Zhaowei Chen, Hongcheng Gao, Yao Tang, Jian Yang, et al. Representation entanglement for generation: Training diffusion transformers is much easier than you think. arXiv preprint arXiv:2507.01467, 2025. 2 [63] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Open-vocabulary panoptic segmentation with text-to-image diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2955‚Äì2966, 2023. 2 [64] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Finegrained text to image generation with attentional generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1316‚Äì 1324, 2018. 21 [65] Jiawei Yang, Tianhong Li, Lijie Fan, Yonglong Tian, and Yue Wang. Latent denoising makes good visual tokenizers. arXiv preprint arXiv:2507.15856, 2025. 6 [66] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2 [67] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. ArXiv, abs/2410.06940, 2024. 2, 8, 12 [68] Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang. Cross-modal contrastive learning for text-toimage generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 833‚Äì842, 2021. 21 [69] Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Polania Cabrera, Varun Jampani, Deqing Sun, and Ming-Hsuan Yang. A tale of two features: Stable diffusion complements dino for zero-shot semantic correspondence. Advances in Neural Information Processing Systems, 36:45533‚Äì45547, 2023. 2 [70] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. arXiv preprint arXiv:2306.09305, 2023. 2, 8, 12 [71] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. 2 [72] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. 2024. 2, 3 [73] Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In Forty-first International Conference on Machine Learning, 2024. 13 [74] Y Zhou, R Zhang, C Chen, C Li, C Tensmeyer, T Yu, J Gu, J Xu, and T Sun. Lafite: Towards language-free training for text-to-image generation. arxiv. arXiv preprint arXiv:2111.13792, 3, 2022. 21 [75] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. Dm-gan: Dynamic memory generative adversarial networks for textto-image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5802‚Äì5810, 2019. 21 11"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 12, "text": "Supplementary\nA. Implementation Details\n12\nA.1. Baselines . . . . . . . . . . . . . . . . . . .\n12\nA.2. Evaluation Metric\n. . . . . . . . . . . . . .\n12\nA.3. Hyperparameter\n. . . . . . . . . . . . . . .\n12\nB. Experiment\n12\nB.1. Ablations . . . . . . . . . . . . . . . . . . .\n12\nB.2. Extending to Pixel Diffusion.\n. . . . . . . .\n13\nB.3. Additional Qualitative Results.\n. . . . . . .\n13\nB.4. Cross-Sample Score Visualization . . . . . .\n21\nB.5. Text-to-Image Generation\n. . . . . . . . . .\n21\nA. Implementation Details\nA.1. Baselines\nWe introduce the baselines of the leading generative sys-\ntems as follows:\n‚Ä¢ ADM [6] leverages classifier for guiding diffusion sam-\npling to improve generation.\n‚Ä¢ LDM [43] presents latent diffusion, enabling fast, high-\nresolution generation by training diffusion models in a\nlatent space.\n‚Ä¢ MDTv2 [11] combines masked token modeling with\ndiffusion transformers to learn visual representations.\n‚Ä¢ VAR [55] introduces next-scale prediction to autore-\ngressive generative models.\n‚Ä¢ LlamaGen [49] shows vanilla autoregressive models\ncould achieve strong generation performance at scale,\noutperforming diffusion baselines.\n‚Ä¢ RandAR [39] proposes a decoder-only autoregressive\nmodel that utilizes position instruction tokens to gener-\nate image tokens in arbitrary orders.\n‚Ä¢ MaskDiT [70] uses masked input patches and an\nasymmetric encoder-decoder to achieve faster diffusion\nmodel training.\n‚Ä¢ DiT [40] proposes a scalable transformer architecture\nbased on AdaIN-zero for diffusion model training.\n‚Ä¢ SiT [31] further improves the efficiency and scalability\non DiT by introducing flow matching.\n‚Ä¢ REPA [67] analyzes the alignment between feature\nquality and generation fidelity of diffusion backbone\nand accelerates diffusion model training by aligning dif-\nfusion feature with pre-trained vision encoders.\n‚Ä¢ REPA-E [26] enables representation learning inside\ndiffusion backbones by unlocking the latent encoder.\n‚Ä¢ DDT [61] proposes a diffusion architecture that sepa-\nrates semantic encoding from high-frequency decoding\nto accelerate convergence during training.\n‚Ä¢ SRA [20] introduces a simple approach to align cross-\nlayer diffusion backbone features to improve training\nefficiency without a pre-trained vision encoder.\n‚Ä¢ Dispersive Loss [59] introduces a simple regularization\nloss that encourages internal representations to disperse\nin the hidden space to improve diffusion model training.\nA.2. Evaluation Metric\nWe use the conventional evaluation pipelines for class-\nconditional generative models, following ADM [6]. Specif-\nically, we introduce the focusing concept of each metric:\n‚Ä¢ Fr¬¥echet Inception Distance (FID) [14] evaluates the\nfeature distance of generated images and the reference\nsamples. Lower FID usually suggests better generation\nfidelity and diversity.\n‚Ä¢ Inception Score (IS) [46] measures image quality and\ndiversity based on how confidently a classifier recog-\nnizes each image and how varied the generated classes\nare. A higher Inception Score indicates a more mean-\ningful image within each class.\n‚Ä¢ Precision and recall [25]. Precision captures the re-\nalism of generated images, while recall captures their\ndiversity relative to real data.\nA.3. Hyperparameter\nIn Table 5,we introduce the hyperparameter setting for mod-\nels reported at Table 3.\nB. Experiment\nB.1. Ablations\nGroupDiff-f: group size. We additionally investigate into\nthe group size in GroupDiff-f setting.\nFigure 9 shows\nthe which images shares the same group during infer-\nence. We compare the uncurated samples from GroupDiff-\nf-{1,2,3,4} in Figure 10 and Figure 11. Our observation\non GroupDiff-f aligns that of GroupDiff-l, where increas-\ning the group size considerably improves the generation fi-\ndelity.\nGroupDiff-f-1\nGroupDiff-f-2\nGroupDiff-f-4\nGroupDiff-f-8\nSample-1\nSample-2\nSample-6\nSample-8\nSample-3\nSample-4\nSample-5\nSample-7\nFigure 9. Group attention illustration. In each row, samples in\nthe sample group shares the same color block.\nGroupDiff-l* : query method.\nBeyond training from\nscratch, resuming from individual diffusion offers an effi-\ncient solution to adding GroupDiff over existing pipelines.\nThus, we also explore different query methods under this\n12", "clean_text": "Supplementary A. Implementation Details 12 A.1. Baselines . . . . . . . . . . . . . . . . . . . 12 A.2. Evaluation Metric . . . . . . . . . . . . . . 12 A.3. Hyperparameter . . . . . . . . . . . . . . . 12 B. Experiment 12 B.1. Ablations . . . . . . . . . . . . . . . . . . . 12 B.2. Extending to Pixel Diffusion. . . . . . . . . 13 B.3. Additional Qualitative Results. . . . . . . . 13 B.4. Cross-Sample Score Visualization . . . . . . 21 B.5. Text-to-Image Generation . . . . . . . . . . 21 A. Implementation Details A.1. Baselines We introduce the baselines of the leading generative systems as follows: ‚Ä¢ ADM [6] leverages classifier for guiding diffusion sampling to improve generation. ‚Ä¢ LDM [43] presents latent diffusion, enabling fast, highresolution generation by training diffusion models in a latent space. ‚Ä¢ MDTv2 [11] combines masked token modeling with diffusion transformers to learn visual representations. ‚Ä¢ VAR [55] introduces next-scale prediction to autoregressive generative models. ‚Ä¢ LlamaGen [49] shows vanilla autoregressive models could achieve strong generation performance at scale, outperforming diffusion baselines. ‚Ä¢ RandAR [39] proposes a decoder-only autoregressive model that utilizes position instruction tokens to generate image tokens in arbitrary orders. ‚Ä¢ MaskDiT [70] uses masked input patches and an asymmetric encoder-decoder to achieve faster diffusion model training. ‚Ä¢ DiT [40] proposes a scalable transformer architecture based on AdaIN-zero for diffusion model training. ‚Ä¢ SiT [31] further improves the efficiency and scalability on DiT by introducing flow matching. ‚Ä¢ REPA [67] analyzes the alignment between feature quality and generation fidelity of diffusion backbone and accelerates diffusion model training by aligning diffusion feature with pre-trained vision encoders. ‚Ä¢ REPA-E [26] enables representation learning inside diffusion backbones by unlocking the latent encoder. ‚Ä¢ DDT [61] proposes a diffusion architecture that separates semantic encoding from high-frequency decoding to accelerate convergence during training. ‚Ä¢ SRA [20] introduces a simple approach to align crosslayer diffusion backbone features to improve training efficiency without a pre-trained vision encoder. ‚Ä¢ Dispersive Loss [59] introduces a simple regularization loss that encourages internal representations to disperse in the hidden space to improve diffusion model training. A.2. Evaluation Metric We use the conventional evaluation pipelines for classconditional generative models, following ADM [6]. Specifically, we introduce the focusing concept of each metric: ‚Ä¢ Fr¬¥echet Inception Distance (FID) [14] evaluates the feature distance of generated images and the reference samples. Lower FID usually suggests better generation fidelity and diversity. ‚Ä¢ Inception Score (IS) [46] measures image quality and diversity based on how confidently a classifier recognizes each image and how varied the generated classes are. A higher Inception Score indicates a more meaningful image within each class. ‚Ä¢ Precision and recall [25]. Precision captures the realism of generated images, while recall captures their diversity relative to real data. A.3. Hyperparameter In Table 5,we introduce the hyperparameter setting for models reported at Table 3. B. Experiment B.1. Ablations GroupDiff-f: group size. We additionally investigate into the group size in GroupDiff-f setting. Figure 9 shows the which images shares the same group during inference. We compare the uncurated samples from GroupDifff-{1,2,3,4} in Figure 10 and Figure 11. Our observation on GroupDiff-f aligns that of GroupDiff-l, where increasing the group size considerably improves the generation fidelity. GroupDiff-f-1 GroupDiff-f-2 GroupDiff-f-4 GroupDiff-f-8 Sample-1 Sample-2 Sample-6 Sample-8 Sample-3 Sample-4 Sample-5 Sample-7 Figure 9. Group attention illustration. In each row, samples in the sample group shares the same color block. GroupDiff-l* : query method. Beyond training from scratch, resuming from individual diffusion offers an efficient solution to adding GroupDiff over existing pipelines. Thus, we also explore different query methods under this 12"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 13, "text": "DiT-XL/2\nSiT-XL/2\nSiT-XL/2-REPA\nGroupDiff-4\nGroupDiff-4*\nGroupDiff-4\nGroupDiff-4*\nGroupDiff-4*\nArchitecture\nInput dim.\n32 √ó 32√ó 4\n32 √ó 32 √ó 4\n32 √ó 32 √ó 4\n32 √ó 32 √ó 4\n32 √ó 32 √ó 4\nNum. layers\n28\n28\n28\n28\n28\nHidden dim.\n1,152\n1,152\n1,152\n1,152\n1,152\nNum. heads\n16\n16\n16\n16\n16\nOptimization\nResume\n-\nDiT-XL/2-7M\n-\nSiT-XL/2-7M\nREPA-4M\nTraining Iteration\n4M\n500K\n4M\n500K\n500K\nBatch Size\n256\n256\n256\n256\n256\nOptimzier\nAdamW\nAdamW\nAdamW\nAdamW\nAdamW\nlr\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\nbetas\n(0.9, 0.999)\n(0.9, 0.999)\n(0.9, 0.999)\n(0.9, 0.999)\n(0.9, 0.999)\nweight decay\n0.01\n0.01\n0.01\n0.01\n0.01\nGroupDiff\nMode\nGroupDiff-l\nGroupDiff-l\nGroupDiff-l\nGroupDiff-l\nGroupDiff-l\nQuery Method\nCLIP-L\nCLIP-L\nCLIP-L\nCLIP-L\nCLIP-L\nœÑimg\n0.7\n0.7\n0.7\n0.7\n0.7\nGroup Size\n4\n4\n4\n4\n4\nNoise Var.\n50\n50\n50\n50\n0\nInference\nSteps\n250\n250\n250\n250\n250\nGuidance Scale\n1.70\n1.60\n2.35\n1.85\n2.575\nGuidance Interval\n(0,1)\n(0,1)\n(0.25,1.0)\n(0.15,1.0)\n(0.25,0.75)\nTable 5. Hyperparameter setup.\nMethod\nQuery Method\nFID ‚Üì\nIS ‚Üë\nPre.‚Üë\nRec.‚Üë\nSiT-XL/2\n-\n2.06\n270.3\n0.82\n0.59\n+ GroupDiff-4*\nClass\n1.76\n283.5\n0.81\n0.61\n+ GroupDiff-4*\nCLIP-L\n1.40\n290.7\n0.79\n0.64\nTable 6. Ablation: query method.\n‚àó: continue training from\npre-trained checkpoint for an additional 100 epochs.\nsetting. Table 6 shows CLIP-L yields the optimality perfor-\nmance while the simplest GroupDiff-4‚àóobtains a consider-\nable improvement (14.5%) over the baseline, highlighting\nthe effectiveness of cross-sample attention.\nB.2. Extending to Pixel Diffusion.\nWe further validate GroupDiff on pixel diffusion systems.\nAs shown in Table 7, GroupDiff-4 with JiT-B/16 delivers\na substantial 15.8% improvement with only 100 additional\ntraining steps when resumed from a pre-trained model. This\nagain highlights the effectiveness of cross-sample collabo-\nration in pixel diffusion and its strong potential for broader\napplicability.\nMethod\nparams\nFID\nIS\nADM-G [6]\n559M\n7.72\n172.7\nRIN [19]\n320M\n3.95\n216\nSiD [73], UViT/2\n2B\n2.44\n256.3\nPixelFlow [3], XL/4\n677M\n1.98\n282.1\nPixNerd [60], XL/16\n700M\n2.15\n297\nJiT-H/16 [28]\n953M\n1.86\n303.4\nJiT-B/16 [28]\n131M\n3.66\n275.1\n+ our GroupDiff-4*\n131M\n3.08\n245.6\nTable 7. System-level performance of pixel diffusion models\nevaluated on ImageNet 256√ó256. ‚àó: continue training from pre-\ntrained checkpoint for an additional 100 epochs.\nB.3. Additional Qualitative Results.\nWe provide additional uncurated samples generated by\nGroupDiff-4 in Figures 14‚Äì26.\n13", "clean_text": "DiT-XL/2 SiT-XL/2 SiT-XL/2-REPA GroupDiff-4 GroupDiff-4* GroupDiff-4 GroupDiff-4* GroupDiff-4* Architecture Input dim. 32 √ó 32√ó 4 32 √ó 32 √ó 4 32 √ó 32 √ó 4 32 √ó 32 √ó 4 32 √ó 32 √ó 4 Num. layers 28 28 28 28 28 Hidden dim. 1,152 1,152 1,152 1,152 1,152 Num. heads 16 16 16 16 16 Optimization Resume DiT-XL/2-7M SiT-XL/2-7M REPA-4M Training Iteration 4M 500K 4M 500K 500K Batch Size 256 256 256 256 256 Optimzier AdamW AdamW AdamW AdamW AdamW lr 0.0001 0.0001 0.0001 0.0001 0.0001 betas (0.9, 0.999) (0.9, 0.999) (0.9, 0.999) (0.9, 0.999) (0.9, 0.999) weight decay 0.01 0.01 0.01 0.01 0.01 GroupDiff Mode GroupDiff-l GroupDiff-l GroupDiff-l GroupDiff-l GroupDiff-l Query Method CLIP-L CLIP-L CLIP-L CLIP-L CLIP-L œÑimg 0.7 0.7 0.7 0.7 0.7 Group Size 4 4 4 4 4 Noise Var. 50 50 50 50 0 Inference Steps 250 250 250 250 250 Guidance Scale 1.70 1.60 2.35 1.85 2.575 Guidance Interval (0,1) (0,1) (0.25,1.0) (0.15,1.0) (0.25,0.75) Table 5. Hyperparameter setup. Method Query Method FID ‚Üì IS ‚Üë Pre.‚Üë Rec.‚Üë SiT-XL/2 2.06 270.3 0.82 0.59 + GroupDiff-4* Class 1.76 283.5 0.81 0.61 + GroupDiff-4* CLIP-L 1.40 290.7 0.79 0.64 Table 6. Ablation: query method. ‚àó: continue training from pre-trained checkpoint for an additional 100 epochs. setting. Table 6 shows CLIP-L yields the optimality performance while the simplest GroupDiff-4‚àóobtains a considerable improvement (14.5%) over the baseline, highlighting the effectiveness of cross-sample attention. B.2. Extending to Pixel Diffusion. We further validate GroupDiff on pixel diffusion systems. As shown in Table 7, GroupDiff-4 with JiT-B/16 delivers a substantial 15.8% improvement with only 100 additional training steps when resumed from a pre-trained model. This again highlights the effectiveness of cross-sample collaboration in pixel diffusion and its strong potential for broader applicability. Method params FID IS ADM-G [6] 559M 7.72 172.7 RIN [19] 320M 3.95 216 SiD [73], UViT/2 2B 2.44 256.3 PixelFlow [3], XL/4 677M 1.98 282.1 PixNerd [60], XL/16 700M 2.15 297 JiT-H/16 [28] 953M 1.86 303.4 JiT-B/16 [28] 131M 3.66 275.1 + our GroupDiff-4* 131M 3.08 245.6 Table 7. System-level performance of pixel diffusion models evaluated on ImageNet 256√ó256. ‚àó: continue training from pretrained checkpoint for an additional 100 epochs. B.3. Additional Qualitative Results. We provide additional uncurated samples generated by GroupDiff-4 in Figures 14‚Äì26. 13"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 14, "text": "GroupDiff-f-1\nGroupDiff-f-2\nGroupDiff-f-4\nGroupDiff-f-8\nGroupDiff-f-1\nGroupDiff-f-2\nGroupDiff-f-4\nGroupDiff-f-8\nGroupDiff-f-1\nGroupDiff-f-2\nGroupDiff-f-4\nGroupDiff-f-8\nFigure 10. Uncurated generation results of GroupDiff-f without classifier-free guidance. Examples of class-conditional generation on\nImageNet 256√ó256. GroupDiff with a larger group size consistently obtains better generation fidelity.\n14", "clean_text": "GroupDiff-f-1 GroupDiff-f-2 GroupDiff-f-4 GroupDiff-f-8 GroupDiff-f-1 GroupDiff-f-2 GroupDiff-f-4 GroupDiff-f-8 GroupDiff-f-1 GroupDiff-f-2 GroupDiff-f-4 GroupDiff-f-8 Figure 10. Uncurated generation results of GroupDiff-f without classifier-free guidance. Examples of class-conditional generation on ImageNet 256√ó256. GroupDiff with a larger group size consistently obtains better generation fidelity. 14"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 15, "text": "GroupDiff-f-1\nGroupDiff-f-2\nGroupDiff-f-4\nGroupDiff-f-8\nGroupDiff-f-1\nGroupDiff-f-2\nGroupDiff-f-4\nGroupDiff-f-8\nGroupDiff-f-1\nGroupDiff-f-2\nGroupDiff-f-4\nGroupDiff-f-8\nFigure 11. Uncurated generation results of GroupDiff-f without classifier-free guidance. Examples of class-conditional generation on\nImageNet 256√ó256.GroupDiff with a larger group size consistently obtains better generation fidelity.\n15", "clean_text": "GroupDiff-f-1 GroupDiff-f-2 GroupDiff-f-4 GroupDiff-f-8 GroupDiff-f-1 GroupDiff-f-2 GroupDiff-f-4 GroupDiff-f-8 GroupDiff-f-1 GroupDiff-f-2 GroupDiff-f-4 GroupDiff-f-8 Figure 11. Uncurated generation results of GroupDiff-f without classifier-free guidance. Examples of class-conditional generation on ImageNet 256√ó256.GroupDiff with a larger group size consistently obtains better generation fidelity. 15"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 16, "text": "Figure 12. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúloggerhead sea\nturtle‚Äù (33).\nFigure 13. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúmacaw‚Äù (88).\nFigure 14. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúsulphur-crested\ncockatoo, Kakatoe galerita, Cacatua galerita‚Äù (89).\n16", "clean_text": "Figure 12. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúloggerhead sea turtle‚Äù (33). Figure 13. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúmacaw‚Äù (88). Figure 14. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúsulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita‚Äù (89). 16"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 17, "text": "Figure 15. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúgolden retriever‚Äù\n(207).\nFigure 16. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚ÄúSiberian husky‚Äù\n(250).\nFigure 17. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúwhite wolf, Arctic\nwolf, Canis lupus tundrarum‚Äù (270).\n17", "clean_text": "Figure 15. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúgolden retriever‚Äù (207). Figure 16. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚ÄúSiberian husky‚Äù (250). Figure 17. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúwhite wolf, Arctic wolf, Canis lupus tundrarum‚Äù (270). 17"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 18, "text": "Figure 18. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚ÄúArctic fox, white\nfox, Alopex lagopus‚Äù (279).\nFigure 19. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúacoustic guitar‚Äù\n(402).\nFigure 20. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúballoon‚Äù (417).\n18", "clean_text": "Figure 18. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚ÄúArctic fox, white fox, Alopex lagopus‚Äù (279). Figure 19. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúacoustic guitar‚Äù (402). Figure 20. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúballoon‚Äù (417). 18"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 19, "text": "Figure 21. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúbaseball‚Äù (429).\nFigure 22. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúfire engine, fire\ntruck‚Äù (555).\nFigure 23. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúspace shuttle‚Äù\n(812).\n19", "clean_text": "Figure 21. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúbaseball‚Äù (429). Figure 22. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúfire engine, fire truck‚Äù (555). Figure 23. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúspace shuttle‚Äù (812). 19"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 20, "text": "Figure 24. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúcheeseburger‚Äù\n(933).\nFigure 25. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúcoral reef‚Äù (973).\nFigure 26. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúvolcano‚Äù (980).\n20", "clean_text": "Figure 24. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúcheeseburger‚Äù (933). Figure 25. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúcoral reef‚Äù (973). Figure 26. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúvolcano‚Äù (980). 20"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 21, "text": "Method\nType\nFID\nAttnGAN [64]\nGAN\n35.49\nDM-GAN [75]\nGAN\n32.64\nVQ-Diffusion [13]\nDiffusion\n19.75\nDF-GAN [52]\nGAN\n19.32\nXMC-GAN [68]\nGAN\n9.33\nFrido [10]\nDiffusion\n8.97\nLAFITE [74]\nGAN\n8.12\nU-Net [1]\nDiffusion\n7.32\nU-ViT-S/2 [1]\nDiffusion\n5.95\nU-ViT/S/2 (Deep) [1]\nDiffusion\n5.45\nMMDiT [9]\nDiffusion\n5.3\nDiT-XL/2 w/ Cross-Attention [40]\nDiffusion\n6.95\n+ our GroupDiff-4\nDiffusion\n6.65\nTable 8. Quantitative comparison on text-to-image generation\n(MS-COCO).\nB.4. Cross-Sample Score Visualization\nAdditionally, we show the relation between FID and cross-\nsample score computed by the group-level mean and max\nof the attention score in Figure 27.\nFID\nCross-Sample Score (%)\nFigure 27.\nFID vs Cross-Sample Score (group-level) Our\nGroupDiff shows a strong correlation (0.94) between cross-\nattention to other samples and generation quality.\nB.5. Text-to-Image Generation\nWe also validate GroupDiff in text-to-image generation. We\nmostly follow the experimental setup used in U-ViT [1] un-\nless otherwise specified: we train the model from scratch on\na train split of the MS-COCO dataset and use a validation\nsplit for evaluation. We use DiT-XL/2 with Cross-Attention\nand train it for 150K iterations with a batch size of 256.\nWe use the frozen CLIP text encoder to extract text prompts\nfrom captions. Table 8 shows that GroupDiff remains effec-\ntive in the T2I generation setting without bells and whistles,\nhighlighting the importance of applying cross-sample atten-\ntion even with text conditions.\n21", "clean_text": "Method Type FID AttnGAN [64] GAN 35.49 DM-GAN [75] GAN 32.64 VQ-Diffusion [13] Diffusion 19.75 DF-GAN [52] GAN 19.32 XMC-GAN [68] GAN 9.33 Frido [10] Diffusion 8.97 LAFITE [74] GAN 8.12 U-Net [1] Diffusion 7.32 U-ViT-S/2 [1] Diffusion 5.95 U-ViT/S/2 (Deep) [1] Diffusion 5.45 MMDiT [9] Diffusion 5.3 DiT-XL/2 w/ Cross-Attention [40] Diffusion 6.95 + our GroupDiff-4 Diffusion 6.65 Table 8. Quantitative comparison on text-to-image generation (MS-COCO). B.4. Cross-Sample Score Visualization Additionally, we show the relation between FID and crosssample score computed by the group-level mean and max of the attention score in Figure 27. FID Cross-Sample Score (%) Figure 27. FID vs Cross-Sample Score (group-level) Our GroupDiff shows a strong correlation (0.94) between crossattention to other samples and generation quality. B.5. Text-to-Image Generation We also validate GroupDiff in text-to-image generation. We mostly follow the experimental setup used in U-ViT [1] unless otherwise specified: we train the model from scratch on a train split of the MS-COCO dataset and use a validation split for evaluation. We use DiT-XL/2 with Cross-Attention and train it for 150K iterations with a batch size of 256. We use the frozen CLIP text encoder to extract text prompts from captions. Table 8 shows that GroupDiff remains effective in the T2I generation setting without bells and whistles, highlighting the importance of applying cross-sample attention even with text conditions. 21"}
