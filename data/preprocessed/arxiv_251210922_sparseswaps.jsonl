{"pdf_id": "arxiv_251210922_sparseswaps", "page": 1, "text": "SPARSESWAPS:\nTRACTABLE LLM PRUNING MASK\nREFINEMENT AT SCALE\nMax Zimmer, Christophe Roux, Moritz Wagner, Deborah Hendrych, Sebastian Pokutta\nDepartment for AI in Society, Science, and Technology, Zuse Institute Berlin, Germany\nInstitute of Mathematics, Technische Universit¨at Berlin, Germany\n{zimmer,roux,wagner,hendrych,pokutta}@zib.de\nABSTRACT\nThe resource requirements of Neural Networks can be significantly reduced\nthrough pruning – the removal of seemingly less important parameters. How-\never, with the rise of Large Language Models (LLMs), full retraining to recover\npruning-induced performance degradation is often prohibitive and classical ap-\nproaches such as global magnitude pruning are suboptimal on Transformer archi-\ntectures. State-of-the-art methods hence solve a layer-wise mask selection prob-\nlem, the problem of finding a pruning mask which minimizes the per-layer pruning\nerror on a small set of calibration data. Exactly solving this problem to optimal-\nity using Integer Programming (IP) solvers is computationally infeasible due to\nits combinatorial nature and the size of the search space, and existing approaches\ntherefore rely on approximations or heuristics. In this work, we demonstrate that\nthe mask selection problem can be made drastically more tractable at LLM scale.\nTo that end, we decouple the rows by enforcing equal sparsity levels per row. This\nallows us to derive optimal 1-swaps (exchanging one kept and one pruned weight)\nthat can be computed efficiently using the Gram matrix of the calibration data.\nUsing these observations, we propose a tractable and simple 1-swap algorithm\nthat warm starts from any pruning mask, runs efficiently on GPUs at LLM scale,\nand is essentially hyperparameter-free. We demonstrate that our approach reduces\nper-layer pruning error by up to 60% over Wanda (Sun et al., 2023) and con-\nsistently improves perplexity and zero-shot accuracy across state-of-the-art GPT\narchitectures.\n1\nINTRODUCTION\nPruning after training (Han et al., 2015; Gale et al., 2019; Lin et al., 2020; Hoefler et al., 2021;\nZimmer et al., 2025) is a state-of-the-art technique to reduce the resource requirements of neural\nnetworks. A simple yet effective approach to obtain such sparse models starts from a pretrained\ndense model, removes seemingly unimportant parameters based on their magnitude, and requires\nretraining to compensate for pruning-induced performance degradation. However, while the inex-\npensive, data-free magnitude criterion has often achieved strong performance on traditional archi-\ntectures (Gale et al., 2019; Zimmer et al., 2023b), pruning has undergone a paradigm shift with the\nrise of large pretrained foundation models, particularly LLMs.\nFirst, the size of the models has shifted the focus toward retraining-free pruning criteria, as re-\ntraining is often computationally expensive if not infeasible, with parameter-efficient fine-tuning\n(Lialin et al., 2023; Zimmer et al., 2023a) being an exception. Secondly, systematic activation out-\nliers (Dettmers et al., 2022) and highly important super-weights (Yu et al., 2025) in sufficiently\nlarge Transformers (Vaswani et al., 2017) have rendered magnitude pruning no better than random\npruning for LLMs (Sun et al., 2023; Yin et al., 2023). Lastly, state-of-the-art methods (Frantar &\nAlistarh, 2023; Sun et al., 2023; Zhang et al., 2024) prune layer-wise: they split the pruning problem\ninto per-layer subproblems, pruning layers sequentially and independently using a small calibration\ndataset to estimate parameter importance. Rather than optimizing the global loss, such approaches\nminimize a per-layer local pruning loss. Specifically, for a single layer with calibration input matrix\n1\narXiv:2512.10922v1  [cs.LG]  11 Dec 2025", "clean_text": "SPARSESWAPS: TRACTABLE LLM PRUNING MASK REFINEMENT AT SCALE Max Zimmer, Christophe Roux, Moritz Wagner, Deborah Hendrych, Sebastian Pokutta Department for AI in Society, Science, and Technology, Zuse Institute Berlin, Germany Institute of Mathematics, Technische Universit¨at Berlin, Germany {zimmer,roux,wagner,hendrych,pokutta}@zib.de ABSTRACT The resource requirements of Neural Networks can be significantly reduced through pruning – the removal of seemingly less important parameters. However, with the rise of Large Language Models (LLMs), full retraining to recover pruning-induced performance degradation is often prohibitive and classical approaches such as global magnitude pruning are suboptimal on Transformer architectures. State-of-the-art methods hence solve a layer-wise mask selection problem, the problem of finding a pruning mask which minimizes the per-layer pruning error on a small set of calibration data. Exactly solving this problem to optimality using Integer Programming (IP) solvers is computationally infeasible due to its combinatorial nature and the size of the search space, and existing approaches therefore rely on approximations or heuristics. In this work, we demonstrate that the mask selection problem can be made drastically more tractable at LLM scale. To that end, we decouple the rows by enforcing equal sparsity levels per row. This allows us to derive optimal 1-swaps (exchanging one kept and one pruned weight) that can be computed efficiently using the Gram matrix of the calibration data. Using these observations, we propose a tractable and simple 1-swap algorithm that warm starts from any pruning mask, runs efficiently on GPUs at LLM scale, and is essentially hyperparameter-free. We demonstrate that our approach reduces per-layer pruning error by up to 60% over Wanda (Sun et al., 2023) and consistently improves perplexity and zero-shot accuracy across state-of-the-art GPT architectures. 1 INTRODUCTION Pruning after training (Han et al., 2015; Gale et al., 2019; Lin et al., 2020; Hoefler et al., 2021; Zimmer et al., 2025) is a state-of-the-art technique to reduce the resource requirements of neural networks. A simple yet effective approach to obtain such sparse models starts from a pretrained dense model, removes seemingly unimportant parameters based on their magnitude, and requires retraining to compensate for pruning-induced performance degradation. However, while the inexpensive, data-free magnitude criterion has often achieved strong performance on traditional architectures (Gale et al., 2019; Zimmer et al., 2023b), pruning has undergone a paradigm shift with the rise of large pretrained foundation models, particularly LLMs. First, the size of the models has shifted the focus toward retraining-free pruning criteria, as retraining is often computationally expensive if not infeasible, with parameter-efficient fine-tuning (Lialin et al., 2023; Zimmer et al., 2023a) being an exception. Secondly, systematic activation outliers (Dettmers et al., 2022) and highly important super-weights (Yu et al., 2025) in sufficiently large Transformers (Vaswani et al., 2017) have rendered magnitude pruning no better than random pruning for LLMs (Sun et al., 2023; Yin et al., 2023). Lastly, state-of-the-art methods (Frantar & Alistarh, 2023; Sun et al., 2023; Zhang et al., 2024) prune layer-wise: they split the pruning problem into per-layer subproblems, pruning layers sequentially and independently using a small calibration dataset to estimate parameter importance. Rather than optimizing the global loss, such approaches minimize a per-layer local pruning loss. Specifically, for a single layer with calibration input matrix 1 arXiv:2512.10922v1 [cs.LG] 11 Dec 2025"}
{"pdf_id": "arxiv_251210922_sparseswaps", "page": 2, "text": "X ∈Rdin×B and weights W ∈Rdout×din, the objective becomes\nmin\nM ∥WX −(M ⊙W)X∥2\nF ,\n(1)\nwhere M ∈{0, 1}dout×din is a binary pruning mask achieving a desired level of sparsity, e.g.,\n∥M∥0 ≤k for unstructured sparsity, and ⊙denotes the element-wise multiplication or Hadamard\nproduct. Here, B = N · L with N being the number of samples in the calibration batch and L being\nthe sequence length.\nSolving this combinatorial mask selection problem to optimality is NP-hard due to feature correla-\ntions: selecting k of dout · din weights yields a cardinality-constrained binary quadratic program (a\nbest-subset selection variant). Even for a single row i, the problem reduces to\nmin\nmi\n\r\rw⊤\ni X −(mi ⊙wi)⊤X\n\r\r2\nF = min\nmi\nB\nX\nk=1\n\n\ndin\nX\nj=1\n(1 −mij)wijXjk\n\n\n2\n,\nwhere wi ∈Rdin and mi ∈{0, 1}din denote the i-th row of W and M, respectively. While IP\nsolvers could theoretically provide optimal solutions, the combinatorial search over mask entries\nmakes this infeasible for LLMs. In practice, existing methods therefore relax Equation 1 or approx-\nimate it.\nHowever, with deployed LLMs now serving millions of users, it becomes increasingly worthwhile\nto invest substantial resources to obtain pruned models that reach high performance, because the\npruning cost is paid once during training whereas inference costs scale with the number of requests.\nIn this work, we revisit the per-layer mask selection problem and demonstrate that it can be op-\nerationalized at LLM scale, enabling monotone improvements with each optimization step rather\nthan relying on proxy importance scores. To that end, we observe that enforcing equal sparsity-\nlevel across rows ensures row-wise separability that yields independent objectives. This makes the\nproblem drastically more tractable and leads to good practical performance for LLMs. Instead of\ntrying to obtain exact solutions via IP solvers, we instead propose a GPU-accelerated local optimiza-\ntion algorithm based on 1-swaps (exchanging one kept and one pruned weight) that perform exact\nand efficient local refinement with incremental cost updates using the Gram matrix G = XX⊤to\nmonotonically decrease the objective from any warm start.\nThe resulting method, which we term SparseSwaps, can start from any warm-start mask, evaluates\nthe exact per-row quadratic loss, and is scalable, parallelizable across rows, almost hyperparameter-\nfree, and deterministic for a fixed warm start. With only few 1-swap iterations, it can reduce the per-\nlayer pruning error by up to 60% compared to Wanda and improves final perplexity and zero-shot\naccuracy across architectures. Our approach is a post-hoc refinement of existing pruning methods\nthat can significantly improve upon the state of the art for unstructured, per-row, or N:M sparsity.\nContributions.\nOur contributions are as follows:\n1. Making the Mask Selection problem tractable. We observe that a) enforcing equal spar-\nsity levels per row decouples the rows, and that b) optimal 1-swaps (exchanging one kept\nand one pruned weight) can be evaluated efficiently using the Gram matrix G = XX⊤of\nthe calibration data, ensuring efficient lookups when determining the most beneficial swap.\n2. SparseSwaps: a practical post-hoc pruning algorithm. Building on these observations,\nwe propose SparseSwaps, a plug-and-play 1-swap refinement that starts from any warm-\nstart mask and monotonically decreases the exact per-row objective under per-row or N:M\nconstraints. In particular, SparseSwaps is almost hyperparameter-free, completely paral-\nlelizable across rows and scalable to LLMs.\n3. Computational study. We verify our hypotheses on state-of-the-art Generative Pretrained\nTransformer (GPT) architectures and demonstrate that SparseSwaps delivers large reduc-\ntions in local pruning error (up to 60% per-layer error reduction over Wanda) and strong\nperplexity and zero-shot gains across a wide range of different LLMs. We conduct a series\nof ablations highlighting the advantages and drawbacks of the proposed approach.\n2", "clean_text": "X ∈Rdin×B and weights W ∈Rdout×din, the objective becomes min M ∥WX −(M ⊙W)X∥2 F , (1) where M ∈{0, 1}dout×din is a binary pruning mask achieving a desired level of sparsity, e.g., ∥M∥0 ≤k for unstructured sparsity, and ⊙denotes the element-wise multiplication or Hadamard product. Here, B = N · L with N being the number of samples in the calibration batch and L being the sequence length. Solving this combinatorial mask selection problem to optimality is NP-hard due to feature correlations: selecting k of dout · din weights yields a cardinality-constrained binary quadratic program (a best-subset selection variant). Even for a single row i, the problem reduces to min mi w⊤ i X −(mi ⊙wi)⊤X 2 F = min mi B X k=1   din X j=1 (1 −mij)wijXjk   2 , where wi ∈Rdin and mi ∈{0, 1}din denote the i-th row of W and M, respectively. While IP solvers could theoretically provide optimal solutions, the combinatorial search over mask entries makes this infeasible for LLMs. In practice, existing methods therefore relax Equation 1 or approximate it. However, with deployed LLMs now serving millions of users, it becomes increasingly worthwhile to invest substantial resources to obtain pruned models that reach high performance, because the pruning cost is paid once during training whereas inference costs scale with the number of requests. In this work, we revisit the per-layer mask selection problem and demonstrate that it can be operationalized at LLM scale, enabling monotone improvements with each optimization step rather than relying on proxy importance scores. To that end, we observe that enforcing equal sparsitylevel across rows ensures row-wise separability that yields independent objectives. This makes the problem drastically more tractable and leads to good practical performance for LLMs. Instead of trying to obtain exact solutions via IP solvers, we instead propose a GPU-accelerated local optimization algorithm based on 1-swaps (exchanging one kept and one pruned weight) that perform exact and efficient local refinement with incremental cost updates using the Gram matrix G = XX⊤to monotonically decrease the objective from any warm start. The resulting method, which we term SparseSwaps, can start from any warm-start mask, evaluates the exact per-row quadratic loss, and is scalable, parallelizable across rows, almost hyperparameterfree, and deterministic for a fixed warm start. With only few 1-swap iterations, it can reduce the perlayer pruning error by up to 60% compared to Wanda and improves final perplexity and zero-shot accuracy across architectures. Our approach is a post-hoc refinement of existing pruning methods that can significantly improve upon the state of the art for unstructured, per-row, or N:M sparsity. Contributions. Our contributions are as follows: 1. Making the Mask Selection problem tractable. We observe that a) enforcing equal sparsity levels per row decouples the rows, and that b) optimal 1-swaps (exchanging one kept and one pruned weight) can be evaluated efficiently using the Gram matrix G = XX⊤of the calibration data, ensuring efficient lookups when determining the most beneficial swap. 2. SparseSwaps: a practical post-hoc pruning algorithm. Building on these observations, we propose SparseSwaps, a plug-and-play 1-swap refinement that starts from any warmstart mask and monotonically decreases the exact per-row objective under per-row or N:M constraints. In particular, SparseSwaps is almost hyperparameter-free, completely parallelizable across rows and scalable to LLMs. 3. Computational study. We verify our hypotheses on state-of-the-art Generative Pretrained Transformer (GPT) architectures and demonstrate that SparseSwaps delivers large reductions in local pruning error (up to 60% per-layer error reduction over Wanda) and strong perplexity and zero-shot gains across a wide range of different LLMs. We conduct a series of ablations highlighting the advantages and drawbacks of the proposed approach. 2"}
{"pdf_id": "arxiv_251210922_sparseswaps", "page": 3, "text": "Further related work.\nPost-training pruning has a long history, and while magnitude pruning\n(Janowsky, 1989; Han et al., 2015) is among the most popular criteria, it is not the only one (cf.\nLeCun et al., 1989; Hassibi & Stork, 1993; Molchanov et al., 2016; Yeom et al., 2019); see Hoe-\nfler et al. (2021) for a comprehensive review. Despite their simplicity, magnitude-based methods\nhave been shown to produce sparse models competitive with far more complex algorithms for con-\nvolutional architectures (Gale et al., 2019; Zimmer et al., 2023b). For LLMs, however, magnitude\npruning is argued to be unsuitable (Yin et al., 2023). Consequently, there is growing interest in\ncriteria beyond magnitude that achieve high performance on LLMs, and do so without requiring an\nexpensive retraining procedure (Kwon et al., 2022; Frantar & Alistarh, 2023; Sun et al., 2023; Zhang\net al., 2024). In this work, we develop a post-hoc refinement of existing methods, rather than propos-\ning a new criterion. A related approach, DSnoT (Zhang et al., 2023), also performs iterative weight\nswaps but differs significantly in its optimization strategy. Inspired by dynamic sparse training (cf.\nEvci et al., 2020), DSnoT prunes and regrows weights based on expected reconstruction-error im-\nprovements, using feature means and variances as surrogates. While effective, it does not guarantee\na monotonic decrease in the true pruning error, whereas our method does. We compare the two\nempirically and find that SparseSwaps consistently outperforms DSnoT.\nSubset selection and IP approaches. To solve Equation 1 to global optimality, which can be for-\nmulated as a mixed-integer nonlinear program (MINLP), several efficient open-source solvers are\navailable, including SCIP (Bolusani et al., 2024), Bonmin (Bonami et al., 2008), SHOT (Lundell\net al., 2022) and Boscia (Hendrych et al., 2025), among others. While we demonstrate how the\nproblem can be made drastically more tractable, explicit solution remains very time-consuming for\nlarge instances; we therefore opt for a GPU-friendly 1-swap approach that avoids moving large\ntensors to the CPU for IP solvers. We leave such an extension for future work.\n2\nMETHODOLOGY\nIn the following, we use uppercase letters for matrices (W, X, M) and lowercase letters for scalars\nand vectors. Matrix entries are denoted Wij for the element in row i, column j. Rows of matrices are\ndenoted with lowercase subscripts: wi represents the i-th row of matrix W. Row and column slices\nuse colon notation: Xj,: for the j-th row and X:,k for the k-th column. We use ⊙for element-wise\nmultiplication, ∥·∥F for Frobenius norm, and ∥·∥2 for ℓ2 norm.\n2.1\nPRELIMINARIES\nBefore describing our proposed method, we make several assumptions and observations that make\nthe problem tractable.\n2.1.1\nEQUAL SPARSITY-LEVEL ACROSS ROWS DOES NOT NEED TO BE DETRIMENTAL\nFirst, note that the objective in Equation 1 decomposes into a sum of dout row-wise quadratics,\n∥WX −(M ⊙W)X∥2\nF =\ndout\nX\ni=1\n\r\rw⊤\ni X −(mi ⊙wi)⊤X\n\r\r2\n2\nwhere wi ∈Rdin and mi ∈{0, 1}din denote the i-th row of W and M, respectively. This alone\ndoes not make the corresponding minimization problem row-separable under unstructured sparsity,\nsince the matrix cardinality constraint couples rows. In contrast, semi-structured patterns like per-\nrow sparsity (keep k per row) or N:M (prune M −N per block of M weights) enforce equal\nper-row sparsity, meaning that the rows are fully decoupled by definition. We therefore focus on the\ndecoupled case, allowing to treat each row separately and reducing the problem to\nmin\nmi\n\r\rw⊤\ni X −(mi ⊙wi)⊤X\n\r\r2\nF = min\nmi\nB\nX\nk=1\n\n\ndin\nX\nj=1\n(1 −mij)wijXjk\n\n\n2\n(2)\nfor each row i ∈{1, . . . , dout}. Note that, for LLMs, Sun et al. (2023) observe that row-wise sparsity\nbenefits performance for both Wanda and magnitude pruning. We therefore argue that enforcing\nper-row sparsity rather than unstructured sparsity is justified and need not harm final performance,\nat least for LLMs.\n3", "clean_text": "Further related work. Post-training pruning has a long history, and while magnitude pruning (Janowsky, 1989; Han et al., 2015) is among the most popular criteria, it is not the only one (cf. LeCun et al., 1989; Hassibi & Stork, 1993; Molchanov et al., 2016; Yeom et al., 2019); see Hoefler et al. (2021) for a comprehensive review. Despite their simplicity, magnitude-based methods have been shown to produce sparse models competitive with far more complex algorithms for convolutional architectures (Gale et al., 2019; Zimmer et al., 2023b). For LLMs, however, magnitude pruning is argued to be unsuitable (Yin et al., 2023). Consequently, there is growing interest in criteria beyond magnitude that achieve high performance on LLMs, and do so without requiring an expensive retraining procedure (Kwon et al., 2022; Frantar & Alistarh, 2023; Sun et al., 2023; Zhang et al., 2024). In this work, we develop a post-hoc refinement of existing methods, rather than proposing a new criterion. A related approach, DSnoT (Zhang et al., 2023), also performs iterative weight swaps but differs significantly in its optimization strategy. Inspired by dynamic sparse training (cf. Evci et al., 2020), DSnoT prunes and regrows weights based on expected reconstruction-error improvements, using feature means and variances as surrogates. While effective, it does not guarantee a monotonic decrease in the true pruning error, whereas our method does. We compare the two empirically and find that SparseSwaps consistently outperforms DSnoT. Subset selection and IP approaches. To solve Equation 1 to global optimality, which can be formulated as a mixed-integer nonlinear program (MINLP), several efficient open-source solvers are available, including SCIP (Bolusani et al., 2024), Bonmin (Bonami et al., 2008), SHOT (Lundell et al., 2022) and Boscia (Hendrych et al., 2025), among others. While we demonstrate how the problem can be made drastically more tractable, explicit solution remains very time-consuming for large instances; we therefore opt for a GPU-friendly 1-swap approach that avoids moving large tensors to the CPU for IP solvers. We leave such an extension for future work. 2 METHODOLOGY In the following, we use uppercase letters for matrices (W, X, M) and lowercase letters for scalars and vectors. Matrix entries are denoted Wij for the element in row i, column j. Rows of matrices are denoted with lowercase subscripts: wi represents the i-th row of matrix W. Row and column slices use colon notation: Xj,: for the j-th row and X:,k for the k-th column. We use ⊙for element-wise multiplication, ∥·∥F for Frobenius norm, and ∥·∥2 for ℓ2 norm. 2.1 PRELIMINARIES Before describing our proposed method, we make several assumptions and observations that make the problem tractable. 2.1.1 EQUAL SPARSITY-LEVEL ACROSS ROWS DOES NOT NEED TO BE DETRIMENTAL First, note that the objective in Equation 1 decomposes into a sum of dout row-wise quadratics, ∥WX −(M ⊙W)X∥2 F = dout X i=1 w⊤ i X −(mi ⊙wi)⊤X 2 2 where wi ∈Rdin and mi ∈{0, 1}din denote the i-th row of W and M, respectively. This alone does not make the corresponding minimization problem row-separable under unstructured sparsity, since the matrix cardinality constraint couples rows. In contrast, semi-structured patterns like perrow sparsity (keep k per row) or N:M (prune M −N per block of M weights) enforce equal per-row sparsity, meaning that the rows are fully decoupled by definition. We therefore focus on the decoupled case, allowing to treat each row separately and reducing the problem to min mi w⊤ i X −(mi ⊙wi)⊤X 2 F = min mi B X k=1   din X j=1 (1 −mij)wijXjk   2 (2) for each row i ∈{1, . . . , dout}. Note that, for LLMs, Sun et al. (2023) observe that row-wise sparsity benefits performance for both Wanda and magnitude pruning. We therefore argue that enforcing per-row sparsity rather than unstructured sparsity is justified and need not harm final performance, at least for LLMs. 3"}
{"pdf_id": "arxiv_251210922_sparseswaps", "page": 4, "text": "2.1.2\nAVOIDING INTERMEDIATE VALUE CACHING THROUGH THE GRAM MATRIX\nFORMULATION\nNaively caching all B ·din intermediate products wijXjk in Equation 2 to evaluate candidate masks\nis prohibitive. To illustrate the scale, consider a single row of the largest matrix in a LLAMA-2-7B\nTransformer block: the up proj matrix with input dimension din = 4096. With N = 128 samples\nand sequence length L = 4096 (so B = N · L = 524,288), caching all products wijXjk for that\nrow requires 524,288 × 4096 ≈2.15 billion float32 values (about 8.6GB); across all 11,008 rows\nthis totals about 94.6TB.\nA straightforward way to circumvent this issue is to consider a single row and derive a compact\nformulation of the per-row loss through the Gram matrix G\ndef\n= XX⊤∈Rdin×din. For notational\nconvenience, we drop the row index i throughout the remainder of this section and write w ∈Rdin\nfor the row’s weight vector and m ∈{0, 1}din for its mask. The per-row loss from Equation 2 is\nL\ndef\n=\n\r\rw⊤X −(m ⊙w)⊤X\n\r\r2\nF =\n\r\r(w −m ⊙w)⊤X\n\r\r2\nF = (w −m ⊙w)⊤G(w −m ⊙w).\nHence, the loss depends on X only through the Gram matrix G, which can be accumulated on-the-fly\nas calibration samples pass through the layer: G = PB\nb=1 X:,bX⊤\n:,b. Unlike the per-row formulation\nin the introduction, which would require caching all B · din intermediate products wjXjk, we only\nneed to maintain the din × din matrix G, which is a reduction from O(B · din) to O(d2\nin) since din\nis typically much smaller than B.\nRemark 1. A different (but in practice slightly less efficient) perspective on this reduction is through\nthe unitary invariance of the Frobenius norm used in our pruning objective: for any matrix A and\nunitary matrix U (i.e., U −1 = U ⊤), we have ∥AU∥F = ∥A∥F . This property enables significant\ncomputational savings through Singular Value Decomposition (SVD) compression. Precisely, let\nX = UΣV ⊤be the SVD of calibration data X ∈Rdin×B. Since B > din, we can write Σ =\n[Σ′ | 0] with Σ′ ∈Rdin×din containing the singular values on its diagonal. The compressed\nrepresentation is simply X′ = UΣ′ ∈Rdin×din. Letting wp = w −m ⊙w for brevity, the key\ninsight is that pruning decisions remain equivalent under this compression:\n∥wpX∥2\nF =\n\r\rwpUΣV ⊤\r\r2\nF = ∥wpUΣ∥2\nF = ∥wpU[Σ′ | 0]∥2\nF = ∥wpUΣ′∥2\nF = ∥wpX′∥2\nF ,\nwhere we used unitary invariance w.r.t. V and that the zero columns do not contribute to the Frobe-\nnius norm. Equivalently, we have\nX′X′⊤= UΣ′Σ′⊤U ⊤= UΣΣ⊤U ⊤= XX⊤= G,\nsince ΣΣ⊤= Σ′Σ′⊤(the zero columns of Σ do not contribute). Since all subsequent operations\ndepend solely on G, we accumulate G directly during calibration and avoid the SVD entirely.\n2.1.3\nEFFICIENT 1-SWAP EVALUATION THROUGH EFFICIENT COST LOOKUPS AND UPDATES\nWhile the global mask selection problem is NP-hard, we can still make efficient progress via local\nsearch. Starting from any feasible mask m ∈{0, 1}din, the idea is to iteratively perform 1-swaps\nthat exchange one kept and one pruned weight to reduce L while preserving the sparsity level. The\nkey observation is that each candidate swap can be evaluated in O(1) time using G and an auxiliary\ncorrelation vector c. To that end, let P\ndef\n= {j : mj = 0} denote the set of currently pruned weight\nindices and analogously U\ndef\n= {j : mj = 1} denote the set of unpruned (kept) weight indices.\nLetting further ϕj\ndef\n= X⊤\nj,: ∈RB denote the j-th row (or feature vector)of X, we can write\n(w −m ⊙w)⊤X =\ndin\nX\nj=1\n(1 −mj)wjXj,: =\nX\nj∈P\nwjϕ⊤\nj = r⊤,\nwhere we define the reconstruction residual r\ndef\n= P\nj∈P wjϕj ∈RB, the total contribution of all\npruned weights to the layer output. Hence, clearly, the loss is L = ∥r∥2\n2 = r⊤r.\nWe define the correlation vector c = (c1, . . . , cdin)⊤∈Rdin with entries\nci\ndef\n= ⟨ϕi, r⟩= ⟨ϕi,\nX\nj∈P\nwjϕj⟩=\nX\nj∈P\nwj⟨ϕi, ϕj⟩=\nX\nj∈P\nwjGij,\nwhich measures how each feature ϕi correlates with the current residual. In vector form, c =\nG · ((1 −m) ⊙w).\n4", "clean_text": "2.1.2 AVOIDING INTERMEDIATE VALUE CACHING THROUGH THE GRAM MATRIX FORMULATION Naively caching all B ·din intermediate products wijXjk in Equation 2 to evaluate candidate masks is prohibitive. To illustrate the scale, consider a single row of the largest matrix in a LLAMA-2-7B Transformer block: the up proj matrix with input dimension din = 4096. With N = 128 samples and sequence length L = 4096 (so B = N · L = 524,288), caching all products wijXjk for that row requires 524,288 × 4096 ≈2.15 billion float32 values (about 8.6GB); across all 11,008 rows this totals about 94.6TB. A straightforward way to circumvent this issue is to consider a single row and derive a compact formulation of the per-row loss through the Gram matrix G def = XX⊤∈Rdin×din. For notational convenience, we drop the row index i throughout the remainder of this section and write w ∈Rdin for the row’s weight vector and m ∈{0, 1}din for its mask. The per-row loss from Equation 2 is L def = w⊤X −(m ⊙w)⊤X 2 F = (w −m ⊙w)⊤X 2 F = (w −m ⊙w)⊤G(w −m ⊙w). Hence, the loss depends on X only through the Gram matrix G, which can be accumulated on-the-fly as calibration samples pass through the layer: G = PB b=1 X:,bX⊤ :,b. Unlike the per-row formulation in the introduction, which would require caching all B · din intermediate products wjXjk, we only need to maintain the din × din matrix G, which is a reduction from O(B · din) to O(d2 in) since din is typically much smaller than B. Remark 1. A different (but in practice slightly less efficient) perspective on this reduction is through the unitary invariance of the Frobenius norm used in our pruning objective: for any matrix A and unitary matrix U (i.e., U −1 = U ⊤), we have ∥AU∥F = ∥A∥F . This property enables significant computational savings through Singular Value Decomposition (SVD) compression. Precisely, let X = UΣV ⊤be the SVD of calibration data X ∈Rdin×B. Since B > din, we can write Σ = [Σ′ | 0] with Σ′ ∈Rdin×din containing the singular values on its diagonal. The compressed representation is simply X′ = UΣ′ ∈Rdin×din. Letting wp = w −m ⊙w for brevity, the key insight is that pruning decisions remain equivalent under this compression: ∥wpX∥2 F = wpUΣV ⊤ 2 F = ∥wpUΣ∥2 F = ∥wpU[Σ′ | 0]∥2 F = ∥wpUΣ′∥2 F = ∥wpX′∥2 F , where we used unitary invariance w.r.t. V and that the zero columns do not contribute to the Frobenius norm. Equivalently, we have X′X′⊤= UΣ′Σ′⊤U ⊤= UΣΣ⊤U ⊤= XX⊤= G, since ΣΣ⊤= Σ′Σ′⊤(the zero columns of Σ do not contribute). Since all subsequent operations depend solely on G, we accumulate G directly during calibration and avoid the SVD entirely. 2.1.3 EFFICIENT 1-SWAP EVALUATION THROUGH EFFICIENT COST LOOKUPS AND UPDATES While the global mask selection problem is NP-hard, we can still make efficient progress via local search. Starting from any feasible mask m ∈{0, 1}din, the idea is to iteratively perform 1-swaps that exchange one kept and one pruned weight to reduce L while preserving the sparsity level. The key observation is that each candidate swap can be evaluated in O(1) time using G and an auxiliary correlation vector c. To that end, let P def = {j : mj = 0} denote the set of currently pruned weight indices and analogously U def = {j : mj = 1} denote the set of unpruned (kept) weight indices. Letting further ϕj def = X⊤ j,: ∈RB denote the j-th row (or feature vector)of X, we can write (w −m ⊙w)⊤X = din X j=1 (1 −mj)wjXj,: = X j∈P wjϕ⊤ j = r⊤, where we define the reconstruction residual r def = P j∈P wjϕj ∈RB, the total contribution of all pruned weights to the layer output. Hence, clearly, the loss is L = ∥r∥2 2 = r⊤r. We define the correlation vector c = (c1, . . . , cdin)⊤∈Rdin with entries ci def = ⟨ϕi, r⟩= ⟨ϕi, X j∈P wjϕj⟩= X j∈P wj⟨ϕi, ϕj⟩= X j∈P wjGij, which measures how each feature ϕi correlates with the current residual. In vector form, c = G · ((1 −m) ⊙w). 4"}
{"pdf_id": "arxiv_251210922_sparseswaps", "page": 5, "text": "Swap cost formula.\nA 1-swap removes index u ∈U from the unpruned set (making it pruned)\nand adds index p ∈P to the unpruned set (making it unpruned). The new residual is r′ = r +\nwuϕu −wpϕp, and the change in loss is\n∆Lu,p = ∥r′∥2\n2 −∥r∥2\n2 = ∥r + wuϕu −wpϕp∥2\n2 −∥r∥2\n2\n= 2wu⟨ϕu, r⟩+ w2\nu∥ϕu∥2\n2 −2wp⟨ϕp, r⟩+ w2\np∥ϕp∥2\n2 −2wuwp⟨ϕu, ϕp⟩.\nUsing ci = ⟨ϕi, r⟩and Gij = ⟨ϕi, ϕj⟩, this simplifies to\n∆Lu,p = 2wucu + w2\nuGuu −2wpcp + w2\npGpp −2wuwpGup.\n(3)\nGiven the precomputed Gram matrix G and correlation vector c, each swap evaluation requires only\nscalar lookups. Evaluating all possible swaps therefore costs O(|U| · |P|) total. By systematically\ntesting all ((din −|P|) · |P|) possible 1-swap operations (adding one of |U| = din −|P| unpruned\nweights to P, removing one of |P| pruned weights from P) evaluating the improvement using the\nabove expression, we iteratively pick a best swap and update the mask until we have reached a\nsatisfactory solution or one optimal w.r.t. 1-swap operations. The only issue that remains is to\nupdate the correlation vector after each swap.\nCorrelation vector update.\nAfter accepting a swap (u∗, p∗), the residual changes to r′ = r +\nwu∗ϕu∗−wp∗ϕp∗. The correlation vector updates as\nci ←ci + wu∗Gi,u∗−wp∗Gi,p∗,\n(4)\nor in vector form, c ←c + wu∗G:,u∗−wp∗G:,p∗. This only requires accessing two columns of G\nand costs O(din).\nWhy picking p and u separately is suboptimal.\nThe interaction term −2wuwpGup in Equation 3\nshows that the best u depends on the chosen p (and vice versa). Consequently, selecting p and u\nbased on their individual effects can yield a detrimental swap, as the following example for the scalar\ncase with B = 1 and din = 4 shows. Let the current pruned weight contributions be {+10, −1},\nso r = 9 and L = 81, and let the unpruned weight contributions be {+9, −9}. The best 1-swap\nis to unprune the −1 contribution and prune the −9 contribution, giving r′ = 10 + (−9) = 1 and\nL′ = 1. However, if we instead greedily remove the best p in isolation, we unprune +10 since\n(9 −10)2 = 1 is minimal. We must then add one index; the best addition in isolation to the original\npruned-weight-contributions {+10, −1} is −9. In combination, the greedily chosen swap leads to\nr′ = −1 + (−9) = −10 and L′ = 100, worse than the starting point. The error stems precisely\nfrom ignoring the interaction term when selecting (p, u).\n2.2\nTHE SPARSESWAPS ALGORITHM\nBuilding upon the preceding observations, we present our complete algorithm. The method takes\nas input a weight matrix W ∈Rdout×din, the Gram matrix G = XX⊤∈Rdin×din (accumulated\nduring calibration), and a warmstart pruning mask M init ∈{0, 1}dout×din that already satisfies the\ndesired sparsity constraints, e.g., obtained from Wanda (Sun et al., 2024) or RIA (Zhang et al., 2024).\nThe algorithm enforces any sparsity pattern that operates per-row, including per-row sparsity (fixed\nnumber of zeros per row, cf. Sun et al. (2023)) and structured N:M sparsity patterns (e.g., 2:4 or 4:8,\nMishra et al. (2021)). All swap operations maintain the sparsity constraints throughout optimization;\nfor N:M sparsity, swaps are restricted to occur only within the same N:M blocks, while for per-\nrow sparsity, the total number of pruned weights per row remains constant. Even though each swap\nonly changes two mask entries, the cumulative effect of multiple swaps can dramatically reduce\nreconstruction error compared to the initial solution.\nWe explain the main phases of the algorithm:\nPreparation: We initialize with the warmstart mask M init. The Gram matrix G is precomputed\nonce per layer by accumulating G = P\nb X:,bX⊤\n:,b during the calibration forward pass.\nRow processing (Lines 2-5): For each row i, we extract weights w and current mask m, define\npruned and unpruned index sets P and U, and compute the initial correlation vector c = G · ((1 −\nm) ⊙w).\n5", "clean_text": "Swap cost formula. A 1-swap removes index u ∈U from the unpruned set (making it pruned) and adds index p ∈P to the unpruned set (making it unpruned). The new residual is r′ = r + wuϕu −wpϕp, and the change in loss is ∆Lu,p = ∥r′∥2 2 −∥r∥2 2 = ∥r + wuϕu −wpϕp∥2 2 −∥r∥2 2 = 2wu⟨ϕu, r⟩+ w2 u∥ϕu∥2 2 −2wp⟨ϕp, r⟩+ w2 p∥ϕp∥2 2 −2wuwp⟨ϕu, ϕp⟩. Using ci = ⟨ϕi, r⟩and Gij = ⟨ϕi, ϕj⟩, this simplifies to ∆Lu,p = 2wucu + w2 uGuu −2wpcp + w2 pGpp −2wuwpGup. (3) Given the precomputed Gram matrix G and correlation vector c, each swap evaluation requires only scalar lookups. Evaluating all possible swaps therefore costs O(|U| · |P|) total. By systematically testing all ((din −|P|) · |P|) possible 1-swap operations (adding one of |U| = din −|P| unpruned weights to P, removing one of |P| pruned weights from P) evaluating the improvement using the above expression, we iteratively pick a best swap and update the mask until we have reached a satisfactory solution or one optimal w.r.t. 1-swap operations. The only issue that remains is to update the correlation vector after each swap. Correlation vector update. After accepting a swap (u∗, p∗), the residual changes to r′ = r + wu∗ϕu∗−wp∗ϕp∗. The correlation vector updates as ci ←ci + wu∗Gi,u∗−wp∗Gi,p∗, (4) or in vector form, c ←c + wu∗G:,u∗−wp∗G:,p∗. This only requires accessing two columns of G and costs O(din). Why picking p and u separately is suboptimal. The interaction term −2wuwpGup in Equation 3 shows that the best u depends on the chosen p (and vice versa). Consequently, selecting p and u based on their individual effects can yield a detrimental swap, as the following example for the scalar case with B = 1 and din = 4 shows. Let the current pruned weight contributions be {+10, −1}, so r = 9 and L = 81, and let the unpruned weight contributions be {+9, −9}. The best 1-swap is to unprune the −1 contribution and prune the −9 contribution, giving r′ = 10 + (−9) = 1 and L′ = 1. However, if we instead greedily remove the best p in isolation, we unprune +10 since (9 −10)2 = 1 is minimal. We must then add one index; the best addition in isolation to the original pruned-weight-contributions {+10, −1} is −9. In combination, the greedily chosen swap leads to r′ = −1 + (−9) = −10 and L′ = 100, worse than the starting point. The error stems precisely from ignoring the interaction term when selecting (p, u). 2.2 THE SPARSESWAPS ALGORITHM Building upon the preceding observations, we present our complete algorithm. The method takes as input a weight matrix W ∈Rdout×din, the Gram matrix G = XX⊤∈Rdin×din (accumulated during calibration), and a warmstart pruning mask M init ∈{0, 1}dout×din that already satisfies the desired sparsity constraints, e.g., obtained from Wanda (Sun et al., 2024) or RIA (Zhang et al., 2024). The algorithm enforces any sparsity pattern that operates per-row, including per-row sparsity (fixed number of zeros per row, cf. Sun et al. (2023)) and structured N:M sparsity patterns (e.g., 2:4 or 4:8, Mishra et al. (2021)). All swap operations maintain the sparsity constraints throughout optimization; for N:M sparsity, swaps are restricted to occur only within the same N:M blocks, while for perrow sparsity, the total number of pruned weights per row remains constant. Even though each swap only changes two mask entries, the cumulative effect of multiple swaps can dramatically reduce reconstruction error compared to the initial solution. We explain the main phases of the algorithm: Preparation: We initialize with the warmstart mask M init. The Gram matrix G is precomputed once per layer by accumulating G = P b X:,bX⊤ :,b during the calibration forward pass. Row processing (Lines 2-5): For each row i, we extract weights w and current mask m, define pruned and unpruned index sets P and U, and compute the initial correlation vector c = G · ((1 − m) ⊙w). 5"}
{"pdf_id": "arxiv_251210922_sparseswaps", "page": 6, "text": "Algorithm 1 SparseSwaps: 1-Swap Pruning Optimization\nRequire: W ∈Rdout×din, Gram matrix G = XX⊤∈Rdin×din, warmstart mask M init, Tmax\nEnsure: Improved pruning mask M\n1: M ←M init\n▷Initialize with warmstart solution\n2: for i = 1 to dout do\n▷Process each row independently\n3:\nw ←Wi,:, m ←Mi,:\n▷Extract row weights and mask\n4:\nP ←{j : mj = 0}, U ←{j : mj = 1}\n▷Pruned and unpruned sets\n5:\nc ←G · ((1 −m) ⊙w)\n▷Initialize correlation vector\n6:\nfor t = 1 to Tmax do\n7:\n(p∗, u∗) ←arg min(p,u) ∆Lu,p\n▷Best swap via Equation 3\n8:\nif ∆Lu∗,p∗< 0 then\n▷Swap improves objective\n9:\nmp∗←1, mu∗←0\n▷Perform swap\n10:\nP ←(P \\ {p∗}) ∪{u∗}, U ←(U \\ {u∗}) ∪{p∗}\n11:\nc ←c + wu∗G:,u∗−wp∗G:,p∗\n▷Update correlation vector\n12:\nelse\n13:\nbreak\n▷Local optimum reached\n14:\nend if\n15:\nend for\n16:\nMi,: ←m\n▷Store optimized row\n17: end for\n1-Swap optimization (Lines 6-15): We iteratively find the swap (p∗, u∗) minimizing ∆Lu,p (cf.\nEquation 3) among feasible pairs, evaluating each candidate in O(1) time. If ∆Lu∗,p∗< 0, we\naccept the swap and update the correlation vector via Equation 4; otherwise we terminate. At all\ntimes, the swaps are appropriately constrained: per-row sparsity allows any swap maintaining |P|\nconstant, while N:M sparsity restricts swaps to within the same N:M blocks.\nThe algorithm has complexity O(dout·Tmax·(|P|·|U|+din)) per layer, where Tmax is the maximum\nnumber of swap iterations per row. The |P| · |U| term comes from evaluating all candidate swaps\n(each in O(1) time via Equation 3), and the din term from the correlation vector update (Equation 4).\nIn practice, several factors further reduce runtime. First, we find that even Tmax = 1 or Tmax = 2\ncan drastically reduce the local pruning error; values around Tmax = 25 often suffice to significantly\nlower model perplexity, with diminishing returns beyond Tmax = 100. Second, row-wise processing\ncan be batched and vectorized, enabling parallel swap cost computations and mask updates, and rows\ncan be distributed across GPUs if needed. Third, the Gram matrix G is computed once per layer and\nshared across all rows, and several summands of Equation 3 can be similarly precomputed once per\nlayer.\n3\nEXPERIMENTAL RESULTS\nWe outline our general experimental approach, detailing datasets, architectures, and metrics. Our\ncode is publicly available at github.com/ZIB-IOL/SparseSwaps. Our study focuses on language\nmodeling within Natural Language Processing (NLP). We use pretrained models from Hugging-\nFace (Wolf et al., 2020), specifically LLAMA-3.1-8B (Grattafiori et al., 2024), GEMMA-2-9B\n(Riviere et al., 2024), YI-1.5-9B (Young et al., 2025), DEEPSEEK-7B-BASE (Bi et al., 2024), and\nQWEN2.5-7B (Yang et al., 2025). For calibration, we randomly draw sequences of 2048 tokens\nfrom the C4 dataset (Raffel et al., 2020). For validation, we similarly pick 100 sequences from the\nvalidation split. The model performance is assessed via perplexity on the WikiText dataset (Merity\net al., 2016) and zero-shot accuracy on the EleutherAI evaluation set (Gao et al., 2023). Following\nSun et al. (2023), we prune all linear layers, excluding the embedding and final linear head, with uni-\nform sparsity allocation across layers. We provide experiments for unstructured and semi-structured\nsparsity patterns (Mishra et al., 2021). We use multiple random seeds throughout our experiments.\n3.1\nMASK REFINEMENT AT SCALE\nWe begin by verifying the effectiveness of SparseSwaps. We make the following observations:\n6", "clean_text": "Algorithm 1 SparseSwaps: 1-Swap Pruning Optimization Require: W ∈Rdout×din, Gram matrix G = XX⊤∈Rdin×din, warmstart mask M init, Tmax Ensure: Improved pruning mask M 1: M ←M init ▷Initialize with warmstart solution 2: for i = 1 to dout do ▷Process each row independently 3: w ←Wi,:, m ←Mi,: ▷Extract row weights and mask 4: P ←{j : mj = 0}, U ←{j : mj = 1} ▷Pruned and unpruned sets 5: c ←G · ((1 −m) ⊙w) ▷Initialize correlation vector 6: for t = 1 to Tmax do 7: (p∗, u∗) ←arg min(p,u) ∆Lu,p ▷Best swap via Equation 3 8: if ∆Lu∗,p∗< 0 then ▷Swap improves objective 9: mp∗←1, mu∗←0 ▷Perform swap 10: P ←(P \\ {p∗}) ∪{u∗}, U ←(U \\ {u∗}) ∪{p∗} 11: c ←c + wu∗G:,u∗−wp∗G:,p∗ ▷Update correlation vector 12: else 13: break ▷Local optimum reached 14: end if 15: end for 16: Mi,: ←m ▷Store optimized row 17: end for 1-Swap optimization (Lines 6-15): We iteratively find the swap (p∗, u∗) minimizing ∆Lu,p (cf. Equation 3) among feasible pairs, evaluating each candidate in O(1) time. If ∆Lu∗,p∗< 0, we accept the swap and update the correlation vector via Equation 4; otherwise we terminate. At all times, the swaps are appropriately constrained: per-row sparsity allows any swap maintaining |P| constant, while N:M sparsity restricts swaps to within the same N:M blocks. The algorithm has complexity O(dout·Tmax·(|P|·|U|+din)) per layer, where Tmax is the maximum number of swap iterations per row. The |P| · |U| term comes from evaluating all candidate swaps (each in O(1) time via Equation 3), and the din term from the correlation vector update (Equation 4). In practice, several factors further reduce runtime. First, we find that even Tmax = 1 or Tmax = 2 can drastically reduce the local pruning error; values around Tmax = 25 often suffice to significantly lower model perplexity, with diminishing returns beyond Tmax = 100. Second, row-wise processing can be batched and vectorized, enabling parallel swap cost computations and mask updates, and rows can be distributed across GPUs if needed. Third, the Gram matrix G is computed once per layer and shared across all rows, and several summands of Equation 3 can be similarly precomputed once per layer. 3 EXPERIMENTAL RESULTS We outline our general experimental approach, detailing datasets, architectures, and metrics. Our code is publicly available at github.com/ZIB-IOL/SparseSwaps. Our study focuses on language modeling within Natural Language Processing (NLP). We use pretrained models from HuggingFace (Wolf et al., 2020), specifically LLAMA-3.1-8B (Grattafiori et al., 2024), GEMMA-2-9B (Riviere et al., 2024), YI-1.5-9B (Young et al., 2025), DEEPSEEK-7B-BASE (Bi et al., 2024), and QWEN2.5-7B (Yang et al., 2025). For calibration, we randomly draw sequences of 2048 tokens from the C4 dataset (Raffel et al., 2020). For validation, we similarly pick 100 sequences from the validation split. The model performance is assessed via perplexity on the WikiText dataset (Merity et al., 2016) and zero-shot accuracy on the EleutherAI evaluation set (Gao et al., 2023). Following Sun et al. (2023), we prune all linear layers, excluding the embedding and final linear head, with uniform sparsity allocation across layers. We provide experiments for unstructured and semi-structured sparsity patterns (Mishra et al., 2021). We use multiple random seeds throughout our experiments. 3.1 MASK REFINEMENT AT SCALE We begin by verifying the effectiveness of SparseSwaps. We make the following observations: 6"}
{"pdf_id": "arxiv_251210922_sparseswaps", "page": 7, "text": "Table 1: LLAMA-3.1-8B: Perplexity (↓) and mean relative reduction in pruning error (↑) versus\nnumber of 1-swap iterations for 50% and 60% unstructured sparsity using Wanda warmstart.\nNumber of 1-swap iterations\nSparsity\nMetric\n0\n1\n2\n5\n10\n25\n50\n100\n200\n50%\nAvg. rel. error reduction (%)\n0.00\n6.34\n8.77\n12.51\n16.38\n23.52\n30.04\n36.48\n38.95\nPerplexity\n10.13\n10.31\n10.40\n10.41\n10.39\n10.38\n10.27\n10.30\n10.34\n60%\nAvg. rel. error reduction (%)\n0.00\n8.04\n11.04\n15.34\n19.64\n26.92\n33.58\n39.99\n43.74\nPerplexity\n21.52\n21.26\n21.51\n21.17\n21.01\n20.38\n19.74\n18.96\n19.17\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\nLayer\n0\n10\n20\n30\n40\n50\n60\n70\n80\nRelative reduction in pruning error (%)\nPer-layer reconstruction improvement over Wanda\nattn.q proj\nattn.k proj\nattn.v proj\nattn.o proj\nmlp.gate proj\nmlp.up proj\nFigure 1: Per-layer relative reduction in local pruning error compared to Wanda. The plot shows\nresult for LLAMA-3.1-8B, 60% unstructured sparsity and 100 1-swap iterations.\nSparseSwaps consistently improves state-of-the-art methods. Table 2 summarizes the main re-\nsults and reports perplexity (upper half, lower is better) and zero-shot accuracy (lower half, higher\nis better) for warmstart masks (Wanda, RIA) as well as their refinements using DSnoT and Spars-\neSwaps. For both 60% unstructured and 2:4 semi-structured sparsity, SparseSwaps (with 100 1-swap\niterations) consistently reduces perplexity and improves zero-shot accuracy over Wanda and RIA\nwarm start masks. While DSnoT similarly yields improvements, it falls short of SparseSwaps. Note\nthat we left the pruning criterion of DSnoT, which partially uses the Wanda saliency, unchanged,\neven when using RIA warmstart. For unstructured RIA, we report results when enforcing a per-\nrow sparsity constraint; while RIA yields good (and slightly better) results when enforcing truely\nunstructured sparsity, we decided to include the results for the per-row setting as this allows direct\nrefinement of the mask with SparseSwaps and DSnoT.\nSparseSwaps successfully optimizes the per-layer pruning loss. Figure 1 shows the per-layer\nreductions in local pruning error relative to a Wanda Warmstart, grouping layers by their corre-\nsponding Transformer block of LLAMA-3.1-8B. We observe drastic improvements of close to\n70% compared to Wanda, demonstrating that SparseSwaps is able to successfully optimize the local\nloss. The attn.o proj seems to consistently benefit the most across blocks, with reductions of\nthe objective in Equation 1 ranging between 40%-60%.\nLarge local error reductions do not always imply reduced perplexity. From Table 2 we observe\nsubstantial perplexity gains, especially when sparsity more strongly degrades model quality (cf.\nTable 4 in the appendix, which shows more drastic improvements when using magnitude pruning,\nwhich more strongly degrades model quality). In contrast, when quality is less affected (e.g., at\n50% sparsity where Wanda performs well), SparseSwaps yields limited perplexity gains despite\nsignificant local error reductions: Table 1 reports perplexity and average relative error reduction (%)\nversus the number of 1-swap iterations. Zero iterations correspond to the Wanda warm start; one or\nmore iterations correspond to SparseSwaps from Wanda. At 50% sparsity, a single 1-swap iteration\nlowers relative error by 6.34%, and 200 iterations by nearly 40%, yet perplexity does not improve,\nbut rather slightly increases. This suggests further reducing local error can overfit the calibration\ndata and may not translate to better perplexity, although we note that the perplexity increase is\nrelatively small. These results emphasize that while the reduction of local error is a useful proxy\nfor perplexity reduction when pruning has a higher negative impact on the model, the local error of\nEquation 1 remains an approximation to the reconstruction error of the entire model.\n7", "clean_text": "Table 1: LLAMA-3.1-8B: Perplexity (↓) and mean relative reduction in pruning error (↑) versus number of 1-swap iterations for 50% and 60% unstructured sparsity using Wanda warmstart. Number of 1-swap iterations Sparsity Metric 0 1 2 5 10 25 50 100 200 50% Avg. rel. error reduction (%) 0.00 6.34 8.77 12.51 16.38 23.52 30.04 36.48 38.95 Perplexity 10.13 10.31 10.40 10.41 10.39 10.38 10.27 10.30 10.34 60% Avg. rel. error reduction (%) 0.00 8.04 11.04 15.34 19.64 26.92 33.58 39.99 43.74 Perplexity 21.52 21.26 21.51 21.17 21.01 20.38 19.74 18.96 19.17 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 Layer 0 10 20 30 40 50 60 70 80 Relative reduction in pruning error (%) Per-layer reconstruction improvement over Wanda attn.q proj attn.k proj attn.v proj attn.o proj mlp.gate proj mlp.up proj Figure 1: Per-layer relative reduction in local pruning error compared to Wanda. The plot shows result for LLAMA-3.1-8B, 60% unstructured sparsity and 100 1-swap iterations. SparseSwaps consistently improves state-of-the-art methods. Table 2 summarizes the main results and reports perplexity (upper half, lower is better) and zero-shot accuracy (lower half, higher is better) for warmstart masks (Wanda, RIA) as well as their refinements using DSnoT and SparseSwaps. For both 60% unstructured and 2:4 semi-structured sparsity, SparseSwaps (with 100 1-swap iterations) consistently reduces perplexity and improves zero-shot accuracy over Wanda and RIA warm start masks. While DSnoT similarly yields improvements, it falls short of SparseSwaps. Note that we left the pruning criterion of DSnoT, which partially uses the Wanda saliency, unchanged, even when using RIA warmstart. For unstructured RIA, we report results when enforcing a perrow sparsity constraint; while RIA yields good (and slightly better) results when enforcing truely unstructured sparsity, we decided to include the results for the per-row setting as this allows direct refinement of the mask with SparseSwaps and DSnoT. SparseSwaps successfully optimizes the per-layer pruning loss. Figure 1 shows the per-layer reductions in local pruning error relative to a Wanda Warmstart, grouping layers by their corresponding Transformer block of LLAMA-3.1-8B. We observe drastic improvements of close to 70% compared to Wanda, demonstrating that SparseSwaps is able to successfully optimize the local loss. The attn.o proj seems to consistently benefit the most across blocks, with reductions of the objective in Equation 1 ranging between 40%-60%. Large local error reductions do not always imply reduced perplexity. From Table 2 we observe substantial perplexity gains, especially when sparsity more strongly degrades model quality (cf. Table 4 in the appendix, which shows more drastic improvements when using magnitude pruning, which more strongly degrades model quality). In contrast, when quality is less affected (e.g., at 50% sparsity where Wanda performs well), SparseSwaps yields limited perplexity gains despite significant local error reductions: Table 1 reports perplexity and average relative error reduction (%) versus the number of 1-swap iterations. Zero iterations correspond to the Wanda warm start; one or more iterations correspond to SparseSwaps from Wanda. At 50% sparsity, a single 1-swap iteration lowers relative error by 6.34%, and 200 iterations by nearly 40%, yet perplexity does not improve, but rather slightly increases. This suggests further reducing local error can overfit the calibration data and may not translate to better perplexity, although we note that the perplexity increase is relatively small. These results emphasize that while the reduction of local error is a useful proxy for perplexity reduction when pruning has a higher negative impact on the model, the local error of Equation 1 remains an approximation to the reconstruction error of the entire model. 7"}
{"pdf_id": "arxiv_251210922_sparseswaps", "page": 8, "text": "Table 2: Perplexity (↓, lower is better) and zero-shot accuracy (↑, higher is better) comparison on\nWikiText and EleutherAI evaluation set. We report DSnoT and SparseSwaps refinement with Wanda\nand RIA warmstart for unstructured 60% sparsity and semi-structured 2:4 sparsity. Best values are\nhighlighted in bold. We omit standard deviations for legibility.\nPerplexity ↓\nLLAMA-3.1\nGEMMA-2\nYI-1.5\nDEEPSEEK\nQWEN2.5\nMethod\nSparsity\n8B\n9B\n9B\n7B\n7B\nWanda\n60%\n21.94\n16.74\n11.40\n11.41\n13.75\n+ DSnoT\n60%\n21.94\n16.69\n11.38\n11.40\n13.75\n+ SparseSwaps\n60%\n19.75\n16.01\n10.07\n10.93\n13.16\nRIA\n60%\n19.73\n16.19\n10.73\n11.80\n12.63\n+ DSnoT\n60%\n19.73\n16.22\n10.73\n11.80\n12.63\n+ SparseSwaps\n60%\n18.47\n15.44\n9.98\n10.79\n12.47\nWanda\n2:4\n24.82\n17.45\n11.76\n11.77\n14.53\n+ DSnoT\n2:4\n22.79\n16.79\n10.84\n11.70\n14.40\n+ SparseSwaps\n2:4\n20.17\n16.30\n10.73\n11.70\n13.95\nRIA\n2:4\n23.96\n16.88\n11.29\n12.03\n13.58\n+ DSnoT\n2:4\n24.26\n16.82\n10.57\n12.03\n13.85\n+ SparseSwaps\n2:4\n20.90\n16.33\n10.50\n11.80\n13.28\nAccuracy ↑\nLLAMA-3.1\nGEMMA-2\nYI-1.5\nDEEPSEEK\nQWEN2.5\nMethod\nSparsity\n8B\n9B\n9B\n7B\n7B\nWanda\n60%\n48.18%\n63.39%\n53.59%\n50.74%\n59.26%\n+ DSnoT\n60%\n48.18%\n63.49%\n53.79%\n50.75%\n59.26%\n+ SparseSwaps\n60%\n50.78%\n63.84%\n54.84%\n51.02%\n60.15%\nRIA\n60%\n49.56%\n64.37%\n52.81%\n50.92%\n59.84%\n+ DSnoT\n60%\n49.56%\n64.43%\n52.96%\n50.83%\n59.81%\n+ SparseSwaps\n60%\n51.02%\n64.32%\n54.45%\n51.47%\n61.22%\nWanda\n2:4\n46.80%\n63.73%\n52.58%\n51.02%\n59.52%\n+ DSnoT\n2:4\n47.01%\n63.66%\n52.16%\n50.78%\n59.09%\n+ SparseSwaps\n2:4\n48.83%\n64.70%\n52.43%\n50.36%\n59.92%\nRIA\n2:4\n47.87%\n63.87%\n52.68%\n51.22%\n58.66%\n+ DSnoT\n2:4\n47.13%\n64.17%\n51.36%\n49.86%\n59.72%\n+ SparseSwaps\n2:4\n49.90%\n64.60%\n52.30%\n51.46%\n60.31%\n3.2\nEFFICIENCY AND HYPERPARAMETER ABLATIONS\nResource requirements. SparseSwaps is more resource-intensive than DSnoT and, as a drop-in\nrefinement, requires at least the resources of the chosen warm-start method. Beyond that, Spars-\neSwaps needs memory to store the Gram matrix G ∈Rdin×din (once per layer) and the correlation\nvector c ∈Rdin (per row), and compute to perform the 1-swaps; see the preceding section for the\ntheoretical complexity. While we have argued in the introduction that the additional compute can be\njustified when amortized over many LLM inference requests, we note that the overhead grows only\nlinearly with the number of 1-swap iterations Tmax. Table 1 shows that few iterations already yield\nsubstantial gains in both perplexity and local error reduction, especially at higher sparsity.\nTable 3 reports wall-clock times for pruning LLAMA-3.1-8B to 60% sparsity on a single H100\nGPU. The Tmax = 0 baseline includes calibration data sampling, Wanda pruning, Gram matrix\ncomputation, and evaluation; each additional iteration of SparseSwaps adds a relatively small over-\nhead. For comparison, Wanda and SparseGPT take approximately 4 and 10 minutes, respectively.\nWe note that our implementation can be further optimized and that the algorithm is fully paralleliz-\nable across rows.\nEffect of the number of reconstruction samples. Figure 2 in the appendix shows the perplexity\nversus the number of reconstruction samples for 50% and 60% unstructured sparsity when using\nWanda as well as SparseSwaps with a Wanda warmstart. We observe that the perplexity decreases\n8", "clean_text": "Table 2: Perplexity (↓, lower is better) and zero-shot accuracy (↑, higher is better) comparison on WikiText and EleutherAI evaluation set. We report DSnoT and SparseSwaps refinement with Wanda and RIA warmstart for unstructured 60% sparsity and semi-structured 2:4 sparsity. Best values are highlighted in bold. We omit standard deviations for legibility. Perplexity ↓ LLAMA-3.1 GEMMA-2 YI-1.5 DEEPSEEK QWEN2.5 Method Sparsity 8B 9B 9B 7B 7B Wanda 60% 21.94 16.74 11.40 11.41 13.75 + DSnoT 60% 21.94 16.69 11.38 11.40 13.75 + SparseSwaps 60% 19.75 16.01 10.07 10.93 13.16 RIA 60% 19.73 16.19 10.73 11.80 12.63 + DSnoT 60% 19.73 16.22 10.73 11.80 12.63 + SparseSwaps 60% 18.47 15.44 9.98 10.79 12.47 Wanda 2:4 24.82 17.45 11.76 11.77 14.53 + DSnoT 2:4 22.79 16.79 10.84 11.70 14.40 + SparseSwaps 2:4 20.17 16.30 10.73 11.70 13.95 RIA 2:4 23.96 16.88 11.29 12.03 13.58 + DSnoT 2:4 24.26 16.82 10.57 12.03 13.85 + SparseSwaps 2:4 20.90 16.33 10.50 11.80 13.28 Accuracy ↑ LLAMA-3.1 GEMMA-2 YI-1.5 DEEPSEEK QWEN2.5 Method Sparsity 8B 9B 9B 7B 7B Wanda 60% 48.18% 63.39% 53.59% 50.74% 59.26% + DSnoT 60% 48.18% 63.49% 53.79% 50.75% 59.26% + SparseSwaps 60% 50.78% 63.84% 54.84% 51.02% 60.15% RIA 60% 49.56% 64.37% 52.81% 50.92% 59.84% + DSnoT 60% 49.56% 64.43% 52.96% 50.83% 59.81% + SparseSwaps 60% 51.02% 64.32% 54.45% 51.47% 61.22% Wanda 2:4 46.80% 63.73% 52.58% 51.02% 59.52% + DSnoT 2:4 47.01% 63.66% 52.16% 50.78% 59.09% + SparseSwaps 2:4 48.83% 64.70% 52.43% 50.36% 59.92% RIA 2:4 47.87% 63.87% 52.68% 51.22% 58.66% + DSnoT 2:4 47.13% 64.17% 51.36% 49.86% 59.72% + SparseSwaps 2:4 49.90% 64.60% 52.30% 51.46% 60.31% 3.2 EFFICIENCY AND HYPERPARAMETER ABLATIONS Resource requirements. SparseSwaps is more resource-intensive than DSnoT and, as a drop-in refinement, requires at least the resources of the chosen warm-start method. Beyond that, SparseSwaps needs memory to store the Gram matrix G ∈Rdin×din (once per layer) and the correlation vector c ∈Rdin (per row), and compute to perform the 1-swaps; see the preceding section for the theoretical complexity. While we have argued in the introduction that the additional compute can be justified when amortized over many LLM inference requests, we note that the overhead grows only linearly with the number of 1-swap iterations Tmax. Table 1 shows that few iterations already yield substantial gains in both perplexity and local error reduction, especially at higher sparsity. Table 3 reports wall-clock times for pruning LLAMA-3.1-8B to 60% sparsity on a single H100 GPU. The Tmax = 0 baseline includes calibration data sampling, Wanda pruning, Gram matrix computation, and evaluation; each additional iteration of SparseSwaps adds a relatively small overhead. For comparison, Wanda and SparseGPT take approximately 4 and 10 minutes, respectively. We note that our implementation can be further optimized and that the algorithm is fully parallelizable across rows. Effect of the number of reconstruction samples. Figure 2 in the appendix shows the perplexity versus the number of reconstruction samples for 50% and 60% unstructured sparsity when using Wanda as well as SparseSwaps with a Wanda warmstart. We observe that the perplexity decreases 8"}
{"pdf_id": "arxiv_251210922_sparseswaps", "page": 9, "text": "Table 3: Wall-clock time for applying SparseSwaps to LLAMA-3.1-8B at 60% sparsity on a single\nH100 GPU.\nTmax\n0\n1\n2\n5\n10\n25\nWall-clock time\n8m15s\n10m17s\n12m7s\n17m20s\n26m13s\n52m29s\ndrastically when using more samples, which leads to SparseSwaps slightly outperforming Wanda\nfor 50% sparsity, despite its advantage typically being larger at higher sparsity. We emphasize that\nthe number of reconstruction samples does not affect SparseSwaps’s swap evaluation efficiency: the\nGram matrix G = XX⊤has fixed size din × din regardless of B.\n4\nCONCLUSION\nWe revisited the mask selection problem for post-training pruning and showed that it can be made\nsubstantially more tractable, even at LLM scale. We observed that row decoupling via equal per-\nrow sparsity yields independent subproblems, and that individual 1-swaps can be evaluated in O(1)\ntime using the Gram matrix G = XX⊤. This enables tractable optimization of the true row-\nwise quadratic loss on GPUs. The resulting method, SparseSwaps, is warm-start agnostic, nearly\nhyperparameter-free, and scalable. It consistently reduces per-layer pruning error and improves\nperplexity and zero-shot accuracy across modern GPT architectures.\nOur work is not without limitations. While per-row sparsity is not necessarily detrimental for LLMs,\nour approach is restricted to that setting and only partially adapts to truly unstructured sparsity; in\nits current form, the algorithm can handle unstructured sparsity but cannot reallocate sparsity levels\nacross rows. Furthermore, runtime and memory remain non-trivial for large architectures.\nACKNOWLEDGMENTS\nThis research was partially supported by the DFG Cluster of Excellence MATH+ (EXC-2046/1,\nproject id 390685689) funded by the Deutsche Forschungsgemeinschaft (DFG) as well as by the\nGerman Federal Ministry of Research, Technology and Space (fund number 16IS23025B).\n9", "clean_text": "Table 3: Wall-clock time for applying SparseSwaps to LLAMA-3.1-8B at 60% sparsity on a single H100 GPU. Tmax 0 1 2 5 10 25 Wall-clock time 8m15s 10m17s 12m7s 17m20s 26m13s 52m29s drastically when using more samples, which leads to SparseSwaps slightly outperforming Wanda for 50% sparsity, despite its advantage typically being larger at higher sparsity. We emphasize that the number of reconstruction samples does not affect SparseSwaps’s swap evaluation efficiency: the Gram matrix G = XX⊤has fixed size din × din regardless of B. 4 CONCLUSION We revisited the mask selection problem for post-training pruning and showed that it can be made substantially more tractable, even at LLM scale. We observed that row decoupling via equal perrow sparsity yields independent subproblems, and that individual 1-swaps can be evaluated in O(1) time using the Gram matrix G = XX⊤. This enables tractable optimization of the true rowwise quadratic loss on GPUs. The resulting method, SparseSwaps, is warm-start agnostic, nearly hyperparameter-free, and scalable. It consistently reduces per-layer pruning error and improves perplexity and zero-shot accuracy across modern GPT architectures. Our work is not without limitations. While per-row sparsity is not necessarily detrimental for LLMs, our approach is restricted to that setting and only partially adapts to truly unstructured sparsity; in its current form, the algorithm can handle unstructured sparsity but cannot reallocate sparsity levels across rows. Furthermore, runtime and memory remain non-trivial for large architectures. ACKNOWLEDGMENTS This research was partially supported by the DFG Cluster of Excellence MATH+ (EXC-2046/1, project id 390685689) funded by the Deutsche Forschungsgemeinschaft (DFG) as well as by the German Federal Ministry of Research, Technology and Space (fund number 16IS23025B). 9"}
{"pdf_id": "arxiv_251210922_sparseswaps", "page": 10, "text": "REFERENCES\nXiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding,\nKai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan,\nDaya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang,\nErhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, A. X. Liu,\nBo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong\nMa, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong\nRuan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun,\nMinghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong\nWu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu, Yanhong Xu,\nDejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang,\nLiyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang\nZhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou.\nDeepSeek\nLLM: Scaling Open-Source Language Models with Longtermism, January 2024. URL http:\n//arxiv.org/abs/2401.02954.\nSuresh Bolusani, Mathieu Besanc¸on, Ksenia Bestuzheva, Antonia Chmiela, Jo˜ao Dion´ısio, Tim\nDonkiewicz, Jasper van Doornmalen, Leon Eifler, Mohammed Ghannam, Ambros Gleixner,\nChristoph Graczyk, Katrin Halbig, Ivo Hedtke, Alexander Hoen, Christopher Hojny, Rolf van der\nHulst, Dominik Kamp, Thorsten Koch, Kevin Kofler, Jurgen Lentz, Julian Manns, Gioni Mexi,\nErik M¨uhmer, Marc E. Pfetsch, Franziska Schl¨osser, Felipe Serrano, Yuji Shinano, Mark Turner,\nStefan Vigerske, Dieter Weninger, and Lixing Xu. The SCIP Optimization Suite 9.0. Techni-\ncal report, Optimization Online, February 2024. URL https://optimization-online.\norg/2024/02/the-scip-optimization-suite-9-0/.\nPierre Bonami, Lorenz T Biegler, Andrew R Conn, G´erard Cornu´ejols, Ignacio E Grossmann, Carl D\nLaird, Jon Lee, Andrea Lodi, Franc¸ois Margot, Nicolas Sawaya, et al. An algorithmic framework\nfor convex mixed integer nonlinear programs. Discrete optimization, 5(2):186–204, 2008.\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8(): 8-bit matrix multi-\nplication for transformers at scale. August 2022.\nUtku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery:\nMaking all tickets winners. In Hal Daum´e III and Aarti Singh (eds.), Proceedings of the 37th\nInternational Conference on Machine Learning, volume 119 of Proceedings of Machine Learning\nResearch, pp. 2943–2952. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.\npress/v119/evci20a.html.\nElias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in\none-shot. In International Conference on Machine Learning, pp. 10323–10337. PMLR, 2023.\nTrevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv\npreprint arXiv:1902.09574, 2019.\nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Fos-\nter, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muen-\nnighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lin-\ntang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework\nfor few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/\n10256836.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad\nAl-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan,\nAnirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Ko-\nrenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava\nSpataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux,\nChaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret,\nChunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius,\nDaniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary,\nDhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab\nAlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco\n10", "clean_text": "REFERENCES Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, A. X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. DeepSeek LLM: Scaling Open-Source Language Models with Longtermism, January 2024. URL http: //arxiv.org/abs/2401.02954. Suresh Bolusani, Mathieu Besanc¸on, Ksenia Bestuzheva, Antonia Chmiela, Jo˜ao Dion´ısio, Tim Donkiewicz, Jasper van Doornmalen, Leon Eifler, Mohammed Ghannam, Ambros Gleixner, Christoph Graczyk, Katrin Halbig, Ivo Hedtke, Alexander Hoen, Christopher Hojny, Rolf van der Hulst, Dominik Kamp, Thorsten Koch, Kevin Kofler, Jurgen Lentz, Julian Manns, Gioni Mexi, Erik M¨uhmer, Marc E. Pfetsch, Franziska Schl¨osser, Felipe Serrano, Yuji Shinano, Mark Turner, Stefan Vigerske, Dieter Weninger, and Lixing Xu. The SCIP Optimization Suite 9.0. Technical report, Optimization Online, February 2024. URL https://optimization-online. org/2024/02/the-scip-optimization-suite-9-0/. Pierre Bonami, Lorenz T Biegler, Andrew R Conn, G´erard Cornu´ejols, Ignacio E Grossmann, Carl D Laird, Jon Lee, Andrea Lodi, Franc¸ois Margot, Nicolas Sawaya, et al. An algorithmic framework for convex mixed integer nonlinear programs. Discrete optimization, 5(2):186–204, 2008. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8(): 8-bit matrix multiplication for transformers at scale. August 2022. Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making all tickets winners. In Hal Daum´e III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 2943–2952. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr. press/v119/evci20a.html. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International Conference on Machine Learning, pp. 10323–10337. PMLR, 2023. Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv preprint arXiv:1902.09574, 2019. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/ 10256836. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco 10"}
{"pdf_id": "arxiv_251210922_sparseswaps", "page": 11, "text": "Guzm´an, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind That-\ntai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Kore-\nvaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra,\nIvan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Ma-\nhadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu,\nJianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jong-\nsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala,\nKarthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid\nEl-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren\nRantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin,\nLovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi,\nMahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew\nOldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Ku-\nmar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoy-\nchev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur C¸ elebi, Patrick Alrassy, Pengchuan\nZhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan,\nPunit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ra-\nmon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Ro-\nhit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan\nSilva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell,\nSeohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng\nShen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer\nWhitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman,\nTara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mi-\nhaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor\nKerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, V´ıtor Albiero, Vladan Petrovic, Weiwei\nChu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang\nWang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Gold-\nschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning\nMao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh,\nAayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria,\nAhuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein,\nAmanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, An-\ndrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, An-\nnie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel,\nAshwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leon-\nhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu\nNi, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Mon-\ntalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao\nZhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia\nGao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide\nTestuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le,\nDustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily\nHahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smoth-\ners, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni,\nFrank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia\nSwee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan,\nHakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harri-\nson Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj,\nIgor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James\nGeboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jen-\nnifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang,\nJoe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Jun-\njie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy\nMatosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang,\nKunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell,\nLei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa,\nManav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias\nReso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L.\n11", "clean_text": "Guzm´an, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur C¸ elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, V´ıtor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. 11"}
{"pdf_id": "arxiv_251210922_sparseswaps", "page": 12, "text": "Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike\nClark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari,\nMunish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan\nSinghal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong,\nNorman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent,\nParth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar,\nPolina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Ro-\ndriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy,\nRaymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin\nMehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon,\nSasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ra-\nmaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha,\nShishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal,\nSoji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satter-\nfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj\nSubramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo\nKoehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook\nShaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar,\nVishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li,\nWenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu,\nXiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi,\nYenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen\nHao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen,\nZhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The Llama 3 Herd of Models, November 2024. URL\nhttp://arxiv.org/abs/2407.21783.\nSong Han, Jeff Pool, John Tran, and William Dally.\nLearning both weights and connections\nfor efficient neural networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Gar-\nnett (eds.), Advances in Neural Information Processing Systems, volume 28. Curran Asso-\nciates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/\nae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf.\nBabak Hassibi and David Stork.\nSecond order derivatives for network pruning: Optimal brain\nsurgeon. In S. Hanson, J. Cowan, and C. Giles (eds.), Advances in Neural Information Processing\nSystems, volume 5. Morgan-Kaufmann, 1993. URL https://proceedings.neurips.\ncc/paper/1992/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf.\nDeborah Hendrych, Hannah Troppens, Mathieu Besanc¸on, and Sebastian Pokutta. Convex inte-\nger optimization with frank-wolfe methods. Mathematical Programming Computation, 2025.\ndoi: 10.1007/s12532-025-00288-w.\nURL https://link.springer.com/article/\n10.1007/s12532-025-00288-w.\nTorsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste.\nSparsity in\ndeep learning: Pruning and growth for efficient inference and training in neural networks. arXiv\npreprint arXiv:2102.00554, January 2021.\nSteven A. Janowsky. Pruning versus clipping in neural networks. Phys. Rev. A, 39:6600–6603, Jun\n1989. doi: 10.1103/PhysRevA.39.6600.\nWoosuk Kwon, Sehoon Kim, Michael W. Mahoney, Joseph Hassoun, Kurt Keutzer, and Amir Gho-\nlami. A fast post-training pruning framework for transformers. March 2022.\nYann LeCun, John S. Denker, and Sara A. Solla. Optimal brain damage. In David S. Touretzky (ed.),\nAdvances in Neural Information Processing Systems 2, [NIPS Conference, Denver, Colorado,\nUSA, November 27-30, 1989], pp. 598–605. Morgan Kaufmann, 1989. URL http://papers.\nnips.cc/paper/250-optimal-brain-damage.\nVladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. Scaling down to scale up: A guide to\nparameter-efficient fine-tuning. March 2023.\nTao Lin, Sebastian U. Stich, Luis Barba, Daniil Dmitriev, and Martin Jaggi. Dynamic model pruning\nwith feedback. In International Conference on Learning Representations, 2020.\n12", "clean_text": "Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The Llama 3 Herd of Models, November 2024. URL http://arxiv.org/abs/2407.21783. Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/ ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf. Babak Hassibi and David Stork. Second order derivatives for network pruning: Optimal brain surgeon. In S. Hanson, J. Cowan, and C. Giles (eds.), Advances in Neural Information Processing Systems, volume 5. Morgan-Kaufmann, 1993. URL https://proceedings.neurips. cc/paper/1992/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf. Deborah Hendrych, Hannah Troppens, Mathieu Besanc¸on, and Sebastian Pokutta. Convex integer optimization with frank-wolfe methods. Mathematical Programming Computation, 2025. doi: 10.1007/s12532-025-00288-w. URL https://link.springer.com/article/ 10.1007/s12532-025-00288-w. Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. arXiv preprint arXiv:2102.00554, January 2021. Steven A. Janowsky. Pruning versus clipping in neural networks. Phys. Rev. A, 39:6600–6603, Jun 1989. doi: 10.1103/PhysRevA.39.6600. Woosuk Kwon, Sehoon Kim, Michael W. Mahoney, Joseph Hassoun, Kurt Keutzer, and Amir Gholami. A fast post-training pruning framework for transformers. March 2022. Yann LeCun, John S. Denker, and Sara A. Solla. Optimal brain damage. In David S. Touretzky (ed.), Advances in Neural Information Processing Systems 2, [NIPS Conference, Denver, Colorado, USA, November 27-30, 1989], pp. 598–605. Morgan Kaufmann, 1989. URL http://papers. nips.cc/paper/250-optimal-brain-damage. Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. Scaling down to scale up: A guide to parameter-efficient fine-tuning. March 2023. Tao Lin, Sebastian U. Stich, Luis Barba, Daniil Dmitriev, and Martin Jaggi. Dynamic model pruning with feedback. In International Conference on Learning Representations, 2020. 12"}
{"pdf_id": "arxiv_251210922_sparseswaps", "page": 13, "text": "Andreas Lundell, Jan Kronqvist, and Tapio Westerlund. The supporting hyperplane optimization\ntoolkit for convex minlp. Journal of Global Optimization, 84(1):1–41, 2022.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels. September 2016.\nAsit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh,\nChong Yu, and Paulius Micikevicius. Accelerating sparse deep neural networks. April 2021.\nPavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional\nneural networks for resource efficient inference. November 2016.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020.\nMorgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L´eonard\nHussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram´e, Johan Ferret, Peter Liu, Pouya\nTafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy\nJerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt\nHoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna\nWalton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic,\nAmanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben\nBastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris\nWelty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vi-\njaykumar, Dominika Rogozi´nska, Dustin Herbison, Elisa Bandy, Emma Wang, Eric Noland, Er-\nica Moreira, Evan Senter, Evgenii Eltyshev, Francesco Visin, Gabriel Rasskin, Gary Wei, Glenn\nCameron, Gus Martins, Hadi Hashemi, Hanna Klimczak-Pluci´nska, Harleen Batra, Harsh Dhand,\nIvan Nardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Peng\nZhou, Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost van Amersfoort,\nJosh Gordon, Josh Lipschultz, Josh Newlan, Ju-yeong Ji, Kareem Mohamed, Kartikeya Badola,\nKat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish Greene,\nLars Lowe Sjoesund, Lauren Usui, Laurent Sifre, Lena Heuermann, Leticia Lago, Lilly Mc-\nNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas Dixon, Luciano Martins, Machel Reid,\nManvinder Singh, Mark Iverson, Martin G¨orner, Mat Velloso, Mateo Wirth, Matt Davidow,\nMatt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal, Mehran Kazemi, Michael Moyni-\nhan, Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi Rahman, Mohit Khatwani, Natalie Dao,\nNenshad Bardoliwalla, Nesh Devanathan, Neta Dumai, Nilay Chauhan, Oscar Wahltinez, Pankil\nBotarda, Parker Barnes, Paul Barham, Paul Michel, Pengchong Jin, Petko Georgiev, Phil Culli-\nton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena Jana, Reza Ardeshir Rokni,\nRishabh Agarwal, Ryan Mullins, Samaneh Saadat, Sara Mc Carthy, Sarah Cogan, Sarah Perrin,\nS´ebastien M. R. Arnold, Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ron-\nstrom, Susan Chan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tomas Kocisky, Tulsee\nDoshi, Vihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren Barkley, Wei\nWei, Wenming Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong, Zichuan\nWei, Victor Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Ludovic Peran, Tris Warkentin, Eli\nCollins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, D. Sculley, Jeanine Banks, Anca Dra-\ngan, Slav Petrov, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Fara-\nbet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin, Kathleen Kenealy,\nRobert Dadashi, and Alek Andreev. Gemma 2: Improving Open Language Models at a Practical\nSize, October 2024. URL http://arxiv.org/abs/2408.00118.\nMingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. A simple and effective pruning approach\nfor large language models. June 2023.\nMingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. A Simple and Effective Pruning Approach\nfor Large Language Models, May 2024. URL http://arxiv.org/abs/2306.11695.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\ntion processing systems, 30, 2017.\n13", "clean_text": "Andreas Lundell, Jan Kronqvist, and Tapio Westerlund. The supporting hyperplane optimization toolkit for convex minlp. Journal of Global Optimization, 84(1):1–41, 2022. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. September 2016. Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius. Accelerating sparse deep neural networks. April 2021. Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efficient inference. November 2016. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020. Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L´eonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram´e, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris Welty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vijaykumar, Dominika Rogozi´nska, Dustin Herbison, Elisa Bandy, Emma Wang, Eric Noland, Erica Moreira, Evan Senter, Evgenii Eltyshev, Francesco Visin, Gabriel Rasskin, Gary Wei, Glenn Cameron, Gus Martins, Hadi Hashemi, Hanna Klimczak-Pluci´nska, Harleen Batra, Harsh Dhand, Ivan Nardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Peng Zhou, Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost van Amersfoort, Josh Gordon, Josh Lipschultz, Josh Newlan, Ju-yeong Ji, Kareem Mohamed, Kartikeya Badola, Kat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish Greene, Lars Lowe Sjoesund, Lauren Usui, Laurent Sifre, Lena Heuermann, Leticia Lago, Lilly McNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas Dixon, Luciano Martins, Machel Reid, Manvinder Singh, Mark Iverson, Martin G¨orner, Mat Velloso, Mateo Wirth, Matt Davidow, Matt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal, Mehran Kazemi, Michael Moynihan, Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi Rahman, Mohit Khatwani, Natalie Dao, Nenshad Bardoliwalla, Nesh Devanathan, Neta Dumai, Nilay Chauhan, Oscar Wahltinez, Pankil Botarda, Parker Barnes, Paul Barham, Paul Michel, Pengchong Jin, Petko Georgiev, Phil Culliton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena Jana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat, Sara Mc Carthy, Sarah Cogan, Sarah Perrin, S´ebastien M. R. Arnold, Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ronstrom, Susan Chan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tomas Kocisky, Tulsee Doshi, Vihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren Barkley, Wei Wei, Wenming Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong, Zichuan Wei, Victor Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Ludovic Peran, Tris Warkentin, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, D. Sculley, Jeanine Banks, Anca Dragan, Slav Petrov, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin, Kathleen Kenealy, Robert Dadashi, and Alek Andreev. Gemma 2: Improving Open Language Models at a Practical Size, October 2024. URL http://arxiv.org/abs/2408.00118. Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. A simple and effective pruning approach for large language models. June 2023. Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. A Simple and Effective Pruning Approach for Large Language Models, May 2024. URL http://arxiv.org/abs/2306.11695. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 13"}
{"pdf_id": "arxiv_251210922_sparseswaps", "page": 14, "text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick\nvon Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,\nMariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020 Conference on Empirical Methods in Natu-\nral Language Processing: System Demonstrations, pp. 38–45, Online, October 2020. Associ-\nation for Computational Linguistics.\ndoi: 10.18653/v1/2020.emnlp-demos.6.\nURL https:\n//aclanthology.org/2020.emnlp-demos.6.\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li,\nDayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin\nYang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang,\nLe Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi\nTang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan,\nYuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 Technical Report, January 2025.\nURL http://arxiv.org/abs/2412.15115.\nSeul-Ki Yeom, Philipp Seegerer, Sebastian Lapuschkin, Alexander Binder, Simon Wiedemann,\nKlaus-Robert M¨uller, and Wojciech Samek. Pruning by explaining: A novel criterion for deep\nneural network pruning. December 2019.\nLu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia, Mykola Pechenizkiy,\nYi Liang, Zhangyang Wang, and Shiwei Liu. Outlier weighed layerwise sparsity (owl): A missing\nsecret sauce for pruning llms to high sparsity. October 2023.\nAlex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Guoyin Wang, Heng\nLi, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue,\nSenbin Yang, Shiming Yang, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu,\nPengcheng Nie, Yanpeng Li, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan\nLiu, and Zonghong Dai. Yi: Open Foundation Models by 01.AI, January 2025. URL http:\n//arxiv.org/abs/2403.04652.\nMengxia Yu, De Wang, Qi Shan, Colorado J. Reed, and Alvin Wan. The Super Weight in Large\nLanguage Models, July 2025. URL http://arxiv.org/abs/2411.07191.\nYingtao Zhang, Haoli Bai, Haokun Lin, Jialin Zhao, Lu Hou, and Carlo Vittorio Cannistraci. Plug-\nand-play: An efficient post-training pruning method for large language models. In The Twelfth\nInternational Conference on Learning Representations, 2024. URL https://openreview.\nnet/forum?id=Tr0lPx9woF.\nYuxin Zhang, Lirui Zhao, Mingbao Lin, Yunyun Sun, Yiwu Yao, Xingjia Han, Jared Tanner, Shiwei\nLiu, and Rongrong Ji. Dynamic sparse no training: Training-free fine-tuning for sparse llms.\nOctober 2023.\nMax Zimmer, Megi Andoni, Christoph Spiegel, and Sebastian Pokutta. Perp: Rethinking the prune-\nretrain paradigm in the era of llms. arXiv preprint arXiv:2312.15230, December 2023a. URL\nhttps://arxiv.org/abs/2312.15230.\nMax Zimmer, Christoph Spiegel, and Sebastian Pokutta. How I Learned To Stop Worrying And\nLove Retraining.\nIn International Conference on Learning Representations, 2023b.\nURL\nhttps://openreview.net/forum?id=_nF5imFKQI.\nMax Zimmer, Christoph Spiegel, and Sebastian Pokutta.\nCompression-aware training of neu-\nral networks using Frank–Wolfe, pp. 137–168.\nDe Gruyter, Berlin, Boston, 2025.\nISBN\n9783111376776.\ndoi: doi:10.1515/9783111376776-010.\nURL https://doi.org/10.\n1515/9783111376776-010.\n14", "clean_text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 38–45, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https: //aclanthology.org/2020.emnlp-demos.6. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 Technical Report, January 2025. URL http://arxiv.org/abs/2412.15115. Seul-Ki Yeom, Philipp Seegerer, Sebastian Lapuschkin, Alexander Binder, Simon Wiedemann, Klaus-Robert M¨uller, and Wojciech Samek. Pruning by explaining: A novel criterion for deep neural network pruning. December 2019. Lu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia, Mykola Pechenizkiy, Yi Liang, Zhangyang Wang, and Shiwei Liu. Outlier weighed layerwise sparsity (owl): A missing secret sauce for pruning llms to high sparsity. October 2023. Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Guoyin Wang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yanpeng Li, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open Foundation Models by 01.AI, January 2025. URL http: //arxiv.org/abs/2403.04652. Mengxia Yu, De Wang, Qi Shan, Colorado J. Reed, and Alvin Wan. The Super Weight in Large Language Models, July 2025. URL http://arxiv.org/abs/2411.07191. Yingtao Zhang, Haoli Bai, Haokun Lin, Jialin Zhao, Lu Hou, and Carlo Vittorio Cannistraci. Plugand-play: An efficient post-training pruning method for large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=Tr0lPx9woF. Yuxin Zhang, Lirui Zhao, Mingbao Lin, Yunyun Sun, Yiwu Yao, Xingjia Han, Jared Tanner, Shiwei Liu, and Rongrong Ji. Dynamic sparse no training: Training-free fine-tuning for sparse llms. October 2023. Max Zimmer, Megi Andoni, Christoph Spiegel, and Sebastian Pokutta. Perp: Rethinking the pruneretrain paradigm in the era of llms. arXiv preprint arXiv:2312.15230, December 2023a. URL https://arxiv.org/abs/2312.15230. Max Zimmer, Christoph Spiegel, and Sebastian Pokutta. How I Learned To Stop Worrying And Love Retraining. In International Conference on Learning Representations, 2023b. URL https://openreview.net/forum?id=_nF5imFKQI. Max Zimmer, Christoph Spiegel, and Sebastian Pokutta. Compression-aware training of neural networks using Frank–Wolfe, pp. 137–168. De Gruyter, Berlin, Boston, 2025. ISBN 9783111376776. doi: doi:10.1515/9783111376776-010. URL https://doi.org/10. 1515/9783111376776-010. 14"}
{"pdf_id": "arxiv_251210922_sparseswaps", "page": 15, "text": "A\nAPPENDIX\nA.1\nFURTHER RESULTS\nTable 4: Perplexity (↓, lower is better) comparison on WikiText. We report SparseSwaps refinement\nwith magnitude warmstart for 50% and 60% sparsity. Best values are highlighted in bold. We omit\nstandard deviations for legibility.\nPerplexity ↓\nLLAMA-3.1\nGEMMA-2\nDEEPSEEK\nMethod\nSparsity\n8B\n9B\n7B\nMagnitude\n50%\n68.89\n31.87\n25.05\n+ SparseSwaps\n50%\n52.26\n19.11\n16.23\nMagnitude\n60%\n3486.26\n184.52\n330.07\n+ SparseSwaps\n60%\n264.92\n60.04\n80.24\n0\n100\n200\n300\n400\n500\nnumber of samples\n10.2\n10.4\n10.6\nperplexity\nLLaMA-3.1-8B (50% sparsity)\nWanda\nSparseSwaps\n(a) 50% unstructured sparsity\n0\n100\n200\n300\n400\n500\nnumber of samples\n19\n20\n21\n22\n23\nperplexity\nLLaMA-3.1-8B (60% sparsity)\nWanda\nSparseSwaps\n(b) 60% unstructured sparsity\nFigure 2: Perplexity versus the number of reconstruction samples for unstructured sparsity using\nWanda warmstart.\n15", "clean_text": "A APPENDIX A.1 FURTHER RESULTS Table 4: Perplexity (↓, lower is better) comparison on WikiText. We report SparseSwaps refinement with magnitude warmstart for 50% and 60% sparsity. Best values are highlighted in bold. We omit standard deviations for legibility. Perplexity ↓ LLAMA-3.1 GEMMA-2 DEEPSEEK Method Sparsity 8B 9B 7B Magnitude 50% 68.89 31.87 25.05 + SparseSwaps 50% 52.26 19.11 16.23 Magnitude 60% 3486.26 184.52 330.07 + SparseSwaps 60% 264.92 60.04 80.24 0 100 200 300 400 500 number of samples 10.2 10.4 10.6 perplexity LLaMA-3.1-8B (50% sparsity) Wanda SparseSwaps (a) 50% unstructured sparsity 0 100 200 300 400 500 number of samples 19 20 21 22 23 perplexity LLaMA-3.1-8B (60% sparsity) Wanda SparseSwaps (b) 60% unstructured sparsity Figure 2: Perplexity versus the number of reconstruction samples for unstructured sparsity using Wanda warmstart. 15"}
