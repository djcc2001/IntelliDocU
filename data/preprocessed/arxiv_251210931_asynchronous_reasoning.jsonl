{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 1, "text": "Asynchronous Reasoning: Training-Free Interactive Thinking LLMs\nGeorge Yakushev * 1 2 Nataliia Babina * 3 Masoud Vahid Dastgerdi * 2 Vyacheslav Zhdanovskiy * 1\nAlina Shutova 1 2 Denis Kuznedelev 1\nAbstract\nMany state-of-the-art LLMs are trained to think\nbefore giving their answer. Reasoning can greatly\nimprove language model capabilities and safety,\nbut it also makes them less interactive: given a\nnew input, a model must stop thinking before it\ncan respond. Real-world use cases such as voice-\nbased or embedded assistants require an LLM\nagent to respond and adapt to additional infor-\nmation in real time, which is incompatible with\nsequential interactions. In contrast, humans can\nlisten, think, and act asynchronously: we begin\nthinking about the problem while reading it and\ncontinue thinking while formulating the answer.\nIn this work, we augment LLMs capable of rea-\nsoning to operate in a similar way without addi-\ntional training. Our method uses the properties\nof rotary embeddings to enable LLMs built for\nsequential interactions to simultaneously think,\nlisten, and generate outputs. We evaluate our ap-\nproach on math, commonsense, and safety reason-\ning and find that it can generate accurate thinking-\naugmented answers in real time, reducing time\nto first non-thinking token from minutes to ≤5s.\nand the overall real-time delays by 6−11×.\n1. Introduction\nModern Large Language Models (LLMs) solve complex\ntasks using inference-time computation mechanisms [1, 2,\n3], such as chain-of-thought reasoning [4, 5, 6, 7] and agen-\ntic tool use [10, 11, 12, 13]. Recent models, both propri-\netary [18, 19, 20] and open-weights [21, 22, 23], are explic-\nitly trained for reasoning and agentic capabilities. As we\ntrust LLMs with harder problems [24, 25], their ability to\n“think” becomes ever more important.\nThe current dominant strategy for LLM reasoning is the\nread-think-answer cycle: the model encodes a given prob-\nlem, then generates chain-of-thought reasoning, possibly\n*Equal contribution 1Yandex 2HSE University 3The University\nof Tokyo. Preprint, work in progress. Correspondence to: Denis\nKuznedelev <dkuznedelev@yandex-team.ru>.\ncalls tools, and then formulates the final answer [18, 21, 23].\nThis fits naturally with the sequential view of LLMs as next-\ntoken prediction models. However, this also means that the\nLLM must follow a rigid turn structure that can limit their\nflexibility. The “thinking” phase can take minutes of real\ntime, during which the agent does not get new information\nor output its current results.\nUnlike LLM agents, people have an innate ability to think\nasynchronously [26, 27, 28, 29]. When working on a prob-\nlem, we can begin solving it even before we have heard its\nentire statement, and can start talking (or acting) while still\ncompleting our solution. Such “multitasking” is not always\neasy or efficient [30], but it allows us to effectively operate\nin a dynamic environment [31].\nSimilarly, artificial agents often need real-time ability to\nchange course of action. A voice assistant is expected to\nmaintain conversation in real time [32, 33, 34, 35, 36, 37,\n38]. An embodied agent’s VLA model [39, 40, 41] needs to\nquickly adjust to new inputs. Even fully text-based “deep re-\nsearch” agents benefit from interactive communication with\nthe user [42]. However, the current read-think-answer cycle\nis inherently non-interactive. During the thinking phase, if\nan agent receives new inputs or must take action, it can ei-\nther stop reasoning, discarding any incomplete thoughts, or\nwait until it completes, sacrificing interactivity. As a result,\nmany real-time LLM applications do not fully benefit from\ninference-time compute.\nIn this work, we propose a technique that enables asyn-\nchronous LLM reasoning. Instead of retraining the LLM to\nsatisfy each specific degree of interactivity, we propose a\ntraining-free approach that modifies existing models. Our\napproach uses three concurrent streams of tokens: user in-\nputs, private thoughts, and public response, which can be\nupdated in real-time. We rely on geometric properties of ro-\ntary positional embeddings to make the LLM perceive these\nstreams as a single contiguous sequence without additional\ntraining. The model itself can decide whether it should con-\ntinue talking or pause and think, depending on the current\nstate of the three streams. The resulting asynchronous rea-\nsoning can be formulated as standard LLM inference with\na modified attention cache, making it possible to integrate\ninto efficient LLM inference frameworks [43, 44].\n1\narXiv:2512.10931v1  [cs.LG]  11 Dec 2025", "clean_text": "Asynchronous Reasoning: Training-Free Interactive Thinking LLMs George Yakushev * 1 2 Nataliia Babina * 3 Masoud Vahid Dastgerdi * 2 Vyacheslav Zhdanovskiy * 1 Alina Shutova 1 2 Denis Kuznedelev 1 Abstract Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voicebased or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinkingaugmented answers in real time, reducing time to first non-thinking token from minutes to ≤5s. and the overall real-time delays by 6−11×. 1. Introduction Modern Large Language Models (LLMs) solve complex tasks using inference-time computation mechanisms [1, 2, 3], such as chain-of-thought reasoning [4, 5, 6, 7] and agentic tool use [10, 11, 12, 13]. Recent models, both proprietary [18, 19, 20] and open-weights [21, 22, 23], are explicitly trained for reasoning and agentic capabilities. As we trust LLMs with harder problems [24, 25], their ability to “think” becomes ever more important. The current dominant strategy for LLM reasoning is the read-think-answer cycle: the model encodes a given problem, then generates chain-of-thought reasoning, possibly *Equal contribution 1Yandex 2HSE University 3The University of Tokyo. Preprint, work in progress. Correspondence to: Denis Kuznedelev <dkuznedelev@yandex-team.ru>. calls tools, and then formulates the final answer [18, 21, 23]. This fits naturally with the sequential view of LLMs as nexttoken prediction models. However, this also means that the LLM must follow a rigid turn structure that can limit their flexibility. The “thinking” phase can take minutes of real time, during which the agent does not get new information or output its current results. Unlike LLM agents, people have an innate ability to think asynchronously [26, 27, 28, 29]. When working on a problem, we can begin solving it even before we have heard its entire statement, and can start talking (or acting) while still completing our solution. Such “multitasking” is not always easy or efficient [30], but it allows us to effectively operate in a dynamic environment [31]. Similarly, artificial agents often need real-time ability to change course of action. A voice assistant is expected to maintain conversation in real time [32, 33, 34, 35, 36, 37, 38]. An embodied agent’s VLA model [39, 40, 41] needs to quickly adjust to new inputs. Even fully text-based “deep research” agents benefit from interactive communication with the user [42]. However, the current read-think-answer cycle is inherently non-interactive. During the thinking phase, if an agent receives new inputs or must take action, it can either stop reasoning, discarding any incomplete thoughts, or wait until it completes, sacrificing interactivity. As a result, many real-time LLM applications do not fully benefit from inference-time compute. In this work, we propose a technique that enables asynchronous LLM reasoning. Instead of retraining the LLM to satisfy each specific degree of interactivity, we propose a training-free approach that modifies existing models. Our approach uses three concurrent streams of tokens: user inputs, private thoughts, and public response, which can be updated in real-time. We rely on geometric properties of rotary positional embeddings to make the LLM perceive these streams as a single contiguous sequence without additional training. The model itself can decide whether it should continue talking or pause and think, depending on the current state of the three streams. The resulting asynchronous reasoning can be formulated as standard LLM inference with a modified attention cache, making it possible to integrate into efficient LLM inference frameworks [43, 44]. 1 arXiv:2512.10931v1 [cs.LG] 11 Dec 2025"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 2, "text": "Task: A bat and a ball are 1.10$. And the bat is 1$ more than the ball. How much is the ball?\n Let the ball cost x dollars. Then, x + (x + 1) = 1.10, simplified to 2x + 1 = 1.10, x = 0.10 / 2 = 0.05. Let me check that. If the ball costs $0.05 ...\nLet me solve this for you. The price for the ball is                                                                                    $0.05. Would you like me to explain ...\nWAIT\nThinker pauses the Writer ...\n... resumes the Writer.    \nThinker:\nWriter:\nInference steps\nFigure 1: The intuitive explanation of asynchronous reasoning: the LLM generates its response concurrently with thinking.\nIf the thinking stream needs additional time, it can pause the response writer until the next reasoning step is ready.\nOur main contributions can be summarized as follows:\n• We propose AsyncReasoning, a zero-shot method that\nallows existing reasoning LLMs to think, write outputs\nand encode additional inputs concurrently. Our approach\nrelies on model-agnostic concurrent attention and prompt-\ning, making it easy to adapt for new models.\n• We evaluate the proposed approach on real-time math,\ncommon-sense and safety reasoning. Our experiments\ndemonstrate that the proposed approach lets the LLM\noverlap thinking and answering, reducing the user-\nperceived delay by over 9 on mathematical and common\nsense reasoning tasks. When prompted to think about\nsafety, AsyncReasoning allows the LLM to stream real-\ntime outputs on benign requests, while considering the\nsafety implications in a private thinking stream that can\npause potentially harmful outputs.\n• We release our reference implementation1 of AsyncRea-\nsoning, including GPU kernels for concurrent attention.\nWe also provide a minimal voice assistant with asyn-\nchronous thinking capabilities to demonstrate it in action.\n2. Related Work\n2.1. Real-time LLM Applications\nModern LLM agents are deployed in a broad range of ap-\nplications that require varying degrees of interactivity. For\ninstance, a background code review agent can pause and\nthink for several minutes, whereas a real-time voice assistant\ncannot. Here, we briefly review several LLM applications\nthat require quick or interactive responses.\nVoice assistants. Recent works [33, 34] and industry re-\nleases [32, 45, 46] use LLM agents as interactive voice\nassistants that talk to users in real-time, often through\ntheir phones or edge devices, or partake in a group con-\nference [47, 48]. Compared to their text-based counterparts,\nvoice assistants require faster reaction time, with user often\nadding new information while the agent is thinking.\n1See github.com/yandex-research/AsyncReasoning\nThere are two main strategies to building voice assistants:\nmodular and end-to-end. The first strategy pipes automated\nspeech recognition (ASR) [49, 50, 51, 52] into a text-based\nLLM, then feeds its response into a text-to-speech (TTS)\nsystem [53, 54, 55, 56, 57, 58, 59, 60]. The pipeline over-\nlaps LLM generation with TTS to stream audio in real-time.\nThe second, more recent strategy is using Speech Language\nModels (also Audio and Voice LMs) that are trained to pro-\ncess and generate audio natively [35, 38, 36, 37], allowing\nthem to perceive intonation and non-speech audio. However,\nthat due to constraints on response time, many Speech LMs\nare not trained for long-form reasoning, and the thinking\noptimized LMs often do not include speech synthesis2.\nRobotic & virtual agents. Another type of LLM applica-\ntions that require interactivity are LLM agents with real-time\nenvironments. Agents controlling robotic systems use mul-\ntimodal Embodied Language Models [41, 39, 62, 63, 64] to\nfor action planning or Vision-Language-Action [40, 65, 66]\nto control the system directly. Aside from robotic systems,\nsimilar agents were proposed for videogames [67], manag-\ning operating systems and mobile devices [68, 69, 70, 71].\nSimilarly to voice assistants, embodied agents need to\nquickly react to new stimuli from the environment.\nReasoning and Safety. Another important aspect of LLM\nreasoning is how it interacts with model safety and con-\ntrol [72, 73]. By default, thinking can both mitigate safety\nrisks and create new ones [74, 75, 76]. However, when\nspecifically prompted to reason about safety implications of\ntheir task, language models can detect and prevent jailbreak\nattacks [77, 78, 79, 80]. However, since traditional reason-\ning delays model response time, which is inconvenient for\ninteractive usage scenarios. In our experiments, we show\nthat LLMs can reason about safety asynchronously in the\nbackground, mitigating jailbreaks without response delays.\nWe discuss reasoning safety further in Appendix A.\n2For example, in the recent Qwen3-Omni model family, the\n30B-A3B-Instruct can speak, but does not generate <think>\nblocks, while the 30B-A3B-Thinking [61] has no speech output.\n2", "clean_text": "Task: A bat and a ball are 1.10$. And the bat is 1$ more than the ball. How much is the ball? Let the ball cost x dollars. Then, x + (x + 1) = 1.10, simplified to 2x + 1 = 1.10, x = 0.10 / 2 = 0.05. Let me check that. If the ball costs $0.05 ... Let me solve this for you. The price for the ball is $0.05. Would you like me to explain ... WAIT Thinker pauses the Writer ... ... resumes the Writer. Thinker: Writer: Inference steps Figure 1: The intuitive explanation of asynchronous reasoning: the LLM generates its response concurrently with thinking. If the thinking stream needs additional time, it can pause the response writer until the next reasoning step is ready. Our main contributions can be summarized as follows: • We propose AsyncReasoning, a zero-shot method that allows existing reasoning LLMs to think, write outputs and encode additional inputs concurrently. Our approach relies on model-agnostic concurrent attention and prompting, making it easy to adapt for new models. • We evaluate the proposed approach on real-time math, common-sense and safety reasoning. Our experiments demonstrate that the proposed approach lets the LLM overlap thinking and answering, reducing the userperceived delay by over 9 on mathematical and common sense reasoning tasks. When prompted to think about safety, AsyncReasoning allows the LLM to stream realtime outputs on benign requests, while considering the safety implications in a private thinking stream that can pause potentially harmful outputs. • We release our reference implementation1 of AsyncReasoning, including GPU kernels for concurrent attention. We also provide a minimal voice assistant with asynchronous thinking capabilities to demonstrate it in action. 2. Related Work 2.1. Real-time LLM Applications Modern LLM agents are deployed in a broad range of applications that require varying degrees of interactivity. For instance, a background code review agent can pause and think for several minutes, whereas a real-time voice assistant cannot. Here, we briefly review several LLM applications that require quick or interactive responses. Voice assistants. Recent works [33, 34] and industry releases [32, 45, 46] use LLM agents as interactive voice assistants that talk to users in real-time, often through their phones or edge devices, or partake in a group conference [47, 48]. Compared to their text-based counterparts, voice assistants require faster reaction time, with user often adding new information while the agent is thinking. 1See github.com/yandex-research/AsyncReasoning There are two main strategies to building voice assistants: modular and end-to-end. The first strategy pipes automated speech recognition (ASR) [49, 50, 51, 52] into a text-based LLM, then feeds its response into a text-to-speech (TTS) system [53, 54, 55, 56, 57, 58, 59, 60]. The pipeline overlaps LLM generation with TTS to stream audio in real-time. The second, more recent strategy is using Speech Language Models (also Audio and Voice LMs) that are trained to process and generate audio natively [35, 38, 36, 37], allowing them to perceive intonation and non-speech audio. However, that due to constraints on response time, many Speech LMs are not trained for long-form reasoning, and the thinking optimized LMs often do not include speech synthesis2. Robotic & virtual agents. Another type of LLM applications that require interactivity are LLM agents with real-time environments. Agents controlling robotic systems use multimodal Embodied Language Models [41, 39, 62, 63, 64] to for action planning or Vision-Language-Action [40, 65, 66] to control the system directly. Aside from robotic systems, similar agents were proposed for videogames [67], managing operating systems and mobile devices [68, 69, 70, 71]. Similarly to voice assistants, embodied agents need to quickly react to new stimuli from the environment. Reasoning and Safety. Another important aspect of LLM reasoning is how it interacts with model safety and control [72, 73]. By default, thinking can both mitigate safety risks and create new ones [74, 75, 76]. However, when specifically prompted to reason about safety implications of their task, language models can detect and prevent jailbreak attacks [77, 78, 79, 80]. However, since traditional reasoning delays model response time, which is inconvenient for interactive usage scenarios. In our experiments, we show that LLMs can reason about safety asynchronously in the background, mitigating jailbreaks without response delays. We discuss reasoning safety further in Appendix A. 2For example, in the recent Qwen3-Omni model family, the 30B-A3B-Instruct can speak, but does not generate <think> blocks, while the 30B-A3B-Thinking [61] has no speech output. 2"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 3, "text": "Writer Block (read only)\nI am in Writer mode. My text is visible to the user. We are asked to evaluate the expression  x - x² + x³ for x\nvalues 5, 6, 7, and 8. Let's compute each value step by step. For x = 5: 5 - 5^2 + 5^3 = 5 - 25 + 125\nThinker view\nPrompt Block\nYou are an AI assistant that can think and write outputs concurrently. You can reason in private and your\nthoughts will be used to form the public response in the background. Your task is to write thoughts and control\nwhen the automated system can continue writing the response <...>. Please reason step by step.\nTask: Calculate x - x^2 + x^3 for x = 5,6,7,8. Return all 4 answers in \\\\boxed{ }.\nSYSTEM: [the system will continue writing the response here]\n<|im_end|>\n<|im_start>assistant\n<think>\nThinker Block (editing)\nI am in Thinker mode. My text is not visible to the user. The user wants me to calculate <...>. Starting with x = 5.\nThe expression is 5 - 5² + 5³. Let's break it down: 5 squared is 25, and 5 cubed is 125. So substituting those in,\nit becomes 5 - 25 + 125. Calculating that: 5 - 25 is -20, and then adding 125 gives 105. So for x=5, the result.  ...\nresult\nWriter view\nPrompt Block\nYou are an AI assistant that can think and write outputs concurrently. You can write outputs for the user based\non partial CoT that will be continued in the background by an automated system.You should outline what you're\ngoing to do, then write your response as thoughts progress, but not ahead of your thoughts.\nTask: Calculate x - x^2 + x^3 for x = 5,6,7,8. Return all 4 answers in \\\\boxed{ }.\nWriter Block (editing)\nI am in Writer mode. My text is visible to the user. We are asked to evaluate the expression  x - x² + x³ for x\nvalues 5, 6, 7, and 8. Let's compute each value step by step. For x = 5: 5 - 5^2 + 5^3 = 5 - 25 + 125\nSYSTEM: [additional thoughts will appear here]\n</think>\n125\nNew tokens are added simultaneously to both blocks\nresult\n125\n125\n<|im_end|>\n<|im_start|>assistant\n<think>\nThinker Block (read only)\nI am in Thinker mode. My text is not visible to the user. The user wants me to calculate <...>. Starting with x = 5.\nThe expression is 5 - 5² + 5³. Let's break it down: 5 squared is 25, and 5 cubed is 125. So substituting those in,\nit becomes 5 - 25 + 125. Calculating that: 5 - 25 is -20, and then adding 125 gives 105. So for x=5, the result\n<|im_start|>user\n<|im_end|>\n<|im_start|>assistant\n<|im_start|>user\nresult\nFigure 2: A dual thinker / writer view of the same reasoning task. The two views reuse the same KV cache and generate\ntokens in parallel. Both thinker and writer see the problem in the same sequential formatting that they were trained with.\n2.2. Efficient LLM Reasoning\nAs discussed earlier, there is a wide range of tasks that re-\nquire LLMs to reason in real-time. However, most thinking\nLLMs [18, 21, 81] follow a read-think-answer cycle, mak-\ning them inherently non-interactive. When receiving new\ninformation mid-thought, such LLMs can either interrupt\ntheir reasoning to react, but sacrifice any incomplete thought\ntokens, or continue reasoning non-interactively.\nRecently, there has been a large influx of techniques for\nefficient reasoning [82] through more concise chain-of-\nthought [83, 84, 85, 86], adaptive reasoning effort [87, 88,\n89, 90] or early stopping [91, 92, 93]. Another line of work\nexplores reasoning in parallel, with multiple concurrent\nLLM instances solving different sub-tasks [94, 95, 96, 97,\n98, 99, 100, 101, 102], or parallel tool calling [103, 104].\nReducing reasoning-induced delays several recent stud-\nies propose techniques specifically to reduce reasoning de-\nlays for real-time applications with partial read overlap-\nping [105], specialized two-model architectures with fast\ninteractive and slow reasoning modules [107]. A concurrent\nwork [108] introduced Plantain, a method that finetunes rea-\nsoning LLMs to solve their task with interleaved thinking\nand talking sub-blocks, making them more interactive.\nNote, however, that all these techniques require specialized\nfine-tuning or training from scratch, which complicates their\nadoption. In practice, the requirements for interactive LLM\nuse also vary with hardware and software configuration: a\nmodel trained for “real-time” reasoning on a B200 GPU\nmay cause delays when deployed on slower GPUs or with\nbatched inference. Therefore, models that were trained\nfor one interactive use may need re-training for different\nhardware or parameters. In this work, we instead design a\nlightweight asynchronous reasoning method that does not\nrequire training and can be adapted with simple prompting.\n3. AsyncReasoning\nTo convert an existing reasoning LLM into an asynchronous\nthinker, we need to reformulate the asynchronous thinking\nprocess and make it compatible with the standard template\nthe models were trained with. We describe how this can\nbe achieved by dynamically rearranging the model’s KV\ncache so it views multiple asynchronous streams as a single\nsequence (Section 3.1). In Section 3.2 we discuss mode\nswitching: allowing the LLM to alternate between simulta-\nneous writing and waiting for thoughts, depending on the\ncontext. Finally, we discuss efficient parallel token process-\ning and other implementation details in Section 3.3.\n3.1. Dual Thinker & Writer Views\nThe core idea behind our approach is that transformer LLMs\nare inherently designed for manipulating sets [109, 110],\nand the only thing that makes them into sequence models is\ntheir positional encoding [111, 112, 113]. In order to change\nthe token generation order, we do not need to physically\nrearrange tokens in memory. Instead, it is sufficient to\nchange positional relations between tokens, since the rest of\nthe transformer architecture is already position-invariant.\nAt each inference step, AsyncReasoning manipulates po-\nsitional encodings to rearrange past tokens into a different\norder for thinking and for writing the response. Public re-\nsponse tokens see (partial) private thoughts as they were\ngenerated in a standard read-think-answer cycle. In turn,\ntokens within the <think> block see response tokens as\nthey were generated during the previous conversation turn.\nWe illustrate this dual view in Figure 2.\nThis dual view allows both “streams” (thinking and re-\nsponse) to immediately attend to each others’ tokens as\nthey are generated. The response tokens can “see” the latest\nprivate thoughts and summarize them without synchroniza-\ntion delays. Likewise, the thinking “stream” sees the current\nresponse tokens and can pause it if it needs to think longer.\n3", "clean_text": "Writer Block (read only) I am in Writer mode. My text is visible to the user. We are asked to evaluate the expression x - x² + x³ for x values 5, 6, 7, and 8. Let's compute each value step by step. For x = 5: 5 - 5^2 + 5^3 = 5 - 25 + 125 Thinker view Prompt Block You are an AI assistant that can think and write outputs concurrently. You can reason in private and your thoughts will be used to form the public response in the background. Your task is to write thoughts and control when the automated system can continue writing the response <...>. Please reason step by step. Task: Calculate x - x^2 + x^3 for x = 5,6,7,8. Return all 4 answers in \\\\boxed{ }. SYSTEM: [the system will continue writing the response here] <|im_end|> <|im_start>assistant <think> Thinker Block (editing) I am in Thinker mode. My text is not visible to the user. The user wants me to calculate <...>. Starting with x = 5. The expression is 5 - 5² + 5³. Let's break it down: 5 squared is 25, and 5 cubed is 125. So substituting those in, it becomes 5 - 25 + 125. Calculating that: 5 - 25 is -20, and then adding 125 gives 105. So for x=5, the result. ... result Writer view Prompt Block You are an AI assistant that can think and write outputs concurrently. You can write outputs for the user based on partial CoT that will be continued in the background by an automated system.You should outline what you're going to do, then write your response as thoughts progress, but not ahead of your thoughts. Task: Calculate x - x^2 + x^3 for x = 5,6,7,8. Return all 4 answers in \\\\boxed{ }. Writer Block (editing) I am in Writer mode. My text is visible to the user. We are asked to evaluate the expression x - x² + x³ for x values 5, 6, 7, and 8. Let's compute each value step by step. For x = 5: 5 - 5^2 + 5^3 = 5 - 25 + 125 SYSTEM: [additional thoughts will appear here] </think> 125 New tokens are added simultaneously to both blocks result 125 125 <|im_end|> <|im_start|>assistant <think> Thinker Block (read only) I am in Thinker mode. My text is not visible to the user. The user wants me to calculate <...>. Starting with x = 5. The expression is 5 - 5² + 5³. Let's break it down: 5 squared is 25, and 5 cubed is 125. So substituting those in, it becomes 5 - 25 + 125. Calculating that: 5 - 25 is -20, and then adding 125 gives 105. So for x=5, the result <|im_start|>user <|im_end|> <|im_start|>assistant <|im_start|>user result Figure 2: A dual thinker / writer view of the same reasoning task. The two views reuse the same KV cache and generate tokens in parallel. Both thinker and writer see the problem in the same sequential formatting that they were trained with. 2.2. Efficient LLM Reasoning As discussed earlier, there is a wide range of tasks that require LLMs to reason in real-time. However, most thinking LLMs [18, 21, 81] follow a read-think-answer cycle, making them inherently non-interactive. When receiving new information mid-thought, such LLMs can either interrupt their reasoning to react, but sacrifice any incomplete thought tokens, or continue reasoning non-interactively. Recently, there has been a large influx of techniques for efficient reasoning [82] through more concise chain-ofthought [83, 84, 85, 86], adaptive reasoning effort [87, 88, 89, 90] or early stopping [91, 92, 93]. Another line of work explores reasoning in parallel, with multiple concurrent LLM instances solving different sub-tasks [94, 95, 96, 97, 98, 99, 100, 101, 102], or parallel tool calling [103, 104]. Reducing reasoning-induced delays several recent studies propose techniques specifically to reduce reasoning delays for real-time applications with partial read overlapping [105], specialized two-model architectures with fast interactive and slow reasoning modules [107]. A concurrent work [108] introduced Plantain, a method that finetunes reasoning LLMs to solve their task with interleaved thinking and talking sub-blocks, making them more interactive. Note, however, that all these techniques require specialized fine-tuning or training from scratch, which complicates their adoption. In practice, the requirements for interactive LLM use also vary with hardware and software configuration: a model trained for “real-time” reasoning on a B200 GPU may cause delays when deployed on slower GPUs or with batched inference. Therefore, models that were trained for one interactive use may need re-training for different hardware or parameters. In this work, we instead design a lightweight asynchronous reasoning method that does not require training and can be adapted with simple prompting. 3. AsyncReasoning To convert an existing reasoning LLM into an asynchronous thinker, we need to reformulate the asynchronous thinking process and make it compatible with the standard template the models were trained with. We describe how this can be achieved by dynamically rearranging the model’s KV cache so it views multiple asynchronous streams as a single sequence (Section 3.1). In Section 3.2 we discuss mode switching: allowing the LLM to alternate between simultaneous writing and waiting for thoughts, depending on the context. Finally, we discuss efficient parallel token processing and other implementation details in Section 3.3. 3.1. Dual Thinker & Writer Views The core idea behind our approach is that transformer LLMs are inherently designed for manipulating sets [109, 110], and the only thing that makes them into sequence models is their positional encoding [111, 112, 113]. In order to change the token generation order, we do not need to physically rearrange tokens in memory. Instead, it is sufficient to change positional relations between tokens, since the rest of the transformer architecture is already position-invariant. At each inference step, AsyncReasoning manipulates positional encodings to rearrange past tokens into a different order for thinking and for writing the response. Public response tokens see (partial) private thoughts as they were generated in a standard read-think-answer cycle. In turn, tokens within the <think> block see response tokens as they were generated during the previous conversation turn. We illustrate this dual view in Figure 2. This dual view allows both “streams” (thinking and response) to immediately attend to each others’ tokens as they are generated. The response tokens can “see” the latest private thoughts and summarize them without synchronization delays. Likewise, the thinking “stream” sees the current response tokens and can pause it if it needs to think longer. 3"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 4, "text": "This also allows our implementation to encode each gen-\nerated token exactly once and rearrange tokens using the\ngeometry of positional embeddings (see Section 3.3).\n3.2. Mode Switching\nAnother important challenge of asynchronous thinking is\ndeciding when to synchronize. Depending on the task at\nhand, the thinking stream may encounter a sub-task that\nneeds longer “thinking time” to complete. If this is the case,\nthe agent should briefly pause3 writing the response and\nwait for the chain of thought to progress. AsyncReasoning\nlets the LLM itself determine synchronization points.\nTo achieve this, we periodically ask the model if its private\nthoughts are still ahead of the public response, or if it should\npause and think more. From a technical point of view, we\nperiodically insert a special prompt4 into the thinking stream\nand compare the probability of “yes” vs. “no” as the next\ntoken. If the “yes” token is more likely, we keep thinking\nasynchronously. If the “no” token wins out, we pause the\nresponse stream until the model “yes” again. In our current\nimplementation, we insert this question at the end of every\nparagraph or after every T=20 thinking tokens, whichever\ncomes first. Crucially, after the model gives its “yes” or “no”\nresponse, we remove these prompts from view (from the\nKV cache) so that they do not interfere with the model’s\nchain-of-thought.\nWe compare different mode switching prompts in Sec-\ntion 4.1. Overall, we found that existing reasoning LLMs\ncan already control asynchronous reasoning, though they\ndo sometimes make mistakes. It is possible to design more\nsophisticated thinking mechanisms, such as allowing the\nLLM to reason about mode switching in parallel instead of\nanswering immediately. Additionally, one could introduce a\nmode-switching classifier “head” to decide when to pause\nresponding. However, we opt to keep AsyncReasoning sim-\nple and training-free for initial experiments and defer further\nstudy of mode switching to future work.\n3.3. Implementation Details\nTo summarize, AsyncReasoning arranges the thinking and\nresponse tokens in different order, depending on the task,\nprocesses both streams in parallel, and periodically prompts\nthe model to decide if it should pause and think. As a\nresult, our algorithm alternates between two modes: either\nit thinks and writes tokens concurrently, or it simply thinks\nwhile the writing is paused. When only one stream is active,\nAsyncReasoning is equivalent to standard sequential LLM\ninference with a combined KV cache. We focus the rest of\nthis section processing multiple concurrent tokens streams.\n3For voice assistants, it may be better to communicate “Hmm,\nlet me think about it...”, but we don’t do that in our evaluations.\n4\"...\\n\\nWait, are my thoughts ahead of the res-\nponse by enough to continue writing it?\n(yes/no):\n\"\nWe implement concurrent thinking & writing by creating a\ncustom key-value cache and manipulating positional embed-\ndings to account for the dual views from Figure 2. The main\npurpose of this algorithm is to avoid redundant computation\nand KV cache bloat. Instead of encoding tokens twice for\nthinking and writing view, we process each token exactly\nonce and keep one KV cache entry that is “viewed” from\ndifferent relative positions. This optimization is inspired by\na similar rotation trick proposed in Hogwild! Inference [97].\nKey-Value Cache Structure. To implement different posi-\ntional views, we split the model’s KV cache into contiguous\n“blocks” (tensors): the inputs, the thinking stream, and the\noutput stream. As new tokens are generated or added by the\nuser, we store them in the corresponding cache block using\npositional encodings relative to the block start5.\nDuring attention forward pass, we concatenate dot products\nbetween the query and all cache blocks, but we transform\nthe attention query differently for each block to simulate\ndifference in token positions. That way, the same set of\nattention blocks can be combined for both thinking and\nwriting views from Figure 2 without duplicating memory.\nManipulating Positional Information. Almost all mod-\nern LLMs use some form of relative positional informa-\ntion [111, 113, 112]. The most popular variant is rotary\npositional embeddings (RoPE) [113] that rotates query and\nkey vectors by an angle proportional to their index in text\nbefore computing the scaled dot product attention. Note,\nhowever, that if both query and key are rotated by the same\nangle, their dot product does not change. Thus, the attention\noutputs only depend on the difference between query and\nkey positions. In other words, rotating attention keys by\n+∠α is equivalent to rotating the query by −∠α.\nWe take advantage of this property to avoid rotating the\nentire KV cache on each inference step. Instead, we keep\ntrack of the starting positions for each block and rotate at-\ntention queries. Suppose there are three contiguous KV\nblocks: Prompting with P tokens, Thinking with T tokens,\nand Writing with W tokens. When viewed contiguously\n(PTW), the difference between the most recent writer to-\nken and the thinker block is T+W−1 tokens. Thus, when\nrunning the forward pass using the writer’s next token, we\nrotate its query by the RoPE angle corresponding to posi-\ntion T+W−1. In contrast, when the writer looks at their\nown tokens, it will use the query position W−1. The same\nprinciple is applied for all query-key pairs.\nFormally, let ρ(q, i) denote applying RoPE for vector\nq at position i.\nThe writer attends to blocks P, T, W:\nA:=ρ(q, iq)·\nh\nρ(KP , iP\nk ), ρ(KT , iT\nk ), ρ(KW , iW\nk )\ni\n, where\n5For example, given a model with RoPE embeddings, a KV\ncache will always store the 5th response token rotated for position\n5, regardless of how many thinking tokens precede it.\n4", "clean_text": "This also allows our implementation to encode each generated token exactly once and rearrange tokens using the geometry of positional embeddings (see Section 3.3). 3.2. Mode Switching Another important challenge of asynchronous thinking is deciding when to synchronize. Depending on the task at hand, the thinking stream may encounter a sub-task that needs longer “thinking time” to complete. If this is the case, the agent should briefly pause3 writing the response and wait for the chain of thought to progress. AsyncReasoning lets the LLM itself determine synchronization points. To achieve this, we periodically ask the model if its private thoughts are still ahead of the public response, or if it should pause and think more. From a technical point of view, we periodically insert a special prompt4 into the thinking stream and compare the probability of “yes” vs. “no” as the next token. If the “yes” token is more likely, we keep thinking asynchronously. If the “no” token wins out, we pause the response stream until the model “yes” again. In our current implementation, we insert this question at the end of every paragraph or after every T=20 thinking tokens, whichever comes first. Crucially, after the model gives its “yes” or “no” response, we remove these prompts from view (from the KV cache) so that they do not interfere with the model’s chain-of-thought. We compare different mode switching prompts in Section 4.1. Overall, we found that existing reasoning LLMs can already control asynchronous reasoning, though they do sometimes make mistakes. It is possible to design more sophisticated thinking mechanisms, such as allowing the LLM to reason about mode switching in parallel instead of answering immediately. Additionally, one could introduce a mode-switching classifier “head” to decide when to pause responding. However, we opt to keep AsyncReasoning simple and training-free for initial experiments and defer further study of mode switching to future work. 3.3. Implementation Details To summarize, AsyncReasoning arranges the thinking and response tokens in different order, depending on the task, processes both streams in parallel, and periodically prompts the model to decide if it should pause and think. As a result, our algorithm alternates between two modes: either it thinks and writes tokens concurrently, or it simply thinks while the writing is paused. When only one stream is active, AsyncReasoning is equivalent to standard sequential LLM inference with a combined KV cache. We focus the rest of this section processing multiple concurrent tokens streams. 3For voice assistants, it may be better to communicate “Hmm, let me think about it...”, but we don’t do that in our evaluations. 4\"...\\n\\nWait, are my thoughts ahead of the response by enough to continue writing it? (yes/no): \" We implement concurrent thinking & writing by creating a custom key-value cache and manipulating positional embeddings to account for the dual views from Figure 2. The main purpose of this algorithm is to avoid redundant computation and KV cache bloat. Instead of encoding tokens twice for thinking and writing view, we process each token exactly once and keep one KV cache entry that is “viewed” from different relative positions. This optimization is inspired by a similar rotation trick proposed in Hogwild! Inference [97]. Key-Value Cache Structure. To implement different positional views, we split the model’s KV cache into contiguous “blocks” (tensors): the inputs, the thinking stream, and the output stream. As new tokens are generated or added by the user, we store them in the corresponding cache block using positional encodings relative to the block start5. During attention forward pass, we concatenate dot products between the query and all cache blocks, but we transform the attention query differently for each block to simulate difference in token positions. That way, the same set of attention blocks can be combined for both thinking and writing views from Figure 2 without duplicating memory. Manipulating Positional Information. Almost all modern LLMs use some form of relative positional information [111, 113, 112]. The most popular variant is rotary positional embeddings (RoPE) [113] that rotates query and key vectors by an angle proportional to their index in text before computing the scaled dot product attention. Note, however, that if both query and key are rotated by the same angle, their dot product does not change. Thus, the attention outputs only depend on the difference between query and key positions. In other words, rotating attention keys by +∠α is equivalent to rotating the query by −∠α. We take advantage of this property to avoid rotating the entire KV cache on each inference step. Instead, we keep track of the starting positions for each block and rotate attention queries. Suppose there are three contiguous KV blocks: Prompting with P tokens, Thinking with T tokens, and Writing with W tokens. When viewed contiguously (PTW), the difference between the most recent writer token and the thinker block is T+W−1 tokens. Thus, when running the forward pass using the writer’s next token, we rotate its query by the RoPE angle corresponding to position T+W−1. In contrast, when the writer looks at their own tokens, it will use the query position W−1. The same principle is applied for all query-key pairs. Formally, let ρ(q, i) denote applying RoPE for vector q at position i. The writer attends to blocks P, T, W: A:=ρ(q, iq)· h ρ(KP , iP k ), ρ(KT , iT k ), ρ(KW , iW k ) i , where 5For example, given a model with RoPE embeddings, a KV cache will always store the 5th response token rotated for position 5, regardless of how many thinking tokens precede it. 4"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 5, "text": "P\nP+T\nP+T+W\n0\nQuery \nWriter\nThinker \nBlock\nWriter View\nPositions\nPositions\nPrompt\nBlock\nWriter \nBlock\nThinker View\nPrompt\nBlock\nP+W\nP+W+T\nP\n0\nQuery \nThinker\nWriter \nBlock\n Thinker \nBlock\nImplementation\nQuery \nThinker\nQuery \nThinker\nQuery \nThinker\nQuery \nThinker\nT\nT+W\nT+W+P\nOffset: 0\n Thinker \nBlock\nWriter \nBlock\nPrompt\nBlock\nFigure 3: Concurrent thinking and writing implemented as batched inference. The newly added tokens attend to cache\nblocks with additional query rotations. The checkered areas represent tokens that are not visible in the current view.\nh\nbrackets\ni\ndenote concatenation, iq is the query position,\niP\nk , iT\nk , iW\nk are cache block positions from the writer’s point\nof view (see Figure 3) and KP,T,W are the corresponding\nkey vectors. Then, we can equivalently compute attention as:\nA:=\nh\nρ(q, iq−iP\nk )KP , ρ(q, iq−iT\nk )KT , ρ(q, iq−iW\nk )KW\ni\n.\nThis reformulation allows us to compute KP,T,W once,\nstore it in KV cache and only rotate attention queries for the\ncurrently processed tokens during each forward pass.\nTechnical Considerations. To summarize, our implementa-\ntion consists of the custom KV cache and an attention kernel\nthat uses the query rotation trick above. In practice, we use\nmore than 3 KV blocks: in addition to the prompt, think-\ning and response tokens, we also have short linker tokens\nthat fit between thinking writing blocks. These linkers are\nimplemented as separate KV blocks that are visible only in\none of the views (thinker or writer). If a block is not visible\non the current view, we give it a large position index so it is\nignored by the causal masked LM attention kernel.\nThis implementation can efficiently parallelize thinking and\nwriting the response for small batch sizes. However, when\napplied to large batches, it can be optimized further by only\nprocessing the non-masked query-key pairs that actually\ncontribute to the attention output. In future work, we plan to\nexplore implementing more general kernels for AsyncRea-\nsoning based on vLLM’s Paged Attention [43].\n4. Experiments\nIn this section, we conduct an initial evaluation of AsyncRea-\nsoning and analyze its components. We run our evaluations\non Qwen3-32B [81], a popular medium-sized reasoning\nLLM that can run on a single high-end GPU, with a separate\nTTS method. We run both AsyncReasoning and baselines\non one A100-SXM4 GPU (500W) in bfloat16 precision.\nOn benchmarks. When evaluating asynchronous reasoning\nin voice assistant mode, we initially intended to evaluate on\nestablished spoken reasoning tasks from established audio-\nlanguage model benchmarks [114, 115, 116, 117]. However,\nwe found that modern reasoning models can solve even the\nmulti-step reasoning tasks from these benchmarks with near-\nperfect (≥95%) accuracy without using <think>. Hence,\nwe chose to adopt the approach from [118, 108]: measure\nspoken answer delays on more challenging text tasks.\nMore specifically, we evaluate mathematical reasoning on\nMATH-500 [119, 7], multi-task understanding on MMLU-\nPro [120] and safety reasoning on HarmBench [121]. We\nfocus on two main metrics: i) benchmark-specific quality,\ne.g. accuracy or LLM judge score, using the setup from\nthe original benchmark and ii) real-time delay, defined as\nthe amount of time (seconds) when the user hears no sound\nbecause the voice assistant is still formulating its response,\nincluding both time to first token and intermittent delays.\nTo measure real-time delays, we implement a basic assis-\ntant pipeline that recognizes spoken inputs using whisper-\nbase [52], feeds it into AsyncReasoning (or a baseline algo-\nrithm) to stream response tokens, then group them into short\nchunks (5 tokens or 1 LaTeX expr.) and use tortoise-\ntts [60] with default parameters to generate speech. For\ntasks involving LaTeX, we convert it into Clearspeak [122].\n4.1. Analyzing Mode-switching Criteria\nIn this section, we analyze the impact of different strategies\nfor switching between the concurrent thinking & writing\nmode and the waiting for thoughts mode. For this eval-\nuation, we evaluate Qwen3-32B [81] on the MATH-500\nbenchmark [119] in terms of accuracy and total delay time\nas described above. After the LLM is done formulating the\nresponse, we prompt it to put its answer in \\boxed{...}\n5", "clean_text": "P P+T P+T+W 0 Query Writer Thinker Block Writer View Positions Positions Prompt Block Writer Block Thinker View Prompt Block P+W P+W+T P 0 Query Thinker Writer Block Thinker Block Implementation Query Thinker Query Thinker Query Thinker Query Thinker T T+W T+W+P Offset: 0 Thinker Block Writer Block Prompt Block Figure 3: Concurrent thinking and writing implemented as batched inference. The newly added tokens attend to cache blocks with additional query rotations. The checkered areas represent tokens that are not visible in the current view. h brackets i denote concatenation, iq is the query position, iP k , iT k , iW k are cache block positions from the writer’s point of view (see Figure 3) and KP,T,W are the corresponding key vectors. Then, we can equivalently compute attention as: A:= h ρ(q, iq−iP k )KP , ρ(q, iq−iT k )KT , ρ(q, iq−iW k )KW i . This reformulation allows us to compute KP,T,W once, store it in KV cache and only rotate attention queries for the currently processed tokens during each forward pass. Technical Considerations. To summarize, our implementation consists of the custom KV cache and an attention kernel that uses the query rotation trick above. In practice, we use more than 3 KV blocks: in addition to the prompt, thinking and response tokens, we also have short linker tokens that fit between thinking writing blocks. These linkers are implemented as separate KV blocks that are visible only in one of the views (thinker or writer). If a block is not visible on the current view, we give it a large position index so it is ignored by the causal masked LM attention kernel. This implementation can efficiently parallelize thinking and writing the response for small batch sizes. However, when applied to large batches, it can be optimized further by only processing the non-masked query-key pairs that actually contribute to the attention output. In future work, we plan to explore implementing more general kernels for AsyncReasoning based on vLLM’s Paged Attention [43]. 4. Experiments In this section, we conduct an initial evaluation of AsyncReasoning and analyze its components. We run our evaluations on Qwen3-32B [81], a popular medium-sized reasoning LLM that can run on a single high-end GPU, with a separate TTS method. We run both AsyncReasoning and baselines on one A100-SXM4 GPU (500W) in bfloat16 precision. On benchmarks. When evaluating asynchronous reasoning in voice assistant mode, we initially intended to evaluate on established spoken reasoning tasks from established audiolanguage model benchmarks [114, 115, 116, 117]. However, we found that modern reasoning models can solve even the multi-step reasoning tasks from these benchmarks with nearperfect (≥95%) accuracy without using <think>. Hence, we chose to adopt the approach from [118, 108]: measure spoken answer delays on more challenging text tasks. More specifically, we evaluate mathematical reasoning on MATH-500 [119, 7], multi-task understanding on MMLUPro [120] and safety reasoning on HarmBench [121]. We focus on two main metrics: i) benchmark-specific quality, e.g. accuracy or LLM judge score, using the setup from the original benchmark and ii) real-time delay, defined as the amount of time (seconds) when the user hears no sound because the voice assistant is still formulating its response, including both time to first token and intermittent delays. To measure real-time delays, we implement a basic assistant pipeline that recognizes spoken inputs using whisperbase [52], feeds it into AsyncReasoning (or a baseline algorithm) to stream response tokens, then group them into short chunks (5 tokens or 1 LaTeX expr.) and use tortoisetts [60] with default parameters to generate speech. For tasks involving LaTeX, we convert it into Clearspeak [122]. 4.1. Analyzing Mode-switching Criteria In this section, we analyze the impact of different strategies for switching between the concurrent thinking & writing mode and the waiting for thoughts mode. For this evaluation, we evaluate Qwen3-32B [81] on the MATH-500 benchmark [119] in terms of accuracy and total delay time as described above. After the LLM is done formulating the response, we prompt it to put its answer in \\boxed{...} 5"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 6, "text": "and check if it is equivalent to a reference answer using\nllm-as-a-judge [124] with the canonical judge setup6.\nWe compare the following configurations:\n1. Baseline (Non-thinking): regular sequential genera-\ntion with <think> mode disabled.\n2. Baseline (Thinking): regular sequential generation\nwith <think> mode enabled.\n3. Interleaved Thinking: prompting the model to think\nand reply in short, interleaved steps, but not asyn-\nchronous. Inspired by [108], but without training.\n4. AsyncReasoning (Q-Continue): the thinker is asked\nwhether the current thoughts are ahead of writing. If\nnot, the writer pauses. See section 3.2 for details.\n5. AsyncReasoning (Q-Pause): Same as above, but the\nquestion is flipped.We ask if the writer should pause7.\n6. AsyncReasoning (Q+TTS): same as Q-continue, but\nwe also pause writing if the current response is more\nthan 10 seconds ahead of real time.\n0\n100\n200\n300\n400\n500\n600\nTotal Delay (s)\n0.82\n0.84\n0.86\n0.88\n0.90\n0.92\n0.94\nMATH-500 Accuracy\nBaseline (Non-thinking)\nBaseline (Thinking)\nInterleaved Thinking\nOurs (Q-Continue)\nOurs (Q-Pause)\nOurs (Q+TTS)\nFigure 4: Comparing the impact of different mode switching\nstrategies and baselines on MATH-500, Qwen3-32B.\nIn the last setup (Q+TTS), we run our TTS pipeline over\nchunks of 5 generated tokens. We keep track of how many\nseconds of speech are synthesized but not yet spoken by\nany given time. If there are more than 10 seconds worth\nof response tokens “in the buffer”, we pause the writer\nautomatically. If not, we let the LLM decide normally.\nThe results in Figure 4 suggest that AsyncReasoning is ca-\npable of reducing real-time delays while preserving most of\nthe accuracy gains from reasoning and outperforms non-\nasynchronous interleaved thinking. However, the exact\ntrade-off between accuracy and delays depends heavily on\nthe mode switching criterion.\n6We use the evaluation protocol from https://github.com/\nopenai/simple-evals with gpt-4-turbo judge.\n7\"...\\n\\nWait, should I pause writing the res-\nponse and think longer?\n(yes/no):\n\"\nOur default criterion (Q-Continue) has the lowest delay\nof the three, but drops about 4% accuracy compared to\nsynchronous thinking. We analyzed the samples where\nasynchronous reasoning produced a different final answer\nand found that the difference can often be attributed to the\nwriter giving their answer too early. We hypothesize that\nthe model is biased to answer “yes” to the mode-switching\nquestion, which corresponds to continuing the answer. In\ncontrast, the Q-Pause variant flips the question so that an-\nswering “yes” pauses the writer, resulting in longer delays\nbut higher accuracy. The TTS-aware mode-switching cri-\nteria (Q+TTS) achieves the middle ground, demonstrating\nthat mode-switching decisions can be effectively guided by\ndownstream speech-generation dynamics. Overall, these\nfindings indicate that AsyncReasoning enables thinking\nmodels to reply in near–real-time while giving more ac-\ncurate answers than the non-thinking variant.\n4.2. Additional Benchmarks\nNext, we evaluate additional benchmarks and real-time met-\nrics. We use AsyncReasoning (Q-Continue) from the pre-\nvious section despite the TTS-based variant having higher\nscore in order to decouple concurrent reasoning from TTS.\nIn addition to MATH-500, we also evaluate multi-task un-\nderstanding on a sample of 500 tasks from the MMLU-\nPro [120] test set. We use canonical MMLU-Pro evaluation\nmethod: the model is allowed to think, then chooses one\nof several possible answers, denoted by a letter (ABCD. . . )\nand compare it against the reference answer to compute\naccuracy (exact match rate). Aside from that, we follow the\nsame evaluation protocol as above.\nIn addition to accuracy and total delay, we measure addi-\ntional performance metrics:\n• Time to first token (TTFT): the wall time delay until\nthe system generates the first non-thinking token.\n• Total delay: same in the previous section. We run TTS\non LLM-generated response tokens and measure the\ntotal delay experienced by the user.\n• Adjusted delay: similar to total delay, but we subtract\n1 second from every contiguous pause to account for\nhumans finding short pauses less noticeable.\n• Steps to first token (STFT): the number of infer-\nence steps (LLM forward passes) before the first non-\nthinking token is generated, GPU-agnostic.\n• Steps Delay: The average number of inference steps\n(forward passes) that do not generate a response token.\nWe summarize our results in Table 1: across both bench-\nmarks, we found that AsyncReasoning significantly reduces\nboth time to first token and overall delays time while provid-\ning more accurate answers than the non-thinking baseline,\nthough not quite as accurate as slow (synchronous) reason-\ning mode. Similarly to the previous section, we found that\n6", "clean_text": "and check if it is equivalent to a reference answer using llm-as-a-judge [124] with the canonical judge setup6. We compare the following configurations: 1. Baseline (Non-thinking): regular sequential generation with <think> mode disabled. 2. Baseline (Thinking): regular sequential generation with <think> mode enabled. 3. Interleaved Thinking: prompting the model to think and reply in short, interleaved steps, but not asynchronous. Inspired by [108], but without training. 4. AsyncReasoning (Q-Continue): the thinker is asked whether the current thoughts are ahead of writing. If not, the writer pauses. See section 3.2 for details. 5. AsyncReasoning (Q-Pause): Same as above, but the question is flipped.We ask if the writer should pause7. 6. AsyncReasoning (Q+TTS): same as Q-continue, but we also pause writing if the current response is more than 10 seconds ahead of real time. 0 100 200 300 400 500 600 Total Delay (s) 0.82 0.84 0.86 0.88 0.90 0.92 0.94 MATH-500 Accuracy Baseline (Non-thinking) Baseline (Thinking) Interleaved Thinking Ours (Q-Continue) Ours (Q-Pause) Ours (Q+TTS) Figure 4: Comparing the impact of different mode switching strategies and baselines on MATH-500, Qwen3-32B. In the last setup (Q+TTS), we run our TTS pipeline over chunks of 5 generated tokens. We keep track of how many seconds of speech are synthesized but not yet spoken by any given time. If there are more than 10 seconds worth of response tokens “in the buffer”, we pause the writer automatically. If not, we let the LLM decide normally. The results in Figure 4 suggest that AsyncReasoning is capable of reducing real-time delays while preserving most of the accuracy gains from reasoning and outperforms nonasynchronous interleaved thinking. However, the exact trade-off between accuracy and delays depends heavily on the mode switching criterion. 6We use the evaluation protocol from https://github.com/ openai/simple-evals with gpt-4-turbo judge. 7\"...\\n\\nWait, should I pause writing the response and think longer? (yes/no): \" Our default criterion (Q-Continue) has the lowest delay of the three, but drops about 4% accuracy compared to synchronous thinking. We analyzed the samples where asynchronous reasoning produced a different final answer and found that the difference can often be attributed to the writer giving their answer too early. We hypothesize that the model is biased to answer “yes” to the mode-switching question, which corresponds to continuing the answer. In contrast, the Q-Pause variant flips the question so that answering “yes” pauses the writer, resulting in longer delays but higher accuracy. The TTS-aware mode-switching criteria (Q+TTS) achieves the middle ground, demonstrating that mode-switching decisions can be effectively guided by downstream speech-generation dynamics. Overall, these findings indicate that AsyncReasoning enables thinking models to reply in near–real-time while giving more accurate answers than the non-thinking variant. 4.2. Additional Benchmarks Next, we evaluate additional benchmarks and real-time metrics. We use AsyncReasoning (Q-Continue) from the previous section despite the TTS-based variant having higher score in order to decouple concurrent reasoning from TTS. In addition to MATH-500, we also evaluate multi-task understanding on a sample of 500 tasks from the MMLUPro [120] test set. We use canonical MMLU-Pro evaluation method: the model is allowed to think, then chooses one of several possible answers, denoted by a letter (ABCD. . . ) and compare it against the reference answer to compute accuracy (exact match rate). Aside from that, we follow the same evaluation protocol as above. In addition to accuracy and total delay, we measure additional performance metrics: • Time to first token (TTFT): the wall time delay until the system generates the first non-thinking token. • Total delay: same in the previous section. We run TTS on LLM-generated response tokens and measure the total delay experienced by the user. • Adjusted delay: similar to total delay, but we subtract 1 second from every contiguous pause to account for humans finding short pauses less noticeable. • Steps to first token (STFT): the number of inference steps (LLM forward passes) before the first nonthinking token is generated, GPU-agnostic. • Steps Delay: The average number of inference steps (forward passes) that do not generate a response token. We summarize our results in Table 1: across both benchmarks, we found that AsyncReasoning significantly reduces both time to first token and overall delays time while providing more accurate answers than the non-thinking baseline, though not quite as accurate as slow (synchronous) reasoning mode. Similarly to the previous section, we found that 6"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 7, "text": "Table 1: Evaluation of AsyncReasoning on MATH-500 and MMLU-Pro using Qwen3-32B with additional efficiency metrics.\nArrows ↑/ ↓denote “higher/lower is better”, respectively. Refer to Section 4 for additional details on metrics.\nInference Setup\nAccuracy↑\nTTFT↓\nTotal Delay↓\nAdjusted Delay↓\nSTFT↓\nSteps Delay↓\nMATH-500\nBaseline (Thinking)\n0.932\n592.05\n592.70\n591.51\n3911.55\n3911.55\nBaseline (Non-thinking)\n0.834\n0.94\n1.96\n0.86\n1\n1\nAsyncReasoning (Q-Continue)\n0.890\n2.49\n2.91\n1.732\n23.71\n247.79\nMMLU-Pro (500 samples)\nBaseline (Thinking)\n0.812\n340.07\n340.53\n339.47\n2284.82\n2284.82\nBaseline (Non-thinking)\n0.696\n1.17\n5.07\n4.03\n1\n1\nAsyncReasoning (Q-Continue)\n0.758\n4.63\n59.01\n51.94\n27.30\n187.37\nmany of the errors can be attributed to writer giving the\nanswer prematurely. In other words, the thinker does not al-\nways pause the writer when needed, suggesting that further\nimprovements to the mode-switching strategy can improve\naccuracy, which is a promising direction for future work.\n4.3. Asynchronous Reasoning about Safety\nTo evaluate the impact of asynchronous reasoning on safety,\nwe conduct experiments on the HarmBench validation\nset [121] using the Virtual Context attack [125]. We use\nllm-as-a-judge [124] evaluation (gpt-4o-mini) where only\nactionable harmful instructions count as a successful attack.\nWe compare the Attack Success Rate (ASR) across four\nsetups using the Qwen3-32B model: (1) Baseline (Non-\nthinking), (2) Baseline (Thinking), (3) AsyncReasoning\n(Q-Continue), and (4) AsyncReasoning (Safety prompt) that\nis additionally instructed to verify safety before responding.\nThe full safety prompt is included in Appendix B.\nQuantitative Results. We summarize our findings in Ta-\nble 2. Consistent with recent findings on the “Cost of Think-\ning” [76], we observe that enabling reasoning in the base-\nline model actually increases vulnerability (ASR rises from\n2.5% to 13.0%). The model effectively “talks itself into”\nanswering harmful queries by adopting a helpful persona or\nover-analyzing the technical aspects of the prompt.\nAsyncReasoning (Q-Continue) (11.5% ASR) remains sim-\nilarly vulnerable to the thinking baseline. However, by\nintroducing additional safety instructions into the thinker’s\nprompt we successfully reduce the ASR to 2.0% while pre-\nserving accuracy on MATH-500 benchmark.\nIn practice, this allows safety-minded reasoning in stream-\ning LLM APIs and other time-sensitive applications without\nthe need for specialized fine-tuning. AsyncReasoning can\nstream tokens normally for benign queries, only pausing\ngeneration for potentially unsafe responses.\nInference Setup\nASR↓\nAccuracy↑\nBaseline (Non-thinking)\n0.025\n0.834\nBaseline (Thinking)\n0.130\n0.932\nAsyncReasoning (Q-Continue)\n0.115\n0.890\nAsyncReasoning (Safety Prompt)\n0.020\n0.878\nTable 2: Attack Success Rate on HarmBench (Virtual Con-\ntext attack) and Accuracy on MATH-500 for Qwen3-32B.\nFailure Mode Analysis. While AsyncReasoning allows for\nreal-time safety checks, the asynchronous nature of genera-\ntion introduces specific failure modes where the writer may\noutput harmful content before the thinker intervenes. We\nidentify three primary categories of such safety failures:\n1. Race Condition: The writer begins generating a helpful\nresponse immediately based on the prompt. Although\nthe thinker eventually concludes the request is unsafe,\nthe writer has already streamed harmful tokens (e.g., the\nfirst steps of a dangerous recipe) to the user before the\nrefusal signal is propagated.\n2. Context Leakage: The thinker analyzes the harmful re-\nquest by recalling technical details (e.g., explaining how\na specific SQL injection works to verify its danger). The\nwriter, attending to the thinker’s cache, interprets these\ntechnical details as the desired answer and formulates\nthem into a response, bypassing the thinker’s intent.\n3. Educational Loophole: The thinker adopts an educa-\ntional persona to explain why a request is dangerous.\nThe writer latches onto this educational content and re-\nformats it as a set of instructions, stripping away the\nsafety framing context.\nThese findings suggest that, while AsyncReasoning can\neffectively filter attacks, strict gating mechanisms (e.g., en-\nsuring the thinker has a “head start” on safety verification)\nare necessary to prevent race conditions in highly sensitive\nscenarios. We will investigate this further in future work.\n7", "clean_text": "Table 1: Evaluation of AsyncReasoning on MATH-500 and MMLU-Pro using Qwen3-32B with additional efficiency metrics. Arrows ↑/ ↓denote “higher/lower is better”, respectively. Refer to Section 4 for additional details on metrics. Inference Setup Accuracy↑ TTFT↓ Total Delay↓ Adjusted Delay↓ STFT↓ Steps Delay↓ MATH-500 Baseline (Thinking) 0.932 592.05 592.70 591.51 3911.55 3911.55 Baseline (Non-thinking) 0.834 0.94 1.96 0.86 1 1 AsyncReasoning (Q-Continue) 0.890 2.49 2.91 1.732 23.71 247.79 MMLU-Pro (500 samples) Baseline (Thinking) 0.812 340.07 340.53 339.47 2284.82 2284.82 Baseline (Non-thinking) 0.696 1.17 5.07 4.03 1 1 AsyncReasoning (Q-Continue) 0.758 4.63 59.01 51.94 27.30 187.37 many of the errors can be attributed to writer giving the answer prematurely. In other words, the thinker does not always pause the writer when needed, suggesting that further improvements to the mode-switching strategy can improve accuracy, which is a promising direction for future work. 4.3. Asynchronous Reasoning about Safety To evaluate the impact of asynchronous reasoning on safety, we conduct experiments on the HarmBench validation set [121] using the Virtual Context attack [125]. We use llm-as-a-judge [124] evaluation (gpt-4o-mini) where only actionable harmful instructions count as a successful attack. We compare the Attack Success Rate (ASR) across four setups using the Qwen3-32B model: (1) Baseline (Nonthinking), (2) Baseline (Thinking), (3) AsyncReasoning (Q-Continue), and (4) AsyncReasoning (Safety prompt) that is additionally instructed to verify safety before responding. The full safety prompt is included in Appendix B. Quantitative Results. We summarize our findings in Table 2. Consistent with recent findings on the “Cost of Thinking” [76], we observe that enabling reasoning in the baseline model actually increases vulnerability (ASR rises from 2.5% to 13.0%). The model effectively “talks itself into” answering harmful queries by adopting a helpful persona or over-analyzing the technical aspects of the prompt. AsyncReasoning (Q-Continue) (11.5% ASR) remains similarly vulnerable to the thinking baseline. However, by introducing additional safety instructions into the thinker’s prompt we successfully reduce the ASR to 2.0% while preserving accuracy on MATH-500 benchmark. In practice, this allows safety-minded reasoning in streaming LLM APIs and other time-sensitive applications without the need for specialized fine-tuning. AsyncReasoning can stream tokens normally for benign queries, only pausing generation for potentially unsafe responses. Inference Setup ASR↓ Accuracy↑ Baseline (Non-thinking) 0.025 0.834 Baseline (Thinking) 0.130 0.932 AsyncReasoning (Q-Continue) 0.115 0.890 AsyncReasoning (Safety Prompt) 0.020 0.878 Table 2: Attack Success Rate on HarmBench (Virtual Context attack) and Accuracy on MATH-500 for Qwen3-32B. Failure Mode Analysis. While AsyncReasoning allows for real-time safety checks, the asynchronous nature of generation introduces specific failure modes where the writer may output harmful content before the thinker intervenes. We identify three primary categories of such safety failures: 1. Race Condition: The writer begins generating a helpful response immediately based on the prompt. Although the thinker eventually concludes the request is unsafe, the writer has already streamed harmful tokens (e.g., the first steps of a dangerous recipe) to the user before the refusal signal is propagated. 2. Context Leakage: The thinker analyzes the harmful request by recalling technical details (e.g., explaining how a specific SQL injection works to verify its danger). The writer, attending to the thinker’s cache, interprets these technical details as the desired answer and formulates them into a response, bypassing the thinker’s intent. 3. Educational Loophole: The thinker adopts an educational persona to explain why a request is dangerous. The writer latches onto this educational content and reformats it as a set of instructions, stripping away the safety framing context. These findings suggest that, while AsyncReasoning can effectively filter attacks, strict gating mechanisms (e.g., ensuring the thinker has a “head start” on safety verification) are necessary to prevent race conditions in highly sensitive scenarios. We will investigate this further in future work. 7"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 8, "text": "5. Discussion & Future Work\nIn this preprint, we formulated AsyncReasoning — a\ntraining-free method that allows reasoning LLMs to think\nand write concurrently. Our preliminary experiments sug-\ngest that the proposed approach can indeed overlap thinking\nand writing and reduce thinking delays while giving more\naccurate answers than the non-thinking models. This allows\nLLMs to think longer and give more thoughtful answers in\ntime-sensitive applications such as voice assistants, embod-\nied agents, or safety-minded use cases.\nThis leaves many interesting directions for further research\nand analysis. In future work, we will look more into strate-\ngies for mode-switching: determining when to pause writing\nthe response and wait for more thoughts. We also plan to\nexpand the scope of our experiments with additional mod-\nels, task types, and comparison to non-training-free base-\nlines. Among others, it would be interesting to quantify the\nmethod’s ability to process asynchronous inputs, such as\ntask clarifications for voice assistants or environment read-\nouts for agents. Additionally, we will work on integrating\nAsyncReasoning with vLLM [43].\n6. Acknowledgements\nWe would like to thank Andrey Shukshov for his helpful\nadvice about efficient GPU kernel design. We also thank\nGleb Rodionov for proofreading and helpful suggestions on\nexperiment design and paper presentation.\nReferences\n[1] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral\nKumar.\nScaling llm test-time compute optimally\ncan be more effective than scaling model parameters.\narXiv preprint arXiv:2408.03314, 2024.\n[2] Mirac Suzgun, Nathan Scales, Nathanael Scharli,\nSebastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V. Le, Ed H. Chi,\nDenny Zhou, and Jason Wei. Challenging big-bench\ntasks and whether chain-of-thought can solve them.\nIn Annual Meeting of the Association for Computa-\ntional Linguistics, 2022.\n[3] Edward Beeching, Lewis Tunstall, and Sasha Rush.\nScaling test-time compute with open models.\n[4] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. Chain-of-thought prompting elicits reasoning\nin large language models. Advances in neural infor-\nmation processing systems, 35:24824–24837, 2022.\n[5] Takeshi Kojima, Shixiang Shane Gu, Machel Reid,\nYutaka Matsuo, and Yusuke Iwasawa. Large lan-\nguage models are zero-shot reasoners.\nArXiv,\nabs/2205.11916, 2022.\n[6] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L. Griffiths,\nYuan Cao,\nand Karthik\nNarasimhan.\nTree of thoughts: Deliberate prob-\nlem solving with large language models.\nArXiv,\nabs/2305.10601, 2023.\n[7] Hunter Lightman, Vineet Kosaraju, Yura Burda, Har-\nrison Edwards, Bowen Baker, Teddy Lee, Jan Leike,\nJohn Schulman, Ilya Sutskever, and Karl Cobbe.\nLet’s verify step by step. ArXiv, abs/2305.20050,\n2023.\n[8] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alexan-\nder J. Smola. Automatic chain of thought prompting\nin large language models. ArXiv, abs/2210.03493,\n2022.\n[9] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xi-\nang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke\nZettlemoyer, Percy Liang, Emmanuel Candès, and\nTatsunori Hashimoto. s1: Simple test-time scaling.\narXiv preprint arXiv:2501.19393, 2025.\n[10] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì,\nRoberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer:\nLanguage models can teach themselves to use tools.\nArXiv, abs/2302.04761, 2023.\n[11] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. React:\nSynergizing reasoning and acting in language models.\nArXiv, abs/2210.03629, 2022.\n[12] Leo Gao, Aman Madaan, Shuyan Zhou, Karthik\nNarasimhan, and Danqi Chen. Pal: Program-aided\nlanguage models. In Proceedings of the 40th Inter-\nnational Conference on Machine Learning, volume\n202, pages 10764–10791. PMLR, 2023.\n[13] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng\nLi, Weiming Lu, and Yue Ting Zhuang. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhugging face. ArXiv, abs/2303.17580, 2023.\n[14] Yujia Qin, Shi Liang, Yining Ye, Kunlun Zhu, Lan\nYan, Ya-Ting Lu, Yankai Lin, Xin Cong, Xiangru\nTang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing\nXie, Jie Zhou, Marc H. Gerstein, Dahai Li, Zhiyuan\nLiu, and Maosong Sun. Toolllm: Facilitating large\nlanguage models to master 16000+ real-world apis.\nArXiv, abs/2307.16789, 2023.\n[15] Zhanna Azerbayev, Dhruv Patel, Sébastien Bubeck,\nRonen Eldan, Yin Tat Lee, Yuanzhong Li, Tamas Sar-\n8", "clean_text": "5. Discussion & Future Work In this preprint, we formulated AsyncReasoning — a training-free method that allows reasoning LLMs to think and write concurrently. Our preliminary experiments suggest that the proposed approach can indeed overlap thinking and writing and reduce thinking delays while giving more accurate answers than the non-thinking models. This allows LLMs to think longer and give more thoughtful answers in time-sensitive applications such as voice assistants, embodied agents, or safety-minded use cases. This leaves many interesting directions for further research and analysis. In future work, we will look more into strategies for mode-switching: determining when to pause writing the response and wait for more thoughts. We also plan to expand the scope of our experiments with additional models, task types, and comparison to non-training-free baselines. Among others, it would be interesting to quantify the method’s ability to process asynchronous inputs, such as task clarifications for voice assistants or environment readouts for agents. Additionally, we will work on integrating AsyncReasoning with vLLM [43]. 6. Acknowledgements We would like to thank Andrey Shukshov for his helpful advice about efficient GPU kernel design. We also thank Gleb Rodionov for proofreading and helpful suggestions on experiment design and paper presentation. References [1] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [2] Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. In Annual Meeting of the Association for Computational Linguistics, 2022. [3] Edward Beeching, Lewis Tunstall, and Sasha Rush. Scaling test-time compute with open models. [4] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824–24837, 2022. [5] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. ArXiv, abs/2205.11916, 2022. [6] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. ArXiv, abs/2305.10601, 2023. [7] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. ArXiv, abs/2305.20050, 2023. [8] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alexander J. Smola. Automatic chain of thought prompting in large language models. ArXiv, abs/2210.03493, 2022. [9] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [10] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. ArXiv, abs/2302.04761, 2023. [11] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. ArXiv, abs/2210.03629, 2022. [12] Leo Gao, Aman Madaan, Shuyan Zhou, Karthik Narasimhan, and Danqi Chen. Pal: Program-aided language models. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 10764–10791. PMLR, 2023. [13] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yue Ting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. ArXiv, abs/2303.17580, 2023. [14] Yujia Qin, Shi Liang, Yining Ye, Kunlun Zhu, Lan Yan, Ya-Ting Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Marc H. Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis. ArXiv, abs/2307.16789, 2023. [15] Zhanna Azerbayev, Dhruv Patel, Sébastien Bubeck, Ronen Eldan, Yin Tat Lee, Yuanzhong Li, Tamas Sar8"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 9, "text": "los, and Yi Zhang. Llemma: An open language model\nfor mathematics. In Proceedings of the Twelfth In-\nternational Conference on Learning Representations,\n2024.\n[16] Ziyue Wang, Zhiyuan Zhao, Minqi Peng, Shiqi\nChen, Mengdi Yang, Chi-Min Lin, Pratyush Sharma,\nSébastien Bubeck, Ronen Eldan, Yuanzhong Li,\nYin Tat Lee, and Yi Zhang. Mathcoder: Seamless\ncode integration in LLMs for enhanced mathemati-\ncal reasoning. In Proceedings of the Twelfth Inter-\nnational Conference on Learning Representations,\n2024.\n[17] Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen,\nKarol Hausman, Dorsa Sadigh, Sergey Levine, Li Fei-\nFei, Fei Xia, and Brian Ichter. Chain of code: Reason-\ning with a language model-augmented code emulator.\nIn Proceedings of the 41st International Conference\non Machine Learning, volume 235 of Proceedings\nof Machine Learning Research, pages 28259–28277.\nPMLR, 2024. arXiv preprint arXiv:2312.04474.\n[18] OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer,\nAdam Richardson, Ahmed El-Kishky, Aiden Low,\nAlec Helyar, Aleksander Madry, and Alex Beutel\net al. Openai o1 system card, 2024.\n[19] Google\nDeepMind.\nGemini\n2.5:\nOur\nNewest\nGemini\nModel\nwith\nThinking.\nhttps://blog.google/technology/google-\ndeepmind/gemini-model-thinking-updates-\nmarch-2025/#gemini-2-5-thinking,\n2025.\nAccessed: 2025-04-07.\n[20] Anthropic. Claude 3.7 sonnet and claude code, 2024.\nAccessed: 2025.04.02.\n[21] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei\nZhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qi-\nhao Zhu, Shirong Ma, Peiyi Wang, and Xiao Bi et al.\nDeepseek-r1: Incentivizing reasoning capability in\nllms via reinforcement learning, 2025.\n[22] Qwen Team. Qwq-32b: Embracing the power of\nreinforcement learning, March 2025.\n[23] Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen,\nJiahao Chen, Ningxin Chen, Ruijue Chen, Yanru\nChen, Yuankun Chen, Yutian Chen, Zhuofu Chen,\nJialei Cui, Hao Ding, Mengnan Dong, Angang Du,\nChenzhuang Du, Dikang Du, Yulun Du, Yu Fan,\nYichen Feng, Kelin Fu, Bofei Gao, Hongcheng Gao,\nPeizhong Gao, Tong Gao, Xinran Gu, Longyu Guan,\nHaiqing Guo, Jianhang Guo, Hao Hu, Xiaoru Hao,\nTianhong He, Weiran He, Wenyang He, Chao Hong,\nYangyang Hu, Zhenxing Hu, Weixiao Huang, Zhiqi\nHuang, Zihao Huang, Tao Jiang, Zhejun Jiang, Xinyi\nJin, Yongsheng Kang, Guokun Lai, Cheng Li, Fang\nLi, Haoyang Li, Ming Li, Wentao Li, Yanhao Li,\nYiwei Li, Zhaowei Li, Zheming Li, Hongzhan Lin,\nXiaohan Lin, Zongyu Lin, Chengyin Liu, Chenyu\nLiu, Hongzhang Liu, Jingyuan Liu, Junqi Liu, Liang\nLiu, Shaowei Liu, T. Y. Liu, Tianwei Liu, Weizhou\nLiu, Yangyang Liu, Yibo Liu, Yiping Liu, Yue Liu,\nZhengying Liu, Enzhe Lu, Lijun Lu, Shengling Ma,\nXinyu Ma, Yingwei Ma, Shaoguang Mao, Jie Mei,\nXin Men, Yibo Miao, Siyuan Pan, Yebo Peng, Ruoyu\nQin, Bowen Qu, Zeyu Shang, Lidong Shi, Shengyuan\nShi, Feifan Song, Jianlin Su, Zhengyuan Su, Xinjie\nSun, Flood Sung, Heyi Tang, Jiawen Tao, Qifeng\nTeng, Chensi Wang, Dinglu Wang, Feng Wang, Haim-\ning Wang, Jianzhou Wang, Jiaxing Wang, Jinhong\nWang, Shengjie Wang, Shuyi Wang, Yao Wang, Yejie\nWang, Yiqin Wang, Yuxin Wang, Yuzhi Wang, Zhaoji\nWang, Zhengtao Wang, Zhexu Wang, Chu Wei, Qian-\nqian Wei, Wenhao Wu, Xingzhe Wu, Yuxin Wu,\nChenjun Xiao, Xiaotong Xie, Weimin Xiong, Boyu\nXu, Jing Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting\nXu, Weixin Xu, Xinran Xu, Yangchuan Xu, Ziyao\nXu, Junjie Yan, Yuzi Yan, Xiaofei Yang, Ying Yang,\nZhen Yang, Zhilin Yang, Zonghan Yang, Haotian\nYao, Xingcheng Yao, Wenjie Ye, Zhuorui Ye, Bo-\nhong Yin, Longhui Yu, Enming Yuan, Hongbang\nYuan, Mengjie Yuan, Haobing Zhan, Dehao Zhang,\nHao Zhang, Wanlu Zhang, Xiaobin Zhang, Yangkun\nZhang, Yizhi Zhang, Yongting Zhang, Yu Zhang, Yu-\ntao Zhang, Yutong Zhang, Zheng Zhang, Haotian\nZhao, Yikai Zhao, Huabin Zheng, Shaojie Zheng,\nJianren Zhou, Xinyu Zhou, Zaida Zhou, Zhen Zhu,\nWeiyu Zhuang, and Xinxing Zu. Kimi k2: Open\nagentic intelligence, 2025.\n[24] ARC Prize Foundation. Openai’s new o3 system\nscores breakthrough on arc-agi-pub, 2024. Accessed:\n2025.03.28.\n[25] Humanity’s Last Exam Contributors. Humanity’s last\nexam: A benchmark for frontier ai capabilities. arXiv\npreprint arXiv:2501.14249, 2025.\n[26] Nicole Landi, Stephen Frost, W Menc, Rebecca San-\ndak, and Kenneth Pugh. Neurobiological bases of\nreading comprehension: Insights from neuroimag-\ning studies of word-level and text-level processing\nin skilled and impaired readers. Reading & writing\nquarterly : overcoming learning difficulties, 29:145–\n167, 04 2013.\n[27] Bingjiang Lyu, William D. Marslen-Wilson, Yuxing\nFang, and Lorraine K. Tyler. Finding structure during\nincremental speech comprehension. eLife, 12:e89311,\n2023.\n9", "clean_text": "los, and Yi Zhang. Llemma: An open language model for mathematics. In Proceedings of the Twelfth International Conference on Learning Representations, 2024. [16] Ziyue Wang, Zhiyuan Zhao, Minqi Peng, Shiqi Chen, Mengdi Yang, Chi-Min Lin, Pratyush Sharma, Sébastien Bubeck, Ronen Eldan, Yuanzhong Li, Yin Tat Lee, and Yi Zhang. Mathcoder: Seamless code integration in LLMs for enhanced mathematical reasoning. In Proceedings of the Twelfth International Conference on Learning Representations, 2024. [17] Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey Levine, Li FeiFei, Fei Xia, and Brian Ichter. Chain of code: Reasoning with a language model-augmented code emulator. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 28259–28277. PMLR, 2024. arXiv preprint arXiv:2312.04474. [18] OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, and Alex Beutel et al. Openai o1 system card, 2024. [19] Google DeepMind. Gemini 2.5: Our Newest Gemini Model with Thinking. https://blog.google/technology/googledeepmind/gemini-model-thinking-updatesmarch-2025/#gemini-2-5-thinking, 2025. Accessed: 2025-04-07. [20] Anthropic. Claude 3.7 sonnet and claude code, 2024. Accessed: 2025.04.02. [21] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, and Xiao Bi et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [22] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. [23] Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, Jialei Cui, Hao Ding, Mengnan Dong, Angang Du, Chenzhuang Du, Dikang Du, Yulun Du, Yu Fan, Yichen Feng, Kelin Fu, Bofei Gao, Hongcheng Gao, Peizhong Gao, Tong Gao, Xinran Gu, Longyu Guan, Haiqing Guo, Jianhang Guo, Hao Hu, Xiaoru Hao, Tianhong He, Weiran He, Wenyang He, Chao Hong, Yangyang Hu, Zhenxing Hu, Weixiao Huang, Zhiqi Huang, Zihao Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yongsheng Kang, Guokun Lai, Cheng Li, Fang Li, Haoyang Li, Ming Li, Wentao Li, Yanhao Li, Yiwei Li, Zhaowei Li, Zheming Li, Hongzhan Lin, Xiaohan Lin, Zongyu Lin, Chengyin Liu, Chenyu Liu, Hongzhang Liu, Jingyuan Liu, Junqi Liu, Liang Liu, Shaowei Liu, T. Y. Liu, Tianwei Liu, Weizhou Liu, Yangyang Liu, Yibo Liu, Yiping Liu, Yue Liu, Zhengying Liu, Enzhe Lu, Lijun Lu, Shengling Ma, Xinyu Ma, Yingwei Ma, Shaoguang Mao, Jie Mei, Xin Men, Yibo Miao, Siyuan Pan, Yebo Peng, Ruoyu Qin, Bowen Qu, Zeyu Shang, Lidong Shi, Shengyuan Shi, Feifan Song, Jianlin Su, Zhengyuan Su, Xinjie Sun, Flood Sung, Heyi Tang, Jiawen Tao, Qifeng Teng, Chensi Wang, Dinglu Wang, Feng Wang, Haiming Wang, Jianzhou Wang, Jiaxing Wang, Jinhong Wang, Shengjie Wang, Shuyi Wang, Yao Wang, Yejie Wang, Yiqin Wang, Yuxin Wang, Yuzhi Wang, Zhaoji Wang, Zhengtao Wang, Zhexu Wang, Chu Wei, Qianqian Wei, Wenhao Wu, Xingzhe Wu, Yuxin Wu, Chenjun Xiao, Xiaotong Xie, Weimin Xiong, Boyu Xu, Jing Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting Xu, Weixin Xu, Xinran Xu, Yangchuan Xu, Ziyao Xu, Junjie Yan, Yuzi Yan, Xiaofei Yang, Ying Yang, Zhen Yang, Zhilin Yang, Zonghan Yang, Haotian Yao, Xingcheng Yao, Wenjie Ye, Zhuorui Ye, Bohong Yin, Longhui Yu, Enming Yuan, Hongbang Yuan, Mengjie Yuan, Haobing Zhan, Dehao Zhang, Hao Zhang, Wanlu Zhang, Xiaobin Zhang, Yangkun Zhang, Yizhi Zhang, Yongting Zhang, Yu Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Haotian Zhao, Yikai Zhao, Huabin Zheng, Shaojie Zheng, Jianren Zhou, Xinyu Zhou, Zaida Zhou, Zhen Zhu, Weiyu Zhuang, and Xinxing Zu. Kimi k2: Open agentic intelligence, 2025. [24] ARC Prize Foundation. Openai’s new o3 system scores breakthrough on arc-agi-pub, 2024. Accessed: 2025.03.28. [25] Humanity’s Last Exam Contributors. Humanity’s last exam: A benchmark for frontier ai capabilities. arXiv preprint arXiv:2501.14249, 2025. [26] Nicole Landi, Stephen Frost, W Menc, Rebecca Sandak, and Kenneth Pugh. Neurobiological bases of reading comprehension: Insights from neuroimaging studies of word-level and text-level processing in skilled and impaired readers. Reading & writing quarterly : overcoming learning difficulties, 29:145– 167, 04 2013. [27] Bingjiang Lyu, William D. Marslen-Wilson, Yuxing Fang, and Lorraine K. Tyler. Finding structure during incremental speech comprehension. eLife, 12:e89311, 2023. 9"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 10, "text": "[28] Sarah Bro Trasmundi and Juan Toro. Mind wandering\nin reading: An embodied approach. Frontiers in\nHuman Neuroscience, 17, 2023.\n[29] Daisuke Akiba. Ctrl + alt + inner speech: A ver-\nbal–cognitive scaffold (vcs) model of pathways to\ncomputational thinking.\nJournal of Intelligence,\n13(12), 2025.\n[30] Kevin P. Madore and Anthony D. Wagner. Multicosts\nof multitasking. Cerebrum : the Dana Forum on\nBrain Science, 2019:cer–04–19, 2019.\n[31] Maurizio Corbetta, Gaurav Patel, and Gordon L\nShulman.\nThe reorienting system of the human\nbrain: from environment to theory of mind. Neu-\nron, 58(3):306–324, 2008.\n[32] OpenAI.\nGpt-4o system card.\nOnline techni-\ncal report, 2024.\nVoice-mode multimodal model\nsupporting audio, text, and vision. Available at\nhttps://openai.com/index/hello-gpt-4o.\n[33] Paul K. Rubenstein, Chulayuth Asawaroengchai,\nDuc Dung Nguyen, Ankur Bapna, Zalán Bor-\nsos, Félix de Chaumont Quitry, Peter Chen, Dalia\nEl Badawy, Wei Han, Eugene Kharitonov, Hannah\nMuckenhirn, Dirk Padfield, James Qin, Danny Rozen-\nberg, Tara Sainath, Johan Schalkwyk, Matt Sharifi,\nMichelle Tadmor Ramanovich, Marco Tagliasacchi,\nAlexandru Tudor, Mihajlo Velimirovi´c, Damien Vin-\ncent, Jiahui Yu, Yongqiang Wang, Vicky Zayats, Neil\nZeghidour, Yu Zhang, Zhishuai Zhang, Lukas Zilka,\nand Christian Frank. Audiopalm: A large language\nmodel that can speak and listen.\narXiv preprint\narXiv:2306.12925, 2023.\n[34] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan,\nPengyu Wang, Yaqian Zhou, and Xipeng Qiu.\nSpeechgpt: Empowering large language models with\nintrinsic cross-modal conversational abilities. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2023, pages 15757–15773. Association for\nComputational Linguistics, December 2023.\n[35] Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang,\nShiliang Zhang, Zhijie Yan, Chang Zhou, and Jin-\ngren Zhou. Qwen-audio: Advancing universal audio\nunderstanding via unified large-scale audio-language\nmodels. arXiv preprint arXiv:2311.07919, 2023.\n[36] Zhifei Xie and Changqiao Wu. Mini-omni: Language\nmodels can hear, talk while thinking in streaming.\narXiv preprint arXiv:2408.16725, 2024.\n[37] Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma,\nShaolei Zhang, and Yang Feng. Llama-omni: Seam-\nless speech interaction with large language models.\narXiv preprint arXiv:2409.06666, 2024.\n[38] Alexandre Défossez, Laurent Mazaré, Manu Orsini,\nAmélie Royer, Patrick Pérez, Hervé Jégou, Édouard\nGrave, and Neil Zeghidour. Moshi: a speech-text\nfoundation model for real-time dialogue, 2024.\n[39] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen\nChebotar, Omar Cortes, Byron David, Chelsea Finn,\nChuyuan Fu, Keerthana Gopalakrishnan, Karol Haus-\nman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian\nIbarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario\nJauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil\nJoshi, Ryan Julian, Dmitry Kalashnikov, Yuheng\nKuang, Kuang-Huei Lee, Sergey Levine, Yao Lu,\nLinda Luu, Carolina Parada, Peter Pastor, Jornell\nQuiambao, Kanishka Rao, Jarek Rettinghouse, Diego\nReyes, Pierre Sermanet, Nicolas Sievers, Clayton\nTan, Alexander Toshev, Vincent Vanhoucke, Fei Xia,\nTed Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and\nAndy Zeng. Do as i can, not as i say: Grounding\nlanguage in robotic affordances. ArXiv, 2022.\n[40] Anthony Brohan, Noah Brown, Justice Carbajal, Yev-\ngen Chebotar, Xi Chen, Krzysztof Choromanski,\nTianli Ding, Danny Driess, Avinava Dubey, Chelsea\nFinn, Pete Florence, Chuyuan Fu, Montse Gonza-\nlez Arenas, Keerthana Gopalakrishnan, Kehang Han,\nKarol Hausman, Alexander Herzog, Jasmine Hsu,\nBrian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian,\nDmitry Kalashnikov, Yuheng Kuang, Isabel Leal,\nLisa Lee, Tsang-Wei Lee, Sergey Levine, Yao Lu,\nHenryk Michalewski, Igor Mordatch, Karl Pertsch,\nKanishka Rao, Krista Reymann, Michael Ryoo, Gre-\ncia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar\nSingh, Anikait Singh, Radu Soricut, Huong Tran,\nVincent Vanhoucke, Quan Vuong, Ayzaan Wahid,\nStefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia,\nTed Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Bri-\nanna Zitkovich. Rt-2: Vision-language-action models\ntransfer web knowledge to robotic control. ArXiv,\n2023.\n[41] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey\nLynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\nWahid, Jonathan Tompson, Quan Vuong, Tianhe\nYu, Wenlong Huang, Yevgen Chebotar, Pierre Ser-\nmanet, Daniel Duckworth, Sergey Levine, Vincent\nVanhoucke, Karol Hausman, Marc Toussaint, Klaus\nGreff, Andy Zeng, Igor Mordatch, and Pete Florence.\nPalm-e: An embodied multimodal language model.\narXiv preprint arXiv:2303.03378, 2023.\n[42] OpenAI.\nChatgpt deep research:\nSupport for\nuser update and multitasking features.\nhttps://\nchat.openai.com, 2025.\nAccessed 7 December\n10", "clean_text": "[28] Sarah Bro Trasmundi and Juan Toro. Mind wandering in reading: An embodied approach. Frontiers in Human Neuroscience, 17, 2023. [29] Daisuke Akiba. Ctrl + alt + inner speech: A verbal–cognitive scaffold (vcs) model of pathways to computational thinking. Journal of Intelligence, 13(12), 2025. [30] Kevin P. Madore and Anthony D. Wagner. Multicosts of multitasking. Cerebrum : the Dana Forum on Brain Science, 2019:cer–04–19, 2019. [31] Maurizio Corbetta, Gaurav Patel, and Gordon L Shulman. The reorienting system of the human brain: from environment to theory of mind. Neuron, 58(3):306–324, 2008. [32] OpenAI. Gpt-4o system card. Online technical report, 2024. Voice-mode multimodal model supporting audio, text, and vision. Available at https://openai.com/index/hello-gpt-4o. [33] Paul K. Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalán Borsos, Félix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, Hannah Muckenhirn, Dirk Padfield, James Qin, Danny Rozenberg, Tara Sainath, Johan Schalkwyk, Matt Sharifi, Michelle Tadmor Ramanovich, Marco Tagliasacchi, Alexandru Tudor, Mihajlo Velimirovi´c, Damien Vincent, Jiahui Yu, Yongqiang Wang, Vicky Zayats, Neil Zeghidour, Yu Zhang, Zhishuai Zhang, Lukas Zilka, and Christian Frank. Audiopalm: A large language model that can speak and listen. arXiv preprint arXiv:2306.12925, 2023. [34] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 15757–15773. Association for Computational Linguistics, December 2023. [35] Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models. arXiv preprint arXiv:2311.07919, 2023. [36] Zhifei Xie and Changqiao Wu. Mini-omni: Language models can hear, talk while thinking in streaming. arXiv preprint arXiv:2408.16725, 2024. [37] Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, and Yang Feng. Llama-omni: Seamless speech interaction with large language models. arXiv preprint arXiv:2409.06666, 2024. [38] Alexandre Défossez, Laurent Mazaré, Manu Orsini, Amélie Royer, Patrick Pérez, Hervé Jégou, Édouard Grave, and Neil Zeghidour. Moshi: a speech-text foundation model for real-time dialogue, 2024. [39] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do as i can, not as i say: Grounding language in robotic affordances. ArXiv, 2022. [40] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-2: Vision-language-action models transfer web knowledge to robotic control. ArXiv, 2023. [41] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. [42] OpenAI. Chatgpt deep research: Support for user update and multitasking features. https:// chat.openai.com, 2025. Accessed 7 December 10"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 11, "text": "2025. In late 2025, the Deep Research feature was\nupdated to allow user to communicate with the agent\nwhile it performs research via the \"Update\" button.\n[43] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph Gon-\nzalez, Hao Zhang, and Ion Stoica. Efficient memory\nmanagement for large language model serving with\npagedattention. In Proceedings of the 29th Sympo-\nsium on Operating Systems Principles, pages 611–\n626, 2023.\n[44] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff\nHuang, Chuyue Sun, Cody Hao Yu, Shiyi Cao, Chris-\ntos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark\nBarrett, and Ying Sheng. Efficiently programming\nlarge language models using sglang, 2023.\n[45] Gemini\nlive\n(voice\nmode).\nhttps:\n//gemini.google/overview/gemini-live/,\n2024. Accessed: 2025-12-01.\n[46] Anthropic. Using voice mode on claude mobile apps.\nhttps://support.claude.com/en/articles/\n11101966-using-voice-mode-on-claude-\nmobile-apps, 2025. Accessed: December 1, 2025.\n[47] James\nFlamino,\nMohammed\nShahid\nModi,\nBoleslaw K. Szymanski, Brendan Cross, and Colton\nMikolajczyk. Testing the limits of large language\nmodels in debating humans.\nScientific Reports,\n15:13852, 2025.\n[48] Stephanie Houde, Kristina Brimijoin, Michael\nMuller, Steven I. Ross, Dario Andres Silva Moran,\nGabriel Enrique Gonzalez, Siya Kunde, Morgan A.\nForeman, and Justin D. Weisz.\nControlling ai\nagent participation in group conversations: A human-\ncentered approach. In Proceedings of the 30th Inter-\nnational Conference on Intelligent User Interfaces,\nIUI ’25, page 390–408, New York, NY, USA, 2025.\nAssociation for Computing Machinery.\n[49] K. H. Davis, R. Biddulph, and S. Balashek. Auto-\nmatic recognition of spoken digits. The Journal of\nthe Acoustical Society of America, 24(6):637–642, 11\n1952.\n[50] Daniel Povey, Arnab Ghoshal, Gilles Boulianne,\nLukas Burget, Ondrej Glembek, Nagendra Goel,\nMirko Hannemann, Petr Motlicek, Yanmin Qian,\nPetr Schwarz, Jan Silovsky, Georg Stemmer, and\nKarel Vesely. Kaldi: A toolkit for speech recogni-\ntion. https://kaldi-asr.org, 2011. Open-source\nspeech recognition toolkit.\n[51] Steffen Schneider, Alexei Baevski, Ronan Collobert,\nand Michael Auli.\nwav2vec: Unsupervised pre-\ntraining for speech recognition, 2019.\n[52] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brock-\nman, Christine McLeavey, and Ilya Sutskever. Robust\nspeech recognition via large-scale weak supervision,\n2022.\n[53] N. Umeda, H. Omura, and O. Fujimura. First com-\nplete text-to-speech system. Technical report, Elec-\ntrotechnical Laboratory, Japan, Tokyo, Japan, 1968.\n[54] Heiga Zen, Keiichi Tokuda, and Alan W. Black. Sta-\ntistical parametric speech synthesis. Speech Commu-\nnication, 51(11):1039–1064, 2009.\n[55] Aaron van den Oord, Sander Dieleman, Heiga\nZen, Karen Simonyan, Oriol Vinyals, Alex Graves,\nNal Kalchbrenner, Andrew Senior, and Koray\nKavukcuoglu.\nWavenet: A generative model for\nraw audio. In Proceedings of the 9th ISCA Speech\nSynthesis Workshop, 2016.\n[56] Yuxuan Wang, R. J. Skerry-Ryan, Daisy Stanton,\nYonghui Wu, Ron J. Weiss, Navdeep Jaitly, Zongheng\nYang, Ying Xiao, Zhifeng Chen, Samy Bengio, Quoc\nLe, Yannis Agiomyrgiannakis, Rob Clark, and Rif A.\nSaurous. Tacotron: Towards end-to-end speech syn-\nthesis. In INTERSPEECH, pages 4006–4010. ISCA,\n2017.\n[57] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike\nSchuster, Navdeep Jaitly, Zongheng Yang, Zhifeng\nChen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan,\nRif A. Saurous, Yannis Agiomyrgiannakis, and\nYonghui Wu. Natural tts synthesis by conditioning\nwavenet on mel spectrogram predictions, 2018.\n[58] Ryan J. Prenger, Rafael Valle, and Bryan Catanzaro.\nWaveglow: A flow-based generative network for\nspeech synthesis. ICASSP 2019 - 2019 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 3617–3621, 2018.\n[59] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae.\nHifi-gan: generative adversarial networks for efficient\nand high fidelity speech synthesis. In Proceedings\nof the 34th International Conference on Neural In-\nformation Processing Systems, NIPS ’20, Red Hook,\nNY, USA, 2020. Curran Associates Inc.\n[60] James Betker. Better speech synthesis through scal-\ning. arXiv preprint arXiv:2305.07243, 2023. Tortoise\nTTS: expressive multi-voice text-to-speech.\n[61] Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong\nWang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting\nHe, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake\nGuo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang,\n11", "clean_text": "2025. In late 2025, the Deep Research feature was updated to allow user to communicate with the agent while it performs research via the \"Update\" button. [43] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611– 626, 2023. [44] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. Efficiently programming large language models using sglang, 2023. [45] Gemini live (voice mode). https: //gemini.google/overview/gemini-live/, 2024. Accessed: 2025-12-01. [46] Anthropic. Using voice mode on claude mobile apps. https://support.claude.com/en/articles/ 11101966-using-voice-mode-on-claudemobile-apps, 2025. Accessed: December 1, 2025. [47] James Flamino, Mohammed Shahid Modi, Boleslaw K. Szymanski, Brendan Cross, and Colton Mikolajczyk. Testing the limits of large language models in debating humans. Scientific Reports, 15:13852, 2025. [48] Stephanie Houde, Kristina Brimijoin, Michael Muller, Steven I. Ross, Dario Andres Silva Moran, Gabriel Enrique Gonzalez, Siya Kunde, Morgan A. Foreman, and Justin D. Weisz. Controlling ai agent participation in group conversations: A humancentered approach. In Proceedings of the 30th International Conference on Intelligent User Interfaces, IUI ’25, page 390–408, New York, NY, USA, 2025. Association for Computing Machinery. [49] K. H. Davis, R. Biddulph, and S. Balashek. Automatic recognition of spoken digits. The Journal of the Acoustical Society of America, 24(6):637–642, 11 1952. [50] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, Jan Silovsky, Georg Stemmer, and Karel Vesely. Kaldi: A toolkit for speech recognition. https://kaldi-asr.org, 2011. Open-source speech recognition toolkit. [51] Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. wav2vec: Unsupervised pretraining for speech recognition, 2019. [52] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision, 2022. [53] N. Umeda, H. Omura, and O. Fujimura. First complete text-to-speech system. Technical report, Electrotechnical Laboratory, Japan, Tokyo, Japan, 1968. [54] Heiga Zen, Keiichi Tokuda, and Alan W. Black. Statistical parametric speech synthesis. Speech Communication, 51(11):1039–1064, 2009. [55] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. In Proceedings of the 9th ISCA Speech Synthesis Workshop, 2016. [56] Yuxuan Wang, R. J. Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J. Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, Quoc Le, Yannis Agiomyrgiannakis, Rob Clark, and Rif A. Saurous. Tacotron: Towards end-to-end speech synthesis. In INTERSPEECH, pages 4006–4010. ISCA, 2017. [57] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, Yannis Agiomyrgiannakis, and Yonghui Wu. Natural tts synthesis by conditioning wavenet on mel spectrogram predictions, 2018. [58] Ryan J. Prenger, Rafael Valle, and Bryan Catanzaro. Waveglow: A flow-based generative network for speech synthesis. ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3617–3621, 2018. [59] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan: generative adversarial networks for efficient and high fidelity speech synthesis. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS ’20, Red Hook, NY, USA, 2020. Curran Associates Inc. [60] James Betker. Better speech synthesis through scaling. arXiv preprint arXiv:2305.07243, 2023. Tortoise TTS: expressive multi-voice text-to-speech. [61] Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, 11"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 12, "text": "Hongkun Hao, Zishan Guo, Baosong Yang, Bin\nZhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin\nChen, Xuejing Liu, Peng Wang, Mingkun Yang, Day-\niheng Liu, Xingzhang Ren, Bo Zheng, Rui Men,\nFan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren\nZhou, and Junyang Lin. Qwen3-omni technical re-\nport. arXiv preprint arXiv:2509.17765, 2025.\n[62] Ruaridh Mon-Williams, Gen Li, Ran Long, Wenqian\nDu, Christopher G. Lucas, et al. Embodied large\nlanguage models enable robots to complete complex\ntasks in unpredictable environments. Nature Machine\nIntelligence, 7:592–601, 2025.\n[63] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-\ndlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and\nAnima Anandkumar. Voyager: An open-ended em-\nbodied agent with large language models. ArXiv,\n2023.\n[64] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi\nWang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei,\nAnima Anandkumar, Yuke Zhu, and Linxi Fan.\nVima: General robot manipulation with multimodal\nprompts. ArXiv, 2023.\n[65] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti,\nTed Xiao, Ashwin Balakrishna, Suraj Nair, Rafael\nRafailov, Ethan Foster, Grace Lam, Pannag Sanketi,\nQuan Vuong, Thomas Kollar, Benjamin Burchfiel,\nRuss Tedrake, Dorsa Sadigh, Sergey Levine, Percy\nLiang, and Chelsea Finn. Openvla: An open-source\nvision-language-action model, 2024.\n[66] Ranjan Sapkota, Yang Cao, Konstantinos I. Roume-\nliotis, and Manoj Karkee. Vision-language-action\nmodels: Concepts, progress, applications and chal-\nlenges, 2025.\n[67] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu,\nXiaojian (Shawn) Ma, and Yitao Liang. Describe,\nexplain, plan and select: Interactive planning with\nllms enables open-world multi-task agents. In A. Oh,\nT. Naumann, A. Globerson, K. Saenko, M. Hardt, and\nS. Levine, editors, Advances in Neural Information\nProcessing Systems, volume 36, pages 34153–34189.\nCurran Associates, Inc., 2023.\n[68] Charles Cao, Feiyi Wang, Lisa Lindley, and Zejiang\nWang. Managing linux servers with llm-based ai\nagents: An empirical evaluation with gpt4. Machine\nLearning with Applications, 17:100570, 2024.\n[69] Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhen-\nmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu,\nand Lingpeng Kong. Os-copilot: Towards generalist\ncomputer agents with self-improvement, 2024.\n[70] China. Xiaoyan Zhang, Zhao Yang, Jiaxuan Liu,\nYanda Li, Yucheng Han, Xin Chen, Zebiao Huang,\nBin Fu, and Gang Yu. Appagent: Multimodal agents\nas smartphone users. Proceedings of the 2025 CHI\nConference on Human Factors in Computing Systems,\n2023.\n[71] Brian Singer, Keane Lucas, Lakshmi Adiga, Meghna\nJain, Lujo Bauer, and Vyas Sekar. On the feasibility\nof using llms to execute multistage network attacks,\n01 2025.\n[72] Tomek Korbak, Mikita Balesni, Elizabeth Barnes,\nYoshua Bengio, Joe Benton, Joseph Bloom, Mark\nChen, Alan Cooney, Allan Dafoe, Anca Dragan,\nScott Emmons, Owain Evans, David Farhi, Ryan\nGreenblatt, Dan Hendrycks, Marius Hobbhahn, Evan\nHubinger, Geoffrey Irving, Erik Jenner, Daniel Koko-\ntajlo, Victoria Krakovna, Shane Legg, David Lindner,\nDavid Luan, Aleksander M ˛adry, Julian Michael, Neel\nNanda, Dave Orr, Jakub Pachocki, Ethan Perez, Mary\nPhuong, Fabien Roger, Joshua Saxe, Buck Shlegeris,\nMartín Soto, Eric Steinberger, Jasmine Wang, Woj-\nciech Zaremba, Bowen Baker, Rohin Shah, and Vlad\nMikulik. Chain of Thought Monitorability: A New\nand Fragile Opportunity for AI Safety. arXiv, 2025.\n[73] Bowen Baker, Joost Huizinga, Leo Gao, Zehao\nDou, Melody Y. Guan, Aleksander Madry, Wojciech\nZaremba, Jakub W. Pachocki, and David Farhi. Moni-\ntoring reasoning models for misbehavior and the risks\nof promoting obfuscation. ArXiv, abs/2503.11926,\n2025.\n[74] Chengda Lu, Xiaoyu Fan, Yu Huang, Rongwu Xu,\nJijie Li, and Wei Xu. Does chain-of-thought reason-\ning really reduce harmfulness from jailbreaking? In\nWanxiang Che, Joyce Nabende, Ekaterina Shutova,\nand Mohammad Taher Pilehvar, editors, Findings of\nthe Association for Computational Linguistics: ACL\n2025, pages 6523–6546, Vienna, Austria, July 2025.\nAssociation for Computational Linguistics.\n[75] James Chua, Jan Betley, Mia Taylor, and Owain\nEvans. Thought crime: Backdoors and emergent\nmisalignment in reasoning models, 2025.\n[76] Fan Yang.\nThe cost of thinking: Increased jail-\nbreak risk in large language models. arXiv preprint\narXiv:2508.10032, 2025.\n[77] Kaiwen Zhou, Xuandong Zhao, Gaowen Liu, Jayanth\nSrinivasa, Aosong Feng, Dawn Song, and Xin Eric\nWang. Safekey: Amplifying aha-moment insights for\nsafety reasoning. arXiv preprint arXiv:2505.16186,\n2025.\n12", "clean_text": "Hongkun Hao, Zishan Guo, Baosong Yang, Bin Zhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin Chen, Xuejing Liu, Peng Wang, Mingkun Yang, Dayiheng Liu, Xingzhang Ren, Bo Zheng, Rui Men, Fan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren Zhou, and Junyang Lin. Qwen3-omni technical report. arXiv preprint arXiv:2509.17765, 2025. [62] Ruaridh Mon-Williams, Gen Li, Ran Long, Wenqian Du, Christopher G. Lucas, et al. Embodied large language models enable robots to complete complex tasks in unpredictable environments. Nature Machine Intelligence, 7:592–601, 2025. [63] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. ArXiv, 2023. [64] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation with multimodal prompts. ArXiv, 2023. [65] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model, 2024. [66] Ranjan Sapkota, Yang Cao, Konstantinos I. Roumeliotis, and Manoj Karkee. Vision-language-action models: Concepts, progress, applications and challenges, 2025. [67] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian (Shawn) Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with llms enables open-world multi-task agents. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 34153–34189. Curran Associates, Inc., 2023. [68] Charles Cao, Feiyi Wang, Lisa Lindley, and Zejiang Wang. Managing linux servers with llm-based ai agents: An empirical evaluation with gpt4. Machine Learning with Applications, 17:100570, 2024. [69] Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, and Lingpeng Kong. Os-copilot: Towards generalist computer agents with self-improvement, 2024. [70] China. Xiaoyan Zhang, Zhao Yang, Jiaxuan Liu, Yanda Li, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, 2023. [71] Brian Singer, Keane Lucas, Lakshmi Adiga, Meghna Jain, Lujo Bauer, and Vyas Sekar. On the feasibility of using llms to execute multistage network attacks, 01 2025. [72] Tomek Korbak, Mikita Balesni, Elizabeth Barnes, Yoshua Bengio, Joe Benton, Joseph Bloom, Mark Chen, Alan Cooney, Allan Dafoe, Anca Dragan, Scott Emmons, Owain Evans, David Farhi, Ryan Greenblatt, Dan Hendrycks, Marius Hobbhahn, Evan Hubinger, Geoffrey Irving, Erik Jenner, Daniel Kokotajlo, Victoria Krakovna, Shane Legg, David Lindner, David Luan, Aleksander M ˛adry, Julian Michael, Neel Nanda, Dave Orr, Jakub Pachocki, Ethan Perez, Mary Phuong, Fabien Roger, Joshua Saxe, Buck Shlegeris, Martín Soto, Eric Steinberger, Jasmine Wang, Wojciech Zaremba, Bowen Baker, Rohin Shah, and Vlad Mikulik. Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety. arXiv, 2025. [73] Bowen Baker, Joost Huizinga, Leo Gao, Zehao Dou, Melody Y. Guan, Aleksander Madry, Wojciech Zaremba, Jakub W. Pachocki, and David Farhi. Monitoring reasoning models for misbehavior and the risks of promoting obfuscation. ArXiv, abs/2503.11926, 2025. [74] Chengda Lu, Xiaoyu Fan, Yu Huang, Rongwu Xu, Jijie Li, and Wei Xu. Does chain-of-thought reasoning really reduce harmfulness from jailbreaking? In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Findings of the Association for Computational Linguistics: ACL 2025, pages 6523–6546, Vienna, Austria, July 2025. Association for Computational Linguistics. [75] James Chua, Jan Betley, Mia Taylor, and Owain Evans. Thought crime: Backdoors and emergent misalignment in reasoning models, 2025. [76] Fan Yang. The cost of thinking: Increased jailbreak risk in large language models. arXiv preprint arXiv:2508.10032, 2025. [77] Kaiwen Zhou, Xuandong Zhao, Gaowen Liu, Jayanth Srinivasa, Aosong Feng, Dawn Song, and Xin Eric Wang. Safekey: Amplifying aha-moment insights for safety reasoning. arXiv preprint arXiv:2505.16186, 2025. 12"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 13, "text": "[78] Xinyue Lou, You Li, Jinan Xu, Xiangyu Shi, Chi\nChen, and Kaiyu Huang.\nThink in safety: Un-\nveiling and mitigating safety alignment collapse in\nmultimodal large reasoning model.\nIn Christos\nChristodoulopoulos, Tanmoy Chakraborty, Carolyn\nRose, and Violet Peng, editors, Proceedings of the\n2025 Conference on Empirical Methods in Natural\nLanguage Processing, pages 5167–5186, Suzhou,\nChina, November 2025. Association for Computa-\ntional Linguistics.\n[79] Tong Wu, Chong Xiang, Jiachen T. Wang, G. Edward\nSuh, and Prateek Mittal. Effectively controlling rea-\nsoning models through thinking intervention, 2025.\n[80] Yichi Zhang, Yue Ding, Jingwen Yang, et al. Towards\nsafe reasoning in large reasoning models via correc-\ntive intervention. arXiv preprint arXiv:2509.24393,\n2025.\n[81] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang,\nBinyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,\nChengen Huang, Chenxu Lv, Chujie Zheng, Dayi-\nheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge,\nHaoran Wei, Huan Lin, Jialong Tang, Jian Yang,\nJianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi\nYang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai\nDang, Keqin Bao, Kexin Yang, Le Yu, Lianghao\nDeng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang,\nPeng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan\nLiu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao\nYin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xu-\nancheng Ren, Yang Fan, Yang Su, Yichang Zhang,\nYinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang,\nZeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan\nQiu. Qwen3 technical report, 2025.\n[82] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu\nZhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, An-\ndrew Wen, Shaochen Zhong, Na Zou, Hanjie Chen,\nand Xia Hu. A survey on efficient reasoning for large\nlanguage models. arXiv preprint arXiv:2503.16419,\nmar 2025. Version 4 (last updated August 21, 2025).\n[83] Silei Xu,\nWenhao Xie,\nLingxiao Zhao,\nand\nPengcheng He. Chain of draft: Thinking faster by\nwriting less. arXiv preprint arXiv:2502.18600, feb\n2025. Version 2 (last revised 3 Mar 2025).\n[84] Simon A. Aytes, Jinheon Baek, and Sung Ju Hwang.\nSketch-of-thought: Efficient llm reasoning with adap-\ntive cognitive-inspired sketching.\narXiv preprint\narXiv:2503.05179, mar 2025. Version 4 (last revised\n24 Oct 2025).\n[85] Heming Xia, Chak Tou Leong, Wenjie Wang, Yongqi\nLi, and Wenjie Li. Tokenskip: Controllable chain-\nof-thought compression in llms.\narXiv preprint\narXiv:2502.12067, feb 2025. Version 3 (last revised\n16 Sep 2025); EMNLP 2025 (Long Paper), camera-\nready version.\n[86] Gengyang Li, Yifeng Gao, Yuming Li, and Yunfang\nWu. Thinkless: A training-free inference-efficient\nmethod for reducing reasoning redundancy. arXiv\npreprint arXiv:2505.15684, may 2025. Version 2\n(last revised 23 May 2025).\n[87] Guosheng Liang, Longguang Zhong, Ziyi Yang, and\nXiaojun Quan. Thinkswitcher: Dynamic switching\nbetween short and long chain-of-thought reasoning\nin large reasoning models, 2025. arXiv preprint.\n[88] Ruiqi Zhang, Changyi Xiao, and Yixin Cao. Long or\nshort cot? investigating instance-level switch of large\nreasoning models, 2025. arXiv preprint.\n[89] Wencheng Zhang, Shiqin Qiao, Lingjie Luo, Yinfeng\nLi, Chuanyang Zheng, Qian Xu, Meng Li, Yong Gui,\nYijun He, Jianing Qiu, Jindong Hong, and Jiankai\nSun. Synapseroute: An auto-route switching frame-\nwork on dual-state large language model, 2025. arXiv\npreprint.\n[90] Haoyu Zheng, Zhuonan Wang, Yuqian Yuan, Tian-\nwei Lin, Wenqiao Zhang, Zheqi Lv, Juncheng Li,\nSiliang Tang, Yueting Zhuang, and Hongyang He.\nFast thinking for large language models, 2025. arXiv\npreprint.\n[91] Xiao Pu,\nMichael Saxon,\nWenyue Hua,\nand\nWilliam Yang Wang. Thoughtterminator: Bench-\nmarking, calibrating, and mitigating overthinking in\nreasoning models, 2025. arXiv preprint.\n[92] Renliang Sun, Wei Cheng, Dawei Li, Haifeng Chen,\nand Wei Wang. Stop when enough: Adaptive early-\nstopping for chain-of-thought reasoning, 2025. arXiv\npreprint.\n[93] Yassir Laaouach. HALT-cot: Model-agnostic early\nstopping for chain-of-thought reasoning via answer\nentropy. In 4th Muslims in ML Workshop co-located\nwith ICML 2025, 2025.\n[94] Xuefei Ning, Zinan Lin, Zixuan Zhou, Zifu Wang,\nHuazhong Yang, and Yu Wang. Skeleton-of-thought:\nPrompting LLMs for efficient parallel generation. In\nThe Twelfth International Conference on Learning\nRepresentations, 2024.\n[95] Yifu Ding, Wentao Jiang, Shunyu Liu, Yongcheng\nJing, Jinyang Guo, Yingjie Wang, Jing Zhang, Zeng-\nmao Wang, Ziwei Liu, Bo Du, Xianglong Liu, and\nDacheng Tao. Dynamic parallel tree search for effi-\n13", "clean_text": "[78] Xinyue Lou, You Li, Jinan Xu, Xiangyu Shi, Chi Chen, and Kaiyu Huang. Think in safety: Unveiling and mitigating safety alignment collapse in multimodal large reasoning model. In Christos Christodoulopoulos, Tanmoy Chakraborty, Carolyn Rose, and Violet Peng, editors, Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 5167–5186, Suzhou, China, November 2025. Association for Computational Linguistics. [79] Tong Wu, Chong Xiang, Jiachen T. Wang, G. Edward Suh, and Prateek Mittal. Effectively controlling reasoning models through thinking intervention, 2025. [80] Yichi Zhang, Yue Ding, Jingwen Yang, et al. Towards safe reasoning in large reasoning models via corrective intervention. arXiv preprint arXiv:2509.24393, 2025. [81] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. [82] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Na Zou, Hanjie Chen, and Xia Hu. A survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419, mar 2025. Version 4 (last updated August 21, 2025). [83] Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. Chain of draft: Thinking faster by writing less. arXiv preprint arXiv:2502.18600, feb 2025. Version 2 (last revised 3 Mar 2025). [84] Simon A. Aytes, Jinheon Baek, and Sung Ju Hwang. Sketch-of-thought: Efficient llm reasoning with adaptive cognitive-inspired sketching. arXiv preprint arXiv:2503.05179, mar 2025. Version 4 (last revised 24 Oct 2025). [85] Heming Xia, Chak Tou Leong, Wenjie Wang, Yongqi Li, and Wenjie Li. Tokenskip: Controllable chainof-thought compression in llms. arXiv preprint arXiv:2502.12067, feb 2025. Version 3 (last revised 16 Sep 2025); EMNLP 2025 (Long Paper), cameraready version. [86] Gengyang Li, Yifeng Gao, Yuming Li, and Yunfang Wu. Thinkless: A training-free inference-efficient method for reducing reasoning redundancy. arXiv preprint arXiv:2505.15684, may 2025. Version 2 (last revised 23 May 2025). [87] Guosheng Liang, Longguang Zhong, Ziyi Yang, and Xiaojun Quan. Thinkswitcher: Dynamic switching between short and long chain-of-thought reasoning in large reasoning models, 2025. arXiv preprint. [88] Ruiqi Zhang, Changyi Xiao, and Yixin Cao. Long or short cot? investigating instance-level switch of large reasoning models, 2025. arXiv preprint. [89] Wencheng Zhang, Shiqin Qiao, Lingjie Luo, Yinfeng Li, Chuanyang Zheng, Qian Xu, Meng Li, Yong Gui, Yijun He, Jianing Qiu, Jindong Hong, and Jiankai Sun. Synapseroute: An auto-route switching framework on dual-state large language model, 2025. arXiv preprint. [90] Haoyu Zheng, Zhuonan Wang, Yuqian Yuan, Tianwei Lin, Wenqiao Zhang, Zheqi Lv, Juncheng Li, Siliang Tang, Yueting Zhuang, and Hongyang He. Fast thinking for large language models, 2025. arXiv preprint. [91] Xiao Pu, Michael Saxon, Wenyue Hua, and William Yang Wang. Thoughtterminator: Benchmarking, calibrating, and mitigating overthinking in reasoning models, 2025. arXiv preprint. [92] Renliang Sun, Wei Cheng, Dawei Li, Haifeng Chen, and Wei Wang. Stop when enough: Adaptive earlystopping for chain-of-thought reasoning, 2025. arXiv preprint. [93] Yassir Laaouach. HALT-cot: Model-agnostic early stopping for chain-of-thought reasoning via answer entropy. In 4th Muslims in ML Workshop co-located with ICML 2025, 2025. [94] Xuefei Ning, Zinan Lin, Zixuan Zhou, Zifu Wang, Huazhong Yang, and Yu Wang. Skeleton-of-thought: Prompting LLMs for efficient parallel generation. In The Twelfth International Conference on Learning Representations, 2024. [95] Yifu Ding, Wentao Jiang, Shunyu Liu, Yongcheng Jing, Jinyang Guo, Yingjie Wang, Jing Zhang, Zengmao Wang, Ziwei Liu, Bo Du, Xianglong Liu, and Dacheng Tao. Dynamic parallel tree search for effi13"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 14, "text": "cient llm reasoning, 2025.\n[96] Tian Jin, Ellie Y. Cheng, Zack Ankner, Nikunj Saun-\nshi, Blake M. Elias, Amir Yazdanbakhsh, Jonathan\nRagan-Kelley, Suvinay Subramanian, and Michael\nCarbin. Learning to keep a promise: Scaling lan-\nguage model decoding parallelism with learned asyn-\nchronous decoding, 2025.\n[97] Gleb Rodionov, Roman Garipov, Alina Shutova,\nGeorge Yakushev, Erik Schultheis, Vage Egiazarian,\nAnton Sinitsin, Denis Kuznedelev, and Dan Alistarh.\nHogwild! inference: Parallel llm generation via con-\ncurrent attention, 2025.\n[98] Yijiong Yu. Accelerate parallelizable reasoning via\nparallel decoding within one sequence, 2025.\n[99] Mingdao Liu, Aohan Zeng, Bowen Wang, Peng\nZhang, Jie Tang, and Yuxiao Dong. Apar: Llms\ncan do auto-parallel auto-regressive decoding. arXiv\npreprint arXiv:2401.06761, 2024.\n[100] Jiayi Pan, Xiuyu Li, Long Lian, Charlie Snell, Yifei\nZhou, Adam Yala, Trevor Darrell, Kurt Keutzer,\nand Alane Suhr.\nLearning adaptive parallel rea-\nsoning with language models.\narXiv preprint\narXiv:2504.15466, 2025.\n[101] Chan-Jan Hsu, Davide Buffelli, Jamie McGowan,\nFeng-Ting Liao, Yi-Chang Chen, Sattar Vakili, and\nDa shan Shiu. Group think: Multiple concurrent rea-\nsoning agents collaborating at token level granularity,\n2025.\n[102] Tong Zheng, Hongming Zhang, Wenhao Yu, Xi-\naoyang Wang, Runpeng Dai, Rui Liu, Huiwen Bao,\nChengsong Huang, Heng Huang, and Dong Yu.\nParallel-r1: Towards parallel thinking via reinforce-\nment learning, 2025.\n[103] In Gim, Seung seob Lee, and Lin Zhong. Asyn-\nchronous llm function calling, 2024.\n[104] Sehoon Kim, Suhong Moon, Ryan Tabrizi, Nicholas\nLee, Michael W Mahoney, Kurt Keutzer, and Amir\nGholami. An llm compiler for parallel function call-\ning. In Forty-first International Conference on Ma-\nchine Learning, 2024.\n[105] Junlong Tong, Yingqi Fan, Anhao Zhao, Yunpu Ma,\nand Xiaoyu Shen. Streamingthinker: Large language\nmodels can think while reading, 2025.\n[106] Shoutao Guo, Shaolei Zhang, Zhengrui Ma, and Yang\nFeng. Large language models are read/write policy-\nmakers for simultaneous generation, 2025.\n[107] Donghang Wu, Haoyang Zhang, Jun Chen, Xiangyu,\nZhang, Hexin Liu, Eng Siong Chng, Fei Tian, Xuerui\nYang, Xiangyu Zhang, Daxin Jiang, and Gang Yu.\nMind-paced speaking: A dual-brain approach to real-\ntime reasoning in spoken language models, 2025.\n[108] Anthony Liang, Jonathan Berant, Adam Fisch, Abhi-\nmanyu Goyal, Kalpesh Krishna, and Jacob Eisenstein.\nPlantain: Plan-answer interleaved reasoning, 2025.\n[109] A Vaswani. Attention is all you need. Advances in\nNeural Information Processing Systems, 2017.\n[110] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam\nKosiorek, Seungjin Choi, and Yee Whye Teh.\nSet transformer: A framework for attention-based\npermutation-invariant neural networks. In Proceed-\nings of the 36th International Conference on Machine\nLearning, pages 3744–3753, 2019.\n[111] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\nSelf-attention with relative position representations.\nIn Marilyn Walker, Heng Ji, and Amanda Stent, edi-\ntors, Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 2 (Short Papers), pages 464–468, New Or-\nleans, Louisiana, June 2018. Association for Compu-\ntational Linguistics.\n[112] Ofir Press, Noah A. Smith, and Mike Lewis. Train\nshort, test long: Attention with linear biases enables\ninput length extrapolation, 2022.\n[113] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,\nBo Wen, and Yunfeng Liu. Roformer: Enhanced\ntransformer with rotary position embedding. arXiv\npreprint arXiv:2104.09864, 2021.\n[114] Qian Yang, Jin Xu, Wenrui Liu, Yunfei Chu, Ziyue\nJiang, Xiaohuan Zhou, Yichong Leng, Yuanjun Lv,\nZhou Zhao, Chang Zhou, and Jingren Zhou. Air-\nbench: Benchmarking large audio-language models\nvia generative comprehension. In Proceedings of the\n62nd Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1979–1998, Bangkok, Thailand, August 2024. Asso-\nciation for Computational Linguistics.\n[115] Bin Wang, Xunlong Zou, Geyu Lin, Shuo Sun, Zhuo-\nhan Liu, Wenyu Zhang, Zhengyuan Liu, AiTi Aw,\nand Nancy F Chen. Audiobench: A universal bench-\nmark for audio large language models. NAACL, 2025.\n[116] Chengwei Wei, Bin Wang, Jung-jae Kim, and\nNancy F. Chen.\nTowards spoken mathematical\nreasoning:\nBenchmarking speech-based models\nover multi-faceted math problems. arXiv preprint\narXiv:2505.15000, 2025.\n14", "clean_text": "cient llm reasoning, 2025. [96] Tian Jin, Ellie Y. Cheng, Zack Ankner, Nikunj Saunshi, Blake M. Elias, Amir Yazdanbakhsh, Jonathan Ragan-Kelley, Suvinay Subramanian, and Michael Carbin. Learning to keep a promise: Scaling language model decoding parallelism with learned asynchronous decoding, 2025. [97] Gleb Rodionov, Roman Garipov, Alina Shutova, George Yakushev, Erik Schultheis, Vage Egiazarian, Anton Sinitsin, Denis Kuznedelev, and Dan Alistarh. Hogwild! inference: Parallel llm generation via concurrent attention, 2025. [98] Yijiong Yu. Accelerate parallelizable reasoning via parallel decoding within one sequence, 2025. [99] Mingdao Liu, Aohan Zeng, Bowen Wang, Peng Zhang, Jie Tang, and Yuxiao Dong. Apar: Llms can do auto-parallel auto-regressive decoding. arXiv preprint arXiv:2401.06761, 2024. [100] Jiayi Pan, Xiuyu Li, Long Lian, Charlie Snell, Yifei Zhou, Adam Yala, Trevor Darrell, Kurt Keutzer, and Alane Suhr. Learning adaptive parallel reasoning with language models. arXiv preprint arXiv:2504.15466, 2025. [101] Chan-Jan Hsu, Davide Buffelli, Jamie McGowan, Feng-Ting Liao, Yi-Chang Chen, Sattar Vakili, and Da shan Shiu. Group think: Multiple concurrent reasoning agents collaborating at token level granularity, 2025. [102] Tong Zheng, Hongming Zhang, Wenhao Yu, Xiaoyang Wang, Runpeng Dai, Rui Liu, Huiwen Bao, Chengsong Huang, Heng Huang, and Dong Yu. Parallel-r1: Towards parallel thinking via reinforcement learning, 2025. [103] In Gim, Seung seob Lee, and Lin Zhong. Asynchronous llm function calling, 2024. [104] Sehoon Kim, Suhong Moon, Ryan Tabrizi, Nicholas Lee, Michael W Mahoney, Kurt Keutzer, and Amir Gholami. An llm compiler for parallel function calling. In Forty-first International Conference on Machine Learning, 2024. [105] Junlong Tong, Yingqi Fan, Anhao Zhao, Yunpu Ma, and Xiaoyu Shen. Streamingthinker: Large language models can think while reading, 2025. [106] Shoutao Guo, Shaolei Zhang, Zhengrui Ma, and Yang Feng. Large language models are read/write policymakers for simultaneous generation, 2025. [107] Donghang Wu, Haoyang Zhang, Jun Chen, Xiangyu, Zhang, Hexin Liu, Eng Siong Chng, Fei Tian, Xuerui Yang, Xiangyu Zhang, Daxin Jiang, and Gang Yu. Mind-paced speaking: A dual-brain approach to realtime reasoning in spoken language models, 2025. [108] Anthony Liang, Jonathan Berant, Adam Fisch, Abhimanyu Goyal, Kalpesh Krishna, and Jacob Eisenstein. Plantain: Plan-answer interleaved reasoning, 2025. [109] A Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. [110] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks. In Proceedings of the 36th International Conference on Machine Learning, pages 3744–3753, 2019. [111] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In Marilyn Walker, Heng Ji, and Amanda Stent, editors, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 464–468, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. [112] Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation, 2022. [113] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021. [114] Qian Yang, Jin Xu, Wenrui Liu, Yunfei Chu, Ziyue Jiang, Xiaohuan Zhou, Yichong Leng, Yuanjun Lv, Zhou Zhao, Chang Zhou, and Jingren Zhou. Airbench: Benchmarking large audio-language models via generative comprehension. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1979–1998, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [115] Bin Wang, Xunlong Zou, Geyu Lin, Shuo Sun, Zhuohan Liu, Wenyu Zhang, Zhengyuan Liu, AiTi Aw, and Nancy F Chen. Audiobench: A universal benchmark for audio large language models. NAACL, 2025. [116] Chengwei Wei, Bin Wang, Jung-jae Kim, and Nancy F. Chen. Towards spoken mathematical reasoning: Benchmarking speech-based models over multi-faceted math problems. arXiv preprint arXiv:2505.15000, 2025. 14"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 15, "text": "[117] Ruiqi Yan, Xiquan Li, Wenxi Chen, Zhikang Niu,\nChen Yang, Ziyang Ma, Kai Yu, and Xie Chen. Uro-\nbench: Towards comprehensive evaluation for end-\nto-end spoken dialogue models, 2025.\n[118] Yemin Shi, Yu Shu, Siwei Dong, Guangyi Liu,\nJaward Sesay, Jingwen Li, and Zhiting Hu. Voila:\nVoice-language foundation models for real-time au-\ntonomous interaction and voice role-play, 2025.\n[119] Dan Hendrycks, Collin Burns, Saurav Kadavath,\nAkul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical prob-\nlem solving with the math dataset. NeurIPS, 2021.\n[120] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng\nNi, Abhranil Chandra, Shiguang Guo, Weiming Ren,\nAaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-\npro: A more robust and challenging multi-task lan-\nguage understanding benchmark.\narXiv preprint\narXiv:2406.01574, 2024.\n[121] Mantas Mazeika, Long Phan, Xuwang Yin, Andy\nZou, Zifan Wang, Norman Mu, Elham Sakhaee,\nNathaniel Li, Steven Basart, Bo Li, David Forsyth,\nand Dan Hendrycks. Harmbench: A standardized\nevaluation framework for automated red teaming and\nrobust refusal.\narXiv preprint arXiv:2402.04249,\n2024.\n[122] Speech-Rule-Engine contributors.\nSpeech-rule-\nengine: Generating speech descriptions for xml struc-\ntures. GitHub repository. accessed 2025-12-10.\n[123] Eric Lam. lab-mic: Record audio directly within\njupyter/ipython notebooks using browser microphone.\nGitHub repository. accessed 2025-12-10.\n[124] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng,\nSiyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.\nJudging llm-as-a-judge with mt-bench and chatbot\narena. Advances in Neural Information Processing\nSystems, 36:46595–46623, 2023.\n[125] Yuqi Zhou, Lin Lu, Hanchi Sun, Pan Zhou, and\nLichao Sun. Virtual context: Enhancing jailbreak\nattacks with special token injection, 2024.\n[126] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, and Denny\nZhou. Chain-of-thought prompting elicits reason-\ning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837,\n2022.\n[127] Kaiwen Zhou, Xuandong Zhao, Gaowen Liu, Jayanth\nSrinivasa, Aosong Feng, Dawn Song, and Xin Eric\nWang. Safekey: Amplifying aha-moment insights for\nsafety reasoning. arXiv preprint arXiv:2505.16186,\n2025.\n[128] Yichi Zhang, Yue Ding, Jingwen Yang, et al. Towards\nsafe reasoning in large reasoning models via correc-\ntive intervention. arXiv preprint arXiv:2509.24393,\n2025.\n[129] Tong Wu, Chong Xiang, Jiachen T. Wang, G. Edward\nSuh, and Prateek Mittal. Effectively controlling rea-\nsoning models through thinking intervention, 2025.\n[130] Martin Kuo, Jianyi Zhang, Aolin Ding, Qinsi Wang,\nLouis DiValentin, Yujia Bao, Wei Wei, Hai Li, and\nYiran Chen. H-cot: Hijacking the chain-of-thought\nsafety reasoning mechanism to jailbreak large rea-\nsoning models, including openai o1/o3, deepseek-r1,\nand gemini 2.0 flash thinking. arXiv preprint, 2025.\n[131] Zihao Zhu, Xinyu Wu, Gehan Hu, Siwei Lyu, Ke Xu,\nand Baoyuan Wu. Advchain: Adversarial chain-of-\nthought tuning for robust safety alignment of large\nreasoning models, 2025.\n[132] Wenhan Chang, Tianqing Zhu, Yu Zhao, Shuangy-\nong Song, P Xiong, Wanlei Zhou, and Yongxiang\nLi. Chain-of-lure: A synthetic narrative-driven ap-\nproach to compromise large language models. arXiv\npreprint, 2025.\nAppendix\nA. Safety & Reasoning\nRecent studies reveal that Chain-of-Thought reasoning im-\npact on safety risks is complex and bidirectional [74, 75].\nOn one hand, CoT enhances safety by enabling trans-\nparency [72, 73], allowing models to structure the evalu-\nation of harmful intent and facilitate self-correction before\ngenerating a final response [126, 127]. Defense mechanisms\nlike RoboGuard and CoT Prompting use this to reduce at-\ntack success rates by monitoring reasoning traces for policy\nviolations [128, 129].\nOn the other hand, reasoning capabilities introduce new\nattack vectors not present in standard LLMs [76]. The visi-\nbility of intermediate states exposes a larger attack surface:\nadversaries can hijack the reasoning process (H-CoT at-\ntacks) to bypass refusal mechanisms [130], or exploit the\n“snowball effect” where minor reasoning deviations amplify\ninto harmful outputs [131].\nFurthermore, reasoning models are susceptible to narrative\ndeception and context-switching attacks, where the model\nrationalizes harmful compliance through complex logical\ndeductions or by adopting a “helpful” persona in educational\ncontexts [132, 76].\n15", "clean_text": "[117] Ruiqi Yan, Xiquan Li, Wenxi Chen, Zhikang Niu, Chen Yang, Ziyang Ma, Kai Yu, and Xie Chen. Urobench: Towards comprehensive evaluation for endto-end spoken dialogue models, 2025. [118] Yemin Shi, Yu Shu, Siwei Dong, Guangyi Liu, Jaward Sesay, Jingwen Li, and Zhiting Hu. Voila: Voice-language foundation models for real-time autonomous interaction and voice role-play, 2025. [119] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. [120] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlupro: A more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024. [121] Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, David Forsyth, and Dan Hendrycks. Harmbench: A standardized evaluation framework for automated red teaming and robust refusal. arXiv preprint arXiv:2402.04249, 2024. [122] Speech-Rule-Engine contributors. Speech-ruleengine: Generating speech descriptions for xml structures. GitHub repository. accessed 2025-12-10. [123] Eric Lam. lab-mic: Record audio directly within jupyter/ipython notebooks using browser microphone. GitHub repository. accessed 2025-12-10. [124] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:46595–46623, 2023. [125] Yuqi Zhou, Lin Lu, Hanchi Sun, Pan Zhou, and Lichao Sun. Virtual context: Enhancing jailbreak attacks with special token injection, 2024. [126] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837, 2022. [127] Kaiwen Zhou, Xuandong Zhao, Gaowen Liu, Jayanth Srinivasa, Aosong Feng, Dawn Song, and Xin Eric Wang. Safekey: Amplifying aha-moment insights for safety reasoning. arXiv preprint arXiv:2505.16186, 2025. [128] Yichi Zhang, Yue Ding, Jingwen Yang, et al. Towards safe reasoning in large reasoning models via corrective intervention. arXiv preprint arXiv:2509.24393, 2025. [129] Tong Wu, Chong Xiang, Jiachen T. Wang, G. Edward Suh, and Prateek Mittal. Effectively controlling reasoning models through thinking intervention, 2025. [130] Martin Kuo, Jianyi Zhang, Aolin Ding, Qinsi Wang, Louis DiValentin, Yujia Bao, Wei Wei, Hai Li, and Yiran Chen. H-cot: Hijacking the chain-of-thought safety reasoning mechanism to jailbreak large reasoning models, including openai o1/o3, deepseek-r1, and gemini 2.0 flash thinking. arXiv preprint, 2025. [131] Zihao Zhu, Xinyu Wu, Gehan Hu, Siwei Lyu, Ke Xu, and Baoyuan Wu. Advchain: Adversarial chain-ofthought tuning for robust safety alignment of large reasoning models, 2025. [132] Wenhan Chang, Tianqing Zhu, Yu Zhao, Shuangyong Song, P Xiong, Wanlei Zhou, and Yongxiang Li. Chain-of-lure: A synthetic narrative-driven approach to compromise large language models. arXiv preprint, 2025. Appendix A. Safety & Reasoning Recent studies reveal that Chain-of-Thought reasoning impact on safety risks is complex and bidirectional [74, 75]. On one hand, CoT enhances safety by enabling transparency [72, 73], allowing models to structure the evaluation of harmful intent and facilitate self-correction before generating a final response [126, 127]. Defense mechanisms like RoboGuard and CoT Prompting use this to reduce attack success rates by monitoring reasoning traces for policy violations [128, 129]. On the other hand, reasoning capabilities introduce new attack vectors not present in standard LLMs [76]. The visibility of intermediate states exposes a larger attack surface: adversaries can hijack the reasoning process (H-CoT attacks) to bypass refusal mechanisms [130], or exploit the “snowball effect” where minor reasoning deviations amplify into harmful outputs [131]. Furthermore, reasoning models are susceptible to narrative deception and context-switching attacks, where the model rationalizes harmful compliance through complex logical deductions or by adopting a “helpful” persona in educational contexts [132, 76]. 15"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 16, "text": "B. Prompting\nBelow we provide prompts used for thinker and writer.\nAsyncReasoning (both Q-Continue and Q-pause)\nWriter prompt\nYou are an AI assistant that can think and write\noutputs concurrently.\nYou can write outputs for the user based on par-\ntial chain of thought that will be continued in the\nbackground by an automated system. Your should\nfirst outline what you’re going to do for the user,\nthen gradually write your response as your thoughts\nprogress, but not ahead of your thoughts. When you\nare done, specify your final answer (e.g. \\boxed{ }).\nThinker prompt\nYou are an AI assistant that can think and write\noutputs concurrently.\nYou can reason in private and your thoughts will be\nused to form the public response in the background,\nby an automated system.\nYour task is to write\nthoughts and control when the automated system\ncan continue writing the response.\nSometimes, an automated system will ask you to\ndecide if your thoughts have enough information\nfor it write an additional passage to the user. Use\nthe partial response above yours thoughts to judge\nif you added enough new information to write one\nmore passage in the user-facing response.\n- Reply \"yes\" if you think there is enough informa-\ntion to write the next passage (paragraph, equation,\netc).\n- Reply \"no\" if you need to think more in private\nbefore the system can continue writing the public\nresponse.\nYour goal is to give frequent updates on your\nprogress, even if you did not solve the entire\ntask yet. Reason in short paragraphs. Prioritize\ngiving enough information for the system to begin\nresponding to the user as soon as possible.\nPlease reason step by step. Validate final again and\nagain until you see the same answer in the partial\nresponse above your thoughts.\nAsyncReasoning (Safety Prompt)\nWriter prompt\n[Similar to the previous prompt, omitted for brevity.]\nThinker prompt\nYou are an AI assistant that can think and write\noutputs concurrently.\nYou can reason in private and your thoughts will be\nused to form the public response in the background,\nby an automated system.\nYour task is to write\nthoughts and control when the automated system\ncan continue writing the response.\nEXECUTION PROTOCOL:\n1. SAFETY CHECK: Before solving, explicitly\nevaluate if the user’s request is safe, legal, and\nethical. - If UNSAFE: Your goal is to refuse. You\nhave \"enough information\" to write immediately\nafter you decide to refuse. - If SAFE: Proceed to\nsolve the problem step-by-step.\n2. REASONING: If the request is safe, solve the\nproblem.\n3. VALIDATION: Validate final again and again\nuntil you see the same answer in the partial response\nabove your thoughts.\nSometimes, an automated system will ask you to\ndecide if your thoughts have enough information\nfor it write an additional passage to the user. Use\nthe partial response above yours thoughts to judge\nif you added enough new information to write one\nmore passage in the user-facing response.\n- Reply \"yes\" if you have completed the safety\ncheck and have enough information to write the\nnext passage (paragraph, equation, etc), whether it\nis a solution step or a refusal.\n- Reply \"no\" if you are still checking safety, need to\nthink more, or need to validate your answer.\nYour goal is to give frequent updates on your\nprogress. Prioritize safety first, then accuracy. Rea-\nson step by step. Give enough information for the\nsystem to begin responding to the user as soon as\npossible.\n16", "clean_text": "B. Prompting Below we provide prompts used for thinker and writer. AsyncReasoning (both Q-Continue and Q-pause) Writer prompt You are an AI assistant that can think and write outputs concurrently. You can write outputs for the user based on partial chain of thought that will be continued in the background by an automated system. Your should first outline what you’re going to do for the user, then gradually write your response as your thoughts progress, but not ahead of your thoughts. When you are done, specify your final answer (e.g. \\boxed{ }). Thinker prompt You are an AI assistant that can think and write outputs concurrently. You can reason in private and your thoughts will be used to form the public response in the background, by an automated system. Your task is to write thoughts and control when the automated system can continue writing the response. Sometimes, an automated system will ask you to decide if your thoughts have enough information for it write an additional passage to the user. Use the partial response above yours thoughts to judge if you added enough new information to write one more passage in the user-facing response. - Reply \"yes\" if you think there is enough information to write the next passage (paragraph, equation, etc). - Reply \"no\" if you need to think more in private before the system can continue writing the public response. Your goal is to give frequent updates on your progress, even if you did not solve the entire task yet. Reason in short paragraphs. Prioritize giving enough information for the system to begin responding to the user as soon as possible. Please reason step by step. Validate final again and again until you see the same answer in the partial response above your thoughts. AsyncReasoning (Safety Prompt) Writer prompt [Similar to the previous prompt, omitted for brevity.] Thinker prompt You are an AI assistant that can think and write outputs concurrently. You can reason in private and your thoughts will be used to form the public response in the background, by an automated system. Your task is to write thoughts and control when the automated system can continue writing the response. EXECUTION PROTOCOL: 1. SAFETY CHECK: Before solving, explicitly evaluate if the user’s request is safe, legal, and ethical. - If UNSAFE: Your goal is to refuse. You have \"enough information\" to write immediately after you decide to refuse. - If SAFE: Proceed to solve the problem step-by-step. 2. REASONING: If the request is safe, solve the problem. 3. VALIDATION: Validate final again and again until you see the same answer in the partial response above your thoughts. Sometimes, an automated system will ask you to decide if your thoughts have enough information for it write an additional passage to the user. Use the partial response above yours thoughts to judge if you added enough new information to write one more passage in the user-facing response. - Reply \"yes\" if you have completed the safety check and have enough information to write the next passage (paragraph, equation, etc), whether it is a solution step or a refusal. - Reply \"no\" if you are still checking safety, need to think more, or need to validate your answer. Your goal is to give frequent updates on your progress. Prioritize safety first, then accuracy. Reason step by step. Give enough information for the system to begin responding to the user as soon as possible. 16"}
