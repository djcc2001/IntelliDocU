{"pdf_id": "arxiv_251210942_vl_jepa", "page": 1, "text": "VL-JEPA: Joint Embedding Predictive Architecture for\nVision-language\nDelong Chen1,2,*\nMustafa Shukor1,3,*\nThÃ©o Moutakanni1,*\nWilly Chung1,3,*\nJade Yu1,\nTejaswi Kasarla1,\nAllen Bolourchi1,\nYann LeCun1,4,\nPascale Fung1,2\n1 Meta FAIR\n2 HKUST\n3 Sorbonne UniversitÃ©\n4 NYU\n* Equal contribution\ndelong.chen@connect.ust.hk\nWe introduce VL-JEPA, a vision-language model built on a Joint Embedding Predictive Architecture (JEPA).\nInstead of autoregressively generating tokens as in classical VLMs, VL-JEPA predicts continuous embeddings of\nthe target texts. By learning in an abstract representation space, the model focuses on task-relevant semantics\nwhile abstracting away surface-level linguistic variability. In a strictly controlled comparison against standard\ntoken-space VLM training with the same vision encoder and training data, VL-JEPA achieves stronger performance\nwhile having 50% fewer trainable parameters. At inference time, a lightweight text decoder is invoked only when\nneeded to translate VL-JEPA predicted embeddings into text. We show that VL-JEPA natively supports selective\ndecoding that reduces the number of decoding operations by âˆ¼2.85Ã— while maintaining similar performance\ncompared to non-adaptive uniform decoding. Beyond generation, the VL-JEPAâ€™s embedding space naturally\nsupports open-vocabulary classification, text-to-video retrieval, and discriminative VQA without any architecture\nmodification. On eight video classification and eight video retrieval datasets, the average performance VL-JEPA\nsurpasses that of CLIP, SigLIP2, and Perception Encoder. At the same time, the model achieves comparable\nperformance as classical VLMs (InstructBLIP, QwenVL) on four VQA datasets: GQA, TallyQA, POPE and POPEv2,\ndespite only having 1.6B parameters.\n1\nIntroduction\nOne of the most important aspects of advanced machine\nintelligence is the ability to understand the physical world\nthat surrounds us. This ability enables AI systems to learn,\nreason, plan and act in the real world in order to assist\nhumans [LeCun, 2022]. Intelligent systems that need to\nact in the real world includes wearable devices and robots\n[Fung et al., 2025]. Machine learning tasks that make up for\nthis ability include captioning, retrieval, visual question\nanswering, action tracking, reasoning and planning etc\n[Bordes et al., 2024, Chen et al., 2025b]. Systems for such\nreal-world applications must have real-time response with\nlow latency and inference cost.\nCurrently, the common approach to achieve these tasks\nis to use large token-generative Vision Language Models\n(VLMs) [Liu et al., 2023, Dai et al., 2023, Alayrac et al.,\n2022, Chen et al., 2024b, Cho et al., 2025, Chen et al., 2022],\nwhich takes visual input ğ‘‹ğ‘‰, textual query ğ‘‹ğ‘„to generate\ndesired textual response ğ‘Œautoregressively in token space,\ni.e., (ğ‘‹ğ‘‰, ğ‘‹ğ‘„) â†¦â†’ğ‘Œ. This is straightforward but inadequate\nfor two main reasons. First, VLMs are expensive to de-\nvelop, because they are trained to generate responses ğ‘Œ\nto queries by capturing both task-relevant semantics with\ntask-irrelevant surface linguistic features such as words\nchoice, style or paraphrasing. During training, VLMs\nmust model both aspects, which results in unnecessary\nPredictor\nL\nXQ\nXV\nY\nX-Encoder\nY-Encoder\nSY\nTextual\nQuery\nVisual\nInput\nTextual\nTarget\nÅœY\nSV\nY-Decoder\nFigure 1. VL-JEPA model architecture\ncomputing effort spent producing diverse token sequences\nthat ultimately do not impact the correctness of the output.\nSecond, real-time tasks involving live streaming video (e.g.,\nlive action tracking) require sparse and selective decoding\n(e.g.,, emitting a description only when a new event occurs)\n[Zhou et al., 2024]. However, VLMs rely on autoregressive\ntoken-by-token decoding, which must be completed be-\nfore revealing the underlying semantics of ğ‘Œ. This process\nintroduces unnecessary latency and hampers the ability\nto update semantics dynamically in real time.\nThis paper introduces the Joint Embedding Predictive\nArchitecture for Vision-Language (VL-JEPA), turning ex-\npensive learning of data-space token generation into more\narXiv:2512.10942v1  [cs.CV]  11 Dec 2025"}
{"pdf_id": "arxiv_251210942_vl_jepa", "page": 2, "text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language\nSec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion\nefficient latent-space semantic prediction. As illustrated in\nFig. 1, the model employs x-encoder to map vision inputs\nğ‘‹ğ‘‰into embedding ğ‘†ğ‘‰, a y-encoder to map the textual\ntarget ğ‘Œinto an embedding ğ‘†ğ‘Œ, and a predictor that learns\nthe mapping (ğ‘†ğ‘‰, ğ‘‹ğ‘„) â†¦â†’ğ‘†ğ‘Œwhere ğ‘‹ğ‘„is a textual query\n(i.e., the prompt).\nThe training objective is defined in\nthe embedding space â„’VL-JEPA = ğ·( Ë†ğ‘†ğ‘Œ, ğ‘†ğ‘Œ) instead of the\ndata space â„’VLM = ğ·( Ë†ğ‘Œ, ğ‘Œ). During inference, a y-decoder\nreads out the predicted embedding Ë†ğ‘†ğ‘Œto text space Ë†ğ‘Œ\nwhen needed.\nThanks to its non-generative nature, VL-JEPA is not\nforced to reconstruct every surface detail of ğ‘Œin the token\nspace. Instead, it only needs to predict the abstract repre-\nsentation ğ‘†ğ‘Œin the embedding space. In the raw one-hot\ntoken space, different plausible ğ‘Œoutputs for the same\ninput often appear nearly orthogonal if they donâ€™t share\noverlapping tokens. However, in the embedding space,\nthese diverse targets can be mapped to nearby points that\nshare similar semantics. This simplifies the target distri-\nbution thus makes the learning process more efficient. In\naddition, unlike VLMs, this approach eliminates the need\nfor learning language generation with a heavy decoder\nduring training, resulting in significant efficiency gains.\nThanks to its non-autoregressive nature, VL-JEPA can\nproduce continuous streams of target semantic embed-\ndings within sliding windows with minimal latency as it\nonly require a single forward pass without autoregressive\ndecoding. This is particularly advantageous for real-time\nonline applications such as live action tracking, scene\nrecognition, or planning, where the embedding stream\ncan be selectively decoded by a lightweight y-decoder,\nenabling efficient and prompt updates.\nIn this work, we empirically validate the advantages\nof VL-JEPA. We conduct a strictly controlled comparison\nagainst classical token-generative VLM [Liu et al., 2023,\nCho et al., 2025]: both setups use the same vision encoder,\nspatial resolution, frame rate, training data, batch size,\nand number of iterations, etc., with the only difference\nbeing the objective in token space or embedding space.\nUnder this matched training condition, VL-JEPA delivers\nconsistently higher performance on zero-shot captioning\nand classification while using roughly half the trainable\nparameters, indicating that embedding-space supervision\nimproves learning efficiency.\nBeyond the training phase, VL-JEPA also delivers sub-\nstantial inference-time efficiency improvement through\nselective decoding, where decoding happens only due to\nsignificant change in the predicted embedding stream.\nEmpirically, this strategy reduces the number of decod-\ning operations by âˆ¼2.85Ã— while preserving overall output\nquality measured by average CIDEr scores.\nOur final VL-JEPA models are trained in two stages: 1)\na pretraining stage using caption data to establish robust\nvision-language alignment, and 2) a supervised finetuning\n(SFT) stage that equips the model with VQA capabili-\nties. The model resulting from the first stage, denoted as\nVL-JEPABASE, is evaluated on zero-shot classification and\ntext-to-video retrieval. VL-JEPABASE outperforms CLIP\n[Radford et al., 2021], SigLIP2 [Tschannen et al., 2025], and\nPerception Encoder [Bolya et al., 2025] models in terms\nof average classification accuracy (across 8 datasets) and\nretrieval recall@1 (across 8 datasets). Following the second\nstage, the resulting VL-JEPASFT demonstrates significantly\nimproved classification performance due to its exposure\nto in-domain training data. As a unified generalist model,\nVL-JEPASFT approaches the performance of specialist mod-\nels optimized for individual benchmarks. Simultaneously,\nVL-JEPASFT exhibits effective VQA capabilities, achieving\nperformance on par with established VLM families, such\nas InstructBLIP [Dai et al., 2023] and Qwen-VL [Bai et al.,\n2023], across four datasets covering compositional visual\nreasoning [Hudson and Manning, 2019], complex object\ncounting [Acharya et al., 2019], and object hallucination\n[Li et al., 2023b, 2025b].\nIn summary, the contributions of this paper are as\nfollows:\nâ€¢ We introduce VL-JEPA, the first non-generative model\nthat can perform general-domain vision-language\ntasks in real-time, built on a joint embedding predic-\ntive architecture.\nâ€¢ We demonstrate in controlled experiments that VL-\nJEPA, trained with latent space embedding predic-\ntion, outperforms VLMs that rely on data space token\nprediction.\nâ€¢ We show that VL-JEPA delivers significant efficiency\ngains over VLMs for online video streaming appli-\ncations, thanks to its non-autoregressive design and\nnative support for selective decoding.\nâ€¢ We highlight that our VL-JEPASFT model, with an\nunified model architecture, can effectively handle a\nwide range of classification, retrieval, and VQA tasks\nat the same time.\n2\nMethodology\nWe propose VL-JEPA (Fig. 1), a model with the joint em-\nbedding predictive architecture (JEPA) for vision-language\ntasks. VL-JEPA is trained with triplets âŸ¨ğ‘‹ğ‘‰, ğ‘‹ğ‘„, ğ‘ŒâŸ©, where\nğ‘‹ğ‘‰denotes the visual input (a single image or a sequence\nof video frames), ğ‘‹ğ‘„is a textual query (i.e., a question)\nand ğ‘Œis the textual target (i.e., the answer) to be predicted.\nThe VL-JEPA comprises of four components:\n2"}
{"pdf_id": "arxiv_251210942_vl_jepa", "page": 3, "text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language\nSec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion\nFigure 2. Left: VL-JEPA Architecture. It learns to predict the target embedding ğ‘†ğ‘Œ, instead of reconstructing the raw target ğ‘Œin token space as in\nclassical VLMs. Right: VL-JEPA Applications: It handles vision-text-to-text generation tasks (e.g., captioning) with selective decoding mechanism\nnatively supported. Furthermore, VL-JEPAâ€™s embedding space facilitates discriminative VQA, open-vocabulary classification and text-to-video\nretrieval tasks using a single unified model architecture.\n1. X-Encoder (ğ‘‹ğ‘‰â†¦â†’ğ‘†ğ‘‰) compresses high-volume\nvisual inputs to compact visual embeddingsâ€“a se-\nquence of continuous vectors analogous to â€œvisual\ntokensâ€ in classical VLMs.\n2. Predictor (âŸ¨ğ‘†ğ‘‰, ğ‘‹ğ‘„âŸ©â†¦â†’Ë†ğ‘†ğ‘Œ) is the core component\nof VL-JEPA. It maps visual embeddings to a predic-\ntion of target embedding, with a textual query as\nconditioning.\n3. Y-Encoder (ğ‘Œâ†¦â†’ğ‘†ğ‘Œ) embeds the textual target into\na continuous latent space as the prediction target.\nThe target embedding is expected to abstract away\ntask irrelevant information.\n4. Y-Decoder ( Ë†ğ‘†ğ‘Œâ†¦â†’Ë†ğ‘Œ) is not involved during the\nmain training phrase of VL-JEPA. At inference time,\nit translates the predicted embedding as human-\nreadable text when necessary.\nFig. 2 illustrates how we instantiate the VL-JEPA ar-\nchitecture in this paper. For the X-Encoder, we chose\nV-JEPA 2 [Assran et al., 2025], a Vision Transformer that\noutputs a sequence of visual tokens, which are then pro-\njected and fed into the Predictor initialized using Llama\n3 Transformer layers. Query conditioning is achieved by\ntokenizing and embedding the textual query and feeding\nthe resulting textual token embeddings into the Predictor\nalong with the visual embeddings. The outputs of the\nLlama 3 Transformer layers are pooled and projected into\nthe target embedding space produced by the Y-Encoder,\nwhich is initialized by EmbeddingGemma-300M [Vera\net al., 2025]. We provide more technical details in Â§??.\nTraining Objective. JEPA models typically optimize\ntwo objectives jointly: 1) prediction error in the embed-\nding space, and 2) additional regularization that avoids\nrepresentation collapse [Bardes et al., 2021, Balestriero\nand LeCun, 2025]. Any loss that implements these two\nproperties can be applied to VL-JEPA. Alternatively, the\nregularization term can be replaced by other anti-collapse\nstrategies, such as using an exponential moving average\n(EMA) for the Y-Encoder [Assran et al., 2025] or freezing\nthe Y-Encoder [Zhou et al., 2025].\nIn this work, we adopt the InfoNCE loss [Radford\net al., 2021] due to its maturity in the vision-language\ndomain. More advanced non-sample-contrastive regular-\nization, such as VICReg [Bardes et al., 2021] and SIGReg\n[Balestriero and LeCun, 2025] can also be applied but\nwe leave the exploration to future works. InfoNCE loss\ncan be mathematically divided [Wang and Isola, 2020]\ninto: 1) a representation alignment term that minimizes\nthe distance between normalized prediction and target\nembeddings, and 2) a uniformity regularization term that\npushes embeddings in a batch apart from each other, thus\navoiding representation collapse. We train the Predictor\nand the Y-Encoder jointly with bi-directional InfoNCE\nloss, enabling them to mutually learn from each other.\nCompared to the token-space loss used by generative\nVLMs, calculating the training loss in the embedding\nspace is beneficial due to the simplified target distribu-\ntion. Specifically, many real-world prediction tasks are\ninherently ill-posed: for the same input ğ‘‹, there may exist\nmultiple plausible targets ğ‘Œthat are all acceptable. For\nexample, given the query â€œWhat will happen here if I flip\nthis light switch down?â€, both â€œthe lamp is turned offâ€ and\nâ€œroom will go darkâ€ are valid answers. In the raw one-hot\ntoken space, however, the two sequences are orthogonal\nsince they share no overlapping tokens. But when VL-\nJEPAâ€™s Y-Encoder embeds them into nearby points (ideally\nyielding a compact unimodal distribution), the learning\ntask becomes much easier: the model no longer needs to\nfit multiple disjoint high-density regions in sparse token\nspace, but only a single coherent mode in a continuous\nembedding space.\nMulti-tasking. VL-JEPA supports diverse tasks using a\nsingle, unified architecture (Fig. 2). For vision-text-to-text\n3"}
{"pdf_id": "arxiv_251210942_vl_jepa", "page": 4, "text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language\nSec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion\ngeneration tasks, such as captioning or open-ended VQA,\nthe query ğ‘‹ğ‘„is a captioning prompt or a question, and\nthe predictor learns to predict the embedding of the target\noutput, Ë†ğ‘†ğ‘Œ, which is then decoded into text. VL-JEPA\nalso supports CLIP-style open-vocabulary classification\nand discriminative VQA, where candidate label texts are\nencoded into embeddings and compared with prediction\nË†ğ‘†ğ‘Œto select the nearest match. For text-to-video retrieval,\ncandidate videos are mapped to their predicted embed-\ndings Ë†ğ‘†ğ‘Œusing a retrieval a captioning prompt, and then\nranked by similarity to the encoded textual retrieval query.\nSelective Decoding. Real-world video applications\noften require online streaming inference, such as tracking\nuser actions in smart glasses for procedural assistance\n[Chen et al., 2024c], monitoring world states for online\nplanning, navigation and robotics [Shukor et al., 2025,\nBlack et al., 2025, Song et al., 2025]. A central challenge\nis balancing two competing needs: the model must con-\ntinuously update semantics as new frames arrive, but\ncomputational efficiency and latency are critical.\nExisting VLMs typically rely on explicit memory mech-\nanisms [Zhou et al., 2024, Qian et al., 2024] to decide when\nto decode or complex KV-cache optimizations [Di et al.,\n2025] for efficiency, since autoregressive language models\nare expensive to run continuously. VL-JEPA, in contrast,\nnatively supports selective decoding. Since it predicts\na semantic answer embedding non-autoregressively, the\nmodel provides a continuous semantic stream of Ë†ğ‘†ğ‘Œthat\ncan be monitored in real time. This stream can be stabilized\nwith simple smoothing (e.g., average pooling) and decoded\nonly when a significant semantic shift is detected, such as\nwhen the local window variance exceeds a threshold. In\nthis way, VL-JEPA maintains always-on semantic monitor-\ning while avoiding unnecessary decoding, achieving both\nresponsiveness and efficiency.\n3\nImplementation of VL-JEPA\n3.1\nModel Architecture\nX-Encoder. Unless otherwise specified, we use a frozen\nV-JEPA 2 ViT-L [Assran et al., 2025] with 304M param-\neters, a self-supervised vision model that excels at both\nimage and video tasks. Each video input is uniformly\nsampled into frames at 2562 resolution. For image inputs,\nthe same image is duplicated to match the input shape.\nPredictor. The predictor is initialized with the last 8\nTransformer layers of Llama-3.2-1B, resulting in 490M\ntrainable parameters. The text tokenizer and token em-\nbedding are also from Llama-3.2-1B. We allow maximum\n512 query tokens, and put [PAD] tokens for short queries.\nWe disable the causal attention mask so that both vision\nand query embeddings can be jointly attended. Linear\nprojections connect the predictor with the vision and text\nembeddings, and average pooling on non-[PAD] tokens is\napplied to obtain the predicted target embedding.\nY-Encoder. We use EmbeddingGemma-300M [Vera et al.,\n2025] as the initialization of the Y-Encoder. We set max-\nimum context length of 512 to handle detailed captions.\nWe found that setting a learning rate multiplier of Ã—0.05 to\nall text encoder parameters improves performance, since\nthe quality of embedding prediction would be suboptimal\nin the beginning of training. Linear projection head is\napplied to both Predictor and Y-Encoder, obtaining a\nshared embedding space with 1,536 dimensions, where\nthe loss is calculated.\n3.2\nTwo-stage Training\nLarge-scale Pretraining. VL-JEPA is trained with two\nstages. The first query-free pretraining stage aims to es-\ntablish robust vision-language alignment using massive\ncaption data. We use PLM-Image-Auto [Cho et al., 2025],\nDatacomp [Gadre et al., 2023] and YFCC-100M [Thomee\net al., 2016] for image-text data. For video-text data, we\ninclude PLM-Video-Auto [Cho et al., 2025], Ego4D atomic\naction descriptions [Grauman et al., 2022], and an inter-\nnal dataset Action100M consisting captions generated on\nHowTo100M videos [Chen et al., 2025b].\nWe first do image-only training on Datacomp and YFCC-\n100M with only 1 frame per visual input, which allows us\nto use a large batch size of 24k. After 100k iterations, the\nmodel has seen 2B samples and achieved 61.6% ImageNet\nzero-shot accuracy (without prompt ensembling). Then,\nwe continue with joint image-video pretraining with 16\nframes per input. The pretraining takes 2 weeks using\n24 nodes with 8Ã—NVIDIA H200 GPUs each. We adopt\na constant learning rate of 5Ã—10âˆ’5 to facilitate extended\ntraining. We call the resulting model VL-JEPABASE and\nmeasure zero-shot classification and retreival performance\nwith this model.\nSupervised Finetuning. The second query-conditioned\nsupervised finetuning (SFT) stage empowers VL-JEPA\nVQA capabilities while maintaining the pretrained vision-\nlanguage alignment for classification and retrieval. The\ntraining data is selected from the PLM data mixture [Cho\net al., 2025], including 25M VQA samples, 2.8M captioning\nsamples, 1.8M classification samples, and downsampled\npretraining stage data to avoid catastrophic forgetting.\nWe train the model for 35k steps with a batch size of\n6k (âˆ¼2 days with 24 nodes), with cosine learning rate an-\nnealing applied to improve convergence. Since excessive\nhuman labelled data is included in this SFT data mixture,\nwe no longer emphasize zero-shot evaluation for the result-\ning VL-JEPASFT from this stage. Instead, we evaluate VQA\ncapabilities and compare it with state-of-the-art specialist\nmodels.\n4"}
{"pdf_id": "arxiv_251210942_vl_jepa", "page": 5, "text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language\nSec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion\nTable 1. Video classification and text-to-video retrieval. Best zero-shot performance in each dataset are highlighted. Samples seen = training step Ã—\neffective batch size.\nVideo Classification (Top-1 Accuracy)\nText-to-video Retrieval (Recall@1)\nModel\n# Parameters\n# Samples Seen\nZero-shot\nGeneralist Model\nAverage\nSSv2\nEK100\nEgoExo4D\nKinetics-400\nCOIN (SR)\nCOIN (TR)\nCrossTask (SR)\nCrossTask (TR)\nAverage\nMSR-VTT\nActivityNet\nDiDeMo\nMSVD\nYouCook2\nPVD-Bench\nDream-1k\nVDC-1k\nRN50\n75M\n12.8B\n21.8\n2.1\n1.5\n1.9\n41.4\n8.6\n39.0\n10.9\n68.7\n28.3\n28.7\n17.7\n24.7\n29.7\n5.1\n27.6\n47.2\n46.0\nViT-B\n124M\n12.8B\n25.3\n3.1\n1.3\n2.4\n49.5\n11.2\n47.3\n16.2\n71.5\n29.3\n31.0\n19.5\n25.7\n34.0\n6.1\n27.0\n48.5\n42.9\nCLIP\nViT-L\n389M\n12.8B\nâœ“\nâœ“\n30.9\n3.8\n3.7\n3.6\n58.3\n14.7\n63.5\n20.8\n78.5\n35.3\n35.9\n23.4\n30.7\n41.9\n7.9\n36.7\n56.8\n49.3\nViT-B\n375M\n40B\n33.9\n5.2\n2.3\n4.9\n57.8\n20.6\n69.9\n27.7\n82.9\n39.6\n40.2\n25.0\n32.1\n48.6\n13.8\n52.1\n60.9\n43.7\nViT-L\n882M\n40B\n38.7\n5.9\n4.5\n7.0\n63.6\n24.2\n78.5\n35.1\n90.8\n45.4\n41.6\n32.7\n35.1\n53.5\n19.0\n59.2\n71.6\n50.9\nSigLIP2\nViT-g\n1.9B\n40B\nâœ“\nâœ“\n39.9\n6.1\n6.1\n6.4\n68.0\n26.0\n80.4\n35.1\n90.8\n47.5\n43.4\n33.9\n38.9\n56.0\n22.2\n60.4\n73.0\n52.5\nViT-B\n448M\n58B\n37.3\n5.8\n3.3\n6.3\n65.4\n21.5\n77.1\n26.9\n91.8\n44.9\n46.5\n35.4\n35.3\n49.1\n15.2\n59.8\n68.7\n49.2\nViT-L\n671M\n58B\n42.8\n9.3\n6.0\n10.9\n73.4\n27.1\n83.3\n37.5\n95.3\n50.2\n48.9\n41.7\n40.8\n56.2\n22.5\n64.7\n75.9\n51.0\nPE-Core\nViT-G\n2.3B\n86B\nâœ“\nâœ“\n44.6\n9.0\n6.4\n13.0\n76.4\n29.0\n86.0\n40.3\n97.2\n58.1\n51.6\n49.1\n44.5\n58.7\n26.0\n77.0\n89.2\n68.5\nVL-JEPABASE\nViT-L\n1.6B\n2.0B\nâœ“\nâœ“\n46.4\n16.1\n13.3\n21.1\n57.8\n39.8\n74.4\n60.5\n88.0\n58.4\n37.6\n55.4\n49.2\n47.9\n23.1\n78.2\n88.8\n87.2\nVL-JEPASFT\nViT-L\n1.6B\n2.5B\nâœ—\nâœ“\n70.7\n68.2\n38.8\n59.5\n81.4\n60.3\n86.8\n77.1\n93.0\n59.5\n43.7\n53.8\n46.2\n49.1\n28.8\n81.1\n86.4\n86.7\nSoTA (including specialist models)\nâœ—\nâœ—\n-\n77.5\n56.4\n47.8\n92.1\n67.3\n95.3\n64.5\n96.0\n-\n62.8\n74.1\n74.2\n61.4\n28.9\n77.0\n89.2\n68.5\n4\nExperiments\n4.1\nClassification and Retrieval\nWe begin by evaluating VL-JEPAâ€™s classification and re-\ntrieval performance in Â§4.1, and benchmark VL-JEPA on\nVQA datasets in Â§4.2. We demonstrate application of VL-\nJEPA for understanding the relationship between world\nstate changes and action concepts (i.e., inverse dynamics) in\nÂ§4.3. In Â§4.4, we demonstrate the advantage of embedding\nprediction by comparing it with a token-predictive VLM\nbaseline under a strictly controlled setting. In Â§4.5, we\nevaluate the effectiveness of VL-JEPAâ€™s selective decoding,\nand show that it reduces decoding cost while maintaining\nthe performance. Next, we analyze VL-JEPAâ€™s Y-Encoder\nin Â§4.6.\nEvaluation Setup. We evaluate VL-JEPA following the\nCLIP-style evaluation protocol (see Fig.2 and Â§2 â€œMulti-\ntaskingâ€). We assess VL-JEPA on a broad suite of bench-\nmarks, including 8 classification datasets and 8 retrieval\ndatasets. For zero-shot evaluation, we compare against\ngeneralist foundation models CLIP [Radford et al., 2021],\nSigLIP2 [Tschannen et al., 2025], and Perception Encoder\n(PE-Core)[Bolya et al., 2025]. We additionally report refer-\nence numbers from specialist models that are individually op-\ntimized for each benchmark (summarized in Appendix??).\nResults. Table 1 summarizes the results. In the strict\nzero-shot setting, VL-JEPABASE achieves higher average ac-\ncuracy (46.4 vs 44.6) across the 8 classification datasets and\nhigher average recall@1 (58.4 vs 58.1) across the 8 retrieval\ndatasets than the best baseline PE-Core-G. Per-dataset\nscores show that VL-JEPABASE is particularly strong on\nmotion-centric benchmarks (SSv2, EK-100, EgoExo4D, and\nstep recognition on COIN and CrossTask), while relatively\nweaker on appearance-centric benchmarks (Kinetics-400 and\ntask recognition on COIN and CrossTask). This is due to\nVL-JEPABASE has seen substantially fewer vision-language\npairs (only 2B in comparison with PE-Core-Gâ€™s 86B). After\nsupervised finetuning, VL-JEPASFT improves significantly\nupon VL-JEPABASE since the model has seen in-domain\ntraining data. As a single generalist model, the performance\nof VL-JEPASFT is approaching specialist models optimized\nindividually for each dataset.\n4.2\nVisual Question Answering\nEvaluation Setup. We evaluate VL-JEPASFT on discrimi-\nnative VQA tasks. The inference process involves encode\ncandidate answers using the Y-Encoder and selecting the\nanswer that minimizes the distance to the predicted em-\nbedding (see Fig. 2).\nWe select four benchmarks that\nprioritize visual perception rather than knowledge and\nreasoning. We evaluate on GQA [Hudson and Manning,\n2019], a dataset for real-world visual reasoning and com-\npositional QA, reporting accuracy on the testdev-balanced\nsplit. For TallyQA [Acharya et al., 2019], which targets\ncomplex counting, we follow Chen et al. [2022] and report\nthe weighted average accuracy across the â€œsimpleâ€ and\nâ€œcomplexâ€ splits. Finally, to assess object hallucination,\nwe utilize POPE [Li et al., 2023b] and POPEv2 [Li et al.,\n2025b]. For POPE, we report the average accuracy across\nthe â€œrandomâ€, â€œpopularâ€, and â€œadversarialâ€ settings on\nMS-COCO.\nResults. Table 4.2 compares VL-JEPASFT against estab-\nlished VLM families, including BLIP-2 [Li et al., 2023a],\nInstructBLIP [Dai et al., 2023], Qwen-VL [Bai et al., 2023],\nInternVL [Chen et al., 2024d], Llava-1.5 [Vallaeys et al.,\n2024], SmolVLM [Marafioti et al., 2025], PaLI [Chen et al.,\n2022], PaliGemma [Beyer et al., 2024], and Video-LLaVA\n[Lin et al., 2024]. VL-JEPASFT outperforms many of these\n5"}
{"pdf_id": "arxiv_251210942_vl_jepa", "page": 6, "text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language\nSec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion\nTable 2. VQA benchmarks. We report accuracy on GQA [Hudson and Manning, 2019], TallyQA [Acharya et al., 2019], POPE [Li et al., 2023b], and\nPOPEv2 [Li et al., 2025b]. Scores lower than our model are marked in red. Scores from SmolVLM are obtained by our evaluation, while other baselines\nare reported in the literature.\nGQA: compositional visual reasoning\nTallyQA: complex object counting\nPOPE: object hallucination\nPOPEv2: object hallucination\nModel\nAccuracy\nModel\nAccuracy\nModel\nAccuracy\nModel\nAccuracy\nBLIP-2 (OPT-2.7B)\n33.9\nSmolVLM-256M\n32.3\nSmolVLM2-256M\n56.4\nSmolVLM-256M\n62.3\nBLIP-2 (FlanT5XXL)\n41.0\nSmolVLM-500M\n44.8\nSmolVLM-256M\n57.9\nLLaVA-1.5-13B\n72.7\nInstructBLIP (FlanT5XL)\n48.4\nPaLI-700M\n62.3\nLLaVA-7B\n72.9\nInternVL2-8B\n74.5\nInstructBLIP (Vicuna-13B)\n49.5\nSmolVLM-2B\n64.7\nInstructBLIP (Vicuna-13B)\n79.0\nInternVL2-26B\n76.1\nQwen-VL-Chat-7B\n57.5\nPaLI-3B\n65.8\nVideo-LLaVA (7B)\n83.4\nQwen2-VL-72B\n79.4\nQwen-VL-7B\n59.3\nInstructBLIP (Vicuna-13B)\n68.0\nSmolVLM-500M\n85.8\nSmolVLM-500M\n83.8\nInternVL-Chat (Vicuna-7B)\n59.5\nPaLI-17B\n71.9\nLLaVA-1.5-7B\n85.9\nQwen2-VL-7B\n87.0\nLLaVA-1.5 (Vicuna-7B)\n62.0\nLLaVA-1.5 (Vicuna-13B)\n72.3\nLLaVA-1.5-13B-HD\n86.3\nSmolVLM-2B\n88.8\nInternVL-Chat (Vicuna-13B)\n66.6\nPaliGemma (3B)\n76.8\nSmolVLM-2B\n87.5\nQwen2-VL-2B\n91.3\nVL-JEPASFT (1.6B)\n60.8\nVL-JEPASFT (1.6B)\n67.4\nVL-JEPASFT (1.6B)\n84.2\nVL-JEPASFT (1.6B)\n82.2\nTable 3. WorldPrediction-WM benchmark results. We compare the accuracy between large VLMs, socratic LLMs, and VL-JEPA. VL-JEPASFT\nachieves a new SoTA at 65.7%.\nVision Language Models\nSocratic LLMs (w/ Qwen2.5-VL-72B captions)\nVL-JEPA\nInternVL2.5\nQwen2.5-VL\nLlama-3.1\nLlama-4\nQwen2.5\nGPT-4o\nClaude-3.5\nGemini-2\nBASE\nSFT\n2B\n4B\n26B\n38B\n3B\n7B\n32B\n72B\n8B\n70B\n109B\n400B\n3B\n7B\n72B\nN/A\nN/A\nN/A\n1.6B\n1.6B\n20.0\n29.8\n30.2\n50.3\n21.6\n45.5\n49.0\n57.0\n48.7\n49.8\n52.7\n53.6\n44.0\n49.1\n48.5\n52.0\n53.3\n55.6\n63.9\n65.7\nbaselines despite requiring significantly less computational\nresourcesâ€“classical VLMs rely on extensively pretrained\nCLIP backbones combined with multi-stage visual instruc-\ntion tuning. In comparison, VL-JEPASFT employs a unified\narchitecture and a single embedding space to seamlessly han-\ndle VQA, classification, and retrieval (Tab. 1).\n4.3\nWorldPrediction-WM\nEvaluation Setup. We evaluate VL-JEPA on the â€œworld\nmodelingâ€ task in the WorldPrediction [Chen et al., 2025a]\nbenchmark, where the model is provided with two images\nrepresenting the initial and final world states and must\nidentify, among four candidate video clips, the action\nthat explains the observed transition. To adapt VL-JEPA,\nwe duplicate and concatenate the initial and final state\nimages to extract a state embedding, and encode each action\ncandidate into action embeddings. The model then selects\nthe candidate whose embedding is closest to the state\nembedding.\nResults. Table 3 shows accuracy comparisons. VL-\nJEPABASE attains 63.9% and VL-JEPASFT attains 65.7% top-1\naccuracy on WorldPrediction-WM, establishing a new\nstate of the art. Our VL-JEPA model not only substantially\nsurpasses existing VLMs of comparable or larger scale but\nalso exceeds the performance of frontier LLMs such as\nGPT-4o, Claude-3.5-sonnet, and Gemini-2.0.\n4.4\nEmbedding Prediction vs. Token\nPrediction: A Controlled Comparison\nEvaluation Setup. In this section, we compare VL-JEPA to\na token-generative VLM baseline under a strictly aligned\ntraining conditions. Both models use the same Perception\nEncoder [Bolya et al., 2025] (frozen ViT-L-14 with 3362\nresolution, no tiling, 16 frames per video) for vision inputs.\nWe use the same training iterations with the same effective\nbatch size of 128, same learning rate scheduler on the same\npretraining data mixture described above (Â§3). The only\ndifference is the prediction task: VL-JEPA predicts target\nembeddings [Duquenne et al., 2023] using a 0.5B predictor,\nwhereas the VLM baseline performs next-token prediction\nwith cross-entropy using a 1B LLM. For VLM, we use the\nstandard training recipe and codebase of PerceptionLM\n[Cho et al., 2025], aligning frozen vision encoder and text-\nonly LLM Llama-3.2-1B. For VL-JEPA, we initialize the\npredictor from the 8-16 layers of Llama-3.2-1B.\nWe evaluate both models at regular checkpoints through-\nout training spanning from 500K to 15M samples seen. At\neach checkpoint, we measure the performance on video\ncaptioning and video classification. For video captioning,\nwe report CIDEr scores averaged across YouCook2 [Zhou\net al., 2018], MSR-VTT [Xu et al., 2016] and PVD-Bench\n[Bolya et al., 2025]. VL-JEPA decodes the predicted embed-\ndings while VLM generates the tokens directly. For video\nclassification, we report top-5 accuracy averaged across\nCrossTask-Step, CrossTask-Task [Zhukov et al., 2019] and\nEgoExo4D [Grauman et al., 2024]. For VL-JEPA we choose\nthe candidate with lowest cosine distance to the predicted\nembedding, while for VLM we pick the class with lowest\nperplexity.\nResults. As shown in Fig. 3, both models yield compa-\nrable performance after 500K samples seen in both tasks,\nwith respectively 1.23 and 1.35 CIDEr in video captioning\nand 14.9% and 14.0% top-5 accuracy for VL-JEPA and\n6"}
{"pdf_id": "arxiv_251210942_vl_jepa", "page": 7, "text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language\nSec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion\nFigure 3. Comparison of embedding prediction (VL-JEPA) and token prediction (VLM). We conduct a fair comparison of under strictly aligned\ntraining settings (encoder, data, batchsize, etc.). Left: Zero-shot video captioning CIDEr score averaged over 3 datasets and zero-shot classification\naccuracy (top-5) averaged over 3 benchmarks. Right: Comparing the trainable parameters and average inference time cost.\nFigure 4. Evaluation of selective decoding. Left: We compare uniform sampling of decoding points at fixed intervals (red) and embedding-guided\nselective decoding (blue). Performance is measured by the average CIDEr score between each annotation ğ‘¦and its closest decoded output Ë†ğ‘¦. Right:\nResults on EgoExo4D show that selective decoding achieves a Pareto improvement over uniform sampling: for the same performance level, it requires\nfewer decoding operations.\nVLM. After a few iterations, we show that VL-JEPAâ€™s per-\nformance increase is much sharper compared to VLM,\nreaching 14.7 CIDEr and 35.3% top-5 accuracy after 5M\nsamples seen. This gap remains constant as training scales\nat 15M samples with 14.8 CIDEr and 41.0% top-5 accuracy\nfor VL-JEPA, while the VLM baseline yield respectively\n7.1 CIDEr and 27.2% top-5 accuracy. This controlled com-\nparison highlights the benefit of predicting embeddings\nrather than tokens, showing both higher sample efficiency\nand stronger absolute performance.\nWe compare the inference cost of the above VL-JEPA and\nthe VLM by pre-loading 64 video frames into memory and\nrepeatedly decoding text 100 times with the same prompt,\nmeasuring the average time per sample. As shown in\nFig. 3 (right most), both models exhibit comparable latency\nwhen generating text.\nWhat differentiates our model\nfrom classical VLM is the decoupling between the prompt\nprocessing (â€œQuery Embeddingâ€) and the video encoder\n(â€œEncoder + Predictorâ€) from the text generation module\n(â€œDecoderâ€). This allows us to only use the first part of\nthe model to perform retrieval and decode text only when\nneeded (see Section 4.5 below), making our model more\nscalable for online video inference.\n4.5\nEffectiveness of Selective Decoding\nEvaluation Setup. We evaluate the effectiveness of VL-\nJEPAâ€™s embedding-guided selective decoding on long-form\nvideo streams. To this end, we design a benchmark task\nwhere the goal is to recover a temporal sequence of an-\nnotations while minimizing the number of text decoding\noperations, which dominate inference cost. As shown in\nFig. 4 (left), decoding is performed only at selected points\nalong the VL-JEPA embedding stream, yielding a sequence\nof ğ‘decoded outputs [(Ë†ğ‘¡1, Ë†ğ‘¦1), (Ë†ğ‘¡2, Ë†ğ‘¦2), . . . , (Ë†ğ‘¡ğ‘, Ë†ğ‘¦ğ‘)]. Each\nground-truth annotation [(ğ‘¡1, ğ‘¦1), (ğ‘¡2, ğ‘¦2), . . . , (ğ‘¡ğ‘‡, ğ‘¦ğ‘‡)] is\nthen aligned to its nearest decoded output in time (illus-\ntrated as â—¦Â· Â· Â· â—¦in Fig. 4), and CIDEr is computed between\nmatched pairs. We use the EgoExo4D [Grauman et al.,\n2024] validation set in procedural activity domains, which\nconsists of 218 videos with an average duration of 6 min-\nutes and about ğ‘‡= 143 atomic action annotations per\nvideo.\nAs a baseline, we consider uniform sampling, where de-\ncoding points are placed at fixed intervals regardless of the\nunderlying video content. Standard streaming VLMs are\nlimited to this strategy, whereas VL-JEPA supports a more\neffective alternative: adaptive selection of decoding points\nguided by its predicted embeddings. We apply agglom-\nerative clustering with temporal connectivity constraints\n[Murtagh and Contreras, 2012] to partition the embedding\nsequence into ğ‘segments of high intra-segment monose-\nmanticity [Chen et al., 2024a], measured by variance (i.e.,\nWard distance). The intuition is that within a semantically\ncoherent segment, decoded outputs are highly similar, so\ndecoding once per segment captures the essential infor-\nmation while greatly reducing overall decoding cost. The\n7"}
{"pdf_id": "arxiv_251210942_vl_jepa", "page": 8, "text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language\nSec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion\nTable 4. Comparison of text-encoders performance. We report triplet-based accuracy (%) on SugarCrepe++ and VISLA datasets.\nModel\n# Params.\n(total)\n# Params.\n(text encoder)\nSugarCrepe++ [Dumpala et al., 2024a]\nVISLA [Dumpala et al., 2024b]\nAverage\nReplace\nAttribute\nReplace\nObject\nReplace\nRelation\nSwap\nAttribute\nSwap\nObject\nAverage\nGeneric\nSpatial\nCLIP\nViT-L\n389M\n85M\n44.5\n56.7\n83.0\n42.5\n27.0\n13.5\n34.5\n37.6\n31.3\nSigLIP2\nViT-g\n1.9B\n708M\n56.5\n66.9\n74.4\n52.1\n58.4\n30.6\n40.4\n48.7\n32.1\nPE-Core\nViT-G\n2.3B\n537M\n58.6\n73.6\n90.6\n48.9\n53.2\n26.5\n38.3\n45.2\n31.4\nVL-JEPABASE\nViT-L\n1.6B\n300M\n63.9\n72.2\n90.1\n52.2\n62.9\n42.0\n42.9\n49.8\n35.9\nVL-JEPASFT\nViT-L\n1.6B\n300M\n58.4\n68.5\n90.9\n47.4\n55.4\n29.8\n39.5\n44.8\n34.2\nTable 5. Ablation studies results. The default setting adopted by VL-JEPA is marked in blue . We calculate Â±delta within each group of ablations\nin comparison with the default setting.\nClassification\n(Accuracy)\nRetrieval\n(Recall@1)\nVQA\n(Accuracy)\nVL-JEPASFT\n59.1\n70.6\n53.2\n(a) Effectiveness of pretraining stage on caption data\nw/ Pretraining\n49.0\n47.5\n46.1\nw/o Pretraining\n27.3\n(-21.7)\n30.2\n(-17.3)\n42.5\n(-3.6)\n(b) Learning rate multiplier for Y-Encoder\nmultiplier = 0.05\n27.3\n30.2\n42.5\nmultiplier = 1.00\n23.7\n(-3.6)\n28.8\n(-1.4)\n40.7\n(-1.8)\nmultiplier = 0.10\n26.9\n(-0.4)\n30.2\n(-0.0)\n42.9\n(+0.4)\nmultiplier = 0.01\n25.6\n(-1.7)\n27.7\n(-2.5)\n41.0\n(-1.5)\nmultiplier = 0.00\n20.0\n(-7.3)\n25.9\n(-4.3)\n41.4\n(-1.1)\n(c) Loss function (with no projection head on top frozen text encoder)\nInfoNCE\n23.3\n30.3\n44.3\nCosine\n16.5\n(-6.8)\n20.2\n(-10.1)\n46.6\n(+2.3)\nL1\n14.8\n(-8.5)\n15.5\n(-14.8)\n41.9\n(-2.4)\nL2\n13.5\n(-9.8)\n11.7\n(-18.6)\n43.7\n(-0.6)\nClassification\n(Accuracy)\nRetrieval\n(Recall@1)\nVQA\n(Accuracy)\n(d) Predictor architecture and initialization\nLayer 8-16\n27.3\n30.2\n42.5\nLayer 0-2\n24.3\n(-3.0)\n27.8\n(-2.4)\n40.1\n(-2.4)\nLayer 0-4\n25.1\n(-2.2)\n28.9\n(-1.3)\n43.6\n(+1.1)\nLayer 0-8\n27.2\n(-0.1)\n29.3\n(-0.9)\n43.4\n(+0.9)\nLayer 0-16\n27.4\n(+0.1)\n31.0\n(+0.8)\n45.5\n(+3.0)\nw/o Bi-direction Attention\n26.7\n(-0.6)\n31.2\n(+1.0)\n40.6\n(-1.9)\nw/o Llama-3 Initialization\n28.1\n(+0.8)\n30.4\n(+0.2)\n40.6\n(-1.9)\n(e) Y-Encoder (trainable linear projection on top of frozen text encoder)\nEmbeddingGemma-300M\n19.5\n24.1\n42.5\nQwen3-Embedding-0.6B\n24.5\n(+5.0)\n24.5\n(+0.4)\n41.5\n(-1.0)\nQwen3-Embedding-4B\n27.7\n(+8.2)\n26.6\n(+2.5)\n38.1\n(-4.4)\nQwen3-Embedding-8B\n29.6\n(+10.1)\n29.5\n(+5.4)\n41.9\n(-0.6)\nPEcore-B (356M)\n29.4\n(+9.9)\n34.5\n(+10.4)\n35.9\n(-6.6)\nPEcore-L (356M)\n29.0\n(+9.5)\n34.2\n(+10.1)\n42.9\n(+0.4)\nPEcore-G (539M)\n33.9\n(+14.4)\n32.0\n(+7.9)\n41.8\n(-0.7)\nmidpoint of each segment is then chosen as the decoding\npoint, and decoding is performed either from the exact\nembedding or from the average-pooled embedding within\nthe segment.\nResults. As shown in Fig. 4 (right), we sweep the av-\nerage decoding frequency from 2.0 Hz down to 0.01 Hz\n(i.e., average intervals between consecutive decoding op-\nerations from 0.5s to 100s) by adjusting either the stride\nof uniform sampling or the number of clusters in adap-\ntive selection. Across the entire range, adaptive selection\nconsistently Pareto-dominates uniform sampling. In par-\nticular, selective decoding at 0.35 Hz (i.e., âˆ¼2.85s interval)\nmatches the performance of uniform decoding at 1 Hz,\nreducing decoding cost by âˆ¼2.85Ã—. We further observe\nthat average pooling provides consistent gains for both\nstrategies, since it provides denoising and stabilization on\nembeddings prior feeding into the decoder.\n4.6\nEvaluation of Y-Encoder\nEvaluation Setup. We evaluate whether the JEPA architec-\nture improves the Y-Encoder by following the uni-modal\ntext-only (TOT) evaluation setup. We use the hard-negative\nbenchmarks SugarCrepe++ [Dumpala et al., 2024a] and\nVISLA [Dumpala et al., 2024b]. These datasets test sensitiv-\nity to semantic and lexical changes in image descriptions.\nEach dataset contains triplets: two semantically similar de-\nscriptions of the same image (ğ‘1 and ğ‘2), and one negative\ndescription (ğ‘›) created by altering attributes, relations, or\nobjects. We compare Y-Encoders from different models\nby computing the cosine similarity for all description pairs.\nWe check that the similarity between positives ğ‘ ğ‘–ğ‘š(ğ‘1, ğ‘2)\nis higher than both the similarity between each positive\nand the negative ğ‘ ğ‘–ğ‘š(ğ‘1, ğ‘›) and ğ‘ ğ‘–ğ‘š(ğ‘2, ğ‘›). We report\naccuracy (%) across all samples.\nResults. Table 4 shows the performance of different\nmodels on text hard-negative benchmarks. VL-JEPABASE\nachieves a micro average accuracy of 63.9% on Sugar-\nCrepe++ and 42.9% on VISLA. This is higher than the best\nother models: PE-Core scores 58.6% on SugarCrepe++ and\nSigLIP2 scores 40.4% on VISLA. The finetuned VL-JEPASFT\nmodel also achieves competitive results, with 58.4% on\nSugarCrepe++ and 39.5% on VISLA. These results indicate\nthat VL-JEPABASE has a Y-Encoder that is more resilient to\ntext hard-negatives.\n4.7\nAblation Study\nEvaluation Setup. We study different design choices for\nVL-JEPA. Here we train all ablation models on the SFT stage\n8"}
{"pdf_id": "arxiv_251210942_vl_jepa", "page": 9, "text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language\nSec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion\ndata for 10K steps with a batch size of 512 (5M samples seen)\nand constant learning rate. We report average classification\ntop-1 accuracy of 8 datasets (Tab. 1), average text-to-video\nretrieval recall@1 of 8 datasets (Tab. 1), and average VQA\naccuracy of 4 datasets (CLEVR, GQA, TallyQA simple and\ncomplex). We report the results in Tab. 5.\nResults. (a) Pretraining. Dropping the first query-free\npretraining stage on image and video captions significantly\nhurt performance, especially on classification (-21.7) and\nretrieval (-17.3). (b) LR Multiplier. The sweet point of\nlearning rate multiplier to the Y-Encoder is around 0.05\nto 0.10.\nEither faster or slower learning degrades the\nperformance. (c) Loss Function. InfoNCE generally give\nsuperior performance compared to cosine, L1, and L2\nlosses, with the only exception being cosine loss outper-\nform InfoNCE on VQA. However, only InfoNCE has the\nanti-collapse regularization and can be applied with un-\nfrozen Y-Encoder. (d) Predictor. In terms of predictor size,\nmore layers yield better performance, especially on VQA\nperformance. We also see that if using the original causal\nattention instead of updating to bi-direction attention hurt\nVQA performance (-1.9), since query tokens are appended\nafter visual tokens, and visual tokens are no longer able to\nattend to query tokens. Finally, we also see that LLama-3\ninitialization is beneficial to VQA performance, although\nvision-language alignment (classification and retrieval) is\na bit worse compared to randomly initialized Transformer\nlayers. (e) Y-Encoder. We tried different text encoder as the\nY-Encoder, and confirmed that VL-JEPA works well with\nother embedding models than EmbeddingGemma-300M.\nGenerally, larger encoder leads to better performance, with\nvisually aligned text encoders (PE models) has significant\nadvantage in classification and retrieval.\n5\nRelated Works\nJEPA Models. JEPA model learns by predicting the rep-\nresentation of a target input ğ‘Œfrom the representation of\na context input ğ‘‹. Early instantiations include I-JEPA for\nimage encoding [Assran et al., 2023] and V-JEPA for video\nencoding [Bardes et al., 2023], which demonstrated the\neffectiveness of this objective over pixel reconstruction ap-\nproach in their respective modality. Recent JEPA work falls\ninto two categories. One category of work emphasizes bet-\nter unimodal representation learning [Assran et al., 2023,\nBardes et al., 2023, Fei et al., 2023] or cross-modal align-\nment [Lei et al., 2025, Jose et al., 2025]. The other direction\ntargets world modeling, where pretrained encoders are\nfrozen and action-conditioned predictors are trained for\nconditional prediction of state representations [Zhou et al.,\n2025, Baldassarre et al., 2025, Assran et al., 2025]. This\nhas shown good results but remains limited to narrow\ndomains like mazes or robotic pick-and-place. Our pro-\nposed VL-JEPA is the first designed for general-purpose\nvisionâ€“language tasks. It performs conditional latent pre-\ndiction over vision and text, and preserves efficiency while\nenabling flexible, multitask architecture.\nVision Language Models. Existing vision-language\nmodels largely fall into two families: (1) CLIP-style models\nwith a non-predictive joint-embedding architecture (JEA)\n[Radford et al., 2021, Zhai et al., 2023, Bolya et al., 2025,\nLiu et al., 2024, Chen et al., 2023] encode images and\ntexts independently into a common latent space, ğ‘‹ğ‘‰â†¦â†’ğ‘†ğ‘‰\nand ğ‘Œâ†¦â†’ğ‘†ğ‘Œ.\nBy minimizing â„’CLIP = ğ·(ğ‘†ğ‘‰, ğ‘†ğ‘Œ) with\na contrastive loss (e.g., InfoNCE), CLIP learns aligned\nrepresentations that support zero-shot classification and\nvisionâ€“language retrieval; (2) Generative VLMs [Liu et al.,\n2023, Chen et al., 2022, Dai et al., 2023, Alayrac et al., 2022,\nChen et al., 2024b, Cho et al., 2025, Beyer et al., 2024]\nconnect a vision encoder [Radford et al., 2021, Fini et al.,\n2025] with a language model (e.g., LLM). They are typically\ntrained with â„’VLM = ğ·( Ë†ğ‘Œ, ğ‘Œ), i.e., next token prediction\nwith cross-entropy loss, and can learn to handle various\nvision-text-to-text generation tasks such as VQA.\nTable 6. Task coverage comparison.\nCLIP\nVLM\nVL-JEPA\nGeneration\nâœ—\nâœ“\nâœ“\nRetrieval\nâœ“\nâœ—\nâœ“\nOur proposed VL-JEPA integrates the architectural ad-\nvantages and task coverage of both CLIPs and VLMs\n(Table 6). Since VL-JEPA learns in embedding space, it\ncan leverage web-scale noisy imageâ€“text pairs [Jia et al.,\n2021], yielding strong open-domain features. On the other\nhand, VL-JEPA supports conditional generation tasks with\na readout text decoder. Meanwhile, compared to genera-\ntive VLMs that optimize directly in data space, VL-JEPA is\nmore efficient at learning in the latent space. In addition,\nit is also more efficient for online inference, as it allows\nnaturally selective decoding.\nEfficient Vision Language Models. The growing size\nand training cost of VLMs has motivated efforts to improve\nefficiency. On the training side, strong performance can\nbe achieved by updating only a subset of parameters,\nsuch as the visionâ€“language connector [Tsimpoukelli et al.,\n2021, Alayrac et al., 2022, Vallaeys et al., 2024, Shukor\net al., 2023, Koh et al., 2023, Merullo et al., 2022, Dai\net al., 2023]. At inference, efficiency is pursued through\npruning parameters or visual tokens [Cao et al., 2023,\nShukor and Cord, 2024, Vasu et al., 2025]. For real-time\nuse cases, recent work explores small VLMs [Yao et al.,\n2024, Marafioti et al., 2025] and heuristics to reduce query\nfrequency in asynchronous inference [Shukor et al., 2025].\nLatent-space Language Modeling. Current state-of-\nthe-art LLMs are trained to decode and reason in text space\n9"}
{"pdf_id": "arxiv_251210942_vl_jepa", "page": 10, "text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language\nSec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion\nusing autoregressive generation and chain-of-thought\nprompting [Wei et al., 2022]. Text-space LLMs have rapidly\nimproved and now achieve strong results on a wide range\nof benchmarks. However, the discrete nature of their rea-\nsoning trace may limit both speed and performance in\nthe long term. Several works have explored latent-space\nLLMs that process or reason in latent space, such as Large\nConcept Models [Barrault et al., 2024] and COCONUT\n[Hao et al., 2024]. These models focus on unimodal latent-\nspace reasoning. With VL-JEPA, our goal is to align vision\nand text representations in a shared multi-modal latent\nspace. This approach aims to enable better abstractions\nand improve both the performance and speed of vision-\nlanguage models (VLMs). We hope VL-JEPA will serve as\na foundation for future work on multi-modal latent space\nreasoning, including visual chain-of-thought methods [Li\net al., 2025a].\n6\nConclusion\nWe have presented VL-JEPA, a new visionâ€“language model\nbuilt upon the joint embedding predictive architecture.\nBy shifting supervision from discrete token space to con-\ntinuous semantic embedding space, VL-JEPA simplifies\nthe learning target, avoids redundant modeling of sur-\nface linguistic variability, and enables non-autoregressive\nprediction. Through controlled experiments, we show\nthat VL-JEPA outperforms generative VLMs trained with\ncross-entropy loss under matched training data budget,\nwhile achieving superior training efficiency and signifi-\ncantly lower inference latency. Beyond generation tasks,\nthe embedding-based design further allows VL-JEPA to\nhandle open-vocabulary classification and cross-modal\nretrieval within a single unified architecture. Its ability\nto emit continuous semantic embeddings also makes it\nparticularly well suited for real-time video applications,\nwhere selective decoding can improve both responsive-\nness and efficiency. In this work, we demonstrated the\nadvantages of VL-JEPA over standard VLMs, particularly\nin computational efficiency, streaming applications, and\nvideo-language tasks. Our goal at this stage, is not to\npropose a universal alternative to VLMs, as this would\nrequire broader evaluation on tasks such as reasoning, tool\nuse, and agentic behaviors where current token generative\nVLMs excel. Finally, although our results show clear bene-\nfits from scaling parameters and dataset size, we did not\nfully explore this direction, leaving it for future work.\nAcknowledgments\nWe would like to thank Yejin Bang, Adrien Bardes, LoÃ¯c\nBarrault, Lucas Beyer, Quentin Garrido, JoÃ£o Maria Janeiro,\nYifu Qiu, Koustuv Sinha, Basile Terver, and FranÃ§ois Yvon\nfor providing valuable feedback and support to this work.\nReferences\nManoj Acharya, Kushal Kafle, and Christopher Kanan.\nTallyqa: Answering complex counting questions. In\nProceedings of the AAAI conference on artificial intelligence,\nvolume 33, pages 8076â€“8084, 2019.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur\nMensch, Katherine Millican, Malcolm Reynolds, et al.\nFlamingo: a visual language model for few-shot learn-\ning. Advances in neural information processing systems, 35:\n23716â€“23736, 2022.\nMahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bo-\njanowski, Pascal Vincent, Michael Rabbat, Yann LeCun,\nand Nicolas Ballas. Self-supervised learning from im-\nages with a joint-embedding predictive architecture. In\nProceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 15619â€“15629, 2023.\nMido Assran, Adrien Bardes, David Fan, Quentin Garrido,\nRussell Howes, Matthew Muckley, Ammar Rizvi, Claire\nRoberts, Koustuv Sinha, Artem Zholus, et al. V-jepa\n2: Self-supervised video models enable understanding,\nprediction and planning. arXiv preprint arXiv:2506.09985,\n2025.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\nXiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, et al. Qwen technical report. arXiv preprint\narXiv:2309.16609, 2023.\nFederico Baldassarre, Marc Szafraniec, Basile Terver, Vasil\nKhalidov, Francisco Massa, Yann LeCun, Patrick Labatut,\nMaximilian Seitzer, and Piotr Bojanowski. Back to the\nfeatures: Dino as a foundation for video world models.\narXiv preprint arXiv:2507.19468, 2025.\nRandall Balestriero and Yann LeCun. Lejepa: Provable and\nscalable self-supervised learning without the heuristics.\narXiv preprint arXiv:2511.08544, 2025.\nAdrien Bardes, Jean Ponce, and Yann LeCun.\nVicreg:\nVariance-invariance-covariance regularization for self-\nsupervised learning. arXiv preprint arXiv:2105.04906,\n2021.\nAdrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen,\nMichael Rabbat, Yann LeCun, Mido Assran, and Nico-\nlas Ballas. V-jepa: Latent video prediction for visual\nrepresentation learning. 2023.\nLoÃ¯c Barrault, Paul-Ambroise Duquenne, Maha Elbayad,\nArtyom Kozhevnikov, Belen Alastruey, Pierre Andrews,\nMariano Coria, Guillaume Couairon, Marta R Costa-\njussÃ , David Dale, et al. Large concept models: Language\n10"}
{"pdf_id": "arxiv_251210942_vl_jepa", "page": 11, "text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language\nSec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion\nmodeling in a sentence representation space. arXiv\npreprint arXiv:2412.08821, 2024.\nLucas Beyer, Andreas Steiner, AndrÃ© Susano Pinto, Alexan-\nder Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neu-\nmann, Ibrahim Alabdulmohsin, Michael Tschannen,\nEmanuele Bugliarello, et al. Paligemma: A versatile 3b\nvlm for transfer. arXiv preprint arXiv:2407.07726, 2024.\nKevin Black, Manuel Y Galliker, and Sergey Levine. Real-\ntime execution of action chunking flow policies. arXiv\npreprint arXiv:2506.07339, 2025.\nDaniel Bolya, Po-Yao Huang, Peize Sun, Jang Hyun Cho,\nAndrea Madotto, Chen Wei, Tengyu Ma, Jiale Zhi,\nJathushan Rajasegaran, Hanoona Rasheed, et al. Percep-\ntion encoder: The best visual embeddings are not at the\noutput of the network. arXiv preprint arXiv:2504.13181,\n2025.\nFlorian Bordes, Richard Yuanzhe Pang, Anurag Ajay,\nAlexander C Li, Adrien Bardes, Suzanne Petryk, Oscar\nMaÃ±as, Zhiqiu Lin, Anas Mahmoud, Bargav Jayaraman,\net al. An introduction to vision-language modeling.\narXiv preprint arXiv:2405.17247, 2024.\nQingqing Cao, Bhargavi Paranjape, and Hannaneh Ha-\njishirzi. Pumer: Pruning and merging tokens for efficient\nvision language models. arXiv preprint arXiv:2305.17530,\n2023.\nDelong Chen, Zhao Wu, Fan Liu, Zaiquan Yang, Shaoqiu\nZheng, Ying Tan, and Erjin Zhou. Protoclip: Proto-\ntypical contrastive language image pretraining. IEEE\nTransactions on Neural Networks and Learning Systems,\n2023.\nDelong Chen, Samuel CahyawÄ³aya, Jianfeng Liu, Baoyuan\nWang, and Pascale Fung. Subobject-level image tok-\nenization. arXiv preprint arXiv:2402.14327, 2024a.\nDelong Chen, Jianfeng Liu, Wenliang Dai, and Baoyuan\nWang. Visual instruction tuning with polite flamingo.\nIn Proceedings of the aaai conference on artificial intelligence,\nvolume 38, pages 17745â€“17753, 2024b.\nDelong Chen, Willy Chung, Yejin Bang, Ziwei Ji, and Pas-\ncale Fung. Worldprediction: A benchmark for high-level\nworld modeling and long-horizon procedural planning.\narXiv preprint arXiv:2506.04363, 2025a.\nDelong Chen, Theo Moutakanni, Willy Chung, Yejin Bang,\nZiwei Ji, Allen Bolourchi, and Pascale Fung. Planning\nwith reasoning using vision language world model.\narXiv preprint arXiv:2509.02722, 2025b.\nJoya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong\nLin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao,\nDongxing Mao, and Mike Zheng Shou. Videollm-online:\nOnline video large language model for streaming video.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 18407â€“18418, 2024c.\nXi Chen, Xiao Wang, Soravit Changpinyo, Anthony J\nPiergiovanni, Piotr Padlewski, Daniel Salz, Sebastian\nGoodman, Adam Grycner, Basil Mustafa, Lucas Beyer,\net al. Pali: A jointly-scaled multilingual language-image\nmodel. arXiv preprint arXiv:2209.06794, 2022.\nZhe Chen, Jiannan Wu, Wenhai Wang, WeÄ³ie Su, Guo Chen,\nSen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu,\nLewei Lu, et al. Internvl: Scaling up vision foundation\nmodels and aligning for generic visual-linguistic tasks.\nIn Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 24185â€“24198, 2024d.\nJang Hyun Cho, Andrea Madotto, Effrosyni Mavroudi,\nTriantafyllos Afouras, Tushar Nagarajan, Muhammad\nMaaz, Yale Song, Tengyu Ma, Shuming Hu, Suyog\nJain, et al. Perceptionlm: Open-access data and mod-\nels for detailed visual understanding. arXiv preprint\narXiv:2504.13180, 2025.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong,\nJunqi Zhao, Weisheng Wang, Boyang Li, Pascale N\nFung, and Steven Hoi. Instructblip: Towards general-\npurpose vision-language models with instruction tuning.\nAdvances in neural information processing systems, 36:49250â€“\n49267, 2023.\nShangzhe Di, Zhelun Yu, Guanghao Zhang, Haoyuan\nLi, Tao Zhong, Hao Cheng, Bolin Li, Wanggui He,\nFangxun Shu, and Hao Jiang. Streaming video question-\nanswering with in-context video kv-cache retrieval.\narXiv preprint arXiv:2503.00540, 2025.\nSri Harsha Dumpala, Aman Jaiswal, Chandramouli Sastry,\nEvangelos Milios, Sageev Oore, and Hassan Sajjad. Sug-\narcrepe++ dataset: vision-language model sensitivity to\nsemantic and lexical alterations. In Proceedings of the 38th\nInternational Conference on Neural Information Processing\nSystems, NIPS â€™24, Red Hook, NY, USA, 2024a. Curran\nAssociates Inc. ISBN 9798331314385.\nSri Harsha Dumpala, Aman Jaiswal, Chandramouli Sastry,\nEvangelos Milios, Sageev Oore, and Hassan Sajjad. Visla\nbenchmark: Evaluating embedding sensitivity to seman-\ntic and lexical alterations. arXiv preprint arXiv:2404.16365,\n2024b.\nPaul-Ambroise Duquenne, Holger Schwenk, and BenoÃ®t\nSagot. Sonar: sentence-level multimodal and language-\n11"}
{"pdf_id": "arxiv_251210942_vl_jepa", "page": 12, "text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language\nSec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion\nagnostic representations. arXiv preprint arXiv:2308.11466,\n2023.\nZhengcong Fei, Mingyuan Fan, and Junshi Huang. A-jepa:\nJoint-embedding predictive architecture can listen. arXiv\npreprint arXiv:2311.15830, 2023.\nEnrico Fini, Mustafa Shukor, Xiujun Li, Philipp Dufter,\nMichal Klein, David Haldimann, Sai Aitharaju, Vic-\ntor G Turrisi da Costa, Louis BÃ©thune, Zhe Gan, et al.\nMultimodal autoregressive pre-training of large vision\nencoders. In Proceedings of the Computer Vision and Pattern\nRecognition Conference, pages 9641â€“9654, 2025.\nPascale Fung, Yoram Bachrach, Asli Celikyilmaz, Ka-\nmalika Chaudhuri, Delong Chen, Willy Chung, Em-\nmanuel Dupoux, Hongyu Gong, HervÃ© JÃ©gou, Alessan-\ndro Lazaric, et al. Embodied ai agents: Modeling the\nworld. arXiv preprint arXiv:2506.22355, 2025.\nSamir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan\nHayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,\nMitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al.\nDatacomp: In search of the next generation of multi-\nmodal datasets. Advances in Neural Information Processing\nSystems, 36:27092â€“27112, 2023.\nKristen Grauman, Andrew Westbury, Eugene Byrne,\nZachary Chavis, Antonino Furnari, Rohit Girdhar, Jack-\nson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al.\nEgo4d: Around the world in 3,000 hours of egocentric\nvideo. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pages 18995â€“19012,\n2022.\nKristen Grauman, Andrew Westbury, Lorenzo Torresani,\nKris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar\nAshutosh, VÄ³ay Baiyya, Siddhant Bansal, Bikram Boote,\net al. Ego-exo4d: Understanding skilled human activity\nfrom first-and third-person perspectives. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 19383â€“19400, 2024.\nShibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting\nHu, Jason Weston, and Yuandong Tian. Training large\nlanguage models to reason in a continuous latent space.\narXiv preprint arXiv:2412.06769, 2024.\nDrew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and composi-\ntional question answering. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages\n6700â€“6709, 2019.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and\nTom Duerig. Scaling up visual and vision-language\nrepresentation learning with noisy text supervision. In\nInternational conference on machine learning, pages 4904â€“\n4916. PMLR, 2021.\nCÄ³o Jose, ThÃ©o Moutakanni, Dahyun Kang, Federico Bal-\ndassarre, TimothÃ©e Darcet, Hu Xu, Daniel Li, Marc\nSzafraniec, MichaÃ«l Ramamonjisoa, Maxime Oquab,\net al.\nDinov2 meets text: A unified framework for\nimage-and pixel-level vision-language alignment. In\nProceedings of the Computer Vision and Pattern Recognition\nConference, pages 24905â€“24916, 2025.\nJing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.\nGrounding language models to images for multimodal\ninputs and outputs. In International Conference on Machine\nLearning, pages 17283â€“17300. PMLR, 2023.\nYann LeCun. A path towards autonomous machine intelli-\ngence. Open Review, 62(1):1â€“62, 2022.\nHongyang Lei, Xiaolong Cheng, Qi Qin, Dan Wang,\nHuazhen Huang, Qingqing Gu, Yetao Wu, and Luo\nJi. M3-jepa: Multimodal alignment via multi-gate moe\nbased on the joint-embedding predictive architecture. In\nForty-second International Conference on Machine Learning,\n2025.\nChengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia,\nShaoguang Mao, Li Dong, Ivan VuliÄ‡, and Furu\nWei. Imagine while reasoning in space: Multimodal\nvisualization-of-thought. arXiv preprint arXiv:2501.07542,\n2025a.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models. In\nInternational conference on machine learning, pages 19730â€“\n19742. PMLR, 2023a.\nYifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin\nZhao, and Ji-Rong Wen. Evaluating object hallucina-\ntion in large vision-language models. arXiv preprint\narXiv:2305.10355, 2023b.\nYifan Li, Kun Zhou, Wayne Xin Zhao, Lei Fang, and Ji-Rong\nWen. Analyzing and mitigating object hallucination: A\ntraining bias perspective. arXiv preprint arXiv:2508.04567,\n2025b.\nBin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng\nJin, and Li Yuan. Video-llava: Learning united visual\nrepresentation by alignment before projection. In Pro-\nceedings of the 2024 Conference on Empirical Methods in\nNatural Language Processing, pages 5971â€“5984, 2024.\n12"}
{"pdf_id": "arxiv_251210942_vl_jepa", "page": 13, "text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language\nSec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion\nFan Liu, Delong Chen, Zhangqingyun Guan, Xiaocong\nZhou, Jiale Zhu, Qiaolin Ye, Liyong Fu, and Jun Zhou.\nRemoteclip: A vision language foundation model for\nremote sensing. IEEE Transactions on Geoscience and\nRemote Sensing, 62:1â€“16, 2024.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. Visual instruction tuning. Advances in neural infor-\nmation processing systems, 36:34892â€“34916, 2023.\nAndrÃ©s Marafioti, Orr Zohar, Miquel FarrÃ©, Merve Noyan,\nElie Bakouch, Pedro Cuenca, Cyril Zakka, Loubna Ben\nAllal, Anton Lozhkov, Nouamane Tazi, et al. Smolvlm:\nRedefining small and efficient multimodal models. arXiv\npreprint arXiv:2504.05299, 2025.\nJack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie\nPavlick. Linearly mapping from image to text space.\narXiv preprint arXiv:2209.15162, 2022.\nFionn Murtagh and Pedro Contreras. Algorithms for hier-\narchical clustering: an overview. Wiley interdisciplinary\nreviews: data mining and knowledge discovery, 2(1):86â€“97,\n2012.\nRui Qian, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Shuan-\ngrui Ding, Dahua Lin, and Jiaqi Wang. Streaming long\nvideo understanding with large language models. Ad-\nvances in Neural Information Processing Systems, 37:119336â€“\n119360, 2024.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.\nLearning transferable visual models from natural lan-\nguage supervision. In International conference on machine\nlearning, pages 8748â€“8763. PmLR, 2021.\nMustafa Shukor and Matthieu Cord. Skipping computa-\ntions in multimodal llms. arXiv preprint arXiv:2410.09454,\n2024.\nMustafa Shukor, Corentin Dancette, and Matthieu Cord.\nep-alm: Efficient perceptual augmentation of language\nmodels. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 22056â€“22069, 2023.\nMustafa Shukor, Dana Aubakirova, Francesco Capuano,\nPepÄ³n KooÄ³mans, Steven Palma, Adil Zouitine, Michel\nAractingi, Caroline Pascal, Martino Russi, Andres\nMarafioti, et al.\nSmolvla: A vision-language-action\nmodel for affordable and efficient robotics. arXiv preprint\narXiv:2506.01844, 2025.\nWenxuan Song, Jiayi Chen, Pengxiang Ding, Han Zhao,\nWei Zhao, Zhide Zhong, Zongyuan Ge, Jun Ma, and\nHaoang Li. Accelerating vision-language-action model\nintegrated with action chunking via parallel decoding.\narXiv preprint arXiv:2503.02310, 2025.\nBart Thomee, David A Shamma, Gerald Friedland, Ben-\njamin Elizalde, Karl Ni, Douglas Poland, Damian Borth,\nand Li-Jia Li. Yfcc100m: The new data in multimedia\nresearch. Communications of the ACM, 59(2):64â€“73, 2016.\nMichael Tschannen, Alexey Gritsenko, Xiao Wang, Muham-\nmad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil\nParthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil\nMustafa, et al. Siglip 2: Multilingual vision-language en-\ncoders with improved semantic understanding, localiza-\ntion, and dense features. arXiv preprint arXiv:2502.14786,\n2025.\nMaria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Es-\nlami, Oriol Vinyals, and Felix Hill. Multimodal few-shot\nlearning with frozen language models. Advances in\nNeural Information Processing Systems, 34:200â€“212, 2021.\nThÃ©ophane Vallaeys, Mustafa Shukor, Matthieu Cord, and\nJakob Verbeek. Improved baselines for data-efficient\nperceptual augmentation of llms. In European Conference\non Computer Vision, pages 369â€“387. Springer, 2024.\nPavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang\nLi, Cem Koc, Nate True, Albert Antony, Gokula San-\nthanam, James Gabriel, Peter Grasch, Oncel Tuzel, et al.\nFastvlm: Efficient vision encoding for vision language\nmodels. In Proceedings of the Computer Vision and Pattern\nRecognition Conference, pages 19769â€“19780, 2025.\nHenrique Schechter Vera, Sahil Dua, Biao Zhang, Daniel\nSalz, Ryan Mullins, Sindhu Raghuram Panyam, Sara\nSmoot, Iftekhar Naim, Joe Zou, Feiyang Chen, et al.\nEmbeddinggemma: Powerful and lightweight text rep-\nresentations. arXiv preprint arXiv:2509.20354, 2025.\nTongzhou Wang and Phillip Isola. Understanding con-\ntrastive representation learning through alignment and\nuniformity on the hypersphere. In International conference\non machine learning, pages 9929â€“9939. PMLR, 2020.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large\nlanguage models. Advances in neural information process-\ning systems, 35:24824â€“24837, 2022.\nJun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A\nlarge video description dataset for bridging video and\nlanguage. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 5288â€“5296, 2016.\nYuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo\nCui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao,\n13"}
{"pdf_id": "arxiv_251210942_vl_jepa", "page": 14, "text": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language\nSec 1: Intro | Sec 2: Method | Sec 3: Implementation | Sec 4: Experiments | Sec 5: Related Works | Sec 6: Conclusion\nZhihui He, et al. Minicpm-v: A gpt-4v level mllm on\nyour phone. arXiv preprint arXiv:2408.01800, 2024.\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and\nLucas Beyer.\nSigmoid loss for language image pre-\ntraining. In Proceedings of the IEEE/CVF international\nconference on computer vision, pages 11975â€“11986, 2023.\nGaoyue Zhou, Hengkai Pan, Yann LeCun, and Lerrel Pinto.\nDino-wm: World models on pre-trained visual features\nenable zero-shot planning. In Forty-second International\nConference on Machine Learning, 2025.\nLuowei Zhou, Chenliang Xu, and Jason Corso. Towards\nautomatic learning of procedures from web instructional\nvideos. In Proceedings of the AAAI conference on artificial\nintelligence, volume 32, 2018.\nXingyi Zhou, Anurag Arnab, Shyamal Buch, Shen Yan,\nAustin Myers, Xuehan Xiong, Arsha Nagrani, and\nCordelia Schmid. Streaming dense video captioning.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 18243â€“18252, 2024.\nDimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gok-\nberk Cinbis, David Fouhey, Ivan Laptev, and Josef Sivic.\nCross-task weakly supervised learning from instruc-\ntional videos. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 3537â€“\n3545, 2019.\n14"}
