{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 1, "text": "Asynchronous Reasoning: Training-Free Interactive Thinking LLMs\nGeorge Yakushev * 1 2 Nataliia Babina * 3 Masoud Vahid Dastgerdi * 2 Vyacheslav Zhdanovskiy * 1\nAlina Shutova 1 2 Denis Kuznedelev 1\nAbstract\nMany state-of-the-art LLMs are trained to think\nbefore giving their answer. Reasoning can greatly\nimprove language model capabilities and safety,\nbut it also makes them less interactive: given a\nnew input, a model must stop thinking before it\ncan respond. Real-world use cases such as voice-\nbased or embedded assistants require an LLM\nagent to respond and adapt to additional infor-\nmation in real time, which is incompatible with\nsequential interactions. In contrast, humans can\nlisten, think, and act asynchronously: we begin\nthinking about the problem while reading it and\ncontinue thinking while formulating the answer.\nIn this work, we augment LLMs capable of rea-\nsoning to operate in a similar way without addi-\ntional training. Our method uses the properties\nof rotary embeddings to enable LLMs built for\nsequential interactions to simultaneously think,\nlisten, and generate outputs. We evaluate our ap-\nproach on math, commonsense, and safety reason-\ning and find that it can generate accurate thinking-\naugmented answers in real time, reducing time\nto first non-thinking token from minutes to ≤5s.\nand the overall real-time delays by 6−11×.\n1. Introduction\nModern Large Language Models (LLMs) solve complex\ntasks using inference-time computation mechanisms [1, 2,\n3], such as chain-of-thought reasoning [4, 5, 6, 7] and agen-\ntic tool use [10, 11, 12, 13]. Recent models, both propri-\netary [18, 19, 20] and open-weights [21, 22, 23], are explic-\nitly trained for reasoning and agentic capabilities. As we\ntrust LLMs with harder problems [24, 25], their ability to\n“think” becomes ever more important.\nThe current dominant strategy for LLM reasoning is the\nread-think-answer cycle: the model encodes a given prob-\nlem, then generates chain-of-thought reasoning, possibly\n*Equal contribution 1Yandex 2HSE University 3The University\nof Tokyo. Preprint, work in progress. Correspondence to: Denis\nKuznedelev <dkuznedelev@yandex-team.ru>.\ncalls tools, and then formulates the final answer [18, 21, 23].\nThis fits naturally with the sequential view of LLMs as next-\ntoken prediction models. However, this also means that the\nLLM must follow a rigid turn structure that can limit their\nflexibility. The “thinking” phase can take minutes of real\ntime, during which the agent does not get new information\nor output its current results.\nUnlike LLM agents, people have an innate ability to think\nasynchronously [26, 27, 28, 29]. When working on a prob-\nlem, we can begin solving it even before we have heard its\nentire statement, and can start talking (or acting) while still\ncompleting our solution. Such “multitasking” is not always\neasy or efficient [30], but it allows us to effectively operate\nin a dynamic environment [31].\nSimilarly, artificial agents often need real-time ability to\nchange course of action. A voice assistant is expected to\nmaintain conversation in real time [32, 33, 34, 35, 36, 37,\n38]. An embodied agent’s VLA model [39, 40, 41] needs to\nquickly adjust to new inputs. Even fully text-based “deep re-\nsearch” agents benefit from interactive communication with\nthe user [42]. However, the current read-think-answer cycle\nis inherently non-interactive. During the thinking phase, if\nan agent receives new inputs or must take action, it can ei-\nther stop reasoning, discarding any incomplete thoughts, or\nwait until it completes, sacrificing interactivity. As a result,\nmany real-time LLM applications do not fully benefit from\ninference-time compute.\nIn this work, we propose a technique that enables asyn-\nchronous LLM reasoning. Instead of retraining the LLM to\nsatisfy each specific degree of interactivity, we propose a\ntraining-free approach that modifies existing models. Our\napproach uses three concurrent streams of tokens: user in-\nputs, private thoughts, and public response, which can be\nupdated in real-time. We rely on geometric properties of ro-\ntary positional embeddings to make the LLM perceive these\nstreams as a single contiguous sequence without additional\ntraining. The model itself can decide whether it should con-\ntinue talking or pause and think, depending on the current\nstate of the three streams. The resulting asynchronous rea-\nsoning can be formulated as standard LLM inference with\na modified attention cache, making it possible to integrate\ninto efficient LLM inference frameworks [43, 44].\n1\narXiv:2512.10931v1  [cs.LG]  11 Dec 2025"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 2, "text": "Task: A bat and a ball are 1.10$. And the bat is 1$ more than the ball. How much is the ball?\n Let the ball cost x dollars. Then, x + (x + 1) = 1.10, simplified to 2x + 1 = 1.10, x = 0.10 / 2 = 0.05. Let me check that. If the ball costs $0.05 ...\nLet me solve this for you. The price for the ball is                                                                                    $0.05. Would you like me to explain ...\nWAIT\nThinker pauses the Writer ...\n... resumes the Writer.    \nThinker:\nWriter:\nInference steps\nFigure 1: The intuitive explanation of asynchronous reasoning: the LLM generates its response concurrently with thinking.\nIf the thinking stream needs additional time, it can pause the response writer until the next reasoning step is ready.\nOur main contributions can be summarized as follows:\n• We propose AsyncReasoning, a zero-shot method that\nallows existing reasoning LLMs to think, write outputs\nand encode additional inputs concurrently. Our approach\nrelies on model-agnostic concurrent attention and prompt-\ning, making it easy to adapt for new models.\n• We evaluate the proposed approach on real-time math,\ncommon-sense and safety reasoning. Our experiments\ndemonstrate that the proposed approach lets the LLM\noverlap thinking and answering, reducing the user-\nperceived delay by over 9 on mathematical and common\nsense reasoning tasks. When prompted to think about\nsafety, AsyncReasoning allows the LLM to stream real-\ntime outputs on benign requests, while considering the\nsafety implications in a private thinking stream that can\npause potentially harmful outputs.\n• We release our reference implementation1 of AsyncRea-\nsoning, including GPU kernels for concurrent attention.\nWe also provide a minimal voice assistant with asyn-\nchronous thinking capabilities to demonstrate it in action.\n2. Related Work\n2.1. Real-time LLM Applications\nModern LLM agents are deployed in a broad range of ap-\nplications that require varying degrees of interactivity. For\ninstance, a background code review agent can pause and\nthink for several minutes, whereas a real-time voice assistant\ncannot. Here, we briefly review several LLM applications\nthat require quick or interactive responses.\nVoice assistants. Recent works [33, 34] and industry re-\nleases [32, 45, 46] use LLM agents as interactive voice\nassistants that talk to users in real-time, often through\ntheir phones or edge devices, or partake in a group con-\nference [47, 48]. Compared to their text-based counterparts,\nvoice assistants require faster reaction time, with user often\nadding new information while the agent is thinking.\n1See github.com/yandex-research/AsyncReasoning\nThere are two main strategies to building voice assistants:\nmodular and end-to-end. The first strategy pipes automated\nspeech recognition (ASR) [49, 50, 51, 52] into a text-based\nLLM, then feeds its response into a text-to-speech (TTS)\nsystem [53, 54, 55, 56, 57, 58, 59, 60]. The pipeline over-\nlaps LLM generation with TTS to stream audio in real-time.\nThe second, more recent strategy is using Speech Language\nModels (also Audio and Voice LMs) that are trained to pro-\ncess and generate audio natively [35, 38, 36, 37], allowing\nthem to perceive intonation and non-speech audio. However,\nthat due to constraints on response time, many Speech LMs\nare not trained for long-form reasoning, and the thinking\noptimized LMs often do not include speech synthesis2.\nRobotic & virtual agents. Another type of LLM applica-\ntions that require interactivity are LLM agents with real-time\nenvironments. Agents controlling robotic systems use mul-\ntimodal Embodied Language Models [41, 39, 62, 63, 64] to\nfor action planning or Vision-Language-Action [40, 65, 66]\nto control the system directly. Aside from robotic systems,\nsimilar agents were proposed for videogames [67], manag-\ning operating systems and mobile devices [68, 69, 70, 71].\nSimilarly to voice assistants, embodied agents need to\nquickly react to new stimuli from the environment.\nReasoning and Safety. Another important aspect of LLM\nreasoning is how it interacts with model safety and con-\ntrol [72, 73]. By default, thinking can both mitigate safety\nrisks and create new ones [74, 75, 76]. However, when\nspecifically prompted to reason about safety implications of\ntheir task, language models can detect and prevent jailbreak\nattacks [77, 78, 79, 80]. However, since traditional reason-\ning delays model response time, which is inconvenient for\ninteractive usage scenarios. In our experiments, we show\nthat LLMs can reason about safety asynchronously in the\nbackground, mitigating jailbreaks without response delays.\nWe discuss reasoning safety further in Appendix A.\n2For example, in the recent Qwen3-Omni model family, the\n30B-A3B-Instruct can speak, but does not generate <think>\nblocks, while the 30B-A3B-Thinking [61] has no speech output.\n2"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 3, "text": "Writer Block (read only)\nI am in Writer mode. My text is visible to the user. We are asked to evaluate the expression  x - x² + x³ for x\nvalues 5, 6, 7, and 8. Let's compute each value step by step. For x = 5: 5 - 5^2 + 5^3 = 5 - 25 + 125\nThinker view\nPrompt Block\nYou are an AI assistant that can think and write outputs concurrently. You can reason in private and your\nthoughts will be used to form the public response in the background. Your task is to write thoughts and control\nwhen the automated system can continue writing the response <...>. Please reason step by step.\nTask: Calculate x - x^2 + x^3 for x = 5,6,7,8. Return all 4 answers in \\\\boxed{ }.\nSYSTEM: [the system will continue writing the response here]\n<|im_end|>\n<|im_start>assistant\n<think>\nThinker Block (editing)\nI am in Thinker mode. My text is not visible to the user. The user wants me to calculate <...>. Starting with x = 5.\nThe expression is 5 - 5² + 5³. Let's break it down: 5 squared is 25, and 5 cubed is 125. So substituting those in,\nit becomes 5 - 25 + 125. Calculating that: 5 - 25 is -20, and then adding 125 gives 105. So for x=5, the result.  ...\nresult\nWriter view\nPrompt Block\nYou are an AI assistant that can think and write outputs concurrently. You can write outputs for the user based\non partial CoT that will be continued in the background by an automated system.You should outline what you're\ngoing to do, then write your response as thoughts progress, but not ahead of your thoughts.\nTask: Calculate x - x^2 + x^3 for x = 5,6,7,8. Return all 4 answers in \\\\boxed{ }.\nWriter Block (editing)\nI am in Writer mode. My text is visible to the user. We are asked to evaluate the expression  x - x² + x³ for x\nvalues 5, 6, 7, and 8. Let's compute each value step by step. For x = 5: 5 - 5^2 + 5^3 = 5 - 25 + 125\nSYSTEM: [additional thoughts will appear here]\n</think>\n125\nNew tokens are added simultaneously to both blocks\nresult\n125\n125\n<|im_end|>\n<|im_start|>assistant\n<think>\nThinker Block (read only)\nI am in Thinker mode. My text is not visible to the user. The user wants me to calculate <...>. Starting with x = 5.\nThe expression is 5 - 5² + 5³. Let's break it down: 5 squared is 25, and 5 cubed is 125. So substituting those in,\nit becomes 5 - 25 + 125. Calculating that: 5 - 25 is -20, and then adding 125 gives 105. So for x=5, the result\n<|im_start|>user\n<|im_end|>\n<|im_start|>assistant\n<|im_start|>user\nresult\nFigure 2: A dual thinker / writer view of the same reasoning task. The two views reuse the same KV cache and generate\ntokens in parallel. Both thinker and writer see the problem in the same sequential formatting that they were trained with.\n2.2. Efficient LLM Reasoning\nAs discussed earlier, there is a wide range of tasks that re-\nquire LLMs to reason in real-time. However, most thinking\nLLMs [18, 21, 81] follow a read-think-answer cycle, mak-\ning them inherently non-interactive. When receiving new\ninformation mid-thought, such LLMs can either interrupt\ntheir reasoning to react, but sacrifice any incomplete thought\ntokens, or continue reasoning non-interactively.\nRecently, there has been a large influx of techniques for\nefficient reasoning [82] through more concise chain-of-\nthought [83, 84, 85, 86], adaptive reasoning effort [87, 88,\n89, 90] or early stopping [91, 92, 93]. Another line of work\nexplores reasoning in parallel, with multiple concurrent\nLLM instances solving different sub-tasks [94, 95, 96, 97,\n98, 99, 100, 101, 102], or parallel tool calling [103, 104].\nReducing reasoning-induced delays several recent stud-\nies propose techniques specifically to reduce reasoning de-\nlays for real-time applications with partial read overlap-\nping [105], specialized two-model architectures with fast\ninteractive and slow reasoning modules [107]. A concurrent\nwork [108] introduced Plantain, a method that finetunes rea-\nsoning LLMs to solve their task with interleaved thinking\nand talking sub-blocks, making them more interactive.\nNote, however, that all these techniques require specialized\nfine-tuning or training from scratch, which complicates their\nadoption. In practice, the requirements for interactive LLM\nuse also vary with hardware and software configuration: a\nmodel trained for “real-time” reasoning on a B200 GPU\nmay cause delays when deployed on slower GPUs or with\nbatched inference. Therefore, models that were trained\nfor one interactive use may need re-training for different\nhardware or parameters. In this work, we instead design a\nlightweight asynchronous reasoning method that does not\nrequire training and can be adapted with simple prompting.\n3. AsyncReasoning\nTo convert an existing reasoning LLM into an asynchronous\nthinker, we need to reformulate the asynchronous thinking\nprocess and make it compatible with the standard template\nthe models were trained with. We describe how this can\nbe achieved by dynamically rearranging the model’s KV\ncache so it views multiple asynchronous streams as a single\nsequence (Section 3.1). In Section 3.2 we discuss mode\nswitching: allowing the LLM to alternate between simulta-\nneous writing and waiting for thoughts, depending on the\ncontext. Finally, we discuss efficient parallel token process-\ning and other implementation details in Section 3.3.\n3.1. Dual Thinker & Writer Views\nThe core idea behind our approach is that transformer LLMs\nare inherently designed for manipulating sets [109, 110],\nand the only thing that makes them into sequence models is\ntheir positional encoding [111, 112, 113]. In order to change\nthe token generation order, we do not need to physically\nrearrange tokens in memory. Instead, it is sufficient to\nchange positional relations between tokens, since the rest of\nthe transformer architecture is already position-invariant.\nAt each inference step, AsyncReasoning manipulates po-\nsitional encodings to rearrange past tokens into a different\norder for thinking and for writing the response. Public re-\nsponse tokens see (partial) private thoughts as they were\ngenerated in a standard read-think-answer cycle. In turn,\ntokens within the <think> block see response tokens as\nthey were generated during the previous conversation turn.\nWe illustrate this dual view in Figure 2.\nThis dual view allows both “streams” (thinking and re-\nsponse) to immediately attend to each others’ tokens as\nthey are generated. The response tokens can “see” the latest\nprivate thoughts and summarize them without synchroniza-\ntion delays. Likewise, the thinking “stream” sees the current\nresponse tokens and can pause it if it needs to think longer.\n3"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 4, "text": "This also allows our implementation to encode each gen-\nerated token exactly once and rearrange tokens using the\ngeometry of positional embeddings (see Section 3.3).\n3.2. Mode Switching\nAnother important challenge of asynchronous thinking is\ndeciding when to synchronize. Depending on the task at\nhand, the thinking stream may encounter a sub-task that\nneeds longer “thinking time” to complete. If this is the case,\nthe agent should briefly pause3 writing the response and\nwait for the chain of thought to progress. AsyncReasoning\nlets the LLM itself determine synchronization points.\nTo achieve this, we periodically ask the model if its private\nthoughts are still ahead of the public response, or if it should\npause and think more. From a technical point of view, we\nperiodically insert a special prompt4 into the thinking stream\nand compare the probability of “yes” vs. “no” as the next\ntoken. If the “yes” token is more likely, we keep thinking\nasynchronously. If the “no” token wins out, we pause the\nresponse stream until the model “yes” again. In our current\nimplementation, we insert this question at the end of every\nparagraph or after every T=20 thinking tokens, whichever\ncomes first. Crucially, after the model gives its “yes” or “no”\nresponse, we remove these prompts from view (from the\nKV cache) so that they do not interfere with the model’s\nchain-of-thought.\nWe compare different mode switching prompts in Sec-\ntion 4.1. Overall, we found that existing reasoning LLMs\ncan already control asynchronous reasoning, though they\ndo sometimes make mistakes. It is possible to design more\nsophisticated thinking mechanisms, such as allowing the\nLLM to reason about mode switching in parallel instead of\nanswering immediately. Additionally, one could introduce a\nmode-switching classifier “head” to decide when to pause\nresponding. However, we opt to keep AsyncReasoning sim-\nple and training-free for initial experiments and defer further\nstudy of mode switching to future work.\n3.3. Implementation Details\nTo summarize, AsyncReasoning arranges the thinking and\nresponse tokens in different order, depending on the task,\nprocesses both streams in parallel, and periodically prompts\nthe model to decide if it should pause and think. As a\nresult, our algorithm alternates between two modes: either\nit thinks and writes tokens concurrently, or it simply thinks\nwhile the writing is paused. When only one stream is active,\nAsyncReasoning is equivalent to standard sequential LLM\ninference with a combined KV cache. We focus the rest of\nthis section processing multiple concurrent tokens streams.\n3For voice assistants, it may be better to communicate “Hmm,\nlet me think about it...”, but we don’t do that in our evaluations.\n4\"...\\n\\nWait, are my thoughts ahead of the res-\nponse by enough to continue writing it?\n(yes/no):\n\"\nWe implement concurrent thinking & writing by creating a\ncustom key-value cache and manipulating positional embed-\ndings to account for the dual views from Figure 2. The main\npurpose of this algorithm is to avoid redundant computation\nand KV cache bloat. Instead of encoding tokens twice for\nthinking and writing view, we process each token exactly\nonce and keep one KV cache entry that is “viewed” from\ndifferent relative positions. This optimization is inspired by\na similar rotation trick proposed in Hogwild! Inference [97].\nKey-Value Cache Structure. To implement different posi-\ntional views, we split the model’s KV cache into contiguous\n“blocks” (tensors): the inputs, the thinking stream, and the\noutput stream. As new tokens are generated or added by the\nuser, we store them in the corresponding cache block using\npositional encodings relative to the block start5.\nDuring attention forward pass, we concatenate dot products\nbetween the query and all cache blocks, but we transform\nthe attention query differently for each block to simulate\ndifference in token positions. That way, the same set of\nattention blocks can be combined for both thinking and\nwriting views from Figure 2 without duplicating memory.\nManipulating Positional Information. Almost all mod-\nern LLMs use some form of relative positional informa-\ntion [111, 113, 112]. The most popular variant is rotary\npositional embeddings (RoPE) [113] that rotates query and\nkey vectors by an angle proportional to their index in text\nbefore computing the scaled dot product attention. Note,\nhowever, that if both query and key are rotated by the same\nangle, their dot product does not change. Thus, the attention\noutputs only depend on the difference between query and\nkey positions. In other words, rotating attention keys by\n+∠α is equivalent to rotating the query by −∠α.\nWe take advantage of this property to avoid rotating the\nentire KV cache on each inference step. Instead, we keep\ntrack of the starting positions for each block and rotate at-\ntention queries. Suppose there are three contiguous KV\nblocks: Prompting with P tokens, Thinking with T tokens,\nand Writing with W tokens. When viewed contiguously\n(PTW), the difference between the most recent writer to-\nken and the thinker block is T+W−1 tokens. Thus, when\nrunning the forward pass using the writer’s next token, we\nrotate its query by the RoPE angle corresponding to posi-\ntion T+W−1. In contrast, when the writer looks at their\nown tokens, it will use the query position W−1. The same\nprinciple is applied for all query-key pairs.\nFormally, let ρ(q, i) denote applying RoPE for vector\nq at position i.\nThe writer attends to blocks P, T, W:\nA:=ρ(q, iq)·\nh\nρ(KP , iP\nk ), ρ(KT , iT\nk ), ρ(KW , iW\nk )\ni\n, where\n5For example, given a model with RoPE embeddings, a KV\ncache will always store the 5th response token rotated for position\n5, regardless of how many thinking tokens precede it.\n4"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 5, "text": "P\nP+T\nP+T+W\n0\nQuery \nWriter\nThinker \nBlock\nWriter View\nPositions\nPositions\nPrompt\nBlock\nWriter \nBlock\nThinker View\nPrompt\nBlock\nP+W\nP+W+T\nP\n0\nQuery \nThinker\nWriter \nBlock\n Thinker \nBlock\nImplementation\nQuery \nThinker\nQuery \nThinker\nQuery \nThinker\nQuery \nThinker\nT\nT+W\nT+W+P\nOffset: 0\n Thinker \nBlock\nWriter \nBlock\nPrompt\nBlock\nFigure 3: Concurrent thinking and writing implemented as batched inference. The newly added tokens attend to cache\nblocks with additional query rotations. The checkered areas represent tokens that are not visible in the current view.\nh\nbrackets\ni\ndenote concatenation, iq is the query position,\niP\nk , iT\nk , iW\nk are cache block positions from the writer’s point\nof view (see Figure 3) and KP,T,W are the corresponding\nkey vectors. Then, we can equivalently compute attention as:\nA:=\nh\nρ(q, iq−iP\nk )KP , ρ(q, iq−iT\nk )KT , ρ(q, iq−iW\nk )KW\ni\n.\nThis reformulation allows us to compute KP,T,W once,\nstore it in KV cache and only rotate attention queries for the\ncurrently processed tokens during each forward pass.\nTechnical Considerations. To summarize, our implementa-\ntion consists of the custom KV cache and an attention kernel\nthat uses the query rotation trick above. In practice, we use\nmore than 3 KV blocks: in addition to the prompt, think-\ning and response tokens, we also have short linker tokens\nthat fit between thinking writing blocks. These linkers are\nimplemented as separate KV blocks that are visible only in\none of the views (thinker or writer). If a block is not visible\non the current view, we give it a large position index so it is\nignored by the causal masked LM attention kernel.\nThis implementation can efficiently parallelize thinking and\nwriting the response for small batch sizes. However, when\napplied to large batches, it can be optimized further by only\nprocessing the non-masked query-key pairs that actually\ncontribute to the attention output. In future work, we plan to\nexplore implementing more general kernels for AsyncRea-\nsoning based on vLLM’s Paged Attention [43].\n4. Experiments\nIn this section, we conduct an initial evaluation of AsyncRea-\nsoning and analyze its components. We run our evaluations\non Qwen3-32B [81], a popular medium-sized reasoning\nLLM that can run on a single high-end GPU, with a separate\nTTS method. We run both AsyncReasoning and baselines\non one A100-SXM4 GPU (500W) in bfloat16 precision.\nOn benchmarks. When evaluating asynchronous reasoning\nin voice assistant mode, we initially intended to evaluate on\nestablished spoken reasoning tasks from established audio-\nlanguage model benchmarks [114, 115, 116, 117]. However,\nwe found that modern reasoning models can solve even the\nmulti-step reasoning tasks from these benchmarks with near-\nperfect (≥95%) accuracy without using <think>. Hence,\nwe chose to adopt the approach from [118, 108]: measure\nspoken answer delays on more challenging text tasks.\nMore specifically, we evaluate mathematical reasoning on\nMATH-500 [119, 7], multi-task understanding on MMLU-\nPro [120] and safety reasoning on HarmBench [121]. We\nfocus on two main metrics: i) benchmark-specific quality,\ne.g. accuracy or LLM judge score, using the setup from\nthe original benchmark and ii) real-time delay, defined as\nthe amount of time (seconds) when the user hears no sound\nbecause the voice assistant is still formulating its response,\nincluding both time to first token and intermittent delays.\nTo measure real-time delays, we implement a basic assis-\ntant pipeline that recognizes spoken inputs using whisper-\nbase [52], feeds it into AsyncReasoning (or a baseline algo-\nrithm) to stream response tokens, then group them into short\nchunks (5 tokens or 1 LaTeX expr.) and use tortoise-\ntts [60] with default parameters to generate speech. For\ntasks involving LaTeX, we convert it into Clearspeak [122].\n4.1. Analyzing Mode-switching Criteria\nIn this section, we analyze the impact of different strategies\nfor switching between the concurrent thinking & writing\nmode and the waiting for thoughts mode. For this eval-\nuation, we evaluate Qwen3-32B [81] on the MATH-500\nbenchmark [119] in terms of accuracy and total delay time\nas described above. After the LLM is done formulating the\nresponse, we prompt it to put its answer in \\boxed{...}\n5"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 6, "text": "and check if it is equivalent to a reference answer using\nllm-as-a-judge [124] with the canonical judge setup6.\nWe compare the following configurations:\n1. Baseline (Non-thinking): regular sequential genera-\ntion with <think> mode disabled.\n2. Baseline (Thinking): regular sequential generation\nwith <think> mode enabled.\n3. Interleaved Thinking: prompting the model to think\nand reply in short, interleaved steps, but not asyn-\nchronous. Inspired by [108], but without training.\n4. AsyncReasoning (Q-Continue): the thinker is asked\nwhether the current thoughts are ahead of writing. If\nnot, the writer pauses. See section 3.2 for details.\n5. AsyncReasoning (Q-Pause): Same as above, but the\nquestion is flipped.We ask if the writer should pause7.\n6. AsyncReasoning (Q+TTS): same as Q-continue, but\nwe also pause writing if the current response is more\nthan 10 seconds ahead of real time.\n0\n100\n200\n300\n400\n500\n600\nTotal Delay (s)\n0.82\n0.84\n0.86\n0.88\n0.90\n0.92\n0.94\nMATH-500 Accuracy\nBaseline (Non-thinking)\nBaseline (Thinking)\nInterleaved Thinking\nOurs (Q-Continue)\nOurs (Q-Pause)\nOurs (Q+TTS)\nFigure 4: Comparing the impact of different mode switching\nstrategies and baselines on MATH-500, Qwen3-32B.\nIn the last setup (Q+TTS), we run our TTS pipeline over\nchunks of 5 generated tokens. We keep track of how many\nseconds of speech are synthesized but not yet spoken by\nany given time. If there are more than 10 seconds worth\nof response tokens “in the buffer”, we pause the writer\nautomatically. If not, we let the LLM decide normally.\nThe results in Figure 4 suggest that AsyncReasoning is ca-\npable of reducing real-time delays while preserving most of\nthe accuracy gains from reasoning and outperforms non-\nasynchronous interleaved thinking. However, the exact\ntrade-off between accuracy and delays depends heavily on\nthe mode switching criterion.\n6We use the evaluation protocol from https://github.com/\nopenai/simple-evals with gpt-4-turbo judge.\n7\"...\\n\\nWait, should I pause writing the res-\nponse and think longer?\n(yes/no):\n\"\nOur default criterion (Q-Continue) has the lowest delay\nof the three, but drops about 4% accuracy compared to\nsynchronous thinking. We analyzed the samples where\nasynchronous reasoning produced a different final answer\nand found that the difference can often be attributed to the\nwriter giving their answer too early. We hypothesize that\nthe model is biased to answer “yes” to the mode-switching\nquestion, which corresponds to continuing the answer. In\ncontrast, the Q-Pause variant flips the question so that an-\nswering “yes” pauses the writer, resulting in longer delays\nbut higher accuracy. The TTS-aware mode-switching cri-\nteria (Q+TTS) achieves the middle ground, demonstrating\nthat mode-switching decisions can be effectively guided by\ndownstream speech-generation dynamics. Overall, these\nfindings indicate that AsyncReasoning enables thinking\nmodels to reply in near–real-time while giving more ac-\ncurate answers than the non-thinking variant.\n4.2. Additional Benchmarks\nNext, we evaluate additional benchmarks and real-time met-\nrics. We use AsyncReasoning (Q-Continue) from the pre-\nvious section despite the TTS-based variant having higher\nscore in order to decouple concurrent reasoning from TTS.\nIn addition to MATH-500, we also evaluate multi-task un-\nderstanding on a sample of 500 tasks from the MMLU-\nPro [120] test set. We use canonical MMLU-Pro evaluation\nmethod: the model is allowed to think, then chooses one\nof several possible answers, denoted by a letter (ABCD. . . )\nand compare it against the reference answer to compute\naccuracy (exact match rate). Aside from that, we follow the\nsame evaluation protocol as above.\nIn addition to accuracy and total delay, we measure addi-\ntional performance metrics:\n• Time to first token (TTFT): the wall time delay until\nthe system generates the first non-thinking token.\n• Total delay: same in the previous section. We run TTS\non LLM-generated response tokens and measure the\ntotal delay experienced by the user.\n• Adjusted delay: similar to total delay, but we subtract\n1 second from every contiguous pause to account for\nhumans finding short pauses less noticeable.\n• Steps to first token (STFT): the number of infer-\nence steps (LLM forward passes) before the first non-\nthinking token is generated, GPU-agnostic.\n• Steps Delay: The average number of inference steps\n(forward passes) that do not generate a response token.\nWe summarize our results in Table 1: across both bench-\nmarks, we found that AsyncReasoning significantly reduces\nboth time to first token and overall delays time while provid-\ning more accurate answers than the non-thinking baseline,\nthough not quite as accurate as slow (synchronous) reason-\ning mode. Similarly to the previous section, we found that\n6"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 7, "text": "Table 1: Evaluation of AsyncReasoning on MATH-500 and MMLU-Pro using Qwen3-32B with additional efficiency metrics.\nArrows ↑/ ↓denote “higher/lower is better”, respectively. Refer to Section 4 for additional details on metrics.\nInference Setup\nAccuracy↑\nTTFT↓\nTotal Delay↓\nAdjusted Delay↓\nSTFT↓\nSteps Delay↓\nMATH-500\nBaseline (Thinking)\n0.932\n592.05\n592.70\n591.51\n3911.55\n3911.55\nBaseline (Non-thinking)\n0.834\n0.94\n1.96\n0.86\n1\n1\nAsyncReasoning (Q-Continue)\n0.890\n2.49\n2.91\n1.732\n23.71\n247.79\nMMLU-Pro (500 samples)\nBaseline (Thinking)\n0.812\n340.07\n340.53\n339.47\n2284.82\n2284.82\nBaseline (Non-thinking)\n0.696\n1.17\n5.07\n4.03\n1\n1\nAsyncReasoning (Q-Continue)\n0.758\n4.63\n59.01\n51.94\n27.30\n187.37\nmany of the errors can be attributed to writer giving the\nanswer prematurely. In other words, the thinker does not al-\nways pause the writer when needed, suggesting that further\nimprovements to the mode-switching strategy can improve\naccuracy, which is a promising direction for future work.\n4.3. Asynchronous Reasoning about Safety\nTo evaluate the impact of asynchronous reasoning on safety,\nwe conduct experiments on the HarmBench validation\nset [121] using the Virtual Context attack [125]. We use\nllm-as-a-judge [124] evaluation (gpt-4o-mini) where only\nactionable harmful instructions count as a successful attack.\nWe compare the Attack Success Rate (ASR) across four\nsetups using the Qwen3-32B model: (1) Baseline (Non-\nthinking), (2) Baseline (Thinking), (3) AsyncReasoning\n(Q-Continue), and (4) AsyncReasoning (Safety prompt) that\nis additionally instructed to verify safety before responding.\nThe full safety prompt is included in Appendix B.\nQuantitative Results. We summarize our findings in Ta-\nble 2. Consistent with recent findings on the “Cost of Think-\ning” [76], we observe that enabling reasoning in the base-\nline model actually increases vulnerability (ASR rises from\n2.5% to 13.0%). The model effectively “talks itself into”\nanswering harmful queries by adopting a helpful persona or\nover-analyzing the technical aspects of the prompt.\nAsyncReasoning (Q-Continue) (11.5% ASR) remains sim-\nilarly vulnerable to the thinking baseline. However, by\nintroducing additional safety instructions into the thinker’s\nprompt we successfully reduce the ASR to 2.0% while pre-\nserving accuracy on MATH-500 benchmark.\nIn practice, this allows safety-minded reasoning in stream-\ning LLM APIs and other time-sensitive applications without\nthe need for specialized fine-tuning. AsyncReasoning can\nstream tokens normally for benign queries, only pausing\ngeneration for potentially unsafe responses.\nInference Setup\nASR↓\nAccuracy↑\nBaseline (Non-thinking)\n0.025\n0.834\nBaseline (Thinking)\n0.130\n0.932\nAsyncReasoning (Q-Continue)\n0.115\n0.890\nAsyncReasoning (Safety Prompt)\n0.020\n0.878\nTable 2: Attack Success Rate on HarmBench (Virtual Con-\ntext attack) and Accuracy on MATH-500 for Qwen3-32B.\nFailure Mode Analysis. While AsyncReasoning allows for\nreal-time safety checks, the asynchronous nature of genera-\ntion introduces specific failure modes where the writer may\noutput harmful content before the thinker intervenes. We\nidentify three primary categories of such safety failures:\n1. Race Condition: The writer begins generating a helpful\nresponse immediately based on the prompt. Although\nthe thinker eventually concludes the request is unsafe,\nthe writer has already streamed harmful tokens (e.g., the\nfirst steps of a dangerous recipe) to the user before the\nrefusal signal is propagated.\n2. Context Leakage: The thinker analyzes the harmful re-\nquest by recalling technical details (e.g., explaining how\na specific SQL injection works to verify its danger). The\nwriter, attending to the thinker’s cache, interprets these\ntechnical details as the desired answer and formulates\nthem into a response, bypassing the thinker’s intent.\n3. Educational Loophole: The thinker adopts an educa-\ntional persona to explain why a request is dangerous.\nThe writer latches onto this educational content and re-\nformats it as a set of instructions, stripping away the\nsafety framing context.\nThese findings suggest that, while AsyncReasoning can\neffectively filter attacks, strict gating mechanisms (e.g., en-\nsuring the thinker has a “head start” on safety verification)\nare necessary to prevent race conditions in highly sensitive\nscenarios. We will investigate this further in future work.\n7"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 8, "text": "5. Discussion & Future Work\nIn this preprint, we formulated AsyncReasoning — a\ntraining-free method that allows reasoning LLMs to think\nand write concurrently. Our preliminary experiments sug-\ngest that the proposed approach can indeed overlap thinking\nand writing and reduce thinking delays while giving more\naccurate answers than the non-thinking models. This allows\nLLMs to think longer and give more thoughtful answers in\ntime-sensitive applications such as voice assistants, embod-\nied agents, or safety-minded use cases.\nThis leaves many interesting directions for further research\nand analysis. In future work, we will look more into strate-\ngies for mode-switching: determining when to pause writing\nthe response and wait for more thoughts. We also plan to\nexpand the scope of our experiments with additional mod-\nels, task types, and comparison to non-training-free base-\nlines. Among others, it would be interesting to quantify the\nmethod’s ability to process asynchronous inputs, such as\ntask clarifications for voice assistants or environment read-\nouts for agents. Additionally, we will work on integrating\nAsyncReasoning with vLLM [43].\n6. Acknowledgements\nWe would like to thank Andrey Shukshov for his helpful\nadvice about efficient GPU kernel design. We also thank\nGleb Rodionov for proofreading and helpful suggestions on\nexperiment design and paper presentation.\nReferences\n[1] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral\nKumar.\nScaling llm test-time compute optimally\ncan be more effective than scaling model parameters.\narXiv preprint arXiv:2408.03314, 2024.\n[2] Mirac Suzgun, Nathan Scales, Nathanael Scharli,\nSebastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V. Le, Ed H. Chi,\nDenny Zhou, and Jason Wei. Challenging big-bench\ntasks and whether chain-of-thought can solve them.\nIn Annual Meeting of the Association for Computa-\ntional Linguistics, 2022.\n[3] Edward Beeching, Lewis Tunstall, and Sasha Rush.\nScaling test-time compute with open models.\n[4] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. Chain-of-thought prompting elicits reasoning\nin large language models. Advances in neural infor-\nmation processing systems, 35:24824–24837, 2022.\n[5] Takeshi Kojima, Shixiang Shane Gu, Machel Reid,\nYutaka Matsuo, and Yusuke Iwasawa. Large lan-\nguage models are zero-shot reasoners.\nArXiv,\nabs/2205.11916, 2022.\n[6] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L. Griffiths,\nYuan Cao,\nand Karthik\nNarasimhan.\nTree of thoughts: Deliberate prob-\nlem solving with large language models.\nArXiv,\nabs/2305.10601, 2023.\n[7] Hunter Lightman, Vineet Kosaraju, Yura Burda, Har-\nrison Edwards, Bowen Baker, Teddy Lee, Jan Leike,\nJohn Schulman, Ilya Sutskever, and Karl Cobbe.\nLet’s verify step by step. ArXiv, abs/2305.20050,\n2023.\n[8] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alexan-\nder J. Smola. Automatic chain of thought prompting\nin large language models. ArXiv, abs/2210.03493,\n2022.\n[9] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xi-\nang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke\nZettlemoyer, Percy Liang, Emmanuel Candès, and\nTatsunori Hashimoto. s1: Simple test-time scaling.\narXiv preprint arXiv:2501.19393, 2025.\n[10] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì,\nRoberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer:\nLanguage models can teach themselves to use tools.\nArXiv, abs/2302.04761, 2023.\n[11] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. React:\nSynergizing reasoning and acting in language models.\nArXiv, abs/2210.03629, 2022.\n[12] Leo Gao, Aman Madaan, Shuyan Zhou, Karthik\nNarasimhan, and Danqi Chen. Pal: Program-aided\nlanguage models. In Proceedings of the 40th Inter-\nnational Conference on Machine Learning, volume\n202, pages 10764–10791. PMLR, 2023.\n[13] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng\nLi, Weiming Lu, and Yue Ting Zhuang. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhugging face. ArXiv, abs/2303.17580, 2023.\n[14] Yujia Qin, Shi Liang, Yining Ye, Kunlun Zhu, Lan\nYan, Ya-Ting Lu, Yankai Lin, Xin Cong, Xiangru\nTang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing\nXie, Jie Zhou, Marc H. Gerstein, Dahai Li, Zhiyuan\nLiu, and Maosong Sun. Toolllm: Facilitating large\nlanguage models to master 16000+ real-world apis.\nArXiv, abs/2307.16789, 2023.\n[15] Zhanna Azerbayev, Dhruv Patel, Sébastien Bubeck,\nRonen Eldan, Yin Tat Lee, Yuanzhong Li, Tamas Sar-\n8"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 9, "text": "los, and Yi Zhang. Llemma: An open language model\nfor mathematics. In Proceedings of the Twelfth In-\nternational Conference on Learning Representations,\n2024.\n[16] Ziyue Wang, Zhiyuan Zhao, Minqi Peng, Shiqi\nChen, Mengdi Yang, Chi-Min Lin, Pratyush Sharma,\nSébastien Bubeck, Ronen Eldan, Yuanzhong Li,\nYin Tat Lee, and Yi Zhang. Mathcoder: Seamless\ncode integration in LLMs for enhanced mathemati-\ncal reasoning. In Proceedings of the Twelfth Inter-\nnational Conference on Learning Representations,\n2024.\n[17] Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen,\nKarol Hausman, Dorsa Sadigh, Sergey Levine, Li Fei-\nFei, Fei Xia, and Brian Ichter. Chain of code: Reason-\ning with a language model-augmented code emulator.\nIn Proceedings of the 41st International Conference\non Machine Learning, volume 235 of Proceedings\nof Machine Learning Research, pages 28259–28277.\nPMLR, 2024. arXiv preprint arXiv:2312.04474.\n[18] OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer,\nAdam Richardson, Ahmed El-Kishky, Aiden Low,\nAlec Helyar, Aleksander Madry, and Alex Beutel\net al. Openai o1 system card, 2024.\n[19] Google\nDeepMind.\nGemini\n2.5:\nOur\nNewest\nGemini\nModel\nwith\nThinking.\nhttps://blog.google/technology/google-\ndeepmind/gemini-model-thinking-updates-\nmarch-2025/#gemini-2-5-thinking,\n2025.\nAccessed: 2025-04-07.\n[20] Anthropic. Claude 3.7 sonnet and claude code, 2024.\nAccessed: 2025.04.02.\n[21] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei\nZhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qi-\nhao Zhu, Shirong Ma, Peiyi Wang, and Xiao Bi et al.\nDeepseek-r1: Incentivizing reasoning capability in\nllms via reinforcement learning, 2025.\n[22] Qwen Team. Qwq-32b: Embracing the power of\nreinforcement learning, March 2025.\n[23] Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen,\nJiahao Chen, Ningxin Chen, Ruijue Chen, Yanru\nChen, Yuankun Chen, Yutian Chen, Zhuofu Chen,\nJialei Cui, Hao Ding, Mengnan Dong, Angang Du,\nChenzhuang Du, Dikang Du, Yulun Du, Yu Fan,\nYichen Feng, Kelin Fu, Bofei Gao, Hongcheng Gao,\nPeizhong Gao, Tong Gao, Xinran Gu, Longyu Guan,\nHaiqing Guo, Jianhang Guo, Hao Hu, Xiaoru Hao,\nTianhong He, Weiran He, Wenyang He, Chao Hong,\nYangyang Hu, Zhenxing Hu, Weixiao Huang, Zhiqi\nHuang, Zihao Huang, Tao Jiang, Zhejun Jiang, Xinyi\nJin, Yongsheng Kang, Guokun Lai, Cheng Li, Fang\nLi, Haoyang Li, Ming Li, Wentao Li, Yanhao Li,\nYiwei Li, Zhaowei Li, Zheming Li, Hongzhan Lin,\nXiaohan Lin, Zongyu Lin, Chengyin Liu, Chenyu\nLiu, Hongzhang Liu, Jingyuan Liu, Junqi Liu, Liang\nLiu, Shaowei Liu, T. Y. Liu, Tianwei Liu, Weizhou\nLiu, Yangyang Liu, Yibo Liu, Yiping Liu, Yue Liu,\nZhengying Liu, Enzhe Lu, Lijun Lu, Shengling Ma,\nXinyu Ma, Yingwei Ma, Shaoguang Mao, Jie Mei,\nXin Men, Yibo Miao, Siyuan Pan, Yebo Peng, Ruoyu\nQin, Bowen Qu, Zeyu Shang, Lidong Shi, Shengyuan\nShi, Feifan Song, Jianlin Su, Zhengyuan Su, Xinjie\nSun, Flood Sung, Heyi Tang, Jiawen Tao, Qifeng\nTeng, Chensi Wang, Dinglu Wang, Feng Wang, Haim-\ning Wang, Jianzhou Wang, Jiaxing Wang, Jinhong\nWang, Shengjie Wang, Shuyi Wang, Yao Wang, Yejie\nWang, Yiqin Wang, Yuxin Wang, Yuzhi Wang, Zhaoji\nWang, Zhengtao Wang, Zhexu Wang, Chu Wei, Qian-\nqian Wei, Wenhao Wu, Xingzhe Wu, Yuxin Wu,\nChenjun Xiao, Xiaotong Xie, Weimin Xiong, Boyu\nXu, Jing Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting\nXu, Weixin Xu, Xinran Xu, Yangchuan Xu, Ziyao\nXu, Junjie Yan, Yuzi Yan, Xiaofei Yang, Ying Yang,\nZhen Yang, Zhilin Yang, Zonghan Yang, Haotian\nYao, Xingcheng Yao, Wenjie Ye, Zhuorui Ye, Bo-\nhong Yin, Longhui Yu, Enming Yuan, Hongbang\nYuan, Mengjie Yuan, Haobing Zhan, Dehao Zhang,\nHao Zhang, Wanlu Zhang, Xiaobin Zhang, Yangkun\nZhang, Yizhi Zhang, Yongting Zhang, Yu Zhang, Yu-\ntao Zhang, Yutong Zhang, Zheng Zhang, Haotian\nZhao, Yikai Zhao, Huabin Zheng, Shaojie Zheng,\nJianren Zhou, Xinyu Zhou, Zaida Zhou, Zhen Zhu,\nWeiyu Zhuang, and Xinxing Zu. Kimi k2: Open\nagentic intelligence, 2025.\n[24] ARC Prize Foundation. Openai’s new o3 system\nscores breakthrough on arc-agi-pub, 2024. Accessed:\n2025.03.28.\n[25] Humanity’s Last Exam Contributors. Humanity’s last\nexam: A benchmark for frontier ai capabilities. arXiv\npreprint arXiv:2501.14249, 2025.\n[26] Nicole Landi, Stephen Frost, W Menc, Rebecca San-\ndak, and Kenneth Pugh. Neurobiological bases of\nreading comprehension: Insights from neuroimag-\ning studies of word-level and text-level processing\nin skilled and impaired readers. Reading & writing\nquarterly : overcoming learning difficulties, 29:145–\n167, 04 2013.\n[27] Bingjiang Lyu, William D. Marslen-Wilson, Yuxing\nFang, and Lorraine K. Tyler. Finding structure during\nincremental speech comprehension. eLife, 12:e89311,\n2023.\n9"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 10, "text": "[28] Sarah Bro Trasmundi and Juan Toro. Mind wandering\nin reading: An embodied approach. Frontiers in\nHuman Neuroscience, 17, 2023.\n[29] Daisuke Akiba. Ctrl + alt + inner speech: A ver-\nbal–cognitive scaffold (vcs) model of pathways to\ncomputational thinking.\nJournal of Intelligence,\n13(12), 2025.\n[30] Kevin P. Madore and Anthony D. Wagner. Multicosts\nof multitasking. Cerebrum : the Dana Forum on\nBrain Science, 2019:cer–04–19, 2019.\n[31] Maurizio Corbetta, Gaurav Patel, and Gordon L\nShulman.\nThe reorienting system of the human\nbrain: from environment to theory of mind. Neu-\nron, 58(3):306–324, 2008.\n[32] OpenAI.\nGpt-4o system card.\nOnline techni-\ncal report, 2024.\nVoice-mode multimodal model\nsupporting audio, text, and vision. Available at\nhttps://openai.com/index/hello-gpt-4o.\n[33] Paul K. Rubenstein, Chulayuth Asawaroengchai,\nDuc Dung Nguyen, Ankur Bapna, Zalán Bor-\nsos, Félix de Chaumont Quitry, Peter Chen, Dalia\nEl Badawy, Wei Han, Eugene Kharitonov, Hannah\nMuckenhirn, Dirk Padfield, James Qin, Danny Rozen-\nberg, Tara Sainath, Johan Schalkwyk, Matt Sharifi,\nMichelle Tadmor Ramanovich, Marco Tagliasacchi,\nAlexandru Tudor, Mihajlo Velimirovi´c, Damien Vin-\ncent, Jiahui Yu, Yongqiang Wang, Vicky Zayats, Neil\nZeghidour, Yu Zhang, Zhishuai Zhang, Lukas Zilka,\nand Christian Frank. Audiopalm: A large language\nmodel that can speak and listen.\narXiv preprint\narXiv:2306.12925, 2023.\n[34] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan,\nPengyu Wang, Yaqian Zhou, and Xipeng Qiu.\nSpeechgpt: Empowering large language models with\nintrinsic cross-modal conversational abilities. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2023, pages 15757–15773. Association for\nComputational Linguistics, December 2023.\n[35] Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang,\nShiliang Zhang, Zhijie Yan, Chang Zhou, and Jin-\ngren Zhou. Qwen-audio: Advancing universal audio\nunderstanding via unified large-scale audio-language\nmodels. arXiv preprint arXiv:2311.07919, 2023.\n[36] Zhifei Xie and Changqiao Wu. Mini-omni: Language\nmodels can hear, talk while thinking in streaming.\narXiv preprint arXiv:2408.16725, 2024.\n[37] Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma,\nShaolei Zhang, and Yang Feng. Llama-omni: Seam-\nless speech interaction with large language models.\narXiv preprint arXiv:2409.06666, 2024.\n[38] Alexandre Défossez, Laurent Mazaré, Manu Orsini,\nAmélie Royer, Patrick Pérez, Hervé Jégou, Édouard\nGrave, and Neil Zeghidour. Moshi: a speech-text\nfoundation model for real-time dialogue, 2024.\n[39] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen\nChebotar, Omar Cortes, Byron David, Chelsea Finn,\nChuyuan Fu, Keerthana Gopalakrishnan, Karol Haus-\nman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian\nIbarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario\nJauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil\nJoshi, Ryan Julian, Dmitry Kalashnikov, Yuheng\nKuang, Kuang-Huei Lee, Sergey Levine, Yao Lu,\nLinda Luu, Carolina Parada, Peter Pastor, Jornell\nQuiambao, Kanishka Rao, Jarek Rettinghouse, Diego\nReyes, Pierre Sermanet, Nicolas Sievers, Clayton\nTan, Alexander Toshev, Vincent Vanhoucke, Fei Xia,\nTed Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and\nAndy Zeng. Do as i can, not as i say: Grounding\nlanguage in robotic affordances. ArXiv, 2022.\n[40] Anthony Brohan, Noah Brown, Justice Carbajal, Yev-\ngen Chebotar, Xi Chen, Krzysztof Choromanski,\nTianli Ding, Danny Driess, Avinava Dubey, Chelsea\nFinn, Pete Florence, Chuyuan Fu, Montse Gonza-\nlez Arenas, Keerthana Gopalakrishnan, Kehang Han,\nKarol Hausman, Alexander Herzog, Jasmine Hsu,\nBrian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian,\nDmitry Kalashnikov, Yuheng Kuang, Isabel Leal,\nLisa Lee, Tsang-Wei Lee, Sergey Levine, Yao Lu,\nHenryk Michalewski, Igor Mordatch, Karl Pertsch,\nKanishka Rao, Krista Reymann, Michael Ryoo, Gre-\ncia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar\nSingh, Anikait Singh, Radu Soricut, Huong Tran,\nVincent Vanhoucke, Quan Vuong, Ayzaan Wahid,\nStefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia,\nTed Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Bri-\nanna Zitkovich. Rt-2: Vision-language-action models\ntransfer web knowledge to robotic control. ArXiv,\n2023.\n[41] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey\nLynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\nWahid, Jonathan Tompson, Quan Vuong, Tianhe\nYu, Wenlong Huang, Yevgen Chebotar, Pierre Ser-\nmanet, Daniel Duckworth, Sergey Levine, Vincent\nVanhoucke, Karol Hausman, Marc Toussaint, Klaus\nGreff, Andy Zeng, Igor Mordatch, and Pete Florence.\nPalm-e: An embodied multimodal language model.\narXiv preprint arXiv:2303.03378, 2023.\n[42] OpenAI.\nChatgpt deep research:\nSupport for\nuser update and multitasking features.\nhttps://\nchat.openai.com, 2025.\nAccessed 7 December\n10"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 11, "text": "2025. In late 2025, the Deep Research feature was\nupdated to allow user to communicate with the agent\nwhile it performs research via the \"Update\" button.\n[43] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph Gon-\nzalez, Hao Zhang, and Ion Stoica. Efficient memory\nmanagement for large language model serving with\npagedattention. In Proceedings of the 29th Sympo-\nsium on Operating Systems Principles, pages 611–\n626, 2023.\n[44] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff\nHuang, Chuyue Sun, Cody Hao Yu, Shiyi Cao, Chris-\ntos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark\nBarrett, and Ying Sheng. Efficiently programming\nlarge language models using sglang, 2023.\n[45] Gemini\nlive\n(voice\nmode).\nhttps:\n//gemini.google/overview/gemini-live/,\n2024. Accessed: 2025-12-01.\n[46] Anthropic. Using voice mode on claude mobile apps.\nhttps://support.claude.com/en/articles/\n11101966-using-voice-mode-on-claude-\nmobile-apps, 2025. Accessed: December 1, 2025.\n[47] James\nFlamino,\nMohammed\nShahid\nModi,\nBoleslaw K. Szymanski, Brendan Cross, and Colton\nMikolajczyk. Testing the limits of large language\nmodels in debating humans.\nScientific Reports,\n15:13852, 2025.\n[48] Stephanie Houde, Kristina Brimijoin, Michael\nMuller, Steven I. Ross, Dario Andres Silva Moran,\nGabriel Enrique Gonzalez, Siya Kunde, Morgan A.\nForeman, and Justin D. Weisz.\nControlling ai\nagent participation in group conversations: A human-\ncentered approach. In Proceedings of the 30th Inter-\nnational Conference on Intelligent User Interfaces,\nIUI ’25, page 390–408, New York, NY, USA, 2025.\nAssociation for Computing Machinery.\n[49] K. H. Davis, R. Biddulph, and S. Balashek. Auto-\nmatic recognition of spoken digits. The Journal of\nthe Acoustical Society of America, 24(6):637–642, 11\n1952.\n[50] Daniel Povey, Arnab Ghoshal, Gilles Boulianne,\nLukas Burget, Ondrej Glembek, Nagendra Goel,\nMirko Hannemann, Petr Motlicek, Yanmin Qian,\nPetr Schwarz, Jan Silovsky, Georg Stemmer, and\nKarel Vesely. Kaldi: A toolkit for speech recogni-\ntion. https://kaldi-asr.org, 2011. Open-source\nspeech recognition toolkit.\n[51] Steffen Schneider, Alexei Baevski, Ronan Collobert,\nand Michael Auli.\nwav2vec: Unsupervised pre-\ntraining for speech recognition, 2019.\n[52] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brock-\nman, Christine McLeavey, and Ilya Sutskever. Robust\nspeech recognition via large-scale weak supervision,\n2022.\n[53] N. Umeda, H. Omura, and O. Fujimura. First com-\nplete text-to-speech system. Technical report, Elec-\ntrotechnical Laboratory, Japan, Tokyo, Japan, 1968.\n[54] Heiga Zen, Keiichi Tokuda, and Alan W. Black. Sta-\ntistical parametric speech synthesis. Speech Commu-\nnication, 51(11):1039–1064, 2009.\n[55] Aaron van den Oord, Sander Dieleman, Heiga\nZen, Karen Simonyan, Oriol Vinyals, Alex Graves,\nNal Kalchbrenner, Andrew Senior, and Koray\nKavukcuoglu.\nWavenet: A generative model for\nraw audio. In Proceedings of the 9th ISCA Speech\nSynthesis Workshop, 2016.\n[56] Yuxuan Wang, R. J. Skerry-Ryan, Daisy Stanton,\nYonghui Wu, Ron J. Weiss, Navdeep Jaitly, Zongheng\nYang, Ying Xiao, Zhifeng Chen, Samy Bengio, Quoc\nLe, Yannis Agiomyrgiannakis, Rob Clark, and Rif A.\nSaurous. Tacotron: Towards end-to-end speech syn-\nthesis. In INTERSPEECH, pages 4006–4010. ISCA,\n2017.\n[57] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike\nSchuster, Navdeep Jaitly, Zongheng Yang, Zhifeng\nChen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan,\nRif A. Saurous, Yannis Agiomyrgiannakis, and\nYonghui Wu. Natural tts synthesis by conditioning\nwavenet on mel spectrogram predictions, 2018.\n[58] Ryan J. Prenger, Rafael Valle, and Bryan Catanzaro.\nWaveglow: A flow-based generative network for\nspeech synthesis. ICASSP 2019 - 2019 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 3617–3621, 2018.\n[59] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae.\nHifi-gan: generative adversarial networks for efficient\nand high fidelity speech synthesis. In Proceedings\nof the 34th International Conference on Neural In-\nformation Processing Systems, NIPS ’20, Red Hook,\nNY, USA, 2020. Curran Associates Inc.\n[60] James Betker. Better speech synthesis through scal-\ning. arXiv preprint arXiv:2305.07243, 2023. Tortoise\nTTS: expressive multi-voice text-to-speech.\n[61] Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong\nWang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting\nHe, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake\nGuo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang,\n11"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 12, "text": "Hongkun Hao, Zishan Guo, Baosong Yang, Bin\nZhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin\nChen, Xuejing Liu, Peng Wang, Mingkun Yang, Day-\niheng Liu, Xingzhang Ren, Bo Zheng, Rui Men,\nFan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren\nZhou, and Junyang Lin. Qwen3-omni technical re-\nport. arXiv preprint arXiv:2509.17765, 2025.\n[62] Ruaridh Mon-Williams, Gen Li, Ran Long, Wenqian\nDu, Christopher G. Lucas, et al. Embodied large\nlanguage models enable robots to complete complex\ntasks in unpredictable environments. Nature Machine\nIntelligence, 7:592–601, 2025.\n[63] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-\ndlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and\nAnima Anandkumar. Voyager: An open-ended em-\nbodied agent with large language models. ArXiv,\n2023.\n[64] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi\nWang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei,\nAnima Anandkumar, Yuke Zhu, and Linxi Fan.\nVima: General robot manipulation with multimodal\nprompts. ArXiv, 2023.\n[65] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti,\nTed Xiao, Ashwin Balakrishna, Suraj Nair, Rafael\nRafailov, Ethan Foster, Grace Lam, Pannag Sanketi,\nQuan Vuong, Thomas Kollar, Benjamin Burchfiel,\nRuss Tedrake, Dorsa Sadigh, Sergey Levine, Percy\nLiang, and Chelsea Finn. Openvla: An open-source\nvision-language-action model, 2024.\n[66] Ranjan Sapkota, Yang Cao, Konstantinos I. Roume-\nliotis, and Manoj Karkee. Vision-language-action\nmodels: Concepts, progress, applications and chal-\nlenges, 2025.\n[67] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu,\nXiaojian (Shawn) Ma, and Yitao Liang. Describe,\nexplain, plan and select: Interactive planning with\nllms enables open-world multi-task agents. In A. Oh,\nT. Naumann, A. Globerson, K. Saenko, M. Hardt, and\nS. Levine, editors, Advances in Neural Information\nProcessing Systems, volume 36, pages 34153–34189.\nCurran Associates, Inc., 2023.\n[68] Charles Cao, Feiyi Wang, Lisa Lindley, and Zejiang\nWang. Managing linux servers with llm-based ai\nagents: An empirical evaluation with gpt4. Machine\nLearning with Applications, 17:100570, 2024.\n[69] Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhen-\nmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu,\nand Lingpeng Kong. Os-copilot: Towards generalist\ncomputer agents with self-improvement, 2024.\n[70] China. Xiaoyan Zhang, Zhao Yang, Jiaxuan Liu,\nYanda Li, Yucheng Han, Xin Chen, Zebiao Huang,\nBin Fu, and Gang Yu. Appagent: Multimodal agents\nas smartphone users. Proceedings of the 2025 CHI\nConference on Human Factors in Computing Systems,\n2023.\n[71] Brian Singer, Keane Lucas, Lakshmi Adiga, Meghna\nJain, Lujo Bauer, and Vyas Sekar. On the feasibility\nof using llms to execute multistage network attacks,\n01 2025.\n[72] Tomek Korbak, Mikita Balesni, Elizabeth Barnes,\nYoshua Bengio, Joe Benton, Joseph Bloom, Mark\nChen, Alan Cooney, Allan Dafoe, Anca Dragan,\nScott Emmons, Owain Evans, David Farhi, Ryan\nGreenblatt, Dan Hendrycks, Marius Hobbhahn, Evan\nHubinger, Geoffrey Irving, Erik Jenner, Daniel Koko-\ntajlo, Victoria Krakovna, Shane Legg, David Lindner,\nDavid Luan, Aleksander M ˛adry, Julian Michael, Neel\nNanda, Dave Orr, Jakub Pachocki, Ethan Perez, Mary\nPhuong, Fabien Roger, Joshua Saxe, Buck Shlegeris,\nMartín Soto, Eric Steinberger, Jasmine Wang, Woj-\nciech Zaremba, Bowen Baker, Rohin Shah, and Vlad\nMikulik. Chain of Thought Monitorability: A New\nand Fragile Opportunity for AI Safety. arXiv, 2025.\n[73] Bowen Baker, Joost Huizinga, Leo Gao, Zehao\nDou, Melody Y. Guan, Aleksander Madry, Wojciech\nZaremba, Jakub W. Pachocki, and David Farhi. Moni-\ntoring reasoning models for misbehavior and the risks\nof promoting obfuscation. ArXiv, abs/2503.11926,\n2025.\n[74] Chengda Lu, Xiaoyu Fan, Yu Huang, Rongwu Xu,\nJijie Li, and Wei Xu. Does chain-of-thought reason-\ning really reduce harmfulness from jailbreaking? In\nWanxiang Che, Joyce Nabende, Ekaterina Shutova,\nand Mohammad Taher Pilehvar, editors, Findings of\nthe Association for Computational Linguistics: ACL\n2025, pages 6523–6546, Vienna, Austria, July 2025.\nAssociation for Computational Linguistics.\n[75] James Chua, Jan Betley, Mia Taylor, and Owain\nEvans. Thought crime: Backdoors and emergent\nmisalignment in reasoning models, 2025.\n[76] Fan Yang.\nThe cost of thinking: Increased jail-\nbreak risk in large language models. arXiv preprint\narXiv:2508.10032, 2025.\n[77] Kaiwen Zhou, Xuandong Zhao, Gaowen Liu, Jayanth\nSrinivasa, Aosong Feng, Dawn Song, and Xin Eric\nWang. Safekey: Amplifying aha-moment insights for\nsafety reasoning. arXiv preprint arXiv:2505.16186,\n2025.\n12"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 13, "text": "[78] Xinyue Lou, You Li, Jinan Xu, Xiangyu Shi, Chi\nChen, and Kaiyu Huang.\nThink in safety: Un-\nveiling and mitigating safety alignment collapse in\nmultimodal large reasoning model.\nIn Christos\nChristodoulopoulos, Tanmoy Chakraborty, Carolyn\nRose, and Violet Peng, editors, Proceedings of the\n2025 Conference on Empirical Methods in Natural\nLanguage Processing, pages 5167–5186, Suzhou,\nChina, November 2025. Association for Computa-\ntional Linguistics.\n[79] Tong Wu, Chong Xiang, Jiachen T. Wang, G. Edward\nSuh, and Prateek Mittal. Effectively controlling rea-\nsoning models through thinking intervention, 2025.\n[80] Yichi Zhang, Yue Ding, Jingwen Yang, et al. Towards\nsafe reasoning in large reasoning models via correc-\ntive intervention. arXiv preprint arXiv:2509.24393,\n2025.\n[81] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang,\nBinyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,\nChengen Huang, Chenxu Lv, Chujie Zheng, Dayi-\nheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge,\nHaoran Wei, Huan Lin, Jialong Tang, Jian Yang,\nJianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi\nYang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai\nDang, Keqin Bao, Kexin Yang, Le Yu, Lianghao\nDeng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang,\nPeng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan\nLiu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao\nYin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xu-\nancheng Ren, Yang Fan, Yang Su, Yichang Zhang,\nYinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang,\nZeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan\nQiu. Qwen3 technical report, 2025.\n[82] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu\nZhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, An-\ndrew Wen, Shaochen Zhong, Na Zou, Hanjie Chen,\nand Xia Hu. A survey on efficient reasoning for large\nlanguage models. arXiv preprint arXiv:2503.16419,\nmar 2025. Version 4 (last updated August 21, 2025).\n[83] Silei Xu,\nWenhao Xie,\nLingxiao Zhao,\nand\nPengcheng He. Chain of draft: Thinking faster by\nwriting less. arXiv preprint arXiv:2502.18600, feb\n2025. Version 2 (last revised 3 Mar 2025).\n[84] Simon A. Aytes, Jinheon Baek, and Sung Ju Hwang.\nSketch-of-thought: Efficient llm reasoning with adap-\ntive cognitive-inspired sketching.\narXiv preprint\narXiv:2503.05179, mar 2025. Version 4 (last revised\n24 Oct 2025).\n[85] Heming Xia, Chak Tou Leong, Wenjie Wang, Yongqi\nLi, and Wenjie Li. Tokenskip: Controllable chain-\nof-thought compression in llms.\narXiv preprint\narXiv:2502.12067, feb 2025. Version 3 (last revised\n16 Sep 2025); EMNLP 2025 (Long Paper), camera-\nready version.\n[86] Gengyang Li, Yifeng Gao, Yuming Li, and Yunfang\nWu. Thinkless: A training-free inference-efficient\nmethod for reducing reasoning redundancy. arXiv\npreprint arXiv:2505.15684, may 2025. Version 2\n(last revised 23 May 2025).\n[87] Guosheng Liang, Longguang Zhong, Ziyi Yang, and\nXiaojun Quan. Thinkswitcher: Dynamic switching\nbetween short and long chain-of-thought reasoning\nin large reasoning models, 2025. arXiv preprint.\n[88] Ruiqi Zhang, Changyi Xiao, and Yixin Cao. Long or\nshort cot? investigating instance-level switch of large\nreasoning models, 2025. arXiv preprint.\n[89] Wencheng Zhang, Shiqin Qiao, Lingjie Luo, Yinfeng\nLi, Chuanyang Zheng, Qian Xu, Meng Li, Yong Gui,\nYijun He, Jianing Qiu, Jindong Hong, and Jiankai\nSun. Synapseroute: An auto-route switching frame-\nwork on dual-state large language model, 2025. arXiv\npreprint.\n[90] Haoyu Zheng, Zhuonan Wang, Yuqian Yuan, Tian-\nwei Lin, Wenqiao Zhang, Zheqi Lv, Juncheng Li,\nSiliang Tang, Yueting Zhuang, and Hongyang He.\nFast thinking for large language models, 2025. arXiv\npreprint.\n[91] Xiao Pu,\nMichael Saxon,\nWenyue Hua,\nand\nWilliam Yang Wang. Thoughtterminator: Bench-\nmarking, calibrating, and mitigating overthinking in\nreasoning models, 2025. arXiv preprint.\n[92] Renliang Sun, Wei Cheng, Dawei Li, Haifeng Chen,\nand Wei Wang. Stop when enough: Adaptive early-\nstopping for chain-of-thought reasoning, 2025. arXiv\npreprint.\n[93] Yassir Laaouach. HALT-cot: Model-agnostic early\nstopping for chain-of-thought reasoning via answer\nentropy. In 4th Muslims in ML Workshop co-located\nwith ICML 2025, 2025.\n[94] Xuefei Ning, Zinan Lin, Zixuan Zhou, Zifu Wang,\nHuazhong Yang, and Yu Wang. Skeleton-of-thought:\nPrompting LLMs for efficient parallel generation. In\nThe Twelfth International Conference on Learning\nRepresentations, 2024.\n[95] Yifu Ding, Wentao Jiang, Shunyu Liu, Yongcheng\nJing, Jinyang Guo, Yingjie Wang, Jing Zhang, Zeng-\nmao Wang, Ziwei Liu, Bo Du, Xianglong Liu, and\nDacheng Tao. Dynamic parallel tree search for effi-\n13"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 14, "text": "cient llm reasoning, 2025.\n[96] Tian Jin, Ellie Y. Cheng, Zack Ankner, Nikunj Saun-\nshi, Blake M. Elias, Amir Yazdanbakhsh, Jonathan\nRagan-Kelley, Suvinay Subramanian, and Michael\nCarbin. Learning to keep a promise: Scaling lan-\nguage model decoding parallelism with learned asyn-\nchronous decoding, 2025.\n[97] Gleb Rodionov, Roman Garipov, Alina Shutova,\nGeorge Yakushev, Erik Schultheis, Vage Egiazarian,\nAnton Sinitsin, Denis Kuznedelev, and Dan Alistarh.\nHogwild! inference: Parallel llm generation via con-\ncurrent attention, 2025.\n[98] Yijiong Yu. Accelerate parallelizable reasoning via\nparallel decoding within one sequence, 2025.\n[99] Mingdao Liu, Aohan Zeng, Bowen Wang, Peng\nZhang, Jie Tang, and Yuxiao Dong. Apar: Llms\ncan do auto-parallel auto-regressive decoding. arXiv\npreprint arXiv:2401.06761, 2024.\n[100] Jiayi Pan, Xiuyu Li, Long Lian, Charlie Snell, Yifei\nZhou, Adam Yala, Trevor Darrell, Kurt Keutzer,\nand Alane Suhr.\nLearning adaptive parallel rea-\nsoning with language models.\narXiv preprint\narXiv:2504.15466, 2025.\n[101] Chan-Jan Hsu, Davide Buffelli, Jamie McGowan,\nFeng-Ting Liao, Yi-Chang Chen, Sattar Vakili, and\nDa shan Shiu. Group think: Multiple concurrent rea-\nsoning agents collaborating at token level granularity,\n2025.\n[102] Tong Zheng, Hongming Zhang, Wenhao Yu, Xi-\naoyang Wang, Runpeng Dai, Rui Liu, Huiwen Bao,\nChengsong Huang, Heng Huang, and Dong Yu.\nParallel-r1: Towards parallel thinking via reinforce-\nment learning, 2025.\n[103] In Gim, Seung seob Lee, and Lin Zhong. Asyn-\nchronous llm function calling, 2024.\n[104] Sehoon Kim, Suhong Moon, Ryan Tabrizi, Nicholas\nLee, Michael W Mahoney, Kurt Keutzer, and Amir\nGholami. An llm compiler for parallel function call-\ning. In Forty-first International Conference on Ma-\nchine Learning, 2024.\n[105] Junlong Tong, Yingqi Fan, Anhao Zhao, Yunpu Ma,\nand Xiaoyu Shen. Streamingthinker: Large language\nmodels can think while reading, 2025.\n[106] Shoutao Guo, Shaolei Zhang, Zhengrui Ma, and Yang\nFeng. Large language models are read/write policy-\nmakers for simultaneous generation, 2025.\n[107] Donghang Wu, Haoyang Zhang, Jun Chen, Xiangyu,\nZhang, Hexin Liu, Eng Siong Chng, Fei Tian, Xuerui\nYang, Xiangyu Zhang, Daxin Jiang, and Gang Yu.\nMind-paced speaking: A dual-brain approach to real-\ntime reasoning in spoken language models, 2025.\n[108] Anthony Liang, Jonathan Berant, Adam Fisch, Abhi-\nmanyu Goyal, Kalpesh Krishna, and Jacob Eisenstein.\nPlantain: Plan-answer interleaved reasoning, 2025.\n[109] A Vaswani. Attention is all you need. Advances in\nNeural Information Processing Systems, 2017.\n[110] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam\nKosiorek, Seungjin Choi, and Yee Whye Teh.\nSet transformer: A framework for attention-based\npermutation-invariant neural networks. In Proceed-\nings of the 36th International Conference on Machine\nLearning, pages 3744–3753, 2019.\n[111] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\nSelf-attention with relative position representations.\nIn Marilyn Walker, Heng Ji, and Amanda Stent, edi-\ntors, Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 2 (Short Papers), pages 464–468, New Or-\nleans, Louisiana, June 2018. Association for Compu-\ntational Linguistics.\n[112] Ofir Press, Noah A. Smith, and Mike Lewis. Train\nshort, test long: Attention with linear biases enables\ninput length extrapolation, 2022.\n[113] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,\nBo Wen, and Yunfeng Liu. Roformer: Enhanced\ntransformer with rotary position embedding. arXiv\npreprint arXiv:2104.09864, 2021.\n[114] Qian Yang, Jin Xu, Wenrui Liu, Yunfei Chu, Ziyue\nJiang, Xiaohuan Zhou, Yichong Leng, Yuanjun Lv,\nZhou Zhao, Chang Zhou, and Jingren Zhou. Air-\nbench: Benchmarking large audio-language models\nvia generative comprehension. In Proceedings of the\n62nd Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1979–1998, Bangkok, Thailand, August 2024. Asso-\nciation for Computational Linguistics.\n[115] Bin Wang, Xunlong Zou, Geyu Lin, Shuo Sun, Zhuo-\nhan Liu, Wenyu Zhang, Zhengyuan Liu, AiTi Aw,\nand Nancy F Chen. Audiobench: A universal bench-\nmark for audio large language models. NAACL, 2025.\n[116] Chengwei Wei, Bin Wang, Jung-jae Kim, and\nNancy F. Chen.\nTowards spoken mathematical\nreasoning:\nBenchmarking speech-based models\nover multi-faceted math problems. arXiv preprint\narXiv:2505.15000, 2025.\n14"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 15, "text": "[117] Ruiqi Yan, Xiquan Li, Wenxi Chen, Zhikang Niu,\nChen Yang, Ziyang Ma, Kai Yu, and Xie Chen. Uro-\nbench: Towards comprehensive evaluation for end-\nto-end spoken dialogue models, 2025.\n[118] Yemin Shi, Yu Shu, Siwei Dong, Guangyi Liu,\nJaward Sesay, Jingwen Li, and Zhiting Hu. Voila:\nVoice-language foundation models for real-time au-\ntonomous interaction and voice role-play, 2025.\n[119] Dan Hendrycks, Collin Burns, Saurav Kadavath,\nAkul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical prob-\nlem solving with the math dataset. NeurIPS, 2021.\n[120] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng\nNi, Abhranil Chandra, Shiguang Guo, Weiming Ren,\nAaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-\npro: A more robust and challenging multi-task lan-\nguage understanding benchmark.\narXiv preprint\narXiv:2406.01574, 2024.\n[121] Mantas Mazeika, Long Phan, Xuwang Yin, Andy\nZou, Zifan Wang, Norman Mu, Elham Sakhaee,\nNathaniel Li, Steven Basart, Bo Li, David Forsyth,\nand Dan Hendrycks. Harmbench: A standardized\nevaluation framework for automated red teaming and\nrobust refusal.\narXiv preprint arXiv:2402.04249,\n2024.\n[122] Speech-Rule-Engine contributors.\nSpeech-rule-\nengine: Generating speech descriptions for xml struc-\ntures. GitHub repository. accessed 2025-12-10.\n[123] Eric Lam. lab-mic: Record audio directly within\njupyter/ipython notebooks using browser microphone.\nGitHub repository. accessed 2025-12-10.\n[124] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng,\nSiyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.\nJudging llm-as-a-judge with mt-bench and chatbot\narena. Advances in Neural Information Processing\nSystems, 36:46595–46623, 2023.\n[125] Yuqi Zhou, Lin Lu, Hanchi Sun, Pan Zhou, and\nLichao Sun. Virtual context: Enhancing jailbreak\nattacks with special token injection, 2024.\n[126] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, and Denny\nZhou. Chain-of-thought prompting elicits reason-\ning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837,\n2022.\n[127] Kaiwen Zhou, Xuandong Zhao, Gaowen Liu, Jayanth\nSrinivasa, Aosong Feng, Dawn Song, and Xin Eric\nWang. Safekey: Amplifying aha-moment insights for\nsafety reasoning. arXiv preprint arXiv:2505.16186,\n2025.\n[128] Yichi Zhang, Yue Ding, Jingwen Yang, et al. Towards\nsafe reasoning in large reasoning models via correc-\ntive intervention. arXiv preprint arXiv:2509.24393,\n2025.\n[129] Tong Wu, Chong Xiang, Jiachen T. Wang, G. Edward\nSuh, and Prateek Mittal. Effectively controlling rea-\nsoning models through thinking intervention, 2025.\n[130] Martin Kuo, Jianyi Zhang, Aolin Ding, Qinsi Wang,\nLouis DiValentin, Yujia Bao, Wei Wei, Hai Li, and\nYiran Chen. H-cot: Hijacking the chain-of-thought\nsafety reasoning mechanism to jailbreak large rea-\nsoning models, including openai o1/o3, deepseek-r1,\nand gemini 2.0 flash thinking. arXiv preprint, 2025.\n[131] Zihao Zhu, Xinyu Wu, Gehan Hu, Siwei Lyu, Ke Xu,\nand Baoyuan Wu. Advchain: Adversarial chain-of-\nthought tuning for robust safety alignment of large\nreasoning models, 2025.\n[132] Wenhan Chang, Tianqing Zhu, Yu Zhao, Shuangy-\nong Song, P Xiong, Wanlei Zhou, and Yongxiang\nLi. Chain-of-lure: A synthetic narrative-driven ap-\nproach to compromise large language models. arXiv\npreprint, 2025.\nAppendix\nA. Safety & Reasoning\nRecent studies reveal that Chain-of-Thought reasoning im-\npact on safety risks is complex and bidirectional [74, 75].\nOn one hand, CoT enhances safety by enabling trans-\nparency [72, 73], allowing models to structure the evalu-\nation of harmful intent and facilitate self-correction before\ngenerating a final response [126, 127]. Defense mechanisms\nlike RoboGuard and CoT Prompting use this to reduce at-\ntack success rates by monitoring reasoning traces for policy\nviolations [128, 129].\nOn the other hand, reasoning capabilities introduce new\nattack vectors not present in standard LLMs [76]. The visi-\nbility of intermediate states exposes a larger attack surface:\nadversaries can hijack the reasoning process (H-CoT at-\ntacks) to bypass refusal mechanisms [130], or exploit the\n“snowball effect” where minor reasoning deviations amplify\ninto harmful outputs [131].\nFurthermore, reasoning models are susceptible to narrative\ndeception and context-switching attacks, where the model\nrationalizes harmful compliance through complex logical\ndeductions or by adopting a “helpful” persona in educational\ncontexts [132, 76].\n15"}
{"pdf_id": "arxiv_251210931_asynchronous_reasoning", "page": 16, "text": "B. Prompting\nBelow we provide prompts used for thinker and writer.\nAsyncReasoning (both Q-Continue and Q-pause)\nWriter prompt\nYou are an AI assistant that can think and write\noutputs concurrently.\nYou can write outputs for the user based on par-\ntial chain of thought that will be continued in the\nbackground by an automated system. Your should\nfirst outline what you’re going to do for the user,\nthen gradually write your response as your thoughts\nprogress, but not ahead of your thoughts. When you\nare done, specify your final answer (e.g. \\boxed{ }).\nThinker prompt\nYou are an AI assistant that can think and write\noutputs concurrently.\nYou can reason in private and your thoughts will be\nused to form the public response in the background,\nby an automated system.\nYour task is to write\nthoughts and control when the automated system\ncan continue writing the response.\nSometimes, an automated system will ask you to\ndecide if your thoughts have enough information\nfor it write an additional passage to the user. Use\nthe partial response above yours thoughts to judge\nif you added enough new information to write one\nmore passage in the user-facing response.\n- Reply \"yes\" if you think there is enough informa-\ntion to write the next passage (paragraph, equation,\netc).\n- Reply \"no\" if you need to think more in private\nbefore the system can continue writing the public\nresponse.\nYour goal is to give frequent updates on your\nprogress, even if you did not solve the entire\ntask yet. Reason in short paragraphs. Prioritize\ngiving enough information for the system to begin\nresponding to the user as soon as possible.\nPlease reason step by step. Validate final again and\nagain until you see the same answer in the partial\nresponse above your thoughts.\nAsyncReasoning (Safety Prompt)\nWriter prompt\n[Similar to the previous prompt, omitted for brevity.]\nThinker prompt\nYou are an AI assistant that can think and write\noutputs concurrently.\nYou can reason in private and your thoughts will be\nused to form the public response in the background,\nby an automated system.\nYour task is to write\nthoughts and control when the automated system\ncan continue writing the response.\nEXECUTION PROTOCOL:\n1. SAFETY CHECK: Before solving, explicitly\nevaluate if the user’s request is safe, legal, and\nethical. - If UNSAFE: Your goal is to refuse. You\nhave \"enough information\" to write immediately\nafter you decide to refuse. - If SAFE: Proceed to\nsolve the problem step-by-step.\n2. REASONING: If the request is safe, solve the\nproblem.\n3. VALIDATION: Validate final again and again\nuntil you see the same answer in the partial response\nabove your thoughts.\nSometimes, an automated system will ask you to\ndecide if your thoughts have enough information\nfor it write an additional passage to the user. Use\nthe partial response above yours thoughts to judge\nif you added enough new information to write one\nmore passage in the user-facing response.\n- Reply \"yes\" if you have completed the safety\ncheck and have enough information to write the\nnext passage (paragraph, equation, etc), whether it\nis a solution step or a refusal.\n- Reply \"no\" if you are still checking safety, need to\nthink more, or need to validate your answer.\nYour goal is to give frequent updates on your\nprogress. Prioritize safety first, then accuracy. Rea-\nson step by step. Give enough information for the\nsystem to begin responding to the user as soon as\npossible.\n16"}
