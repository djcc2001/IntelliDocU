{"pdf_id": "arxiv_251210922_sparseswaps", "page": 1, "text": "SPARSESWAPS:\nTRACTABLE LLM PRUNING MASK\nREFINEMENT AT SCALE\nMax Zimmer, Christophe Roux, Moritz Wagner, Deborah Hendrych, Sebastian Pokutta\nDepartment for AI in Society, Science, and Technology, Zuse Institute Berlin, Germany\nInstitute of Mathematics, Technische Universit¨at Berlin, Germany\n{zimmer,roux,wagner,hendrych,pokutta}@zib.de\nABSTRACT\nThe resource requirements of Neural Networks can be significantly reduced\nthrough pruning – the removal of seemingly less important parameters. How-\never, with the rise of Large Language Models (LLMs), full retraining to recover\npruning-induced performance degradation is often prohibitive and classical ap-\nproaches such as global magnitude pruning are suboptimal on Transformer archi-\ntectures. State-of-the-art methods hence solve a layer-wise mask selection prob-\nlem, the problem of finding a pruning mask which minimizes the per-layer pruning\nerror on a small set of calibration data. Exactly solving this problem to optimal-\nity using Integer Programming (IP) solvers is computationally infeasible due to\nits combinatorial nature and the size of the search space, and existing approaches\ntherefore rely on approximations or heuristics. In this work, we demonstrate that\nthe mask selection problem can be made drastically more tractable at LLM scale.\nTo that end, we decouple the rows by enforcing equal sparsity levels per row. This\nallows us to derive optimal 1-swaps (exchanging one kept and one pruned weight)\nthat can be computed efficiently using the Gram matrix of the calibration data.\nUsing these observations, we propose a tractable and simple 1-swap algorithm\nthat warm starts from any pruning mask, runs efficiently on GPUs at LLM scale,\nand is essentially hyperparameter-free. We demonstrate that our approach reduces\nper-layer pruning error by up to 60% over Wanda (Sun et al., 2023) and con-\nsistently improves perplexity and zero-shot accuracy across state-of-the-art GPT\narchitectures.\n1\nINTRODUCTION\nPruning after training (Han et al., 2015; Gale et al., 2019; Lin et al., 2020; Hoefler et al., 2021;\nZimmer et al., 2025) is a state-of-the-art technique to reduce the resource requirements of neural\nnetworks. A simple yet effective approach to obtain such sparse models starts from a pretrained\ndense model, removes seemingly unimportant parameters based on their magnitude, and requires\nretraining to compensate for pruning-induced performance degradation. However, while the inex-\npensive, data-free magnitude criterion has often achieved strong performance on traditional archi-\ntectures (Gale et al., 2019; Zimmer et al., 2023b), pruning has undergone a paradigm shift with the\nrise of large pretrained foundation models, particularly LLMs.\nFirst, the size of the models has shifted the focus toward retraining-free pruning criteria, as re-\ntraining is often computationally expensive if not infeasible, with parameter-efficient fine-tuning\n(Lialin et al., 2023; Zimmer et al., 2023a) being an exception. Secondly, systematic activation out-\nliers (Dettmers et al., 2022) and highly important super-weights (Yu et al., 2025) in sufficiently\nlarge Transformers (Vaswani et al., 2017) have rendered magnitude pruning no better than random\npruning for LLMs (Sun et al., 2023; Yin et al., 2023). Lastly, state-of-the-art methods (Frantar &\nAlistarh, 2023; Sun et al., 2023; Zhang et al., 2024) prune layer-wise: they split the pruning problem\ninto per-layer subproblems, pruning layers sequentially and independently using a small calibration\ndataset to estimate parameter importance. Rather than optimizing the global loss, such approaches\nminimize a per-layer local pruning loss. Specifically, for a single layer with calibration input matrix\n1\narXiv:2512.10922v1  [cs.LG]  11 Dec 2025"}
{"pdf_id": "arxiv_251210922_sparseswaps", "page": 2, "text": "X ∈Rdin×B and weights W ∈Rdout×din, the objective becomes\nmin\nM ∥WX −(M ⊙W)X∥2\nF ,\n(1)\nwhere M ∈{0, 1}dout×din is a binary pruning mask achieving a desired level of sparsity, e.g.,\n∥M∥0 ≤k for unstructured sparsity, and ⊙denotes the element-wise multiplication or Hadamard\nproduct. Here, B = N · L with N being the number of samples in the calibration batch and L being\nthe sequence length.\nSolving this combinatorial mask selection problem to optimality is NP-hard due to feature correla-\ntions: selecting k of dout · din weights yields a cardinality-constrained binary quadratic program (a\nbest-subset selection variant). Even for a single row i, the problem reduces to\nmin\nmi\n\r\rw⊤\ni X −(mi ⊙wi)⊤X\n\r\r2\nF = min\nmi\nB\nX\nk=1\n\n\ndin\nX\nj=1\n(1 −mij)wijXjk\n\n\n2\n,\nwhere wi ∈Rdin and mi ∈{0, 1}din denote the i-th row of W and M, respectively. While IP\nsolvers could theoretically provide optimal solutions, the combinatorial search over mask entries\nmakes this infeasible for LLMs. In practice, existing methods therefore relax Equation 1 or approx-\nimate it.\nHowever, with deployed LLMs now serving millions of users, it becomes increasingly worthwhile\nto invest substantial resources to obtain pruned models that reach high performance, because the\npruning cost is paid once during training whereas inference costs scale with the number of requests.\nIn this work, we revisit the per-layer mask selection problem and demonstrate that it can be op-\nerationalized at LLM scale, enabling monotone improvements with each optimization step rather\nthan relying on proxy importance scores. To that end, we observe that enforcing equal sparsity-\nlevel across rows ensures row-wise separability that yields independent objectives. This makes the\nproblem drastically more tractable and leads to good practical performance for LLMs. Instead of\ntrying to obtain exact solutions via IP solvers, we instead propose a GPU-accelerated local optimiza-\ntion algorithm based on 1-swaps (exchanging one kept and one pruned weight) that perform exact\nand efficient local refinement with incremental cost updates using the Gram matrix G = XX⊤to\nmonotonically decrease the objective from any warm start.\nThe resulting method, which we term SparseSwaps, can start from any warm-start mask, evaluates\nthe exact per-row quadratic loss, and is scalable, parallelizable across rows, almost hyperparameter-\nfree, and deterministic for a fixed warm start. With only few 1-swap iterations, it can reduce the per-\nlayer pruning error by up to 60% compared to Wanda and improves final perplexity and zero-shot\naccuracy across architectures. Our approach is a post-hoc refinement of existing pruning methods\nthat can significantly improve upon the state of the art for unstructured, per-row, or N:M sparsity.\nContributions.\nOur contributions are as follows:\n1. Making the Mask Selection problem tractable. We observe that a) enforcing equal spar-\nsity levels per row decouples the rows, and that b) optimal 1-swaps (exchanging one kept\nand one pruned weight) can be evaluated efficiently using the Gram matrix G = XX⊤of\nthe calibration data, ensuring efficient lookups when determining the most beneficial swap.\n2. SparseSwaps: a practical post-hoc pruning algorithm. Building on these observations,\nwe propose SparseSwaps, a plug-and-play 1-swap refinement that starts from any warm-\nstart mask and monotonically decreases the exact per-row objective under per-row or N:M\nconstraints. In particular, SparseSwaps is almost hyperparameter-free, completely paral-\nlelizable across rows and scalable to LLMs.\n3. Computational study. We verify our hypotheses on state-of-the-art Generative Pretrained\nTransformer (GPT) architectures and demonstrate that SparseSwaps delivers large reduc-\ntions in local pruning error (up to 60% per-layer error reduction over Wanda) and strong\nperplexity and zero-shot gains across a wide range of different LLMs. We conduct a series\nof ablations highlighting the advantages and drawbacks of the proposed approach.\n2"}
{"pdf_id": "arxiv_251210922_sparseswaps", "page": 3, "text": "Further related work.\nPost-training pruning has a long history, and while magnitude pruning\n(Janowsky, 1989; Han et al., 2015) is among the most popular criteria, it is not the only one (cf.\nLeCun et al., 1989; Hassibi & Stork, 1993; Molchanov et al., 2016; Yeom et al., 2019); see Hoe-\nfler et al. (2021) for a comprehensive review. Despite their simplicity, magnitude-based methods\nhave been shown to produce sparse models competitive with far more complex algorithms for con-\nvolutional architectures (Gale et al., 2019; Zimmer et al., 2023b). For LLMs, however, magnitude\npruning is argued to be unsuitable (Yin et al., 2023). Consequently, there is growing interest in\ncriteria beyond magnitude that achieve high performance on LLMs, and do so without requiring an\nexpensive retraining procedure (Kwon et al., 2022; Frantar & Alistarh, 2023; Sun et al., 2023; Zhang\net al., 2024). In this work, we develop a post-hoc refinement of existing methods, rather than propos-\ning a new criterion. A related approach, DSnoT (Zhang et al., 2023), also performs iterative weight\nswaps but differs significantly in its optimization strategy. Inspired by dynamic sparse training (cf.\nEvci et al., 2020), DSnoT prunes and regrows weights based on expected reconstruction-error im-\nprovements, using feature means and variances as surrogates. While effective, it does not guarantee\na monotonic decrease in the true pruning error, whereas our method does. We compare the two\nempirically and find that SparseSwaps consistently outperforms DSnoT.\nSubset selection and IP approaches. To solve Equation 1 to global optimality, which can be for-\nmulated as a mixed-integer nonlinear program (MINLP), several efficient open-source solvers are\navailable, including SCIP (Bolusani et al., 2024), Bonmin (Bonami et al., 2008), SHOT (Lundell\net al., 2022) and Boscia (Hendrych et al., 2025), among others. While we demonstrate how the\nproblem can be made drastically more tractable, explicit solution remains very time-consuming for\nlarge instances; we therefore opt for a GPU-friendly 1-swap approach that avoids moving large\ntensors to the CPU for IP solvers. We leave such an extension for future work.\n2\nMETHODOLOGY\nIn the following, we use uppercase letters for matrices (W, X, M) and lowercase letters for scalars\nand vectors. Matrix entries are denoted Wij for the element in row i, column j. Rows of matrices are\ndenoted with lowercase subscripts: wi represents the i-th row of matrix W. Row and column slices\nuse colon notation: Xj,: for the j-th row and X:,k for the k-th column. We use ⊙for element-wise\nmultiplication, ∥·∥F for Frobenius norm, and ∥·∥2 for ℓ2 norm.\n2.1\nPRELIMINARIES\nBefore describing our proposed method, we make several assumptions and observations that make\nthe problem tractable.\n2.1.1\nEQUAL SPARSITY-LEVEL ACROSS ROWS DOES NOT NEED TO BE DETRIMENTAL\nFirst, note that the objective in Equation 1 decomposes into a sum of dout row-wise quadratics,\n∥WX −(M ⊙W)X∥2\nF =\ndout\nX\ni=1\n\r\rw⊤\ni X −(mi ⊙wi)⊤X\n\r\r2\n2\nwhere wi ∈Rdin and mi ∈{0, 1}din denote the i-th row of W and M, respectively. This alone\ndoes not make the corresponding minimization problem row-separable under unstructured sparsity,\nsince the matrix cardinality constraint couples rows. In contrast, semi-structured patterns like per-\nrow sparsity (keep k per row) or N:M (prune M −N per block of M weights) enforce equal\nper-row sparsity, meaning that the rows are fully decoupled by definition. We therefore focus on the\ndecoupled case, allowing to treat each row separately and reducing the problem to\nmin\nmi\n\r\rw⊤\ni X −(mi ⊙wi)⊤X\n\r\r2\nF = min\nmi\nB\nX\nk=1\n\n\ndin\nX\nj=1\n(1 −mij)wijXjk\n\n\n2\n(2)\nfor each row i ∈{1, . . . , dout}. Note that, for LLMs, Sun et al. (2023) observe that row-wise sparsity\nbenefits performance for both Wanda and magnitude pruning. We therefore argue that enforcing\nper-row sparsity rather than unstructured sparsity is justified and need not harm final performance,\nat least for LLMs.\n3"}
{"pdf_id": "arxiv_251210922_sparseswaps", "page": 4, "text": "2.1.2\nAVOIDING INTERMEDIATE VALUE CACHING THROUGH THE GRAM MATRIX\nFORMULATION\nNaively caching all B ·din intermediate products wijXjk in Equation 2 to evaluate candidate masks\nis prohibitive. To illustrate the scale, consider a single row of the largest matrix in a LLAMA-2-7B\nTransformer block: the up proj matrix with input dimension din = 4096. With N = 128 samples\nand sequence length L = 4096 (so B = N · L = 524,288), caching all products wijXjk for that\nrow requires 524,288 × 4096 ≈2.15 billion float32 values (about 8.6GB); across all 11,008 rows\nthis totals about 94.6TB.\nA straightforward way to circumvent this issue is to consider a single row and derive a compact\nformulation of the per-row loss through the Gram matrix G\ndef\n= XX⊤∈Rdin×din. For notational\nconvenience, we drop the row index i throughout the remainder of this section and write w ∈Rdin\nfor the row’s weight vector and m ∈{0, 1}din for its mask. The per-row loss from Equation 2 is\nL\ndef\n=\n\r\rw⊤X −(m ⊙w)⊤X\n\r\r2\nF =\n\r\r(w −m ⊙w)⊤X\n\r\r2\nF = (w −m ⊙w)⊤G(w −m ⊙w).\nHence, the loss depends on X only through the Gram matrix G, which can be accumulated on-the-fly\nas calibration samples pass through the layer: G = PB\nb=1 X:,bX⊤\n:,b. Unlike the per-row formulation\nin the introduction, which would require caching all B · din intermediate products wjXjk, we only\nneed to maintain the din × din matrix G, which is a reduction from O(B · din) to O(d2\nin) since din\nis typically much smaller than B.\nRemark 1. A different (but in practice slightly less efficient) perspective on this reduction is through\nthe unitary invariance of the Frobenius norm used in our pruning objective: for any matrix A and\nunitary matrix U (i.e., U −1 = U ⊤), we have ∥AU∥F = ∥A∥F . This property enables significant\ncomputational savings through Singular Value Decomposition (SVD) compression. Precisely, let\nX = UΣV ⊤be the SVD of calibration data X ∈Rdin×B. Since B > din, we can write Σ =\n[Σ′ | 0] with Σ′ ∈Rdin×din containing the singular values on its diagonal. The compressed\nrepresentation is simply X′ = UΣ′ ∈Rdin×din. Letting wp = w −m ⊙w for brevity, the key\ninsight is that pruning decisions remain equivalent under this compression:\n∥wpX∥2\nF =\n\r\rwpUΣV ⊤\r\r2\nF = ∥wpUΣ∥2\nF = ∥wpU[Σ′ | 0]∥2\nF = ∥wpUΣ′∥2\nF = ∥wpX′∥2\nF ,\nwhere we used unitary invariance w.r.t. V and that the zero columns do not contribute to the Frobe-\nnius norm. Equivalently, we have\nX′X′⊤= UΣ′Σ′⊤U ⊤= UΣΣ⊤U ⊤= XX⊤= G,\nsince ΣΣ⊤= Σ′Σ′⊤(the zero columns of Σ do not contribute). Since all subsequent operations\ndepend solely on G, we accumulate G directly during calibration and avoid the SVD entirely.\n2.1.3\nEFFICIENT 1-SWAP EVALUATION THROUGH EFFICIENT COST LOOKUPS AND UPDATES\nWhile the global mask selection problem is NP-hard, we can still make efficient progress via local\nsearch. Starting from any feasible mask m ∈{0, 1}din, the idea is to iteratively perform 1-swaps\nthat exchange one kept and one pruned weight to reduce L while preserving the sparsity level. The\nkey observation is that each candidate swap can be evaluated in O(1) time using G and an auxiliary\ncorrelation vector c. To that end, let P\ndef\n= {j : mj = 0} denote the set of currently pruned weight\nindices and analogously U\ndef\n= {j : mj = 1} denote the set of unpruned (kept) weight indices.\nLetting further ϕj\ndef\n= X⊤\nj,: ∈RB denote the j-th row (or feature vector)of X, we can write\n(w −m ⊙w)⊤X =\ndin\nX\nj=1\n(1 −mj)wjXj,: =\nX\nj∈P\nwjϕ⊤\nj = r⊤,\nwhere we define the reconstruction residual r\ndef\n= P\nj∈P wjϕj ∈RB, the total contribution of all\npruned weights to the layer output. Hence, clearly, the loss is L = ∥r∥2\n2 = r⊤r.\nWe define the correlation vector c = (c1, . . . , cdin)⊤∈Rdin with entries\nci\ndef\n= ⟨ϕi, r⟩= ⟨ϕi,\nX\nj∈P\nwjϕj⟩=\nX\nj∈P\nwj⟨ϕi, ϕj⟩=\nX\nj∈P\nwjGij,\nwhich measures how each feature ϕi correlates with the current residual. In vector form, c =\nG · ((1 −m) ⊙w).\n4"}
{"pdf_id": "arxiv_251210922_sparseswaps", "page": 5, "text": "Swap cost formula.\nA 1-swap removes index u ∈U from the unpruned set (making it pruned)\nand adds index p ∈P to the unpruned set (making it unpruned). The new residual is r′ = r +\nwuϕu −wpϕp, and the change in loss is\n∆Lu,p = ∥r′∥2\n2 −∥r∥2\n2 = ∥r + wuϕu −wpϕp∥2\n2 −∥r∥2\n2\n= 2wu⟨ϕu, r⟩+ w2\nu∥ϕu∥2\n2 −2wp⟨ϕp, r⟩+ w2\np∥ϕp∥2\n2 −2wuwp⟨ϕu, ϕp⟩.\nUsing ci = ⟨ϕi, r⟩and Gij = ⟨ϕi, ϕj⟩, this simplifies to\n∆Lu,p = 2wucu + w2\nuGuu −2wpcp + w2\npGpp −2wuwpGup.\n(3)\nGiven the precomputed Gram matrix G and correlation vector c, each swap evaluation requires only\nscalar lookups. Evaluating all possible swaps therefore costs O(|U| · |P|) total. By systematically\ntesting all ((din −|P|) · |P|) possible 1-swap operations (adding one of |U| = din −|P| unpruned\nweights to P, removing one of |P| pruned weights from P) evaluating the improvement using the\nabove expression, we iteratively pick a best swap and update the mask until we have reached a\nsatisfactory solution or one optimal w.r.t. 1-swap operations. The only issue that remains is to\nupdate the correlation vector after each swap.\nCorrelation vector update.\nAfter accepting a swap (u∗, p∗), the residual changes to r′ = r +\nwu∗ϕu∗−wp∗ϕp∗. The correlation vector updates as\nci ←ci + wu∗Gi,u∗−wp∗Gi,p∗,\n(4)\nor in vector form, c ←c + wu∗G:,u∗−wp∗G:,p∗. This only requires accessing two columns of G\nand costs O(din).\nWhy picking p and u separately is suboptimal.\nThe interaction term −2wuwpGup in Equation 3\nshows that the best u depends on the chosen p (and vice versa). Consequently, selecting p and u\nbased on their individual effects can yield a detrimental swap, as the following example for the scalar\ncase with B = 1 and din = 4 shows. Let the current pruned weight contributions be {+10, −1},\nso r = 9 and L = 81, and let the unpruned weight contributions be {+9, −9}. The best 1-swap\nis to unprune the −1 contribution and prune the −9 contribution, giving r′ = 10 + (−9) = 1 and\nL′ = 1. However, if we instead greedily remove the best p in isolation, we unprune +10 since\n(9 −10)2 = 1 is minimal. We must then add one index; the best addition in isolation to the original\npruned-weight-contributions {+10, −1} is −9. In combination, the greedily chosen swap leads to\nr′ = −1 + (−9) = −10 and L′ = 100, worse than the starting point. The error stems precisely\nfrom ignoring the interaction term when selecting (p, u).\n2.2\nTHE SPARSESWAPS ALGORITHM\nBuilding upon the preceding observations, we present our complete algorithm. The method takes\nas input a weight matrix W ∈Rdout×din, the Gram matrix G = XX⊤∈Rdin×din (accumulated\nduring calibration), and a warmstart pruning mask M init ∈{0, 1}dout×din that already satisfies the\ndesired sparsity constraints, e.g., obtained from Wanda (Sun et al., 2024) or RIA (Zhang et al., 2024).\nThe algorithm enforces any sparsity pattern that operates per-row, including per-row sparsity (fixed\nnumber of zeros per row, cf. Sun et al. (2023)) and structured N:M sparsity patterns (e.g., 2:4 or 4:8,\nMishra et al. (2021)). All swap operations maintain the sparsity constraints throughout optimization;\nfor N:M sparsity, swaps are restricted to occur only within the same N:M blocks, while for per-\nrow sparsity, the total number of pruned weights per row remains constant. Even though each swap\nonly changes two mask entries, the cumulative effect of multiple swaps can dramatically reduce\nreconstruction error compared to the initial solution.\nWe explain the main phases of the algorithm:\nPreparation: We initialize with the warmstart mask M init. The Gram matrix G is precomputed\nonce per layer by accumulating G = P\nb X:,bX⊤\n:,b during the calibration forward pass.\nRow processing (Lines 2-5): For each row i, we extract weights w and current mask m, define\npruned and unpruned index sets P and U, and compute the initial correlation vector c = G · ((1 −\nm) ⊙w).\n5"}
{"pdf_id": "arxiv_251210922_sparseswaps", "page": 6, "text": "Algorithm 1 SparseSwaps: 1-Swap Pruning Optimization\nRequire: W ∈Rdout×din, Gram matrix G = XX⊤∈Rdin×din, warmstart mask M init, Tmax\nEnsure: Improved pruning mask M\n1: M ←M init\n▷Initialize with warmstart solution\n2: for i = 1 to dout do\n▷Process each row independently\n3:\nw ←Wi,:, m ←Mi,:\n▷Extract row weights and mask\n4:\nP ←{j : mj = 0}, U ←{j : mj = 1}\n▷Pruned and unpruned sets\n5:\nc ←G · ((1 −m) ⊙w)\n▷Initialize correlation vector\n6:\nfor t = 1 to Tmax do\n7:\n(p∗, u∗) ←arg min(p,u) ∆Lu,p\n▷Best swap via Equation 3\n8:\nif ∆Lu∗,p∗< 0 then\n▷Swap improves objective\n9:\nmp∗←1, mu∗←0\n▷Perform swap\n10:\nP ←(P \\ {p∗}) ∪{u∗}, U ←(U \\ {u∗}) ∪{p∗}\n11:\nc ←c + wu∗G:,u∗−wp∗G:,p∗\n▷Update correlation vector\n12:\nelse\n13:\nbreak\n▷Local optimum reached\n14:\nend if\n15:\nend for\n16:\nMi,: ←m\n▷Store optimized row\n17: end for\n1-Swap optimization (Lines 6-15): We iteratively find the swap (p∗, u∗) minimizing ∆Lu,p (cf.\nEquation 3) among feasible pairs, evaluating each candidate in O(1) time. If ∆Lu∗,p∗< 0, we\naccept the swap and update the correlation vector via Equation 4; otherwise we terminate. At all\ntimes, the swaps are appropriately constrained: per-row sparsity allows any swap maintaining |P|\nconstant, while N:M sparsity restricts swaps to within the same N:M blocks.\nThe algorithm has complexity O(dout·Tmax·(|P|·|U|+din)) per layer, where Tmax is the maximum\nnumber of swap iterations per row. The |P| · |U| term comes from evaluating all candidate swaps\n(each in O(1) time via Equation 3), and the din term from the correlation vector update (Equation 4).\nIn practice, several factors further reduce runtime. First, we find that even Tmax = 1 or Tmax = 2\ncan drastically reduce the local pruning error; values around Tmax = 25 often suffice to significantly\nlower model perplexity, with diminishing returns beyond Tmax = 100. Second, row-wise processing\ncan be batched and vectorized, enabling parallel swap cost computations and mask updates, and rows\ncan be distributed across GPUs if needed. Third, the Gram matrix G is computed once per layer and\nshared across all rows, and several summands of Equation 3 can be similarly precomputed once per\nlayer.\n3\nEXPERIMENTAL RESULTS\nWe outline our general experimental approach, detailing datasets, architectures, and metrics. Our\ncode is publicly available at github.com/ZIB-IOL/SparseSwaps. Our study focuses on language\nmodeling within Natural Language Processing (NLP). We use pretrained models from Hugging-\nFace (Wolf et al., 2020), specifically LLAMA-3.1-8B (Grattafiori et al., 2024), GEMMA-2-9B\n(Riviere et al., 2024), YI-1.5-9B (Young et al., 2025), DEEPSEEK-7B-BASE (Bi et al., 2024), and\nQWEN2.5-7B (Yang et al., 2025). For calibration, we randomly draw sequences of 2048 tokens\nfrom the C4 dataset (Raffel et al., 2020). For validation, we similarly pick 100 sequences from the\nvalidation split. The model performance is assessed via perplexity on the WikiText dataset (Merity\net al., 2016) and zero-shot accuracy on the EleutherAI evaluation set (Gao et al., 2023). Following\nSun et al. (2023), we prune all linear layers, excluding the embedding and final linear head, with uni-\nform sparsity allocation across layers. We provide experiments for unstructured and semi-structured\nsparsity patterns (Mishra et al., 2021). We use multiple random seeds throughout our experiments.\n3.1\nMASK REFINEMENT AT SCALE\nWe begin by verifying the effectiveness of SparseSwaps. We make the following observations:\n6"}
{"pdf_id": "arxiv_251210922_sparseswaps", "page": 7, "text": "Table 1: LLAMA-3.1-8B: Perplexity (↓) and mean relative reduction in pruning error (↑) versus\nnumber of 1-swap iterations for 50% and 60% unstructured sparsity using Wanda warmstart.\nNumber of 1-swap iterations\nSparsity\nMetric\n0\n1\n2\n5\n10\n25\n50\n100\n200\n50%\nAvg. rel. error reduction (%)\n0.00\n6.34\n8.77\n12.51\n16.38\n23.52\n30.04\n36.48\n38.95\nPerplexity\n10.13\n10.31\n10.40\n10.41\n10.39\n10.38\n10.27\n10.30\n10.34\n60%\nAvg. rel. error reduction (%)\n0.00\n8.04\n11.04\n15.34\n19.64\n26.92\n33.58\n39.99\n43.74\nPerplexity\n21.52\n21.26\n21.51\n21.17\n21.01\n20.38\n19.74\n18.96\n19.17\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\nLayer\n0\n10\n20\n30\n40\n50\n60\n70\n80\nRelative reduction in pruning error (%)\nPer-layer reconstruction improvement over Wanda\nattn.q proj\nattn.k proj\nattn.v proj\nattn.o proj\nmlp.gate proj\nmlp.up proj\nFigure 1: Per-layer relative reduction in local pruning error compared to Wanda. The plot shows\nresult for LLAMA-3.1-8B, 60% unstructured sparsity and 100 1-swap iterations.\nSparseSwaps consistently improves state-of-the-art methods. Table 2 summarizes the main re-\nsults and reports perplexity (upper half, lower is better) and zero-shot accuracy (lower half, higher\nis better) for warmstart masks (Wanda, RIA) as well as their refinements using DSnoT and Spars-\neSwaps. For both 60% unstructured and 2:4 semi-structured sparsity, SparseSwaps (with 100 1-swap\niterations) consistently reduces perplexity and improves zero-shot accuracy over Wanda and RIA\nwarm start masks. While DSnoT similarly yields improvements, it falls short of SparseSwaps. Note\nthat we left the pruning criterion of DSnoT, which partially uses the Wanda saliency, unchanged,\neven when using RIA warmstart. For unstructured RIA, we report results when enforcing a per-\nrow sparsity constraint; while RIA yields good (and slightly better) results when enforcing truely\nunstructured sparsity, we decided to include the results for the per-row setting as this allows direct\nrefinement of the mask with SparseSwaps and DSnoT.\nSparseSwaps successfully optimizes the per-layer pruning loss. Figure 1 shows the per-layer\nreductions in local pruning error relative to a Wanda Warmstart, grouping layers by their corre-\nsponding Transformer block of LLAMA-3.1-8B. We observe drastic improvements of close to\n70% compared to Wanda, demonstrating that SparseSwaps is able to successfully optimize the local\nloss. The attn.o proj seems to consistently benefit the most across blocks, with reductions of\nthe objective in Equation 1 ranging between 40%-60%.\nLarge local error reductions do not always imply reduced perplexity. From Table 2 we observe\nsubstantial perplexity gains, especially when sparsity more strongly degrades model quality (cf.\nTable 4 in the appendix, which shows more drastic improvements when using magnitude pruning,\nwhich more strongly degrades model quality). In contrast, when quality is less affected (e.g., at\n50% sparsity where Wanda performs well), SparseSwaps yields limited perplexity gains despite\nsignificant local error reductions: Table 1 reports perplexity and average relative error reduction (%)\nversus the number of 1-swap iterations. Zero iterations correspond to the Wanda warm start; one or\nmore iterations correspond to SparseSwaps from Wanda. At 50% sparsity, a single 1-swap iteration\nlowers relative error by 6.34%, and 200 iterations by nearly 40%, yet perplexity does not improve,\nbut rather slightly increases. This suggests further reducing local error can overfit the calibration\ndata and may not translate to better perplexity, although we note that the perplexity increase is\nrelatively small. These results emphasize that while the reduction of local error is a useful proxy\nfor perplexity reduction when pruning has a higher negative impact on the model, the local error of\nEquation 1 remains an approximation to the reconstruction error of the entire model.\n7"}
{"pdf_id": "arxiv_251210922_sparseswaps", "page": 8, "text": "Table 2: Perplexity (↓, lower is better) and zero-shot accuracy (↑, higher is better) comparison on\nWikiText and EleutherAI evaluation set. We report DSnoT and SparseSwaps refinement with Wanda\nand RIA warmstart for unstructured 60% sparsity and semi-structured 2:4 sparsity. Best values are\nhighlighted in bold. We omit standard deviations for legibility.\nPerplexity ↓\nLLAMA-3.1\nGEMMA-2\nYI-1.5\nDEEPSEEK\nQWEN2.5\nMethod\nSparsity\n8B\n9B\n9B\n7B\n7B\nWanda\n60%\n21.94\n16.74\n11.40\n11.41\n13.75\n+ DSnoT\n60%\n21.94\n16.69\n11.38\n11.40\n13.75\n+ SparseSwaps\n60%\n19.75\n16.01\n10.07\n10.93\n13.16\nRIA\n60%\n19.73\n16.19\n10.73\n11.80\n12.63\n+ DSnoT\n60%\n19.73\n16.22\n10.73\n11.80\n12.63\n+ SparseSwaps\n60%\n18.47\n15.44\n9.98\n10.79\n12.47\nWanda\n2:4\n24.82\n17.45\n11.76\n11.77\n14.53\n+ DSnoT\n2:4\n22.79\n16.79\n10.84\n11.70\n14.40\n+ SparseSwaps\n2:4\n20.17\n16.30\n10.73\n11.70\n13.95\nRIA\n2:4\n23.96\n16.88\n11.29\n12.03\n13.58\n+ DSnoT\n2:4\n24.26\n16.82\n10.57\n12.03\n13.85\n+ SparseSwaps\n2:4\n20.90\n16.33\n10.50\n11.80\n13.28\nAccuracy ↑\nLLAMA-3.1\nGEMMA-2\nYI-1.5\nDEEPSEEK\nQWEN2.5\nMethod\nSparsity\n8B\n9B\n9B\n7B\n7B\nWanda\n60%\n48.18%\n63.39%\n53.59%\n50.74%\n59.26%\n+ DSnoT\n60%\n48.18%\n63.49%\n53.79%\n50.75%\n59.26%\n+ SparseSwaps\n60%\n50.78%\n63.84%\n54.84%\n51.02%\n60.15%\nRIA\n60%\n49.56%\n64.37%\n52.81%\n50.92%\n59.84%\n+ DSnoT\n60%\n49.56%\n64.43%\n52.96%\n50.83%\n59.81%\n+ SparseSwaps\n60%\n51.02%\n64.32%\n54.45%\n51.47%\n61.22%\nWanda\n2:4\n46.80%\n63.73%\n52.58%\n51.02%\n59.52%\n+ DSnoT\n2:4\n47.01%\n63.66%\n52.16%\n50.78%\n59.09%\n+ SparseSwaps\n2:4\n48.83%\n64.70%\n52.43%\n50.36%\n59.92%\nRIA\n2:4\n47.87%\n63.87%\n52.68%\n51.22%\n58.66%\n+ DSnoT\n2:4\n47.13%\n64.17%\n51.36%\n49.86%\n59.72%\n+ SparseSwaps\n2:4\n49.90%\n64.60%\n52.30%\n51.46%\n60.31%\n3.2\nEFFICIENCY AND HYPERPARAMETER ABLATIONS\nResource requirements. SparseSwaps is more resource-intensive than DSnoT and, as a drop-in\nrefinement, requires at least the resources of the chosen warm-start method. Beyond that, Spars-\neSwaps needs memory to store the Gram matrix G ∈Rdin×din (once per layer) and the correlation\nvector c ∈Rdin (per row), and compute to perform the 1-swaps; see the preceding section for the\ntheoretical complexity. While we have argued in the introduction that the additional compute can be\njustified when amortized over many LLM inference requests, we note that the overhead grows only\nlinearly with the number of 1-swap iterations Tmax. Table 1 shows that few iterations already yield\nsubstantial gains in both perplexity and local error reduction, especially at higher sparsity.\nTable 3 reports wall-clock times for pruning LLAMA-3.1-8B to 60% sparsity on a single H100\nGPU. The Tmax = 0 baseline includes calibration data sampling, Wanda pruning, Gram matrix\ncomputation, and evaluation; each additional iteration of SparseSwaps adds a relatively small over-\nhead. For comparison, Wanda and SparseGPT take approximately 4 and 10 minutes, respectively.\nWe note that our implementation can be further optimized and that the algorithm is fully paralleliz-\nable across rows.\nEffect of the number of reconstruction samples. Figure 2 in the appendix shows the perplexity\nversus the number of reconstruction samples for 50% and 60% unstructured sparsity when using\nWanda as well as SparseSwaps with a Wanda warmstart. We observe that the perplexity decreases\n8"}
{"pdf_id": "arxiv_251210922_sparseswaps", "page": 9, "text": "Table 3: Wall-clock time for applying SparseSwaps to LLAMA-3.1-8B at 60% sparsity on a single\nH100 GPU.\nTmax\n0\n1\n2\n5\n10\n25\nWall-clock time\n8m15s\n10m17s\n12m7s\n17m20s\n26m13s\n52m29s\ndrastically when using more samples, which leads to SparseSwaps slightly outperforming Wanda\nfor 50% sparsity, despite its advantage typically being larger at higher sparsity. We emphasize that\nthe number of reconstruction samples does not affect SparseSwaps’s swap evaluation efficiency: the\nGram matrix G = XX⊤has fixed size din × din regardless of B.\n4\nCONCLUSION\nWe revisited the mask selection problem for post-training pruning and showed that it can be made\nsubstantially more tractable, even at LLM scale. We observed that row decoupling via equal per-\nrow sparsity yields independent subproblems, and that individual 1-swaps can be evaluated in O(1)\ntime using the Gram matrix G = XX⊤. This enables tractable optimization of the true row-\nwise quadratic loss on GPUs. The resulting method, SparseSwaps, is warm-start agnostic, nearly\nhyperparameter-free, and scalable. It consistently reduces per-layer pruning error and improves\nperplexity and zero-shot accuracy across modern GPT architectures.\nOur work is not without limitations. While per-row sparsity is not necessarily detrimental for LLMs,\nour approach is restricted to that setting and only partially adapts to truly unstructured sparsity; in\nits current form, the algorithm can handle unstructured sparsity but cannot reallocate sparsity levels\nacross rows. Furthermore, runtime and memory remain non-trivial for large architectures.\nACKNOWLEDGMENTS\nThis research was partially supported by the DFG Cluster of Excellence MATH+ (EXC-2046/1,\nproject id 390685689) funded by the Deutsche Forschungsgemeinschaft (DFG) as well as by the\nGerman Federal Ministry of Research, Technology and Space (fund number 16IS23025B).\n9"}
{"pdf_id": "arxiv_251210922_sparseswaps", "page": 10, "text": "REFERENCES\nXiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding,\nKai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan,\nDaya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang,\nErhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, A. X. Liu,\nBo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong\nMa, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong\nRuan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun,\nMinghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong\nWu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu, Yanhong Xu,\nDejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang,\nLiyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang\nZhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou.\nDeepSeek\nLLM: Scaling Open-Source Language Models with Longtermism, January 2024. URL http:\n//arxiv.org/abs/2401.02954.\nSuresh Bolusani, Mathieu Besanc¸on, Ksenia Bestuzheva, Antonia Chmiela, Jo˜ao Dion´ısio, Tim\nDonkiewicz, Jasper van Doornmalen, Leon Eifler, Mohammed Ghannam, Ambros Gleixner,\nChristoph Graczyk, Katrin Halbig, Ivo Hedtke, Alexander Hoen, Christopher Hojny, Rolf van der\nHulst, Dominik Kamp, Thorsten Koch, Kevin Kofler, Jurgen Lentz, Julian Manns, Gioni Mexi,\nErik M¨uhmer, Marc E. Pfetsch, Franziska Schl¨osser, Felipe Serrano, Yuji Shinano, Mark Turner,\nStefan Vigerske, Dieter Weninger, and Lixing Xu. The SCIP Optimization Suite 9.0. Techni-\ncal report, Optimization Online, February 2024. URL https://optimization-online.\norg/2024/02/the-scip-optimization-suite-9-0/.\nPierre Bonami, Lorenz T Biegler, Andrew R Conn, G´erard Cornu´ejols, Ignacio E Grossmann, Carl D\nLaird, Jon Lee, Andrea Lodi, Franc¸ois Margot, Nicolas Sawaya, et al. An algorithmic framework\nfor convex mixed integer nonlinear programs. Discrete optimization, 5(2):186–204, 2008.\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8(): 8-bit matrix multi-\nplication for transformers at scale. August 2022.\nUtku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery:\nMaking all tickets winners. In Hal Daum´e III and Aarti Singh (eds.), Proceedings of the 37th\nInternational Conference on Machine Learning, volume 119 of Proceedings of Machine Learning\nResearch, pp. 2943–2952. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.\npress/v119/evci20a.html.\nElias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in\none-shot. In International Conference on Machine Learning, pp. 10323–10337. PMLR, 2023.\nTrevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv\npreprint arXiv:1902.09574, 2019.\nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Fos-\nter, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muen-\nnighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lin-\ntang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework\nfor few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/\n10256836.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad\nAl-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan,\nAnirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Ko-\nrenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava\nSpataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux,\nChaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret,\nChunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius,\nDaniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary,\nDhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab\nAlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco\n10"}
{"pdf_id": "arxiv_251210922_sparseswaps", "page": 11, "text": "Guzm´an, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind That-\ntai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Kore-\nvaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra,\nIvan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Ma-\nhadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu,\nJianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jong-\nsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala,\nKarthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid\nEl-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren\nRantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin,\nLovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi,\nMahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew\nOldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Ku-\nmar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoy-\nchev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur C¸ elebi, Patrick Alrassy, Pengchuan\nZhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan,\nPunit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ra-\nmon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Ro-\nhit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan\nSilva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell,\nSeohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng\nShen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer\nWhitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman,\nTara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mi-\nhaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor\nKerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, V´ıtor Albiero, Vladan Petrovic, Weiwei\nChu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang\nWang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Gold-\nschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning\nMao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh,\nAayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria,\nAhuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein,\nAmanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, An-\ndrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, An-\nnie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel,\nAshwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leon-\nhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu\nNi, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Mon-\ntalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao\nZhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia\nGao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide\nTestuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le,\nDustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily\nHahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smoth-\ners, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni,\nFrank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia\nSwee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan,\nHakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harri-\nson Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj,\nIgor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James\nGeboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jen-\nnifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang,\nJoe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Jun-\njie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy\nMatosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang,\nKunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell,\nLei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa,\nManav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias\nReso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L.\n11"}
{"pdf_id": "arxiv_251210922_sparseswaps", "page": 12, "text": "Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike\nClark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari,\nMunish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan\nSinghal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong,\nNorman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent,\nParth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar,\nPolina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Ro-\ndriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy,\nRaymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin\nMehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon,\nSasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ra-\nmaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha,\nShishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal,\nSoji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satter-\nfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj\nSubramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo\nKoehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook\nShaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar,\nVishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li,\nWenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu,\nXiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi,\nYenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen\nHao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen,\nZhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The Llama 3 Herd of Models, November 2024. URL\nhttp://arxiv.org/abs/2407.21783.\nSong Han, Jeff Pool, John Tran, and William Dally.\nLearning both weights and connections\nfor efficient neural networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Gar-\nnett (eds.), Advances in Neural Information Processing Systems, volume 28. Curran Asso-\nciates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/\nae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf.\nBabak Hassibi and David Stork.\nSecond order derivatives for network pruning: Optimal brain\nsurgeon. In S. Hanson, J. Cowan, and C. Giles (eds.), Advances in Neural Information Processing\nSystems, volume 5. Morgan-Kaufmann, 1993. URL https://proceedings.neurips.\ncc/paper/1992/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf.\nDeborah Hendrych, Hannah Troppens, Mathieu Besanc¸on, and Sebastian Pokutta. Convex inte-\nger optimization with frank-wolfe methods. Mathematical Programming Computation, 2025.\ndoi: 10.1007/s12532-025-00288-w.\nURL https://link.springer.com/article/\n10.1007/s12532-025-00288-w.\nTorsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste.\nSparsity in\ndeep learning: Pruning and growth for efficient inference and training in neural networks. arXiv\npreprint arXiv:2102.00554, January 2021.\nSteven A. Janowsky. Pruning versus clipping in neural networks. Phys. Rev. A, 39:6600–6603, Jun\n1989. doi: 10.1103/PhysRevA.39.6600.\nWoosuk Kwon, Sehoon Kim, Michael W. Mahoney, Joseph Hassoun, Kurt Keutzer, and Amir Gho-\nlami. A fast post-training pruning framework for transformers. March 2022.\nYann LeCun, John S. Denker, and Sara A. Solla. Optimal brain damage. In David S. Touretzky (ed.),\nAdvances in Neural Information Processing Systems 2, [NIPS Conference, Denver, Colorado,\nUSA, November 27-30, 1989], pp. 598–605. Morgan Kaufmann, 1989. URL http://papers.\nnips.cc/paper/250-optimal-brain-damage.\nVladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. Scaling down to scale up: A guide to\nparameter-efficient fine-tuning. March 2023.\nTao Lin, Sebastian U. Stich, Luis Barba, Daniil Dmitriev, and Martin Jaggi. Dynamic model pruning\nwith feedback. In International Conference on Learning Representations, 2020.\n12"}
{"pdf_id": "arxiv_251210922_sparseswaps", "page": 13, "text": "Andreas Lundell, Jan Kronqvist, and Tapio Westerlund. The supporting hyperplane optimization\ntoolkit for convex minlp. Journal of Global Optimization, 84(1):1–41, 2022.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels. September 2016.\nAsit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh,\nChong Yu, and Paulius Micikevicius. Accelerating sparse deep neural networks. April 2021.\nPavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional\nneural networks for resource efficient inference. November 2016.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020.\nMorgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L´eonard\nHussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram´e, Johan Ferret, Peter Liu, Pouya\nTafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy\nJerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt\nHoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna\nWalton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic,\nAmanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben\nBastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris\nWelty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vi-\njaykumar, Dominika Rogozi´nska, Dustin Herbison, Elisa Bandy, Emma Wang, Eric Noland, Er-\nica Moreira, Evan Senter, Evgenii Eltyshev, Francesco Visin, Gabriel Rasskin, Gary Wei, Glenn\nCameron, Gus Martins, Hadi Hashemi, Hanna Klimczak-Pluci´nska, Harleen Batra, Harsh Dhand,\nIvan Nardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Peng\nZhou, Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost van Amersfoort,\nJosh Gordon, Josh Lipschultz, Josh Newlan, Ju-yeong Ji, Kareem Mohamed, Kartikeya Badola,\nKat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish Greene,\nLars Lowe Sjoesund, Lauren Usui, Laurent Sifre, Lena Heuermann, Leticia Lago, Lilly Mc-\nNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas Dixon, Luciano Martins, Machel Reid,\nManvinder Singh, Mark Iverson, Martin G¨orner, Mat Velloso, Mateo Wirth, Matt Davidow,\nMatt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal, Mehran Kazemi, Michael Moyni-\nhan, Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi Rahman, Mohit Khatwani, Natalie Dao,\nNenshad Bardoliwalla, Nesh Devanathan, Neta Dumai, Nilay Chauhan, Oscar Wahltinez, Pankil\nBotarda, Parker Barnes, Paul Barham, Paul Michel, Pengchong Jin, Petko Georgiev, Phil Culli-\nton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena Jana, Reza Ardeshir Rokni,\nRishabh Agarwal, Ryan Mullins, Samaneh Saadat, Sara Mc Carthy, Sarah Cogan, Sarah Perrin,\nS´ebastien M. R. Arnold, Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ron-\nstrom, Susan Chan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tomas Kocisky, Tulsee\nDoshi, Vihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren Barkley, Wei\nWei, Wenming Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong, Zichuan\nWei, Victor Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Ludovic Peran, Tris Warkentin, Eli\nCollins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, D. Sculley, Jeanine Banks, Anca Dra-\ngan, Slav Petrov, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Fara-\nbet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin, Kathleen Kenealy,\nRobert Dadashi, and Alek Andreev. Gemma 2: Improving Open Language Models at a Practical\nSize, October 2024. URL http://arxiv.org/abs/2408.00118.\nMingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. A simple and effective pruning approach\nfor large language models. June 2023.\nMingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. A Simple and Effective Pruning Approach\nfor Large Language Models, May 2024. URL http://arxiv.org/abs/2306.11695.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\ntion processing systems, 30, 2017.\n13"}
{"pdf_id": "arxiv_251210922_sparseswaps", "page": 14, "text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick\nvon Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,\nMariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020 Conference on Empirical Methods in Natu-\nral Language Processing: System Demonstrations, pp. 38–45, Online, October 2020. Associ-\nation for Computational Linguistics.\ndoi: 10.18653/v1/2020.emnlp-demos.6.\nURL https:\n//aclanthology.org/2020.emnlp-demos.6.\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li,\nDayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin\nYang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang,\nLe Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi\nTang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan,\nYuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 Technical Report, January 2025.\nURL http://arxiv.org/abs/2412.15115.\nSeul-Ki Yeom, Philipp Seegerer, Sebastian Lapuschkin, Alexander Binder, Simon Wiedemann,\nKlaus-Robert M¨uller, and Wojciech Samek. Pruning by explaining: A novel criterion for deep\nneural network pruning. December 2019.\nLu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia, Mykola Pechenizkiy,\nYi Liang, Zhangyang Wang, and Shiwei Liu. Outlier weighed layerwise sparsity (owl): A missing\nsecret sauce for pruning llms to high sparsity. October 2023.\nAlex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Guoyin Wang, Heng\nLi, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue,\nSenbin Yang, Shiming Yang, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu,\nPengcheng Nie, Yanpeng Li, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan\nLiu, and Zonghong Dai. Yi: Open Foundation Models by 01.AI, January 2025. URL http:\n//arxiv.org/abs/2403.04652.\nMengxia Yu, De Wang, Qi Shan, Colorado J. Reed, and Alvin Wan. The Super Weight in Large\nLanguage Models, July 2025. URL http://arxiv.org/abs/2411.07191.\nYingtao Zhang, Haoli Bai, Haokun Lin, Jialin Zhao, Lu Hou, and Carlo Vittorio Cannistraci. Plug-\nand-play: An efficient post-training pruning method for large language models. In The Twelfth\nInternational Conference on Learning Representations, 2024. URL https://openreview.\nnet/forum?id=Tr0lPx9woF.\nYuxin Zhang, Lirui Zhao, Mingbao Lin, Yunyun Sun, Yiwu Yao, Xingjia Han, Jared Tanner, Shiwei\nLiu, and Rongrong Ji. Dynamic sparse no training: Training-free fine-tuning for sparse llms.\nOctober 2023.\nMax Zimmer, Megi Andoni, Christoph Spiegel, and Sebastian Pokutta. Perp: Rethinking the prune-\nretrain paradigm in the era of llms. arXiv preprint arXiv:2312.15230, December 2023a. URL\nhttps://arxiv.org/abs/2312.15230.\nMax Zimmer, Christoph Spiegel, and Sebastian Pokutta. How I Learned To Stop Worrying And\nLove Retraining.\nIn International Conference on Learning Representations, 2023b.\nURL\nhttps://openreview.net/forum?id=_nF5imFKQI.\nMax Zimmer, Christoph Spiegel, and Sebastian Pokutta.\nCompression-aware training of neu-\nral networks using Frank–Wolfe, pp. 137–168.\nDe Gruyter, Berlin, Boston, 2025.\nISBN\n9783111376776.\ndoi: doi:10.1515/9783111376776-010.\nURL https://doi.org/10.\n1515/9783111376776-010.\n14"}
{"pdf_id": "arxiv_251210922_sparseswaps", "page": 15, "text": "A\nAPPENDIX\nA.1\nFURTHER RESULTS\nTable 4: Perplexity (↓, lower is better) comparison on WikiText. We report SparseSwaps refinement\nwith magnitude warmstart for 50% and 60% sparsity. Best values are highlighted in bold. We omit\nstandard deviations for legibility.\nPerplexity ↓\nLLAMA-3.1\nGEMMA-2\nDEEPSEEK\nMethod\nSparsity\n8B\n9B\n7B\nMagnitude\n50%\n68.89\n31.87\n25.05\n+ SparseSwaps\n50%\n52.26\n19.11\n16.23\nMagnitude\n60%\n3486.26\n184.52\n330.07\n+ SparseSwaps\n60%\n264.92\n60.04\n80.24\n0\n100\n200\n300\n400\n500\nnumber of samples\n10.2\n10.4\n10.6\nperplexity\nLLaMA-3.1-8B (50% sparsity)\nWanda\nSparseSwaps\n(a) 50% unstructured sparsity\n0\n100\n200\n300\n400\n500\nnumber of samples\n19\n20\n21\n22\n23\nperplexity\nLLaMA-3.1-8B (60% sparsity)\nWanda\nSparseSwaps\n(b) 60% unstructured sparsity\nFigure 2: Perplexity versus the number of reconstruction samples for unstructured sparsity using\nWanda warmstart.\n15"}
