{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 1, "text": "BabyVLM-V2: Toward Developmentally Grounded Pretraining and\nBenchmarking of Vision Foundation Models\nShengao Wang*†, Wenqi Wang∗, Zecheng Wang∗, Max Whitton∗\nMichael Wakeham, Arjun Chandra, Joey Huang, Pengyue Zhu\nHelen Chen‡, David Li‡, Jeffrey Li‡, Shawn L. Li‡, Andrew Zagula‡, Amy Zhao‡, Andrew Zhu‡\nSayaka Nakamura2, Yuki Yamamoto2, Jerry Jun Yokono2\nAaron Mueller, Bryan A. Plummer, Kate Saenko, Venkatesh Saligrama, Boqing Gong\nBoston University,\n2Sony Group Corporation, {wsashawn,wqwang,vicwang0,maxwh,bgong}@bu.edu\nhttps://shawnking98.github.io/BabyVLM-v2/\nAbstract\nEarly children’s developmental trajectories set up a natu-\nral goal for sample-efficient pretraining of vision founda-\ntion models. We introduce BabyVLM-V2, a developmentally\ngrounded framework for infant-inspired vision-language\nmodeling that extensively improves upon BabyVLM-V1\nthrough a longitudinal, multifaceted pretraining set, a\nversatile model, and, most importantly, DevCV Tool-\nbox for cognitive evaluation.\nThe pretraining set maxi-\nmizes coverage while minimizing curation of a longitudinal,\ninfant-centric audiovisual corpus, yielding video-utterance,\nimage-utterance, and multi-turn conversational data that\nmirror infant experiences.\nDevCV Toolbox adapts all\nvision-related measures of the recently released NIH Baby\nToolbox® into a benchmark suite of ten multimodal tasks,\ncovering spatial reasoning, memory, and vocabulary un-\nderstanding aligned with early children’s capabilities. Ex-\nperimental results show that a compact model pretrained\nfrom scratch can achieve competitive performance on De-\nvCV Toolbox, outperforming GPT-4o on some tasks. We\nhope the principled, unified BabyVLM-V2 framework will\naccelerate research in developmentally plausible pretrain-\ning of vision foundation models.\n1. Introduction\nWe formalize our objective: Given a longitudinal, infant-\ncentric audiovisual sample of early children’s sensory expe-\nriences (e.g., Figure 1a), can we learn a foundation model\n*Equal contribution.\n†Project lead.\n‡Equal contribution; work done as interns at Boston University.\nFigure 1. BabyVLM-V2: An extensive, versatile, and develop-\nmentally plausible framework for research in vision foundation\nmodels. Its (a) pretraining set is diverse in format (video, image-\nutterance, and multiple turns), enabling (b) a flexible model. Its (c)\nbenchmark developmentally aligns with the pretraining set’s age\nspan by grounding on the newly released NIH Baby Toolbox®.\n(FM) that is as versatile and capable as the early children’s\nperception? As a further challenge, can we leverage princi-\nples of developmental psychology to create a benchmark as\nan initial step toward artificial developmental intelligence\n(ADI), in both what it is and how to achieve it within the\nconstraints of early children’s limited sensory intake? We\nconsider a resultant model and benchmark developmentally\nplausible if the training data and desired model performance\nclosely mirror those of early children.\nWe\nenvision\nthat\nour\nanswer\nto\nthis\nobjective,\nBabyVLM-V2, will have a threefold impact.\nFirst, by\nmaking the limited training data accessible to independent\nresearchers and friendly to university resources, we will\nbroaden research engagement in developing FMs [18, 59]\nin a time when the scaling law [23] causes research on FMs\n1\narXiv:2512.10932v1  [cs.CV]  11 Dec 2025"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 2, "text": "Table 1. BabyVLM-V2 extensively extends BabyVLM-V1 [56].\nBabyVLM-V1\nBabyVLM-V2 (Ours)\nPretraining\n67k img-utterance\n768k img-utterance\n181k video-utterance\n63k interleaved\nInstruction\nNone\n150k examples\nBenchmarks\n4 tasks, intuitive\n10 tasks, grounded on\nNIH Baby Toolbox®\nVisual vocabulary,\ncaptioning\nVisual\nvocabulary,\ncounting,\nmemory,\nattention,\nspatial\nrea-\nsoning,\nlocalization,\nspatiotemporal\nreason-\ning, executive function\nModels\nInput: text, single\nimg\nInput: text, img, multi-\nimg, video, multi-turn\nOutput: logits\nOutput: language\nto be dominated by industry. Second, we envision that ADI\ncould advance studies in cognitive science and psychology\nby allowing scientists to read into early children’s minds in\nan unprecedented way. Lastly, we believe that the broad-\nened engagement in FMs will improve public understand-\ning, trust, and safe use of FMs and AI in general.\nPreviously, Wang et al. proposed BabyVLM-V1 [56], a\nscaffold for studying ADI from the lens of vision-language\nmodels (VLMs). It consists of 1) an image-text pretrain-\ning set extracted from SAYCam’s head-mounted camera\nrecordings from three children for approximately two hours\nper week from age 6 to 32 months [50], 2) four intuitive\nand developmentally inspired benchmark tasks, and 3) a\npublic codebase for pretraining and evaluation. BabyVLM-\nV1 pretrained a baseline VLM from scratch, whose perfor-\nmance, unfortunately, fell far behind the remarkable capa-\nbilities of early children [7, 34]. Similarly, Vong et al. [54]\ntrained a CLIP-style [44] contrastive model using SAYCam,\nbut with a narrower focus on word-referent mappings rather\nthan general perception. More related work is in Section 2.\nWhile BabyVLM-V1 sets up a basic framework, it lacks\ncrucial elements. Its pretraining set only leverages about\na third of SAYCam’s recordings, causing it to cover only a\ntiny portion of the total visual intake time of a three-year-old\nsince birth [38]. It does not support instruction tuning [64],\nwhich is crucial for a pretrained model to articulate its ca-\npabilities to user instructions. Importantly, its evaluation\nbenchmarks are not based on any established psychology\ntests. Finally, the models trained in BabyVLM-V1 have\nnear-zero open-set performance, and one has to postprocess\ntheir logits for evaluation.\nThis work extends BabyVLM-V1 to a comprehen-\nsive, extensive, and developmentally plausible framework,\nBabyVLM-V2 (see Figure 1), for studying the objective\nposed at the beginning of the paper. Table 1 contrasts the\ntwo frameworks in pretraining, instruction tuning, bench-\nmarks, and baseline models. Notably, we provide DevCV\nToolbox (see Figure 3), a benchmark of ten tasks designed\nusing the NIH Baby Toolbox® [11, 16], which was publicly\nreleased in February 2025 as a “universal assessment for\ndevelopmental and pediatric communities”. We make min-\nimal changes while adapting all of its vision-related mea-\nsures to DevCV Toolbox in order to maintain developmen-\ntal fidelity.\nInterestingly, the DevCV Toolbox tasks are naturally\ndiverse in format, desiring FMs to understand individual\nvideos and images, reason across multiple images, and\nsolve a task in multiple turns. To account for these require-\nments in the pretraining data, we compile video, image-\nutterance, and multi-turn data from the longitudinal, infant-\ncentric videos in SAYCam [50]. As in BabyVLM-V1, we\ninclude a minimal curation process to bring our pretraining\ndata as close to the children’s sensory intake as possible.\nWe validate BabyVLM-V2 through extensive experi-\nments and human performance surveys. A model trained\nfrom scratch within our BabyVLM-V2 framework outper-\nforms GPT-4o in math tasks, highlighting the potential of\ndevelopmentally grounded pretraining.\n2. Related work\nVision FMs refer to general-purpose models [3] often pre-\ntrained on massive visual data [4, 37, 46, 58]. They can\ntackle many vision tasks via a unified interface, such as\nCLIP [44], ALIGN [20], BLIP [26, 27], SAMs [24, 45],\nand vision LLMs [1, 6, 28, 40]. The development of these\npowerful models hinges critically on pretraining [3, 5, 10], a\nprocess that trains a model on a large, generic dataset before\ntuning it to any downstream tasks.\nSample-efficient pretraining. While FMs have been re-\nlying on the scaling law, sample-efficient pretraining has\ngained momentum recently in the language [59] and medi-\ncal [51] domains. To the best of our knowledge, BabyVLM-\nV1 was the first of this kind in vision, and we further their\neffort with a more comprehensive and extensive framework.\nCognitively plausible benchmarking. BabyVLM-V1 [56]\ndesigns four developmentally plausible tasks, which unfor-\ntunately lack grounding on established psychological tests.\nDevBench [52] and KIVA [62] draw inspiration from kid-\noriented tests, yet they are more age-advanced than our\npretraining data. Other cognitively plausible benchmarks\nhave a narrower focus, such as Zorro [19], LRS [25], In-\nfLevel [60], CoreCognition [29], and MEWL [21], and\nModelVsBaby [48]. Table 2 summarizes the differences.\nTools assessing neurodevelopment in children.\nOur\nbenchmark\ntasks\nare\ngrounded\non\nthe\nNIH\nBaby\nToolbox® [11], a standardized tool released in February\n2025 for assessing neurodevelopment in children. It is not\n2"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 3, "text": "Table 2. Comparison of existing developmentally inspired benchmarks.\nBenchmark\nDevelopmental\nTask Diversity\nMultimodal\nTrain\nVal\nTest\nIn-Domain\nOOD\nHuman Data\nModel\nDevBench [52]\n✓\n✓\n✓\n✗\n✗\n✓\n✗\n✓\n✓\n✗\nLabeled-S [54]\n✓\n✗\n✓\n✓\n✗\n✓\n✓\n✗\n✗\n✗\nModelVsBaby [48]\n✓\n✗\n✓\n✓\n✓\n✓\n✗\n✓\n✓\n✗\nMEWL [21]\n✗\n✓\n✓\n✓\n✓\n✓\n✓\n✗\n✓\n✗\nZorro [19]\n✓\n✗\n✗\n✓\n✗\n✓\n✓\n✗\n✗\n✓\nInfLevel [60]\n✓\n✗\n✓\n✗\n✗\n✓\n✗\n✓\n✓\n✗\nLRS [25]\n✓\n✓\n✗\n✗\n✗\n✓\n✗\n✓\n✗\n✗\nCoreCognition [29]\n✓\n✓\n✓\n✗\n✗\n✓\n✗\n✓\n✓\n✗\nBabyVLM [56]\n✗\n✓\n✓\n✓\n✗\n✓\n✓\n✗\n✗\n✓\nDevCV Toolbox (Ours)\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nonly more recent but also more comprehensive and normed\nthan alternatives, such as the Bayley Scales Of Infant and\nToddler Development [2], Mullen Scales of Early Learn-\ning [9], and Battelle Developmental Inventory [39]. Be-\nsides, its design for clinical use validates its credibility over\nthe psychological tests used in research settings.\n3. BabyVLM-V2\n3.1. Data source & the pretraining set\nWe describe SAYCam, the developmental data source, fol-\nlowed by our minimal process to curate the pretraining set.\nSAYCam:\nThe developmental plausibility of our work\nhinges on the use of a visual-audio-text corpus that faith-\nfully samples what early children have seen and heard by\na certain age, which requires the corpus to be 1) longitu-\ndinal and 2) infant-centric. To accomplish this, we use the\nSAYCam dataset [50], which is accessible to all nonprofit\ninstitutes, and will include BabyView [32] in future work.\nSAYCam contains egocentric recordings from three infants\n(left of Figure 1a) taken once every week from roughly 6\nto 32 months old.\nEach recording is approximately two\nhours, and the recordings total 478 hours (see bottom of\nFigure 2 for the recorded time vs. wake and sleep time [38]).\nNotably, the utterances found in SAYCam are mostly from\ncaregivers providing simple verbal instructions and descrip-\ntions to the infants (top of Figure 2). BabyView [32] is an\nongoing effort in the same spirit as SAYCam, but at a larger\nscale and with extra gyroscope/accelerometer sensors.\nData split & the pretraining set. To maximize our use\nof the SAYCam corpus, we designate all video clips con-\ntaining speech to the pretraining split, and evenly divide the\nremaining clips into validation and test splits. Their relative\nsizes are approximately 3:1:1, respectively. We then apply\nminimal processing to facilitate model pretraining while ob-\nserving the children’s sensory intake as much as possible.\nSpecifically, we transcribe all utterances, which are almost\nall from caregivers, using Azure Speech Recognition [36].\nWe then construct three types of pretraining data.\n• Video–utterance pairs. We segment the camera record-\nings into short clips based on transcript boundaries, with\neach clip corresponding to exactly one utterance.\nWe\nthen drop the video clips shorter than 0.5 seconds or\nwith a transcript confidence score below 0.3. Further, we\ncompute video-utterance similarities using X-CLIP [33]\nand only retain the video-utterance pairs with similarities\ngreater than 0.1. This process leaves approximately 181k\nvideo clips in our pretraining set, a total of 138 (out of\n478) hours. We pad 1 second to either side of the clips.\n• Image–utterance pairs. Following BabyVLM-V1, we\nsample at 1 FPS from the video-utterance pairs and com-\npute the CLIP similarity [44] between each frame and its\nutterance. Only frames with CLIP similarities > 0.2 are\nretained, resulting in 768k image-utterance pairs in total.\n• Interleaved text and images. We create sequences of\ninterleaved images and utterance from consecutive video\nsegments, aiming to enable downstream capabilities that\ninvolve conversations. For each video segment, we pair\nthe frame with the highest CLIP similarity with its as-\nsociated utterance and use a sliding window over the re-\nsulting image-utterance pairs to construct the interleaved\nsequences. We randomly choose a window size between\n4 and 8 and employ a stride of half the window size, re-\nsulting in 63k interleaved sequences.\nUnlike BabyVLM-V1’s image-utterance pairs, the mixing\nof three pretraining data formats prepares models for di-\nverse downstream tasks, which can involve videos, multiple\nor single images, and even multi-turn conversations.\n3.2. Pretraining & fine-tuning BabyLLaVA-V2\nUsing our pretraining split, we pretrain BabyLLaVA-V2,\nwhich uses a language model (LLaMA-1.1B [53, 63]) as\na versatile interface to probe various capabilities of a visual\nencoder (ViT-L-16 [8], 300M parameters). A lightweight\nMLP connector [30] projects visual features into the lan-\nguage space. This model architecture (Figure 1b) is the\nsame BabyVLM-V1’s BabyLLaVA-Llama. We pretrain the\nentire model from scratch using the three-stage pipeline de-\nscribed in Appendix A. Finally, we fine-tune the model us-\ning a small, curated instruction set consisting of the tasks as\nin DevCV Toolbox, which we describe next.\n3"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 4, "text": "Time: 15:22\nUtterance: It’s really \ngood with yogurt.\nTime: 17:03\nUtterance: Want to \ntry a little piece of \nthis?\nTime: 18:46\nUtterance: How \nabout another \nblueberry?\nTime: 18:59\nUtterance: Oh that’s \na very sweet one.\nTime: 00:40\nUtterance: You won’t?\nTime: 00:45\nUtterance: Which one \nare you choosing?\nTime: 00:48\nUtterance: Hard, huh?\nTime: 00:50\nUtterance: Don’t use \nyour hand.\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\nmonth\nSleep\nWake\nRecorded\nFigure 2. Top: Video frames and utterances recorded from the infants’ view. Bottom: Recorded wake time vs. wake/sleep time for the ages\nof 6 months to 32 months in SAYCam [50].\n3.3. Age-appropriate benchmarking\nOur objective with BabyVLM-V2 is to design benchmark\ntasks that test age-appropriate visual skills given our pre-\ntraining data’s age span. However, we acknowledge that de-\nvelopmental benchmarking is an ongoing and rapidly evolv-\ning field of research. Early children’s growth rates vary\nsignificantly, and among psychologists and cognitive scien-\ntists, substantial conceptual and methodological disagree-\nments exist regarding the notion of developmental intelli-\ngence and how to properly probe, measure, and benchmark\nit [16]. How can we properly define ADI, then, given the in-\nconsistent measurement techniques in human developmen-\ntal research? To answer this, we consult with two experi-\nenced psychologists specializing in development and learn-\ning. Numerous meetings with them led us to the timely NIH\nBaby Toolbox®, over which we ground the design of our\nbenchmark, DevCV Toolbox.\n3.3.1. Background: NIH Baby Toolbox®\nIn February 2025, a multi-institutional team solicited by the\nNIH publicly released the NIH Baby Toolbox®, envision-\ning it as a standardized evaluation of neurodevelopmental\nintelligence in infants [15]. The NIH Baby Toolbox® di-\nvides developmental function into three domains: Cogni-\ntion, Motor, and Social-Emotional, where the Cognition\ndomain includes the subdomains of Language, Executive\nFunction/Memory, and Math, each consisting of some num-\nber of specific tests, known in the toolbox as measures. See\nTable 3 for a summary of these measures and Appendix B\nfor technical details.\n3.3.2. DevCV Toolbox\nIn this section, we develop a computer vision counterpart,\ncalled task for clarity, for every vision-related measure in\nthe NIH Baby Toolbox®, leading to ten tasks in our DevCV\nToolbox, which are summarized in Table 3 and illustrated\nin Figure 3.\nThe need to adapt measures to tasks. Unlike the prac-\ntice in computer vision, most of the measures originally\nfound in the NIH Baby Toolbox® 1) have only a couple of\ntest examples and 2) are human-oriented but not accessi-\nble to AI models. Additionally, the cartoon stimuli in NIH\nBaby Toolbox® are out-of-domain from our pretraining set,\npreventing their direct use. Hence, we adapt the measures\nto computer vision tasks by standardizing their format and\nequipping each task with thousands of naturalistic examples\n(see Table 3), separated into instruction and test sets accord-\ning to the split defined in the pretraining stage.\nWe construct the tasks using SAYCam to ensure that\nthe examples are in the same domain as the pretraining\ndata, thereby focusing the benchmarking on the models’\nin-domain cognitive capabilities. To provide an additional\ntool to evaluate models’ generalizability, we also compile\nan out-of-domain test set using Ego4D [14] with the same\ntechniques. Below, we detail the construction of Picture Vo-\ncabulary as a representative example, and briefly describe\nthe rest. See Appendix B for more details on the construc-\ntion of DevCV Toolbox.\nPicture vocabulary (≥25 months): The top right of Fig-\nure 3 shows the original picture vocabulary (PV) measure\nfound in the NIH Baby Toolbox®, which assesses the Recep-\ntive Language of children aged 25 months and older. Partic-\nipants are presented with four clipart images on an iPad, and\nan audio prompt instructs them to touch the named image.\nWe adapt PV to DevCV Toolbox using the pipeline in\nFigure 4, to replace the clipart in the NIH Baby Toolbox®\nmanual with objects and actions detected from SAYCam\nvideo frames. Concretely, we sample frames at 1FPS, la-\nbel all objects and actions present using manual transcripts\nand GPT-4o, and then crop out regions for each label us-\ning Grounding-DINO [31]. Low quality crops and labels\nbeyond the child-oriented MAB-CDI vocabulary [35] are\nremoved. Each PV example (e.g., the top left of Figure 3)\nconsists of a language prompt, a target image corresponding\n4"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 5, "text": "Figure 3. DevCV Toolbox tasks and their corresponding NIH Baby Toolbox® measures\nto the prompt, and three distractor images, and we construct\nthe examples in a round-robin manner for diversity. The\ntarget and distractor images are related either semantically\nor phonologically in NIH Baby Toolbox®; therefore, we de-\nrive a distractor distribution over phonology and semantics\nfrom the toolbox and then sample distractor images accord-\ningly. We manually screen the process to ensure quality and\ndiversity. Appendix B presents more details.\nOther tasks. We describe the other tasks in DevCV Tool-\nbox briefly. Construction details are in Appendix B.\n1. Looking while listening (6–24 months) shows infants\ntwo clipart objects, and plays an audio prompt describ-\ning one of them. Eye tracking is used to detect the partic-\nipant’s response. We replace clipart with natural objects\nfrom SAYCam, and eye tracking with multiple choice.\n2. Localization / Mullen visual receptive language #19\n5"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 6, "text": "Table 3. DevCV Toolbox tasks and their corresponding NIH Baby Toolbox® measures (EF/M stands for Executive Function/Memory).\nDevCV Toolbox tasks\n#Instruct/#Test\nModel Input\nNIH Baby Toolbox® measures\nMonths\nSubdomain\nLooking While Listening\n0/1.2k\n2 images\nLooking While Listening\n6-24\nLanguage\nPicture Vocabulary\n63.9k/1.2k\n4 images\nPicture Vocabulary\n25+\nLanguage\nLocalization\n12.3k/2.1k\n1 image\nMullen Receptive Language #19\n1-42\nLanguage\nLeft/Right\n12.3k/2.3k\n4 images\nMullen Visual Reception #29\n1-42\nEF/M\nSpatial Details\n11.8k/1.2k\n4 image\nMullen Visual Reception #20\n1-42\nEF/M\nVisual Delayed Response\n5.2k/0.9k\n5-8 images\nVisual Delayed Response\n22-42\nEF/M\nMemory\n10.0k/0.5k\n29 images\nDelayed Memory\n22-42\nEF/M\nWho Has More (synthetic)\n11.2k/1.8k\n2 images\nWho Has More\n25-42\nMath\nWho Has More (naturalistic)\n6.9k/2.2k\n2 images\nWho Has More\n25-42\nMath\nSubitizing (synthetic)\n0/1.9k\n3 images\nSubitizing\n25-42\nMath\nSubitizing (naturalistic)\n0/0.2k\n3 images\nSubitizing\n25-42\nMath\nObject Counting\n13.7k/3.0k\n1 image\nObject Counting\n25-42\nMath\nFigure 4. Pipeline to adapt the picture vocabulary measure in NIH Baby Toolbox® to DevCV Toolbox.\n(1–42 months) tests an infant’s ability to point at\nsketched objects as they are named. We task a model\nwith localizing an object in a natural video frame.\n3. Left/Right / Mullen visual reception #29 (1–42\nmonths) measures an infant’s attention to detail by in-\nstructing them to match objects by orientation.\n4. Spatial details / Mullen visual reception #29 (1–42\nmonths) measures attention to detail in identical objects\namong distractors of the same type.\n5. Visual delayed response (22–42 months) shows infants\na creature moving behind one of two occluders, and after\na short pause, instructs them to tap the target occluder.\nWe use video clips with prominent objects moving out\nof the field of view.\n6. (Delayed) memory (22–42 months) involves multiple\nturns, each presenting a pair of animals. Participants\nare asked to “feed” the new animal appearing for the\nfirst time, and they receive corrective feedback during\nthe early rounds.\n7. Who has more (25–42 months) shows two images with\nthe same shape in different quantities and asks which im-\nage has more. We replace the shape with natural objects\nas one sub-task, and use entire natural video frames for\nthe other sub-task.\n8. Subitizing (25–42 months) refers to the rapid identifica-\ntion of the number of items in a small set. An infant sees\none to four identical shapes for one second, and then an\naudio prompt requests the count.\n9. Object counting (25–42 months) evaluates a child’s\nability to count up to 12 colored shapes on a screen.\nDuring evaluation, we employ accuracy as the metric.\nThese tasks cover all cognitive measures in NIH Baby\nToolbox® except the non-visual MacArthur-Bates lan-\nguage (9–30 months, 7–18 months), familiarization (6–21\nmonths), verbal counting (25–42 months), and verbal arith-\nmetic (37–42 months). Adult performance data on these\ntasks confirms the validity of our DevCV Toolbox (see Hu-\nman performance in Tables 4 and Appendix C for details).\nIn future work, we hope to complete a survey of children’s\nperformance.\n4. Experiments\nWe\ndesign\nexperiments\nabout\nthe\nkey\nelements\nof\nBabyVLM-V2 framework, aiming to validate the quality of\nthe DevCV Toolbox, as well as illustrate the effectiveness of\nour training data and training recipe. Meanwhile, the exper-\n6"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 7, "text": "Figure 5.\nTask-specific supervised finetuning of LLaVA-\nOneVision-7B and Qwen2.5-VL-7B.\niments position our BabyLLaVA-V2 in context across three\ncognitive subdomains and ten tasks. Note that we exclude\ntwo tasks, Subitizing and Looking While Listening, from\nthe majority of the experiments to test our models’ general-\nization on unseen tasks near the end. Implementation details\nare in Appendix D.\n4.1. Examining DevCV Toolbox\nOverall quality. We validate the quality of DevCV Tool-\nbox by conducting human surveys, detailed in Appendix C.\nAs shown in Table 4, the human volunteers recruited in our\nhome institute achieved near-perfect accuracy on the the\nexecutive functioning/memory subdomain (Spatial Details,\nMemory, Visual Delayed Response) and the math tasks of\nObject Counting and Who Has More. Their accuracy on\nLocalization is slightly low (87.3%), and a follow-up re-\nvealed that it could improve when the volunteers were in-\nstructed to spend more time on the task.\nDifferentiating capability. Table 4 also demonstrates that,\nbetween Human performance and Random guess,\nthere is a sufficiently big room for differentiating various\nmodels. Indeed, the proprietary GPT and Gemini models\nare on the upper end, while our BabyLLaVA-V2 and the\nopen-source models of about the same size as ours are on\nthe lower end, indicating that the tasks in DevCV Toolbox\nare challenging but solvable.\nDevelopmental fidelity. DevCV Toolbox should develop-\nmentally align with the pretraining data’s age span (6–32\nmonths).\nHence, we are in the process of performing a\nlarge-scale children survey about DevCV Toolbox using the\nChildren Helping Science platform [49], though this survey\nwill take a couple of years per our estimation.\n4.2. Validating the instruction tuning dataset\nInstruction tuning addresses the mismatch between pre-\ntraining and downstream tasks, steering models towards the\ndownstream. To validate the effectiveness of our instruction\ntuning data, we use it to supervise the fine-tuning of three\nmodels under two strategies. Figure 5 fine-tunes LLaVA-\nOneVision-7B and Qwen2.5-VL-7B on each task separately\n(see Appendix A for the experiment setup). The consistent\nand relatively big gains from the fine-tuning are highlighted\nin the red top bars, signifying that the instruction data can\neffectively guide the models to the downstream tasks in De-\nvCV Toolbox.\nFurthermore, we experiment with the second fine-tuning\nstrategy that combines the instruction data into a single set.\nTable 5 contrasts it against the first strategy, fine-tuning a\nmodel for each task separately, over our BabyLLaVA-V2.\nThe results show that the overall difference between the\ntwo strategies is marginal. The results on most tasks de-\ncrease under the mixed-tuning setting, which produces a\nsingle unified model rather than multiple per-task models,\nbut some tasks, such as Memory and Spatial Details, can ac-\ntually benefit from the mixed fine-tuning, implying knowl-\nedge transfer or regularization from other tasks.\n4.3. Ablating the pretraining data\nThe speech transcripts in our pretraining set could be noisy\nbecause the naturalistic child-directed utterances are often\nmisaligned with the children’s visual intake. We study their\nimpact on the pretrained models by replacing the transcripts\nwith video captions generated by GPT-4o (see Appendix D\nfor how we prompt GPT-4o). We train BabyLLaVA-V2-\nsynthetic on this altered pretraining dataset and present the\nresults in Table 6. Overall, the synthetic captions improve\nperformance, especially on tasks that demand semantic rea-\nsoning (Picture Vocabulary) and a long attention window\n(Memory).\nHowever, the gains are modest, suggesting\nthat our minimally curated pretraining set already provides\nstrong supervision. In future work, novel pretraining algo-\nrithms can likely mine stronger supervision from this or-\nganic pretraining set.\n4.4. Inspecting BabyLLaVA-V2\nOur BabyLLaVA-V2’s overall performance in Table 4 is en-\ncouraging, on par with the open-source models whose size\nis about the same as ours. Of course, one could argue that\nthose models are not fine-tuned under the BabyVLM-V2\nframework, but they are probably trained on much larger\ndatasets than ours.\nTo further stretch BabyLLaVA-V2, we study its gener-\nalization along two axes: 1) out-of-domain generalization\nand 2) performance over previously unseen tasks.\nOut-of-domain generalization. We have created a sibling\nof DevCV Toolbox by replacing SAYCam with Ego4D.\nBoth are about egocentric videos, but Ego4D is from the\nperspective of grown-ups.\nBabyLLaVA-V2’s overall ac-\ncuracy on this sibling benchmark is 41.1% (vs. 31.8%\nof random guess), significantly lower than its in-domain\nperformance (55.2%) on DevCV Toolbox.\nWe conclude\nthat BabyLLaVA-V2 can generalize beyond its training do-\nmain to some degree, but it is far from human infants’ re-\nmarkable generalization capabilities. Appendix D further\n7"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 8, "text": "Table 4. Performance comparison of different models on DevCV Toolbox (in-domain). Different background colors denote different\nmodel families. We report accuracy (%) for all tasks; the higher, the better.\nModel\nOverall\nCount\nLeftRight\nSpatial\nPV\nMemory\nLocalization\nVisual Delay Response\nWho Has More\nbinary\nmulti-exact\nmulti-adjacent\nsynthetic\nnaturalistic\nUpper bound\nHuman performance\n93.0\n99.1\n94.5\n100\n91.8\n97.9\n87.3\n98.2\n63.6\n95.5\n98.2\n96.4\nProprietary models\nGemini-2.5-flash\n72.7\n71.1\n34.9\n73.8\n91.2\n96.9\n84.8\n75.9\n42.4\n70.3\n87.5\n70.7\nGPT-4o\n74.6\n39.0\n89.8\n92.6\n93.7\n99.7\n81.7\n64.2\n29.3\n62.9\n87.9\n79.3\nGemini-2.5-pro\n82.5\n77.2\n68.8\n90.5\n93.8\n97.8\n88.8\n86.9\n54.0\n87.7\n90.6\n71.7\nGPT-5\n87.6\n69.1\n96.0\n94.5\n95.0\n99.9\n85.2\n95.1\n62.9\n90.1\n88.9\n86.6\nOpen-source models\nLLaVA-OneVision-0.5B\n33.2\n43.5\n33.7\n28.7\n23.5\n24.0\n12.3\n58.9\n7.31\n49.2\n37.3\n46.2\nInternVL3.5-1B\n37.2\n27.9\n32.2\n34.6\n34.4\n25.8\n44.8\n64.1\n11.6\n36.8\n47.8\n49.1\nQwen2.5-VL-3B\n47.0\n29.2\n33.7\n40.0\n71.7\n36.5\n85.8\n66.7\n17.0\n32.7\n51.7\n52.3\nBaby models (Ours)\nBabyLLaVA-V2\n55.2\n44.6\n42.3\n91.3\n27.4\n75.3\n38.8\n57.6\n33.1\n45.6\n98.4\n52.8\nLower bound\nRandom guess\n31.8\n8.33\n33.3\n33.3\n25.0\n25.0\n25.0\n50.0\n12.5\n37.5\n50.0\n50.0\nTable 5. Two supervised fine-tuning strategies. BabyLLaVA-V2-separate denotes models fine-tuned on each task’s instruction dataset\nseparately, and BabyLLaVA-V2-mixed is a single model fine-tuned on the mixed instruction set.\nModel\nOverall\nCount\nLeftRight\nSpatial\nPV\nMemory\nLocalization\nVisual Delay Response\nWho Has More\nbinary\nmulti-exact\nmulti-adjacent\nsynthetic\nnaturalistic\nBabyLLaVA-V2-separate\n56.0\n45.2\n42.5\n87.1\n28.4\n70.7\n43.3\n55.7\n37.0\n49.9\n98.6\n56.4\nBabyLLaVA-V2-mixed\n55.2\n44.6\n42.3\n91.3\n27.4\n75.3\n38.8\n57.6\n33.1\n45.6\n98.4\n52.8\nFigure 6. GPT-4o and our model’s counting performance by dif-\nferent object numbers.\ntests BabyLLaVA-V2’s out-of-domain generalization on the\noriginal NIH Baby Toolbox®.\nUnseen tasks. We have excluded Looking While Listen-\ning and Subitizing from the instruction tuning, which are\nthus unseen by BabyLLaVA-V2. While the two tasks are\nin spirit similar to Picture Vocabulary and Object Counting,\nrespectively, BabyLLaVA-V2 yields near-random-guess re-\nsults on them. We will address this issue in future work by\nimproving the instruction tuning algorithm.\n4.5. Intriguing findings\nFinally, we draw some intriguing “byproduct” findings from\nTable 4, which can improve our understanding of the pro-\nprietary GPT and Gemini models.\nGPT models struggle to count. Object Counting requires\na model to count objects in an image (between 1 and 12),\nand GPT-4o can hardly count beyond 5 (see Figure 6).\nBabyLLaVA-V2 can match or outperform GPT-4o on\nsome cognitive tasks. On Spatial Details and Who Has\nMore, BabyLLaVA-V2 is on par with the four latest GPT\nand Gemini models. Moreover, it even outperforms GPT-\n4o on the math tasks of Object Counting and Who Has\nMore. Figure 6 shows that BabyLLaVA-V2 counts better\nthan GPT-4o given six or more objects.\nGPT vs. Gemini. In general, the proprietary models give\nrise to similar results on DevCV Toolbox. However, when\nwe zoom into the individual tasks, GPT-5 is significantly\nbetter than the rest on Spatial Details, while Gemini models\nare better at Object Counting than the GPT models.\n5. Conclusion\nWe introduced BabyVLM-V2, a framework that fea-\ntures a developmentally plausible pretraining set derived\nfrom the longitudinal SAYCam corpus, a compact VLM\n(BabyLLaVA-V2) trained from scratch, and comprehen-\nsive developmental benchmarks (DevCV Toolbox).\nDe-\nvCV Toolbox adapts all vision-related measures from the\nnewly published NIH Baby Toolbox®. It contains ten mea-\nsures spanning three subdomains (language, executive func-\ntion/memory, and math) and requires a flexible model inter-\nface that can process image, video, and multi-turn dialogue.\nWe demonstrate the potential of developmentally plausi-\nble vision FMs through extensive experiments on our pre-\ntraining and instruction tuning datasets, and we confirm the\nquality of DevCV Toolbox through extensive benchmark-\ning with proprietary and open-source models. This frame-\nwork will serve as a principled platform to broaden research\n8"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 9, "text": "Table 6. Two language sources for pretraining. BabyLLaVA-V2-original is pretrained on our pretraining set whose language is mainly\ncaregivers’ speech transcripts, while BabyLLaVA-V2-synthetic is pretrained on synthetic utterances generated by GPT-4o.\nModel\nOverall\nCount\nLeftRight\nSpatial\nPV\nMemory\nLocalization\nVisual Delay Response\nWho Has More\nbinary\nmulti-exact\nmulti-adjacent\nsynthetic\nnaturalistic\nBabyLLaVA-V2-original\n55.2\n44.6\n42.3\n91.3\n27.4\n75.3\n38.8\n57.6\n33.1\n45.6\n98.4\n52.8\nBabyLLaVA-V2-synthetic\n57.4\n46.7\n35.3\n92.0\n30.7\n87.7\n36.9\n57.8\n38.1\n49.0\n99.2\n57.6\nengagement in vision FMs and accelerate progress toward\ndevelopmentally plausible learning.\nAcknowledgements\nSpecial thanks to Chen Yu, Jessica Sullivan, and Michel C.\nFrank for their wholehearted support and feedback through-\nout the project!\nReferences\n[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin\nGe, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun\nTang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhao-\nhai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu,\nYiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng,\nHang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin.\nQwen2.5-VL Technical Report, 2025.\narXiv:2502.13923\n[cs]. 2\n[2] Palanikumar Balasundaram and Indirapriya Darshini Avu-\nlakunta. Bayley Scales Of Infant and Toddler Development.\nIn StatPearls. StatPearls Publishing, Treasure Island (FL),\n2025. 3\n[3] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Alt-\nman, Simran Arora, Sydney von Arx, Michael S. Bernstein,\nJeannette Bohg, Antoine Bosselut, and Emma Brunskill et\nal. On the Opportunities and Risks of Foundation Models,\n2022. arXiv:2108.07258 [cs]. 2\n[4] Mathilde Caron, Alireza Fathi, Cordelia Schmid, and Ahmet\nIscen.\nWeb-scale visual entity recognition: an llm-driven\ndata approach.\nIn Proceedings of the 38th International\nConference on Neural Information Processing Systems, Red\nHook, NY, USA, 2024. Curran Associates Inc. 2\n[5] Fei-Long Chen, Du-Zhen Zhang, Ming-Lun Han, Xiu-Yi\nChen, Jing Shi, Shuang Xu, and Bo Xu. VLP: A Survey\non Vision-language Pre-training. Machine Intelligence Re-\nsearch, 20(1):38–56, 2023. 2\n[6] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice\nPasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blis-\ntein, Ori Ram, Dan Zhang, and Evan Rosen et al. Gemini\n2.5: Pushing the Frontier with Advanced Reasoning, Multi-\nmodality, Long Context, and Next Generation Agentic Capa-\nbilities, 2025. arXiv:2507.06261 [cs]. 2\n[7] Gil Diesendruck and Paul Bloom. How specific is the shape\nbias? Child Development, 74(1):168–178, 2003. 2\n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In International Conference on Learning Representa-\ntions, 2021. 3, 13\n[9] Ron Dumont, John O. Willis, Kathleen Viezel, and Jamie\nZibulsky. Mullen Scales of Early Learning, AGS Edition,\n1995. In Encyclopedia of Special Education. John Wiley &\nSons, Ltd, 2014. 3\n[10] Dumitru Erhan, Aaron Courville, Yoshua Bengio, and Pas-\ncal Vincent.\nWhy Does Unsupervised Pre-training Help\nDeep Learning?\nIn Proceedings of the Thirteenth Inter-\nnational Conference on Artificial Intelligence and Statistics,\npages 201–208. JMLR Workshop and Conference Proceed-\nings, 2010. ISSN: 1938-7228. 2\n[11] Richard Gershon, Miriam A. Novack, and Aaron J. Kaat.\nThe NIH Infant and Toddler Toolbox: A new standardized\ntool for assessing neurodevelopment in children ages 1–42\nmonths. Child Development, 95(6):2252–2254, 2024. 2, 15\n[12] Richard C. Gershon, Molly V. Wagster, Hugh C. Hendrie,\nNathan A. Fox, Karon F. Cook, and Cindy J. Nowinski. Nih\ntoolbox for assessment of neurological and behavioral func-\ntion. Neurology, 80(11_supplement_3):S2–S6, 2013. 15\n[13] Kristen\nGrauman,\nAndrew\nWestbury,\nEugene\nByrne,\nZachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson\nHamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d:\nAround the world in 3,000 hours of egocentric video.\nIn\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 18995–19012, 2022. 24\n[14] Kristen\nGrauman,\nAndrew\nWestbury,\nEugene\nByrne,\nZachary Chavis, Antonino Furnari, Rohit Girdhar, Jack-\nson Hamburger, Hao Jiang, Miao Liu, and Xingyu Liu et\nal. Ego4d: Around the world in 3,000 hours of egocentric\nvideo. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 18995–\n19012, 2022. 4\n[15] Y. Catherine Han, Courtney K. Blackwell, Elizabeth M.\nDworak, Rachel M. Flynn, Maxwell A. Mansolf, Miriam A.\nNovack, Sarah Pila, and Aaron J. Kaat. NIH Baby Toolbox®\nTechnical Manual. Northwestern University, Evanston, IL,\nversion 1.1 edition, 2025. 4, 15, 16\n[16] Y. Catherine Han, Elizabeth M. Dworak, Maxwell Mansolf,\nHubert Adam, Lihua Yao, Miriam A. Novack, Sarah Pila,\nRachel M. Flynn, Amanda M. Flagg, Vitali Ustsinovich, Kay\nSavio, Greg J. Byrne, Richard C. Gershon, and Aaron J.\nKaat.\nNIH Baby Toolbox® methodology and norms de-\nvelopment. Infant Behavior and Development, 80:102117,\n2025. 2, 4\n[17] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de\nLas Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan\nClark, Tom Hennigan, Eric Noland, Katie Millican, George\nvan den Driessche, Bogdan Damoc, Aurelia Guy, Simon\n9"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 10, "text": "Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol\nVinyals, and Laurent Sifre. Training compute-optimal large\nlanguage models, 2022. 25\n[18] Michael Y. Hu, Aaron Mueller, Candace Ross, Adina\nWilliams, Tal Linzen, Chengxu Zhuang, Ryan Cotterell,\nLeshem Choshen, Alex Warstadt, and Ethan Gotlieb Wilcox.\nFindings of the Second BabyLM Challenge:\nSample-\nEfficient Pretraining on Developmentally Plausible Corpora,\n2024. arXiv:2412.05149 [cs] version: 1. 1\n[19] Philip A. Huebner, Elior Sulem, Fisher Cynthia, and Dan\nRoth. BabyBERTa: Learning More Grammar With Small-\nScale Child-Directed Language. In Proceedings of the 25th\nConference on Computational Natural Language Learning,\npages 624–646, Online, 2021. Association for Computa-\ntional Linguistics. 2, 3\n[20] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom\nDuerig. Scaling Up Visual and Vision-Language Representa-\ntion Learning With Noisy Text Supervision. In Proceedings\nof the 38th International Conference on Machine Learning,\npages 4904–4916. PMLR, 2021. ISSN: 2640-3498. 2\n[21] Guangyuan Jiang, Manjie Xu, Shiji Xin, Wei Liang, Yujia\nPeng, Chi Zhang, and Yixin Zhu. MEWL: Few-shot mul-\ntimodal word learning with referential uncertainty. In Pro-\nceedings of the 40th International Conference on Machine\nLearning, pages 15144–15169. PMLR, 2023. ISSN: 2640-\n3498. 2, 3\n[22] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for\nneural language models, 2020. 25\n[23] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. Scaling Laws for\nNeural Language Models, 2020. arXiv:2001.08361 [cs]. 1\n[24] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and\nRoss Girshick. Segment anything. In 2023 IEEE/CVF In-\nternational Conference on Computer Vision (ICCV), pages\n3992–4003, 2023. 2\n[25] Eliza Kosoy, Emily Rose Reagan, Leslie Lai, Alison Gop-\nnik, and Danielle Krettek Cobb.\nComparing machines\nand children: Using developmental psychology experiments\nto assess the strengths and weaknesses of LaMDA re-\nsponses.\nSSRN Electronic Journal, 2024.\nAvailable at\nSSRN: https://ssrn.com/abstract=4696693 or\nhttp://dx.doi.org/10.2139/ssrn.4696693. 2,\n3\n[26] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBLIP: Bootstrapping Language-Image Pre-training for Uni-\nfied Vision-Language Understanding and Generation.\nIn\nProceedings of the 39th International Conference on Ma-\nchine Learning, pages 12888–12900. PMLR, 2022. ISSN:\n2640-3498. 2\n[27] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBLIP-2: Bootstrapping Language-Image Pre-training with\nFrozen Image Encoders and Large Language Models.\nIn\nProceedings of the 40th International Conference on Ma-\nchine Learning, pages 19730–19742. PMLR, 2023. ISSN:\n2640-3498. 2\n[28] Songtao Li and Hao Tang. Multimodal Alignment and Fu-\nsion: A Survey, 2025. arXiv:2411.17040 [cs] version: 2. 2\n[29] Yijiang Li, Qingying Gao, Tianwei Zhao, Bingyang Wang,\nHaoran Sun, Haiyun Lyu, Robert D. Hawkins, Nuno Vas-\nconcelos, Tal Golan, Dezhi Luo, and Hokin Deng.\nCore\nknowledge deficits in multi-modal language models.\nIn\nForty-second International Conference on Machine Learn-\ning, 2025. 2, 3\n[30] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. In Proceedings of the 37th Inter-\nnational Conference on Neural Information Processing Sys-\ntems, Red Hook, NY, USA, 2023. Curran Associates Inc. 3,\n13\n[31] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang,\nHang Su, Jun Zhu, and Lei Zhang. Grounding DINO: Mar-\nrying DINO with Grounded Pre-training for Open-Set Ob-\nject Detection. In Computer Vision – ECCV 2024: 18th Eu-\nropean Conference, Milan, Italy, September 29–October 4,\n2024, Proceedings, Part XLVII, pages 38–55, Berlin, Hei-\ndelberg, 2024. Springer-Verlag. 4, 14\n[32] Bria Long, Robert Z. Sparks, Violet Xiang, Stefan Sto-\njanov, Zi Yin, Grace E. Keene, Alvin W. M. Tan, Steven Y.\nFeng, Chengxu Zhuang, Virginia A. Marchman, Daniel L. K.\nYamins, and Michael C. Frank.\nThe BabyView dataset:\nHigh-resolution egocentric videos of infants’ and young chil-\ndren’s everyday experiences, 2025. arXiv:2406.10447 [cs].\n3\n[33] Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Ming Yan, Ji Zhang,\nand Rongrong Ji.\nX-clip: End-to-end multi-grained con-\ntrastive learning for video-text retrieval. In Proceedings of\nthe 30th ACM International Conference on Multimedia, page\n638–647, New York, NY, USA, 2022. Association for Com-\nputing Machinery. 3\n[34] Maya Malaviya,\nIlia Sucholutsky,\nKerem Oktar,\nand\n{Thomas L.} Griffiths. Can humans do less-than-one-shot\nlearning? pages 997–1003, 2022. Publisher Copyright: ©\n2022 The Author(s). This work is licensed under a Creative\nCommons Attribution 4.0 International License (CC BY);\n44th Annual Meeting of the Cognitive Science Society: Cog-\nnitive Diversity, CogSci 2022 ; Conference date: 27-07-2022\nThrough 30-07-2022. 2\n[35] Virginia A. Marchman and Philip S. Dale. The MacArthur-\nBates Communicative Development Inventories:\nupdates\nfrom the CDI Advisory Board. Frontiers in Psychology, 14,\n2023. Publisher: Frontiers. 4, 14, 15\n[36] Microsoft. Azure AI Speech | Microsoft Azure. [Accessed\n13-11-2025] https://azure.microsoft.com/en-\nus/products/ai-services/ai-speech. 3\n[37] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,\nMakarand\nTapaswi,\nIvan\nLaptev,\nand\nJosef\nSivic.\nHowTo100M:\nLearning\na\nText-Video\nEmbedding\nby\nWatching Hundred Million Narrated Video Clips. In 2019\n10"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 11, "text": "IEEE/CVF International Conference on Computer Vision\n(ICCV), pages 2630–2640, 2019. ISSN: 2380-7504. 2\n[38] National Heart, Lung, and Blood Institute. How Sleep Works\n- How Much Sleep Is Enough? NHLBI, National Institutes\nof Health (NIH) website, 2022. 2, 3\n[39] Jean. Newborg and Riverside Publishing Company. Battelle\ndevelopmental inventory.\nBDI-2, 2005.\nEdition: 2nd ed.\nPlace: Itasca, Ill Publisher: Riverside Pub. 3\n[40] OpenAI, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam\nPerelman, Aditya Ramesh, Aidan Clark, A. J. Ostrow, Akila\nWelihinda, Alan Hayes, and Alec Radford et al. GPT-4o\nSystem Card, 2024. arXiv:2410.21276 [cs]. 2\n[41] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mah-\nmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell\nHowes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael\nRabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Je-\ngou, Julien Mairal, Patrick Labatut, Armand Joulin, and Pi-\notr Bojanowski. DINOv2: Learning Robust Visual Features\nwithout Supervision, 2024. arXiv:2304.07193 [cs]. 13\n[42] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. Improving Language Understanding by Genera-\ntive Pre-Training. . 13\n[43] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language Models are Unsuper-\nvised Multitask Learners. . 13\n[44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. Learning Transferable Visual\nModels From Natural Language Supervision. In Proceedings\nof the 38th International Conference on Machine Learning,\npages 8748–8763. PMLR, 2021. ISSN: 2640-3498. 2, 3, 15\n[45] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang\nHu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman\nRädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junt-\ning Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-\nYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feicht-\nenhofer. SAM 2: Segment anything in images and videos. In\nThe Thirteenth International Conference on Learning Rep-\nresentations, 2025. 2\n[46] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, Patrick Schramowski, Srivatsa Kundurthy, Katherine\nCrowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia\nJitsev. LAION-5B: an open large-scale dataset for training\nnext generation image-text models. In Proceedings of the\n36th International Conference on Neural Information Pro-\ncessing Systems, pages 25278–25294, Red Hook, NY, USA,\n2022. Curran Associates Inc. 2\n[47] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neu-\nral machine translation of rare words with subword units.\nIn Proceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Papers),\npages 1715–1725, Berlin, Germany, 2016. Association for\nComputational Linguistics. 13\n[48] Saber Sheybani, Sahaj Singh Maini, Aravind Dendukuri, Zo-\nran Tiganj, and Linda B. Smith. ModelVsBaby: a Develop-\nmentally Motivated Benchmark of Out-of-Distribution Ob-\nject Recognition, 2024. 2, 3\n[49] Melissa Kline Struhl, Laura Schulz, and Mark Sheskin et al.\nChildren Helping Science. [Accessed 13-11-2025] https:\n//childrenhelpingscience.com/. 7, 24\n[50] Jessica Sullivan, Michelle Mei, Andrew Perfors, Erica Woj-\ncik, and Michael C. Frank. SAYCam: A Large, Longitudinal\nAudiovisual Dataset Recorded From the Infant’s Perspective.\nOpen Mind, 5:20–29, 2021. 2, 3, 4\n[51] Yuqi Sun, Weimin Tan, Zhuoyao Gu, Ruian He, Siyuan\nChen, Miao Pang, and Bo Yan. A data-efficient strategy for\nbuilding high-performing medical foundation models. Na-\nture Biomedical Engineering, 9(4):539–551, 2025.\nPub-\nlisher: Nature Publishing Group. 2\n[52] Alvin Wei Ming Tan, Chunhua Yu, Bria Lorelle Long, Wan-\njing Anya Ma, Tonya Murray, Rebecca D. Silverman, Ja-\nson D Yeatman, and Michael Frank. Devbench: A multi-\nmodal developmental benchmark for language learning. In\nThe Thirty-eight Conference on Neural Information Process-\ning Systems Datasets and Benchmarks Track, 2024. 2, 3\n[53] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste\nRozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien\nRodriguez, Armand Joulin, Edouard Grave, and Guillaume\nLample. LLaMA: Open and Efficient Foundation Language\nModels, 2023. arXiv:2302.13971 [cs]. 3, 13\n[54] Wai Keen Vong, Wentao Wang, A. Emin Orhan, and Bren-\nden M. Lake. Grounded language acquisition through the\neyes and ears of a single child. Science (New York, N.Y.),\n383(6682):504–511, 2024. 2, 3\n[55] Ao Wang, Lihao Liu, Hui Chen, Zijia Lin, Jungong Han, and\nGuiguang Ding. Yoloe: Real-time seeing anything, 2025. 20\n[56] Shengao Wang, Arjun Chandra, Aoming Liu, Venkatesh\nSaligrama, and Boqing Gong. Babyvlm: Data-efficient pre-\ntraining of vlms inspired by infant learning. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), pages 1380–1390, 2025. 2, 3, 13\n[57] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang,\nand Ming Zhou. Minilm: deep self-attention distillation for\ntask-agnostic compression of pre-trained transformers.\nIn\nProceedings of the 34th International Conference on Neural\nInformation Processing Systems, Red Hook, NY, USA, 2020.\nCurran Associates Inc. 15\n[58] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu,\nXin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui\nWang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, and\nYu Qiao. Internvid: A large-scale video-text dataset for mul-\ntimodal understanding and generation. In The Twelfth Inter-\nnational Conference on Learning Representations, 2024. 2\n[59] Alex Warstadt, Aaron Mueller, Leshem Choshen, Ethan\nWilcox, Chengxu Zhuang, Juan Ciro, Rafael Mosquera,\nBhargavi Paranjabe, Adina Williams, Tal Linzen, and Ryan\nCotterell.\nFindings of the BabyLM Challenge: Sample-\nEfficient Pretraining on Developmentally Plausible Corpora.\nIn Proceedings of the BabyLM Challenge at the 27th Confer-\nence on Computational Natural Language Learning, pages\n11"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 12, "text": "1–34, Singapore, 2023. Association for Computational Lin-\nguistics. 1, 2\n[60] Luca Weihs, Amanda Yuile, Renée Baillargeon, Cynthia\nFisher, Gary Marcus, Roozbeh Mottaghi, and Aniruddha\nKembhavi.\nBenchmarking Progress to Infant-Level Phys-\nical Reasoning in AI. Transactions on Machine Learning\nResearch, 2022. 2, 3\n[61] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1492–1500,\n2017. 13\n[62] Eunice Yiu, Maan Qraitem, Anisa Noor Majhi, Charlie\nWong, Yutong Bai, Shiry Ginosar, Alison Gopnik, and Kate\nSaenko.\nKiVA: Kid-inspired visual analogies for testing\nlarge multimodal models. In The Thirteenth International\nConference on Learning Representations, 2025. 2\n[63] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu.\nTinyLlama: An Open-Source Small Language Model, 2024.\narXiv:2401.02385 [cs]. 3, 13\n[64] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xi-\naofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang,\nFei Wu, and Guoyin Wang.\nInstruction Tuning for Large\nLanguage Models: A Survey, 2025. arXiv:2308.10792 [cs].\n2\n[65] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Fucheng\nWeng, Zehuan Yuan, Ping Luo, Wenyu Liu, and Xinggang\nWang. Bytetrack: Multi-object tracking by associating every\ndetection box. 2022. 20\n12"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 13, "text": "BabyVLM-V2: Toward Developmentally Grounded Pretraining and\nBenchmarking of Vision Foundation Models\nSupplementary Material\nA. Model training\nA.1. BabyLLaVA-V2 architecture\nWe build upon the original BabyLLaVA-Llama model in-\ntroduced in BabyVLM-V1 [56], by giving it the capabil-\nity to process multiple images as input and conduct multi-\nturn visual–linguistic interactions. The model architecture\nconsists of a compact language backbone, a visual encoder,\nand a lightweight multilayer perceptron (MLP) connector\nthat projects visual features into the language space. Un-\nlike BabyVLM-V1, which also experimented with smaller\nbackbones (GPT-2 [43] + ResNeXt-50 [61]), we only adopt\nthe larger variant composed of a LLaMA-1.1B [53, 63]\nlanguage model and a ViT-L-16 [8] visual encoder (300M\nparams). We find that the smaller variant often struggles to\ncomplete complex downstream tasks such as memory, pri-\nmarily due to its limited model capacity, whereas the larger\nconfiguration achieves a better balance between develop-\nmental plausibility and expressive capability.\nA.2. BabyLLaVA-V2 training paradigm\nWe train the entire model from scratch using a four-stage\npipeline, as summarized in Table 7.\nStage 0: Unimodal Training. In the first stage, the lan-\nguage and vision backbones are trained independently to\nacquire the basic representational abilities for each modal-\nity. The language backbone is trained on all transcribed\nutterances using a standard autoregressive loss [42].\nIts\ntokenizer is initialized via Byte-Pair Encoding (BPE) [47]\ntrained on the same corpus, with a fixed vocabulary size of\n6000. The vision backbone is trained using a DINOv2 [41]\nobjective on SAYCam frames. We do not apply any filtering\nduring this stage—except restricting samples to the training\nsplit—since the filtering procedures are primarily designed\nto enforce image–utterance alignment, which is irrelevant\nto unimodal representation learning.\nStage 1: Feature Alignment. This stage corresponds to\nPhase 1 training in LLaVA [30]. Both the vision and lan-\nguage backbones are frozen, and only the MLP connector\nis optimized using an autoregressive loss. The objective is\nto align visual features with the language embedding space,\neffectively bridging the two modalities. To maintain train-\ning stability, we use only the image–utterance subset of the\npretraining data in this stage, postponing exposure to multi-\nimage inputs until later phases.\n1The training dataset, the model checkpoints, the training scripts and\nthe evaluation samples will be released to the public in the near future.\nStage 2: Joint Pretraining. In this stage, the vision back-\nbone remains frozen, while the MLP connector and lan-\nguage backbone are trained jointly on the full mixed-format\npretraining dataset, as described in Section 3.1. This allows\nthe model to learn multimodal grounding over diverse input\nstructures.\nStage 3: Instruction Fine-tuning. Finally, we fine-tune the\nmodel using the mixed instruction dataset, which is a com-\nbination of all the instruction samples mentioned in Table 3.\nThis step enables the model to perform various downstream\ntasks through natural-language prompts. The vision back-\nbone, MLP connector and language backbone are all up-\ndated to learn instruction-following behavior and context-\ndependent reasoning. We apply two different learning rates\nfor different modules in this stage: the learning rate of the\nvision backbone is 1e-5, while that of the MLP connector\nand language backbone is 5e-5.\nMain hyperparameters of all 4 stages are summarized in\nTable 7. All experiments are conducted on four NVIDIA\nA6000 GPUs with 48 GB of VRAM each. Language back-\nbone training completes in less than one hour, while the vi-\nsion backbone completes in 4 days. Next, training the MLP\nconnector requires approximately five hours. Joint pretrain-\ning on the mixed-format dataset takes roughly 34 hours to\nconverge. Finally, instruction tuning takes 60 hours.\nA.3. Open-source model fine-tuning\nWe conduct LoRA finetuning experiments on two open-\nsource models, LLaVA-OneVision-7B and Qwen2.5-VL-\n7B, to evaluate the effectiveness of our instruction-\nfinetuning dataset. Each task is finetuned separately. We\nset the LoRA rank to 64, use a scaling factor of 64, and\napply a dropout rate of 0.05. Training is performed for 5\nepochs with a global batch size of 128, a learning rate of\n1e-4, a weight decay of 0.1, a warmup ratio of 0.03, and a\ncosine learning-rate schedule.\nB. Developmentally aligned benchmarks\nIn Appendix B, we adopt the following organization: Sub-\nsection B.1 describes general implementation details that\nare shared by several tasks, including details on the vocab-\nulary used in DevCV Toolbox, acquisition of SAYCam an-\nnotations, acquisition of Ego4d annotations, and important\ndistinction between SAYCam and Ego4d. Then, each sub-\nsection between 2 and 11 describes how these annotations\nare used to construct one task in DevCV Toolbox each, and\nare each broken up into Original Toolbox Task, Adaptation,\n13"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 14, "text": "Table 7. Training stage specification of BabyLLaVA-V2. Note that for stage 3, different modules have different learning rate, as\nmentioned in Section A.2.\nStage\nTrained modules\nFrozen modules\nDataset\nLoss\nLearning rate\nEpoch\nGlobal batch size\n0-language\nLanguage backbone\nN/A\n283k utterance only\nAutoregressive\n2e-4\n10\n16\n0-vision\nVision backbone\nN/A\n1085k image only\nDINOv2\n1e-4\n100\n64\n1\nMLP connector\nLanguage backbone\n+ vision backbone\n768k image-utterance\nAutoregressive\n3e-3\n5\n128\n2\nMLP connector\n+ language backbone\nVision backbone\n768k image-utterance\n+ 181k video-utterance\n+ 63k multi-turn\nAutoregressive\n2e-4\n5\n128\n3\nMLP connector\n+ language backbone\n+ vision backbone\nNone\n150k instruction finetune\nAutoregressive\n5e-5 | 1e-5\n5\n128\nData Collection, and Example Prompt. Some of these also\ninclude information on Evaluation or Data Composition.\nB.1. Data collection procedures common to all tasks\nVocabulary filtering\nTo ensure that all benchmarks in this work focus on\ndevelopmentally appropriate vocabulary, we draw on the\nMacArthur–Bates Communicative Development Invento-\nries (MAB–CDI): Words and Gestures [35]. The MAB–CDI\nis a standardized instrument assessing early vocabulary\ncomprehension and production in infants and toddlers, cov-\nering familiar words across core semantic categories (e.g.,\nanimals, foods, body parts, actions).\nBecause it is widely regarded as a gold-standard\nreference for early lexical development, we restrict our\nbenchmark vocabulary to words that appear in—or are\nclosely aligned with—those in the MAB–CDI. Accord-\ningly, during visual concept mining from SAYCam and\nEgo4D, we retain only crops whose labels fall within this\ndevelopmentally grounded lexical domain, ensuring that\nevery keyword used across tasks reflects concepts young\nchildren could plausibly understand.\nSAYCam annotations\nTo support all SAYCam-based benchmarks in this work,\nwe build the following unified preprocessing pipeline that\nextracts high-quality image crops for every object and\naction concept appearing in the corpus.\nThis pipeline\nis reused (with task-specific modifications described in\nthe corresponding benchmark sections) across tasks and\nprovides consistent visual grounding for all downstream\ndatasets.\n• Frame-level detection and indexing: We first sample\nSAYCam videos at 1 FPS and run an open-vocabulary de-\ntector (Grounding–DINO [31]) using the GPT-annotated\nlabels associated with each frame as the open set. Let\nS denote the set of all such SAYCam labels. For each\nlabel s ∈S, we construct an index Index(s) that maps\ns to all frames in which it is detected, together with its\nproportionally buffered bounding boxes and GPT-derived\nblurriness scores. This Index(s) structure serves as the\nmaster lookup table for retrieving visual instances of any\nconcept.\n• Normalizing label variants: Raw SAYCam labels s ∈S\noften include plural forms, paraphrases, or compositional\ndescriptions. To ensure consistent visual grounding, we\ncluster lexically or semantically equivalent labels into\nsmall groups based on lexical similarity, plural equiva-\nlence, and phrase containment heuristics. Each label s is\nassigned to its cluster M(s). This allows us to treat vari-\nants of s such as “shoes”, “a shoe”, or “pair of shoes” as\na single underlying concept by retrieving visual instances\nfrom {Index(s′)|s′ ∈M(s)}.\n• Quality filtering: Because SAYCam contains natural-\nistic video frames from children’s head-cam footage,\nmany detections are of low-quality due to motion\nblur, wrong/irrelevant detector predictions, small/partial\nbounding boxes. Therefore, we score each detection re-\nsult using four broad signals: (1) detector confidence, (2)\nCLIP image–text alignment, (3) crop size, and (4) spatial\nclarity (e.g., centeredness). These signals are normalized\nper-concept and combined into a single quality measure.\nWe also employ additional light-weight adjustments to\nensure that within M(s), rare labels are not overwhelmed\nby frequent ones and that exact label matches are pre-\nferred over looser variants.\n• Ensuring lexical and visual diversity: To avoid select-\ning many near-duplicate frames of the same scene, we\napply simple diversity controls. We first ensure that dif-\nferent lexical variants of a concept are represented, and\nthen enforce a minimal temporal spacing between cho-\nsen frames. From this diversified pool, we keep only a\nsmall number (≤10) of final crops per concept, prioritiz-\ning clarity and representativeness.\n• Final output: The result is a compact, high-quality set\nof image crops for every object or action concept in SAY-\nCam. These curated crops act as the visual foundation for\nthe majority of benchmarks built from SAYCam in this\npaper. They guarantee concept fidelity, diversity of visual\n14"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 15, "text": "Table 8. Comparison between SAYCam and Ego4d. Object\nSize is reported in terms of the average % of the frame’s area filled.\nData Source\nparticipants\nNumber of Pixels\nObject Size\nSAYCam\ninfants\n307k (fixed)\n57%\nEgo4d\nadults\nover 2M (average)\n4%\ncontexts, and consistent quality standards across tasks.\nEgo4D Annotations\nFor Ego4D, we do not perform any heavy data clean-\ning or processing due to the native, high-quality annota-\ntions of the dataset.\nFor most of our benchmarks, we\nuse image data from the egotracks split, which con-\ntains densely annotated egocentric video tracks. In addi-\ntion, Picture Vocabulary (see Section B.2) also draws image\ncrops from fho_lta—a subset of Ego4D focused on fu-\nture hand–object interactions—providing additional object-\ncentric visual diversity.\nOverall differences between SAYCam and Ego4d\nHere, we analyze the differences between SAYCam and\nEgo4d which result in BabyLLaVA-V2’s very poor gener-\nalization to Ego4d. Specifically, SAYCam was filmed by 3\nbabies across 4 homes, while Ego4d was filmed by 923 par-\nticipants across 74 sites. There’s also a significant domain\nshift in the size of the frames and the sizes of the objects\nrelative to the frames. See Table 8 for a summary.\nIn addition, although all of the Ego4d examples con-\nstructed in DevCV Toolbox are directly based on objects\nlisted in SAYCam’s vocabulary, their backgrounds may still\ninclude objects that BabyLLaVA-V2 never saw in its train-\ning, and thus detract from its’ overall understanding of the\nscene. For example, we might construct an example from\nEgo4d that asks about the location of a hand, and although\nBabyLLaVA-V2 saw examples of hand during training,\nthe frame is full of other objects to which BabyLLaVA-V2\ncan attribute no meaning.\nIn such a case, context clues\nlearned by BabyLLaVA-V2 about where gloves are usually\nfound relative to their scene, such as at the end of an arm\nor holding onto a known object, are lost, and performance\ndrops correspondingly. Further, we conjecture that this lack\nof generalization stems from not only the explicit action\ncategories included in Ego4d that a baby would never have\nseen (like fixing a car or performing a laboratory exper-\niment), but also from the inherently wider field of view\ncaptured adult demonstrators relative to babies.\nFurther,\nwe argue that even if Ego4d had been filmed of the same\nlocations and actions as SAYCam, we would still observe\na domain shift caused solely because the demonstrators\nare adults, perceiving the world from a higher point of\nview than babies. This point reinforces the uniqueness of\nthe baby domain in the space of egocentric computer vision.\nB.2. Picture Vocabulary\nOriginal Toolbox Task\nOur\ntask\nis\ndirectly\nadapted\nfrom\nthe\nNIH\nBaby\nToolbox® Picture Vocabulary Test (PVT), which evaluates\na participant’s receptive vocabulary by presenting a spoken\ntarget word alongside four images (one correct, three dis-\ntractors) [11]. The goal is to touch the picture matching\nthe target word. Distractors in the original PVT are de-\nsigned to be plausible but incorrect, typically encompassing\ncoarse-categorical, fine-categorical, or phonological simi-\nlarity.\nWhile the full PVT, taken directly from the NIH\nToolbox® [12, 15], includes 373 examples, we identify 52\nexamples intended for early childhood receptive vocabulary\nevaluation through combining all-MiniLM-L6-v2 em-\nbedding similarity [57] comparison to vocabulary in [35]\nand manual inspection.\nWhile the Baby Toolbox PVT uses an IRT-based\ncomputer-adaptive score that converts response patterns\ninto age-normed ability estimates [15], our adaptation sim-\nplifies this to straightforward 4-way accuracy since all items\nin the benchmark are evaluated rather than adaptively se-\nlected.\nOur adaptation preserves the original developmental\nintent while replacing controlled illustrations with natural-\nistic egocentric visual inputs (SAYCam/Ego4D), providing\na grounded benchmark for modeling baby-level vocabulary\ncomprehension in realistic developmental environments.\nAdaptation\nTo adapt the original PVT design to naturalistic corpora,\nwe first map MAB-CDI words r ∈R to corpus vocab-\nularies S: GPT-annotated labels for SAYCam and native\nobjects/actions labels for Ego4D. This produces a set of vi-\nsually grounded targets Gr ⊂S for each CDI anchor r,\nforming a one-to-many mapping r→Gr.\nWe then analyze the 52 baby-level NIH PVT items to\nquantify the original distractor structure. We define three\ncategories: fine-categorical, coarse-categorical, and phono-\nlogical. We manually annotate every NIH distractor to one\nor more of these types accordingly. Note that the original\nPVT includes unrelated distractors and we exclude those\ngiven the difficulty in controlling the quality of unrelated\ndistractors in naturalistic imagery. We obtain the unnormal-\nized distractor-type weights:\nwcoarse = 0.5643,\nwfine = 0.1472,\nwphon = 0.0321.\nUsing these proportions, we construct corpus-specific dis-\ntractor pools from the entire corpus S (because the model\nis only required to identify the correct target concept, not to\ncorrectly recognize or label the distractors):\n• Fine-categorical: We use similarity scoring based on\nCLIP text embeddings [44]. For SAYCam, candidates\n15"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 16, "text": "above a similarity threshold of 0.7 is considered belong-\ning to the same fine-grained category while for Ego4D\nwe use a quantile band [0.997, 0.99973] to also filter out\noverly similar and thus indistinguishable words.\n• Coarse-categorical: We use Kmeans clustering based on\nCLIP text embeddings (SAYCam: K = 100; Ego4D:\nK = 150).\n• Phonological: We use Soundex-based string similarity\nfor both datasets.\nFor each CDI anchor r, we select a ground-truth label\ng ∈Gr and sample three distinct distractors from these\npools using the weights.\nFor SAYCam examples, we\nperform a final round of manual screening to filter out the\ninfeasible examples, while Ego4D examples are filtered\nwith a hybrid procedure combining Gemini2.5-flash\nchecks with a lightweight manual review.\nData Collection\nTo produce high-quality 4-way visual choices, we collect\nimage crops corresponding to every target and distractor la-\nbel.\nFor SAYCam, because Picture Vocabulary requires ex-\ntremely precise, semantically clear images, we modify the\nfully automated pipeline in Section B.1 with the following\nchanges:\n1. Candidates come directly from Index(g) where g ∈Gr\ngiven anchor r.\n2. Human annotators manually filter irrelevant, ambiguous,\nor blurry crops and refine bounding boxes, replacing au-\ntomated quality scores.\nThis process yields a compact, high-precision crop inven-\ntory used for all SAYCam examples.\nFor Ego4D, for the objects, we use the bounding boxes\nfrom the visual_crop field of the EgoTracks bench-\nmark, applying a deterministic buffer (1.2× + 8px mar-\ngin) and requiring a post-buffer normalized area > 0.03.\nFor actions, we use the fho_lta benchmark which con-\ntains abundant action annotations. As there are no explicit\nbounding box annotations, we sample frames from the mid-\ndle 25% of each action frame interval and apply minimal\ncenter-biased cropping to maintain clarity. For each label,\nwe keep 10 candidates while preserving diversity and vi-\nsual fidelity, and we apply a Gemini2.5-flash pass to\neliminate unusable crops.\nDataset composition.\nAs shown in Figure 7, We obtain\n1181 SAYCam examples, covering 344 unique GT labels,\n1311 unique distrator labels, and 1660 unique crops. Due\nto manual filtering, its distractor distribution only loosely\nfollows NIH proportions (shown in Figure 8). Similarly,\nwe obtain 346 Ego4D examples over 124 unique GT labels,\n343 unique distractor labels, and 633 unique images (shown\nin Figure 7) with the corresponding distractor distribution\nFigure 7.\nLabel/Image crop uniqueness comparison between\nPicture Vocabulary Test in NIH Baby Toolbox®, SAYCam, and\nEgo4D.\nFigure 8. Distractor type composition for the Picture Vocabulary\nTest in NIH Baby Toolbox®, SAYCam, and Ego4D. The origi-\nnal PVT contains multi-type overlaps, while our sampling assigns\neach distractor a single type even though some satisfy multiple\ncues. Ego4D uses unrelated distractors only as a rare fallback.\nshown in Figure 8.\nExample Prompt\nEach finalized example is a prompt embedded with 4 image\nchoices for which the following is an example:\n\" Touch the image of ’foot’ (A)\n<image> (B) <image> (C) <image>\n(D) <image> \"\nThe model needs to output one of A, B, C, or D to be\nevaluated.\nB.3. Looking While Listening\nOriginal Toolbox Task\nThe Looking while listening test (LwL) from NIH Baby\nToolbox®aims to evaluate comprehension for object label-\ning and receptive language [15]. The infant is shown two\nclipart images which is followed by an audio prompt de-\nscribing one of them. Eye tracking is used to detect whether\nthe participant is looking at the ground-truth image. Similar\nto PVT, we simplify the original metric to accuracy only.\nAdaptation\nTo adapt LwL to our benchmark in SAYCam, We replace\nclipart with naturalistic image crops from SAYCam, and\neye tracking with multiple choice, similar to Picture Vocab-\nulary.\n16"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 17, "text": "Data collection\nExamples for Looking While Listening are taken directly\nfrom Picture Vocabulary examples.\nExample Prompt\nEach finalized example is a prompt embedded with 2 image\nchoices for which the following is an example:\n\" Touch the image of ’foot’\n(A) <image> (B) <image>\"\nThe model needs to output one of A or B to be evaluated.\nB.4. Localization\nOriginal Toolbox Task\nMuch like Picture Vocabulary,\nthe Mullen Receptive\nLanguage test #19 tests infants on their ability to point at\nsketched target objects as they are named, avoiding con-\nfusing them with the distractor objects. Specifically, after\ngesturing to a group of sketched objects, the psychologist\nasks: Look at these. Where is the cat? If the child points in\nthe direction of the cat, they pass the test.\nAdaptation\nLocalization makes a significant modification to the origi-\nnal NIH Baby Toolbox® measure- In DevCV Toolbox, we\nfind it meaningful to test pointing to objects in their nat-\nuralistic environments, namely, we treat the objects natu-\nrally occurring in the background of the frame as distrac-\ntors rather than inserting unrelated objects. Additionally,\nbecause if is infeasible to ask a model to ’point’, the answer\nchoices are always top left, top right, bottom left, bottom\nright.\nAgain, the objects in this task are real objects from\nSAYCam and Ego4d rather than the sketches used in\nthe NIH Baby Toolbox®, and just like in the NIH Baby\nToolbox®, the prompt is the full frame and the name of the\nobject to be localized.\nData collection\nThe examples for both SAYCam and Ego4d are generated\nusing the centers of the bounding boxes annotated in B.1\nand Ego4d’s egotracks, respectively.\nTo avoid including test examples where a bounding box\nstretches across two answers ambiguously (for example,\nan object in the bottom middle that could reasonably be\ncalled either bottom left or bottom right), we 1) crop each\nframe so that its closest corner is flush with the edges of\nthe object’s bounding box, and 2) enforce a maximum\nbounding box area of 1/4 of the frame’s area (see Figure 9),\nwhich filters out 5.2k of the 7.3k possible test examples.\nIn practice, we find that both of these steps are needed to\nensure fair, reasonably unambiguous examples. We enforce\nno minimum confidence in the SAYCam object annotations\nFigure 9\nand use all object names generated in B.1.\nExample Prompt\nEach finalized example is a prompt embedded with one im-\nage and the same four choices, for which the following is\nan example:\n\"<image>\nPoint at the cup.\nIs it in (A)\nthe top left of the image, (B)\nthe top right, (C) the bottom\nleft, or (D) the bottom right?\"\nThe model needs to output one of top left, top\nright, bottom left, or bottom right to be evalu-\nated.\nB.5. Left/Right\nOriginal Toolbox Task\nLeft/Right is adapted directly from Mullen Visual Recep-\ntion test #29, in which a psychologist shows a child an ob-\nject, then instructs the child to match it with the identical\none. If the child correctly points to the identical object,\n17"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 18, "text": "Figure 10\navoiding confusing it with its own mirror image, they pass\nthe test.\nAdaptation\nThe only modification made while adapting VR test #29 to\nDevCV Toolbox is replacing the clipart objects with real\nobjects from SAYCam and Ego4d. In DevCV Toolbox, the\nbasic format is preserved: a prompt image, followed by a\ncorrect answer and two distractor choices in some random\norder, are presented to the model. The target image is a\nduplicate of the prompt, and the incorrect answers are the\nmirror image of the target.\nSome examples in Left/Right are harder than others; we\nconjecture that difficulty in this task can result from either\n1) naturally symmetric objects, or 2) low resolution objects\n(see Figure 10). Naturally symmetric objects are difficult\nbecause they require an encoding of fine-grained details.\nHowever, low resolution objects are difficult because even\nthough there might be some spatial clues to discriminate\nthe target from its mirror image, if have models can’t\nascribe any semantic meaning to the image, they won’t\nencode any semantic meaning to its details. By filtering out\nsmall bounding boxes, we aim to remove the examples that\nare difficult solely due to low resolution.\nData collection\nFor the SAYCam variant, use the object names and bound-\ning boxes generated in B.1. We enforce no minimum or\nmaximum object size, and for the val and test splits, we en-\nforce a minimum confidence in the bounding box of .85. In\nboth variants, all object crops are zero-padded to (640, 480).\nFor the Ego4d variant, we use object names and bound-\ning boxes from the published Ego4d egotracks annotations\nand include only objects that belong to the vocabulary\ndefined in B.1. To remove examples with poor resolution,\nwe require either a minimum bounding box height or width\nof one fifth of the frame, which filters out about half of the\notherwise qualifying examples.\nExample Prompt\nEach finalized example is a prompt embedded with 1 image\nprompt and 3 image choices for which the following is an\nexample:\n\"<image>\nWhich of the following is the\nsame as this?\n(A) <image> (B)\n<image>, or (C) <image>?\"\nThe model needs to output one of A, B, or C to be evaluated.\nB.6. Spatial Details\nOriginal Toolbox Task\nSimilarly, Mullen Visual Reception test #25 also tests\nunderstanding of details in images. In this test, the child\nis presented with a sketch of a tulip, and the psychologist\nasks: See this flower. Find one just like this. Look for it\nhere, while tracing their finger along a page filled with\nsketches of a tulip, a sunflower, a clover, and a daisy. The\nchild is allowed to refer back to the tulip while choosing\ntheir answer. If the child points to the tulip, they pass the\ntest.\nAdaptation\nAgain, the objects in our benchmark are real, cropped ob-\njects from SAYCam and Ego4d rather than clipart, and they\ncome from more categories than just flower.\nAddition-\nally, because the models cannot \"point\" to the choices, the\nchoices are passed as separate images and the correct an-\nswer is the index (A, B, or C) of the matching image.\nOur final modification to the original measure is that\nto make it more difficult for a computer, we present the\nanswer choices in their naturalistic backgrounds rather than\ncropped as in the NIH Baby Toolbox®. In practice, we find\nthe final modification necessary to make Spatial Details\nrequire a fine-grained understanding of detail, as matching\nidentical images is trivial even for a small vision model.\nData collection\nTo construct examples from both SAYCam and Ego4d, we\nmatch objects with the label, but require that they come\nfrom different videos. The labels for each come from B.1\nand egotracks, respectively. Note that the same object can\nappear multiple times within an example- for instance, the\nsame chair, captured in two separate videos, can show up\nas two of the choices. In such cases, the model is forced to\nrely on spatial details such as orientation, perspective, and\nlighting, to match identical occurrences.\nTo ensure quality, we enforce a minimum object con-\nfidence of .92 in the SAYCam annotations.\nTo increase\ndifficulty, we also require that objects have an area of less\nthan half of the frame’s area.\n18"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 19, "text": "Figure 11. Comparison between sources of occlusion. Left: object\nocclusion from a static camera and moving object. Right: object\nocclusion from a moving camera and static object. Each panel\nshows a top-down view of the scene along with the corresponding\nprojected 2D video depicting the occlusion event.\nExample Prompt\nEach finalized example is a prompt embedded with 1 image\nprompt and 3 image choices for which the following is an\nexample:\n\"<image>\nWhich of the following is the\nsame as this?\n(A) <image> (B)\n<image>, or (C) <image>?\"\nThe model needs to output one of A, B, or C to be evaluated.\nB.7. Visual Delayed Response\nOriginal Toolbox Task\nInspired by Visual Delayed Response in the NIH Baby\nToolbox, we introduce an evaluation task designed to\nassess the spatiotemporal reasoning capabilities of vision-\nlanguage models. More specifically, our task focuses on\nobject tracking and spatial localization over time, requiring\nmodels to process multi-frame/video input to infer spatial\ntrajectory and disappearance of a designated object.\nAdaptation\nIn the original NIH Baby Toolbox task, a cartoon creature\nis placed in a frame with grey walls to its left and right. The\ncreature moves from the center of the frame to behind either\nwall, and the child must identify which wall the creature hid\nbehind. (See Figure 12)\nTranslating this task to real-world videos is challenging,\nas the synthetic examples from the toolbox portray an un-\nrealistically ideal scenario. Each toolbox example depicts\na moving object observed from a static camera perspective,\nwith simplified backgrounds and perfectly smooth motion\ntrajectories. Such controlled scenarios are rare in real-world\nfootage, especially in egocentric videos captured from a\ntoddler’s perspective.\nTo address this challenge we exploit the frequent head\nmovements captured in SAYCam footage, together with the\nFigure 12. Example of Visual Delayed Response task, taken di-\nrectly from the NIH Baby Toolbox.\nfact that many objects in real-world scenes are largely sta-\ntionary. By inverting the source of 2D object motion from a\nstatic camera with moving objects to a moving camera with\nstationary objects (see Figure 11), we are able to expand the\ndataset by over an order of magnitude.\nFormally,\nthe model is provided a video V\n=\n{f1, f2, ...fT } and designated key object k. The video de-\npicts the key object k moving within the field of view and\neventually exiting the visible frame at time t∗≤T. The\nmodel’s objective is to predict the exit region r ∈R, where\nR denotes the set of possible frame boundaries through\nwhich the object may leave.\nWe define two variants of this task, which differ in the\nset of selectable exit regions R provided to the model:\n• Multi-choice setting: Rm = {left, right, top, bottom,\ntop-left, top-right, bottom-left, bottom-right}\n• Binary setting: Rb = {correct, opposite}, Rb ⊆Rm\nThe multi-choice variant provides a comprehensive set\nof possible exit regions, where the model is given eight re-\ngions as selectable options. The binary variant is a sim-\nplified version of the task, where the model only chooses\nbetween two options: the correct exit region or the region\ndirectly opposite to it.\nThe overall task can be summarized as a mapping\nfV DR(V, k) →r, where r ∈R. Here, fV DR represents\nthe function that, given a video V and designated key\nobject k, predicts the exit region r ∈R through which the\nobject leaves the frame.\nData Collection\nSAYCam. Collecting examples for the SAYCam variant\nof Visual Delayed Response can be split into 3 stages: fil-\n19"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 20, "text": "tering with GPT annotations, filtering with object tracking,\nand manual labeling.\nStage 1. We first use the 1 FPS annotations provided by\nGPT in B.1, where each frame is labeled with a \"key object\"\nand \"objects\" attribute. The \"key object\" denotes a singular\nobject being attended to in a particular frame (if any), and\n\"object\" denotes a list of all visible objects within the frame\nof view. We do an initial filtering for candidate clips by\nusing a sliding window over the 1 FPS frames of each long-\nrange video. For a clip to pass the filter, the first half of\nframes in the window must have the same \"key object\", k.\nIn addition, the second half of frames must not have k listed\nas a \"key object\" or be present in the \"objects\" list. From\nthe 422990 initial clips, 17443 are passed as candidate clips\nto the next stage.\nStage 2. We then perform open-set object detection [55]\nover the 1 FPS frames sampled from each candidate clip,\nwhere the only object class to be detected is the \"key object\"\nitself. An object tracking algorithm [65] is also used to track\nthe \"key object\" over the full fps video. The clips are filtered\naccording to the object tracks, where each track must satisfy\nall of the following:\n–\nStart within the middle 70% of the frame\n–\nAppear in at least 10 consecutive frames\n–\nDisappear for at least 10 consecutive frames before the\nfull clip ends\nTo help account for errors in the object detection/tracking,\nwe purposefully loosen the filters and add additional mea-\nsures for sporadic/false detections. From the 17443 initial\nclips, 3908 are passed as candidate clips to the next stage.\nStage 3. The final stage involves manually reviewing and\nhand-labeling each candidate example from the previous\nstage. We label not only for the ground truth exit direc-\ntion, but also for a variety of annotations related to overall\nquality of the clip. In total, we annotate for camera motion,\nscene visibility, camera stability, occlusion, exit direction,\nand presence of multiple objects. A breakdown for each is\nprovided as follows:\n–\nOcclusion: {Fully Occluded, Partially Occluded, Re-\nmains in View}\n–\nCamera Motion: {Static, Moving}\n–\nDirection of Exit: [Up, Down, Left, Right]\n–\nScene Visibility: {Excellent, Good, Fair, Poor}\n–\nCamera Stability: {Very Stable, Stable, Shaky, Very\nShaky}\n–\nMultiple Objects: {True, False}\nWe then filter for valid high-quality clips according to\nthe following criteria:\n–\nObject must become fully occluded\n–\nDirection of exit cannot be contradicting (both left &\nright, or both up & down)\n–\nScene visibility better than \"Poor\"\n–\nCamera stability better than \"Very Shaky\"\nFigure 13. Visualization of evaluation methods for Visual Delayed\nResponse task. Left: Binary evaluation for the binary setting,\nwhere there is only a correct and opposite incorrect option. Right:\nExact and Adjacent evaluation for the multi-region setting, where\nthe correct region for Exact is defined by only the green region,\nand the correct region for Adjacent is defined by both the green\nand yellow regions.\nFrom the 3908 initial clips, 2380 are passed as final clips\nfor the dataset.\nEgo4D. Data collection for Ego4D follows a very sim-\nilar structure to the SAYCam process, with the addition of\ntracked object annotations being already provided by the\nEgo4D dataset. We use a sliding window over each long-\nrange video’s object tracks and filter for all of the following:\n–\nObject is present in first half of window and disappears\nin second half\n–\nObject bounding box ≥40000 pixels ( 13% of screen)\n–\nStarts within the middle 50% frame\nEach clip is also manually reviewed/labelled according\nto the same procedure as SAYCam data collection\nMulti-frame versions.\nSince the average clip can\nrange from 100-150 frames, we manually create multi-\nframe counterparts to each example.\nMore specifically,\nwe look to obtain 1 representative object frame and 3-9\nlinearly sampled frames that best showcase the object\nmotion/disappearance in a given clip. To do this, we first\nfind the specific frame for three different fields: full object\nframe, start occlusion frame, and end occlusion frame. The\nfull object frame is always used as the first frame in the\nmulti-frame sequence, and shows the key object in clear\nview.\nThe start/end occlusion frames mark the interval\nwith which the key object becomes occluded. A random\nnumber of frames (3-9) are linearly sampled along this in-\nterval to complete the multi-frame sequence for a given clip.\nEvaluation\nEvaluation is performed over three separate variants: Ex-\nact and Adjacent in the multi-choice setting, and Binary\nin the binary setting (see Figure 13). Accuracy is used as\nthe metric for evaluation, defined as the fraction of predic-\ntions considered correct across all trials for a given variant.\nIn the multi-choice setting:\n20"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 21, "text": "• Exact: Only the labelled ground truth region is counted\nas correct.\n• Adjacent: Both the labelled ground truth region and its\ntwo adjacent regions are counted as correct. This helps\naccount for small ambiguities in the ground truth label.\nIn the binary setting:\n• Binary: A prediction is correct if it matches the \"correct\"\nregion rather than the \"opposite\" region.\nExample Prompt\nEach finalized example includes a series of <image> tags\nor singular <video> tag, followed by the prompt. To be\nproperly evaluated, the model must output exactly one op-\ntion from the choices given in the prompt.\nExample from binary setting with multi-frame input:\n\"<image><image><image><image>\ndoes the bottle leave the frame\nthrough the right side of the\nframe or the left side of\nthe frame?\nrespond ONLY with\n’right’ or ’left’.\"\nExample from multi-choice setting with video input:\n\"<video>\nwhich part of the frame do the\ntoys leave from?\nrespond ONLY\nwith one of:\n’top’, ’bottom’,\n’left’, ’right’, ’top right’,\n’top left’, ’bottom right’, or\n’bottom left’.\"\nB.8. Memory\nOriginal Toolbox Task\nThe Memory task in the NIH Toolbox is designed to mea-\nsure how well toddlers (22–42 months old) learn and re-\nmember new information using a touchscreen.\nChildren\nplay a short game where they “feed” hungry cartoon ani-\nmals by touching them on the screen. The test is divided\ninto the learning phase and the test phase.\n• Learning phase: children see pairs of animals and are\ntold to touch the new animal—the one they have not fed\nbefore. They complete 10 trials and receive feedback so\nthey can learn the rules and memorize the animals seen\nin this phase.\n• Testing phase: children again see pairs of animals and\ntold to touch the new animal, where each old animal from\nthe learning phase appears twice, each time paired with a\ndifferent new animal. They complete 20 trials and receive\nno feedback so correct responses reflect their memory for\nanimals in learning phase.\nThe animals were selected based on how many 24-month\nold infants were familiar with them according to data from\nthe MB-CDI Wordbank. Performance is scored based on\nwhether the child touches the correct animal in the testing\nphase, along with optional reaction time measures to show\nhow quickly they respond.\nAdaptation\nTo simplify the problem and enlarge the potential dataset\nsize, we define the set of word labels used in the learning\nphase as\nWlearn = {w1, w2, . . . , wk},\nwhere each wi corresponds to an image xi ∈Xlearn. These\nimage–label pairs (xi, wi) serve as the stimuli to be memo-\nrized during the learning phase. We further sample 2k ad-\nditional word labels for the testing phase,\nWtest = {wk+1, wk+2, . . . , w3k},\neach associated with a novel image xj ∈Xtest.\nAt each round t, the Vision–Language Model (VLM) re-\nceives an input consisting of two images and a text prompt:\nIt = {xpt, xqt, Pt},\nwhere xpt, xqt are the image inputs and Pt is the corre-\nsponding prompt.\n• Learning phase: The learning phase contains k rounds:\nIlearn\nt\n=\n(\n{x1, P1},\nt = 1,\n{xt−1, xt, Pt},\n2 ≤t ≤k,\nwhere the two images in the second case are presented in\nrandom order. This setup enables the model to incremen-\ntally associate visual concepts across consecutive rounds\nwithin a single context window.\n• Testing phase: The testing phase consists of 2k rounds,\neach comparing a learned stimulus with a new one:\nItest\nt\n= {xi(t), xj(t), P test\nt\n},\nxi(t) ∈Xlearn, xj(t) ∈Xtest.\nHere, xi(t) is a previously seen image and xj(t) a novel\none. The model must identify which image corresponds\nto the new concept described in P test\nt\n.\nEvaluation\nEach learned concept wi ∈Wlearn is paired with two distinct\nnew concepts:\n(wi, wa(i)), (wi, wb(i)),\na(i), b(i) ∈{k+1, . . . , 3k},\na(i) ̸= b(i),\n(1)\nforming two dyads per old stimulus and a total of 2k dyads\nin the testing phase. To mitigate the influence of random\nguessing, an old stimulus wi is considered successfully re-\nmembered only if both of its dyads are answered correctly:\nri =\n(\n1,\nif both dyads for wi are correct,\n0,\notherwise.\n21"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 22, "text": "Figure 14. A sample of our memory task adaptation. We use the MAB-CDI words detected in SAYCam as the images to be memorized.\nThe overall memory accuracy is then computed as\nAccmem = 1\nk\nk\nX\ni=1\nri.\nIn all experiments, we set k = 5, resulting in a total of\n3k = 15 distinct word–image pairs. This design preserves\nthe spirit of the original Toolbox while adapting the\nprocedure to the VLM’s limited context window.\nWhen\ndesigning the evaluation metric, we follow the structure\nof the original Toolbox with appropriate simplifications.\nSpecifically, we remove the original intermediate 6–8\nmin delay settings between the learning and testing\nphases in our benchmark design.\nFuture extensions\nmay incorporate external memory mechanisms such as\nRetrieval-Augmented Generation (RAG), or introduce\nirrelevant contexts between the two phases to simulate\nreal-world temporal gaps. In this work, however, we focus\nexclusively on assessing the model’s in-context retrieval\nability.\nData collection\nFor the scalability of the memory task, we expanded the\nimage set from the cartoon animals in the original Toolbox\nto the objects in the SAYCam dataset, which also ensures\nthat the items are familiar to children. We used a combi-\nnation of annotation-based search scripts and automated\nvision models, including CLIP for object–text similarity\nand SAM for object segmentation as shown in B.1, to find\nand isolate frames where these objects appeared clearly.\nManual screening was also done after auto-filtering. This\nprocess allowed us to gather real-world visual examples\nof common objects seen by young children, supporting\nthe creation of new learning and memory trials for our\nbenchmark.\nThe visual objects collected from SAYCam\ndataset will serve as our stimuli in the memory task.\nExample Prompt\nEach finalized example is a list of prompts each embedded\nwith 2 image choices, for which the following is an exam-\nple:\n\"Let’s try more.\nTouch the new image.\n(A) <image> or (B) <image>.\"\nThe model needs to output one of A or B to be evaluated.\nB.9. Who Has More\nOriginal Toolbox Task\nIn the NIH Baby Toolbox®, the Who Has More Measure is\npoised as a simple narrative: there are two animals; each\nof them is pictured with some number of the same object.\nWhich animal has more?\nAdaptation\nIn DevCV Toolbox, we remove the narrative aspect and\nreplace the clipart objects with naturalistic SAYCam\nand Ego4d objects.\nIn the Naturalistic adaptation, the\nobjects are not necessarily identical and appear in their\nnaturalistic backgrounds;\nin the Synthetic adaptation,\nthe objects are perfectly identical, cropped, and pasted\nonto black backgrounds in matching layouts. The model\nis prompted to identify whether the first or second has more.\nData collection\nIn the synthetic variants, to pick the two quantities to com-\npare, we first sample a number between one and ten. Then,\nfrom the numbers remaining that are lower than the first\none, we sample the second quantity. We do this to ensure\n22"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 23, "text": "a balanced distribution in the differences in numbers being\ncompared for each answer. The objects being compared\ncome from the annotations in B.1 and egotracks for SAY-\nCam and Ego4d, respectively.\nFor the test sets in the naturalistic adaptations, each ex-\nample is hand annotated by two separate human experts to\ncross-validate annotation quality. Specifically, the first hu-\nman expert labels video frames with an object type and the\nnumber of that object. Next, for each the frames that the\nfirst annotator labeled, the second annotator labels the num-\nber of the named object in each, without access to the first’s\nannotation.\nWith both labels for each frame, we construct an exam-\nple for every pair of frames of with objects of the same type\nfor which both annotators would have arrived at the same\nanswer answer as to which has more had they based their\ndecision solely on their count annotation. As an example,\nsay the first annotator labels frame A as having 5 cups, and\nframe B as having 6 cups. If the second annotator labels 5\ncups in frame A and 7 cups in frame B, we construct a Who\nHas More example from frames A and B (despite the anno-\ntators giving frame B two different labels) because 5<7 and\n6<7. However, if the second annotator instead labeled frame\nB as having 5 cups, we do not construct a Who Has More\nexample from frames A and B, because the two annotators\nwould have given different answers for such an example.\nIn constructing Who Has More, we observe that some\nobjects occur in multiples more than others, and each object\nfollows a unique (and usually nonuniform) distribution of\nquantity- for example, the number of hands visible in a\nframe is usually one or two and rarely another number,\nwhile an object like books could reasonably be seen in any\nquantity between one and ten. Additionally, we observe\nthat given the differences in settings and scene perspective,\nthe distributions of object types as well as quantity per\nobject is inherently different for SAYCam and Ego4d.\nExample Prompt\nEach finalized example is a prompt embedded with 2 image\nchoices for which the following is an example:\nWhich of the following has more\nof shoe?\n(A) <image>, or (B)\n<image>?\"\nThe model needs to output one of A or B to be evaluated.\nB.10. Subitizing\nOriginal Toolbox Task\nIn the NIH Baby Toolbox® , the infant sees one to four\ncolored dots for only one second, then an audio prompt\nrequests the number of dots. Importantly, the dots are not\nshown for long enough to be counted one at a time- Subitize\nis intended to measure the ability to quickly identify small\nquantities, without counting.\nAdaptation\nTo construct Subitizing in DevCV Toolbox, we paste\nobjects onto random locations on black frames, in random\nquantities between one and four.\nTo simulate the \"one\nsecond flash\", we insert empty frames before and after the\nframe including the objects.\nData collection\nIn the SAYCam variant, the objects being pasted come from\nframes cropped by the bounding boxes obtained in Section\nB.1, subjected to a minimum confidence of .95.\nIn the\nEgo4d variant, the bounding boxes come from egotracks,\nand only objects in the MAB-CDI vocabulary are included.\nExample Prompt\nEach finalized example is a prompt embedded with 1 blank\nframe, 1 image prompt, and 1 blank frame for which the\nfollowing is an example:\n<image> <image> <image>\nHow many of apple did you see?\nAnswer with 1, 2, 3, or 4.\"\nThe model needs to output one of 1, 2, 3, or 4 to be evalu-\nated.\nB.11. Object Counting\nOriginal Toolbox Task\nIn the NIH Baby Toolbox®, infants are shown some number\nof an object on a screen, and asked to count them. Unlike\nthe Subitize measure, there is no time limit- participants\nhave time to count each item individually.\nAdaptation\nIn DevCV Toolbox, the examples are constructed in the\nsame way as the Subitizingexamples, except the quantities\nare between one and twelve, and there are no blank frames\ncorresponding with the lack of a time limit.\nData collection\nThe data collection for Object Countingis the same as for\nSubitizing.\nExample Prompt\nEach finalized example is a prompt embedded with 1 image\nprompt, for which the following is an example:\n<image>\nHow many of chair did you see?\nAnswer with a number 1-12.\"\nThe model needs to output a number between 1 and 12 to\nbe evaluated.\n23"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 24, "text": "C. Human survey\nC.1. Small-scale human adult test\nTo confirm the validity of DevCV Toolbox, we collect\nsmall-scale adult performance data on eight of the ten tasks.\nWe omit Looking While Listening and Subitizing as their\nexamples are directly taken from Picture Vocabulary and\nObject Counting, respectively. In total, we have data from\nn=11 adult participants, each completing 10 trials per task\nfor the SAYCam variants of Picture Vocabulary, Localiza-\ntion, Left/Right, Spatial Details, Visual Delayed Response,\nand Object Counting, and 5 trials per task for the SAYCam\nvariants of naturalistic Who Has More and synthetic Who\nHas More, and as well as the Ego4d variants of all tasks\nother than Memory. Participants completed 30 consecu-\ntive rounds of each Memory variant, requiring a maximum\nmemory of 29 distinct images.\nResults for each task can be found in the Human per-\nformance rows of Tables 4 and 9. In summary, our partici-\npants achieved an average accuracy of 93.0 on all SAYCam\ntasks and 93.5 on all Ego4d tasks, for both of which they\nfar outperform any model. From this, we conclude 1) De-\nvCV Toolbox is a valid discriminator of vision FMs with\nadult performance as a strong upper bound, and 2) the SAY-\nCam and Ego4d variants have roughly similar complexity\nand ambiguity for humans.\nC.2. Children Helping Science tests\nTo further examine the developmental fidelity of DevCV\nToolbox, an IRB review process is currently underway to\nextend this survey to a large scale children survey, where\nwe plan to collect response data for each task from children\nof the ages recommended for the corresponding NIH Baby\nToolbox® measure.\nTo this end, we collaborated with expert psychologists\nto develop child-friendly web interfaces for selected tasks\nand prepared them for deployment on the online develop-\nmental research platform Children Helping Science (CHS)\n[49]. CHS is a widely used, home-based platform through\nwhich families can participate in browser-based develop-\nmental studies run by researchers worldwide. By adapting\nour SAYCam-based tasks (PV, VDR and Memory) to CHS,\nwe aim to collect performance from young children under\nconditions analogous to the NIH Baby Toolbox®. At the\ntime of writing, the studies are under review and not yet\nlive. We show two examples of our task UI design in Fig-\nures15 and 16.\nTaking PV as an example (Figure 15), to approximate\nthe modality of the original NIH Baby Toolbox®task, which\nrelies on audio-visual interaction with spoken prompts and\nobserved child responses, we design an audio&video test\npage to verify that instructions and target words can be de-\nlivered clearly via audio and that the child’s webcam setup\nis functioning for basic participation monitoring. The in-\nstruction page provides caregiver-friendly guidance in both\ntext and spoken form. Finally, the trial pages present each\nexample in a clean 2×2 grid of four large image options,\npaired with an audio prompt of the target word, optimizing\nengagement and accessibility for infants and toddlers while\nstaying faithful to the original task format.\nFollowing the PV setup, VDR also has an initial audio\n& video test page, along with an instruction page to provide\ncontext of the experiment to the caregiver. The trial page for\nthis task (see Figure 16) displays the object that should be\ntracked, along with the video clip itself and two selectable\narrows to submit an answer. Since MP4 with interactive dis-\nplay is not yet supported on the website, a GIF is created in\nits place. The beginning 5 seconds of the GIF show the first\nframe with a countdown, then the clip is played as normal\nand followed with another 5 second buffer to show that the\nvideo has ended. To help the caregiver and child understand\nthe experiment, an interactive demo is played as the first 3\ntrials to showcase how each one should be properly done.\nD. Additional experiments & details\nD.1. Out-Of-Domain evaluation\nTo test BabyLLaVA-V2’s capability of generalizing to un-\nseen data domain, we further evaluate it on a set of out-of-\ndomain (OOD) tasks that share the same structure as the\nin-domain benchmarks but differ in their visual domains.\nWe consider two OOD settings: (1) Ego4D-based tasks\nuse egocentric videos from the Ego4D dataset [13], which\nremain first-person and naturalistic but introduce distinct\nenvironments and contexts. (2) BabyToolbox-based tasks\ncorrespond directly to standardized developmental psychol-\nogy and clinical assessments, where the visual stimuli are\nabstract, non-egocentric cartoon images. The detailed test\nresults are reflected in Table 9 and Table 10.\nD.2. Importance of the pretraining stage\nTo evaluate the contribution of the pretraining stage, we\ncompare two variants of BabyLLaVA-V2: (1) the full model\ntrained with Stage 0–2 pretraining before instruction tuning,\nand (2) a randomly initialized model that skips pretraining\nand is trained only with Stage 3. For both variants, we fine-\ntune using different fractions of the instruction dataset and\nevaluate each model on in-domain tasks.\nAs shown in Figure 17, the pretrained model consistently\noutperforms the non-pretrained variant across all data frac-\ntions. The gap is especially pronounced when the instruc-\ntion data is limited, demonstrating that pretraining provides\na strong and sample-efficient initialization for downstream\ninstruction tuning.\nAs the instruction data fraction in-\ncreases, both models improve, reflecting a clear scaling-like\ntrend qualitatively consistent with observations in large-\n24"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 25, "text": "Figure 15. User interface design for our CHS-adapted Picture Vocabulary task.\nFigure 16. User Interface design for the trial page of Visual Delayed Response task on CHS.\nFigure 17. DevCV Toolbox overall performance on different in-\nstruction tuning data fraction.\nscale model studies [17, 22].\nThis suggests that data-\ndependent performance gains also exist in compact, devel-\nopmentally inspired models, while pretraining remains a\ncrucial component for achieving data-efficient learning.\nD.3. Synthetic caption generation\nWe study the impact of noisy visual-alignment in the natu-\nralistic child-directed utterances transcribed in the pretrain-\ning dataset by replacing them with video captions gener-\nated by GPT-4o. To encourage diversity in the generated\ncaptions and ensure they remain close to the style of the\noriginal dataset, we include 10 randomly sampled transcrip-\ntions in each prompt. The transcriptions are sampled from\na pool of the 1,000 highest confidence transcriptions in the\noriginal dataset that contain at least one noun and more than\nthree words. These heuristic filters help ensure that the sam-\npled transcriptions contain stylistic information rather than\nsimple phrases that are common in the dataset like \"wow\"\nor \"let’s go\". The pool of 1,000 transcriptions are manually\nscreened to remove uninformative transcriptions that passed\nthe filtering step. The full prompt to GPT-4o is shown in\nFigure 18 and an example of a generated caption is shown\nin Figure 19.\nFigure 18. Full prompt for pretraining data ablation\n25"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 26, "text": "Table 9. Performance comparison across models on DevCV Toolbox out-of-domain tasks (Ego4D). Different background colors\ndenote different model families. We report accuracy (%) for all tasks.\nModel\nOverall\nCount\nLeftRight\nSpatial\nPV\nMemory\nLocalization\nVisual Delay Response\nWho Has More\nbinary\nmulti-exact\nmulti-adjacent\nsynthetic\nnaturalistic\nUpper Bound\nHuman performance\n93.5\n96.4\n98.2\n96.4\n96.4\n98.8\n90.9\n100\n58.2\n100\n100\n92.7\nProprietary models\nGPT-4o\n67.6\n62.1\n45.1\n94.7\n85.3\n100\n80.4\n45.5\n13.2\n48.3\n84.3\n84.6\nGPT-5\n86.7\n77.5\n88.0\n96.8\n91.9\n100\n88.7\n94.4\n50.3\n82.6\n94.6\n88.5\nGemini-2.5-flash\n77.7\n72.9\n49.6\n86.7\n92.5\n99.2\n88.4\n80.6\n37.1\n70.2\n97.8\n80.1\nGemini-2.5-pro\n88.2\n81.9\n88.0\n94.8\n91.9\n100\n90.2\n91.3\n50.3\n87.9\n96.5\n97.8\nOpen-source models\nLLaVA-OneVision-0.5B\n39.4\n43.9\n32.6\n33.3\n27.7\n22.6\n21.6\n73.0\n15.2\n67.7\n46.8\n49.4\nInternVL3.5-1B\n43.7\n34.7\n34.0\n34.1\n33.8\n24.9\n60.7\n73.9\n16.9\n68.5\n49.0\n49.9\nQwen2.5-VL-3B\n48.1\n35.7\n32.6\n44.1\n41.9\n25.7\n86.7\n79.8\n28.9\n51.1\n50.2\n53.4\nBaby models (Ours)\nBabyLLaVA-V2\n41.1\n33.9\n32.9\n42.4\n29.8\n40.7\n30.0\n55.3\n17.7\n37.1\n86.0\n45.8\nLower Bound\nRandom guess\n31.8\n8.33\n33.3\n33.3\n25.0\n25.0\n25.0\n50.0\n12.5\n37.5\n50.0\n50.0\nTable 10. Performance on NIH Baby Toolbox out-of-domain\ntasks. We report the #correct/#total for all tasks.\nModel\nWho Has More\nCount\nMullen Visual Reception\nBabyLLaVA-V2\n13/24\n2/6\n3/12\nFigure 19. Example of caption generated by GPT-4o\nTable 11. Comparison between Gemini-2.5-flash performance\nwith different prompting strategies\nPrompt Type\nCount\nLeftRight\nstandard\n69\n55\none-shot\n66\n82\nalternate prompt 1\n55\n54\nalternate prompt 2\n67\n56\nD.4. Prompting Experiment\nFinally, we complete a prompting experiment to show\nthe stability of DevCV Toolbox examples with respect to\ncommercial models, the results of which are shown in Table\n11.\nWe select Left/Right and Object Counting for this\nexperiment, as we found that commercial models had the\nlowest and most variable performance on these. For both\ntasks, 100 examples are randomly selected and presented\nto Gemini-2.5-flash with a standard prompt, a one-shot\nprompt, and two variations of the standard prompt, called\nalternate prompt 1 and alternate prompt 2. The standard\nprompt is the one used in all other experiments, and the one\nshot-prompt is a prompt that includes one other example,\nwith its correct answer, prepended to the standard prompt.\nFor Object Counting, alternate prompt 1 does not\ngive the object’s name to be counted, e.g.\n\"<image>\nHow many objects do you see?\",\nwhich\nwe\nsee drops performance, which is intuitive because large\nmodels thrive on context, in this case the name of the\nobject to be counted.\nAlternate prompt 2 gives more\ndetail,\ne.g.\n<image> count the flora very\nclosely, starting from one.\nKeep track\nof which ones have already been counted\nand what number you’ve counted to thus\nfar.\nThen, report how many flora you\ncounted.\".\nUnsurprisingly, alternate prompt 2 does\nnot improve performance, showing that 1) the standard\nprompt was sufficient and 2) Gemini-2.5-flash has capable\ninstruction-following capabilities. For Object Counting, we\nfind that a one-shot prompt does not boost performance.\nFor\nLeft/Right,\nthe\nstandard\nprompt\ngives\neach\nimage\ntoken\ninterleaved\nwith\ntheir\nanswer\nlabels,\ne.g.\n\"<image> Which of the following\n26"}
{"pdf_id": "arxiv_251210932_babyvlm_v2", "page": 27, "text": "is the same as this?\n(A) <image>\n(B) <image> (C) <image>\".\nIn\nalternate\nprompt\n1,\nwe\nundo\nthis\ninterleaving,\nresulting\nin\n\"<image><image><image><image> Which\nof the following is the same as the\nfirst one?\n(A) the second one, (B) the\nthird one, or (C) the fourth one?\".\nIn\nalternate prompt 2, we interleave even more, by giving\nsome descriptive text before the prompt image,\ne.g.\n\"Here is an image:\n<image>.\nWhich of\nthe following is the same as it?\n(A)\n<image>, (B) <image>, or (C) <image>?\".\nIntuitively we expect alternate prompt 2 to be the easiest,\nalternate prompt 1 to be the hardest, and the standard\nprompt to fall in between.\nHowever, we find that none\nof these prompts elicits significantly different perfor-\nmance, however, the one-shot prompt significantly boosts\nperformance.\nThese two findings show the robustness\nof Gemini-2.5-flash, and the complexity of Left/Right,\nrespectively.\n27"}
