{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 1, "text": "Stronger Normalization-Free Transformers\nMingzhi Chen1\nTaiming Lu1\nJiachen Zhu2\nMingjie Sun3\nZhuang Liu1\n1Princeton University\n2NYU\n3Carnegie Mellon University\n...\n82.8%\n82.5%\n82.4%\n82.3%\n82.2%\n82.2%\n81.6%\n81.6%\n...\nViT ImageNet acc\nreplace with\nPoint-wise functions\npoint-wise functions\nNorm Layer\nAttention / FFN\n-0.5\n0.5\n-1\n1\n0\n-2\n-1\n0\n1\n2\na) We search point-wise functions of different shapes as norm layer replacement.\nLayerNorm:\nğ‘¥âˆ’ğœ‡\nâˆš\nğœ2+ğœ–\nDyT: tanh(ğ›¼ğ‘¥)\nDerf: erf(ğ›¼ğ‘¥+ ğ‘ )\nb) Formulation of LayerNorm (LN), DyT, and Derf (ours).\nmethod\nViT acc (â†‘)\nDiT FID (â†“)\nDNA acc (â†‘)\nLN\n82.3%\n45.91\n86.9%\nDyT\n82.5%\n45.66\n86.9%\nDerf\n82.8%\n43.94\n87.3%\nc) Performance across domains.\nFigure 1 We introduce Dynamic erf (Derf), a point-wise function, that outperforms normalization layers and\nother point-wise functions. (a) We identify the feasible function shape for replacing the normalization layer and\npropose a large set of point-wise functions within this space. Evaluating all candidates, we identify and introduce\nDerf as the strongest choice. (b) LayerNorm, DyT (Zhu et al., 2025), and Derf operate in fundamentally different\nways: with channels ğ¶and tokens ğ‘‡, LayerNorm normalizes each channel across the token axis, whereas DyT and\nDerf apply independent scalar mappings to each element. (c) Across ImagenNet-1K classification and generation,\nand DNA sequence modeling, Derf consistently outperforms LayerNorm and DyT. Derf demonstrates that a\npoint-wise function can not only replace normalization but also surpass it.\nAbstract\nAlthough normalization layers have long been viewed as indispensable components of deep learning\narchitectures, the recent introduction of Dynamic Tanh (DyT) (Zhu et al., 2025) has demonstrated\nthat alternatives are possible. The point-wise function DyT constrains extreme values for stable\nconvergence and reaches normalization-level performance; this work seeks further for function designs\nthat can surpass it. We first study how the intrinsic properties of point-wise functions influence\ntraining and performance. Building on these findings, we conduct a large-scale search for a more\neffective function design. Through this exploration, we introduce Derf(ğ‘¥) = erf(ğ›¼ğ‘¥+ ğ‘ ), where erf(ğ‘¥)\nis the rescaled Gaussian cumulative distribution function, and identify it as the most performant\ndesign. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including\nvisual recognition and generation, speech representation, and DNA sequence modeling. Our findings\nsuggest that the performance gains of Derf largely stem from its improved generalization rather than\nstronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for\nnormalization-free Transformer architectures. Our code is available at this link.\n1\narXiv:2512.10938v1  [cs.LG]  11 Dec 2025"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 2, "text": "1 Introduction\nNormalization layers have become a critical component in modern deep neural networks. Since the invention\nof Batch Normalization (Ioffe and Szegedy, 2015), more and more variants have been developed to adapt\nnormalization to various architectures and model modalities (Ba et al., 2016; Salimans and Kingma, 2016;\nUlyanov et al., 2016; Wu and He, 2018; Zhang and Sennrich, 2019). By regulating the distribution of\nintermediate activations, normalization layers have long demonstrated their strong capability in stabilizing\ntraining and accelerating model convergence (Santurkar et al., 2018; Bjorck et al., 2018).\nDue to the inherent formulation of normalization layers, they heavily rely on activation statistics during\ntraining. This introduces additional memory access and synchronization overhead (Zhang and Sennrich,\n2019; Chen et al., 2020; Yang et al., 2022). Moreover, some normalization methods are highly sensitive to\nbatch size, and inappropriate batch settings can lead to unstable training (Wu and He, 2018; Lian and Liu,\n2019; Singh and Krishnan, 2020). These issues motivate recent efforts to develop normalization-free methods.\nAmong these attempts, Dynamic Tanh (Zhu et al., 2025), an S-shaped point-wise function, has emerged as a\nsimple yet effective drop-in replacement for normalization layers. This work has established the foundation\nfor point-wise functions that match the performance of normalization layers, yet functions that can surpass\nthem remain unexplored. In this work, we aim to discover point-wise functions that outperform normalization\nlayers to push toward stronger Transformer architectures (Vaswani et al., 2017; Dosovitskiy, 2021).\nWe first systematically study how the intrinsic properties of point-wise functions affect the training dynamics\nand final performance. Specifically, we focus on four fundamental and representative function properties:\nzero-centeredness, boundedness, center sensitivity, and monotonicity. Each property is independently examined\nthrough controlled experiments on a diverse set of point-wise functions to assess its impact on the training\nresult. This analysis isolates a subset of point-wise functions as effective normalization replacements and\nyields a concrete design principle for normalization-free Transformers.\nGuided by these principles, we identify a set of promising point-wise functions that have the potential to\nsurpass the performance of normalization layers. Within this set, we empirically search for the optimal\ndesigns, among which Dynamic erf (Derf) emerges as a simple yet the most performant function (Figure 1a).\nDerf augments erf(ğ‘¥) with learnable parameters, where the error function erf(ğ‘¥) is an S-shaped, rescaled\ncumulative distribution of a standard Gaussian around zero.\nWe evaluate Derf spanning multiple modalities (vision, language, speech, and DNA sequences); covering various\ntasks (classification, generation, and sequence modeling), under different training paradigms (supervised and\nself-supervised). Across all these settings, Derf consistently surpasses LayerNorm, RMSNorm, and Dynamic\nTanh (Figure 1b). To pinpoint the source of these gains, we measure the training loss in evaluation mode\nafter optimization. Derf exhibits higher training loss than normalization-based models, indicating that its\nsuperior performance stems from stronger generalization rather than enhanced fitting capacity. Overall, our\nwork demonstrates that well-designed point-wise functions can outperform normalization layers.\n2 Background\nNormalization layers. Normalization layers have become pivotal components of modern neural networks.\nAmong the various normalization techniques, Batch Normalization (BN) (Ioffe and Szegedy, 2015), Layer\nNormalization (LN) (Ba et al., 2016), and Root Mean Square Normalization (RMSNorm) (Zhang and Sennrich,\n2019) are the three most widely used in deep learning models.\nğ‘¦= ğ›¾*\nğ‘¥âˆ’ğœ‡\nâˆš\nğœ2 + ğœ–\n+ ğ›½\n(1)\nAll normalization methods adhere to a unified paradigm, formalized in Equation 1, where activations within\neach group are centered and scaled by their mean ğœ‡and standard deviation ğœ(with ğœ–for numerical stability)\nto maintain consistent scale and stable gradient flow. The main distinction among different normalization\nmethods lies in how the activations are grouped when computing ğœ‡and ğœ. For example, LN computes the\nstatistics along the channel dimension for each token independently. Given a token representation ğ‘¥âˆˆRğ¶, the\nmean and variance are computed as Equation 2, where ğ¶denotes the number of hidden features (channels).\n2"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 3, "text": "Zero-centered\nCentered\nNot centered\nBounded\nBounded\nNot bounded\nCenter Sensitive\nSensitive\nNot sensitive\nMonotonic\nMonotonic\nNot monotonic\nFigure 2 Key properties of point-wise function. The four properties: zero-centeredness, boundedness, center sensitivity,\nand monotonicity collectively characterize functional behavior on activations and influence training dynamics. Blue\ncurves represent functions that satisfy each property, while red curves violate them.\nDue to its per-token normalization, LN is particularly well-suited for Transformer architectures, where\nactivations across tokens exhibit diverse statistics.\nğœ‡= 1\nğ¶\nğ¶\nâˆ‘ï¸\nğ‘˜=1\nğ‘¥ğ‘˜,\nğœ2 = 1\nğ¶\nğ¶\nâˆ‘ï¸\nğ‘˜=1\n(ğ‘¥ğ‘˜âˆ’ğœ‡)2,\n(2)\nPoint-wise functions. The strong reliance of normalization layers on activation statistics has motivated\nfurther exploration of statistics-free methods (He and Hofmann, 2024; Heimersheim, 2024; Jha and Reagen,\n2024; Zhu et al., 2025). Among these approaches, point-wise functions (Zhu et al., 2025) have emerged as\nsimple yet effective alternatives to traditional normalization methods. Unlike normalization, a point-wise\nfunction applies the same parametric mapping ğ‘“(ğ‘¥; ğœƒ) to each activation independently. The parameters ğœƒ\nare fixed or learned, rather than being computed from batch-, token-, or channel-level statistics. A recent\nstudy (Zhu et al., 2025) introduces the Dynamic Tanh (DyT) function (Equation 3), where ğ›¼is a learnable\nparameter. This design is motivated by the observation that Layer Normalization often produces an S-shaped\ninput-output mapping in practice. The saturating nature of the tanh function squashes extreme activations,\nthereby fulfilling a role analogous to the re-centering and re-scaling effects of normalization layers.\nDyT(ğ‘¥) = ğ›¾* tanh(ğ›¼ğ‘¥) + ğ›½\n(3)\nWhile DyT has shown similar empirical performance to normalization layers across various Transformer\narchitectures, a comprehensive analysis of the design space for these statistics-free operators remains missing.\nIn this work, we target at the optimal form of the point-wise function as normalization replacement. We identify\nthe function properties crucial for convergence and performance, and then we introduce Derf, a point-wise\nfunction consistently surpassing normalization layers rather than merely matching their performance.\n3 Function Property Analysis\nTraining Transformers without normalization requires understanding the factors that make a point-wise\nfunction stable and effective as a replacement. In this section, we examine four essential properties: zero-\ncenteredness, boundedness, center sensitivity, and monotonicity (see Figure 2). These properties collectively\ncharacterize the fundamental shape of point-wise functions and their behavior on activations. By isolating the\nimpact of each property, we explore its influence on optimization and final performance.\nTo investigate these properties, we replace each normalization layer with a point-wise function of the form:\nğ‘¦= ğ›¾Â· ğ‘“(ğ›¼ğ‘¥) + ğ›½,\n(4)\nwhere ğ‘“(Â·) denotes the chosen base function with learnable ğ›¼rescaling the input. ğ›¾and ğ›½are affine parameters,\nsimilar to those in normalization layers. We begin with three base functions: tanh(ğ‘¥), erf(ğ‘¥), and arctan(ğ‘¥).\nIn subsequent experiments, we modify these functions with controlled transformations to examine the impact\nof each property. All experiments are conducted with ViT-Base (Dosovitskiy, 2021), and top-1 accuracy on\nImageNet-1K (Deng et al., 2009) is reported. In Appendix A, we provide more detailed training results.\n3"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 4, "text": "3.1 Zero-centeredness\nZero-centeredness means that the functionâ€™s outputs are balanced around zero, with positive and negative\nvalues of similar magnitude and symmetry. Because normalization layers inherently recenter activations to the\norigin for stabilizing gradients, maintaining this property could reduce internal covariate shifts and promote\nsmoother gradient flow during training.\nSetup. Under the ViT setup, we manipulate the centering of the functions. For each base function, we\nconsider two types of shifts: horizontal and vertical, defined in Equation 5. In this form, ğœ†horiz and ğœ†vert\nrespectively denote the magnitudes of horizontal and vertical shifts. For both types of shifts, we vary ğœ†over\n{Â± 1\n2, Â±1, Â±2} to examine how increasing deviation from zero-centeredness affects the functionâ€™s behavior. All\nother training settings remain unchanged.\nğ‘“horiz(ğ‘¥) = ğ‘“(ğ‘¥+ ğœ†horiz),\nğ‘“vert(ğ‘¥) = ğ‘“(ğ‘¥) + ğœ†vert,\n(5)\nResults. As shown in Table 1, the results are consistent across different base functions: for horizontal shifts,\nperformance remains largely comparable to the zero-centered base function when |ğœ†horiz| â‰¤0.5. However, as\n|ğœ†horiz| increases, performance gradually degrades, and training diverges when |ğœ†horiz| â‰¥2. Similarly, vertical\nshifts consistently lead to a decline in performance as |ğœ†vert| grows with training failure once |ğœ†vert| â‰¥2. These\nresults show that zero-centeredness is a requirement for stable convergence and effective training.\nfunction\nshift type\n-2\n-1\n-0.5\n-0.1\nğœ†= 0\n+0.1\n+0.5\n+1\n+2\nerf(ğ‘¥)\nhorizontal\nÃ—\n82.0%\n82.5%\n82.6%\n82.6%\n82.7%\n82.5%\n82.1%\nÃ—\nvertical\nÃ—\n81.8%\n82.3%\n82.4%\n82.6%\n82.5%\n82.3%\n81.6%\nÃ—\ntanh(ğ‘¥)\nhorizontal\nÃ—\n82.1%\n82.5%\n82.6%\n82.5%\n82.6%\n82.4%\n82.2%\nÃ—\nvertical\nÃ—\n81.5%\n81.9%\n82.4%\n82.5%\n82.3%\n81.9%\n81.4%\nÃ—\narctan(ğ‘¥)\nhorizontal\nÃ—\n81.9%\n82.3%\n82.3%\n82.3%\n82.4%\n82.2%\n82.0%\nÃ—\nvertical\nÃ—\n81.4%\n81.9%\n82.2%\n82.3%\n82.3%\n82.0%\n81.2%\nÃ—\nTable 1 Results of zero-centeredness on ViT-Base. Horizontal shift corresponds to modifying the input as ğ‘“(ğ›¼ğ‘¥Â± ğœ†),\nwhile vertical shift adds or subtracts a constant to the output as ğ‘“(ğ›¼ğ‘¥) Â± ğœ†. â€œÃ—â€ indicates training failure.\n3.2 Boundedness\nBoundedness refers to the property of a function whose output is constrained within a finite range. Formally,\na function ğ‘“(Â·) is bounded if there exist constants ğ‘, ğ‘âˆˆR such that ğ‘â‰¤ğ‘“(ğ‘¥) â‰¤ğ‘for all ğ‘¥in its domain. This\nensures that activations remain finite and do not accumulate variance across layers. Unbounded functions, in\ncontrast, may induce signal explosion and gradient instability.\nSetup. Under the same ViT setup, we study the role of boundedness with two methods. Firstly, we select\nthree inherently unbounded S-shaped functions (e.g., arcsinh(ğ‘¥)) and compare them with their clamped\nversions shown in Equation 6, where ğ‘“ğ‘¢(ğ‘¥) denotes the unbounded point-wise function, and ğœ†is a chosen\nvalue specifying the clipping range.\nğ‘¦= clip(ğ‘“ğ‘¢(ğ‘¥), âˆ’ğœ†ğ‘¢, ğœ†ğ‘¢),\n(6)\nSecondly, we gradually transition bounded functions (e.g., erf(ğ‘¥)) toward unbounded linear form, defined in\nEquation 7, where ğ‘“ğ‘denotes a bounded point-wise function, and ğœ†controls how quickly the function becomes\nunbounded. We vary ğœ†ğ‘¢over {0.5, 0.8, 1.0, 2.0, 3.0, 5.0} in the first method and ğœ†ğ‘over {0.01, 0.1, 0.5} for the\nsecond. The original unmodified function is also included as a baseline.\nğ‘¦= (1 âˆ’ğœ†)ğ‘“ğ‘(ğ‘¥) + ğœ†ğ‘ğ‘¥,\nğœ†ğ‘âˆˆ(0, 1).\n(7)\nResults. For the first method, among the three unbounded functions in Table 2, only arcsinh(ğ‘¥) and logsign(ğ‘¥)\nconverge effectively, while linear(ğ‘¥) does not. For the convergent functions, their clipped versions consistently\n4"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 5, "text": "3\n6\n9\n12\n15\n18\n6\n12\n18\n0\nlinear(x)\npower23(x)\nlogquad(x)\narcsinh(x)\nlogsign(x)\nFigure 3 Visualization of several unbounded point-wise functions on the\npositive half-axis, illustrating their different growth rates. ğ‘ğ‘Ÿğ‘ğ‘ ğ‘–ğ‘›â„(ğ‘¥)\nrefers to its standard analytical form. The remaining functions are defined\nas linear(ğ‘¥) = ğ‘¥, power23(ğ‘¥) = ğ‘¥\n2\n3 , logsign(ğ‘¥) = sign(ğ‘¥) ln(|ğ‘¥| + 1),\nsmoothsign(ğ‘¥) =\nğ‘¥\n1+|ğ‘¥|, and logquad(ğ‘¥) = sign(ğ‘¥) ln(ğ‘¥2+1). Among them,\nlogquad(ğ‘¥) shows the fastest growth that still ensures stable convergence.\nğœ†ğ‘¢\narcsinh(ğ‘¥)\nlogsign(ğ‘¥)\nlinear(ğ‘¥)\nâˆ’\n82.2%\n82.2%\nÃ—\n0.5\n82.3%\n82.4%\n82.1%\n0.8\n82.3%\n82.4%\n82.2%\n1.0\n82.4%\n82.4%\n82.2%\n2.0\n82.4%\n82.4%\n82.1%\n3.0\n82.4%\n82.3%\n82.1%\n5.0\n82.3%\n82.3%\n82.0%\nTable 2 Results of clamping for bound-\nedness on ViT-Base. Clipped version of\nunbounded functions consistently achieves\nbetter performance than unbounded base-\nlines. â€œâˆ’â€ denotes the original unmodified\nfunction. â€œÃ—â€ indicates training failure.\noutperform the unbounded baselines across all tested ğœ†values. These results indicate that incorporating\nboundedness can improve optimization and result in better performance. For the second, as shown in Table 3,\nthe results are consistent with clipping the intrinsic unbounded functions: the unbounded variant yields\nslightly lower accuracy than the bounded baseline.\nğœ†ğ‘\nerf(ğ‘¥)\ntanh(ğ‘¥)\narctan(ğ‘¥)\nisru(ğ‘¥)\nâˆ’\n82.6%\n82.5%\n82.4%\n82.3%\n0.01\n82.4%\n82.4%\n82.1%\n82.2%\n0.1\n82.3%\n82.3%\n82.1%\n82.1%\n0.5\nÃ—\nÃ—\nÃ—\nÃ—\nTable 3 Results of removing boundedness on ViT-Base. Performance decreases as the function is less bounded. â€œâˆ’â€\ndenotes the original function without modification and â€œÃ—â€ donotes training failure.\nLimitation of growth rate. From Table 2 and Table 3, we observe that there is an upper limit on their\nacceptable growth rate. Large growth rates often lead to training failure. To determine this limit, we evaluate\na family of inherently unbounded functions with varying growth rates, as illustrated in Figure 3. Among them,\nlogquad(ğ‘¥) exhibits the fastest growth that still allows training convergence (see Table 4). Functions with\nfaster growth, such as linear(ğ‘¥) and power23(ğ‘¥), tend to cause optimization divergence in the early stages of\ntraining. This failure occurs because rapidly growing functions fail to suppress variance effectively, leading to\nlarge gradient norms at the start of optimization.\nlogsign(ğ‘¥)\narcsinh(ğ‘¥)\nlogquad(ğ‘¥)\npower23(ğ‘¥)\nlinear(ğ‘¥)\n82.2%\n82.2%\n82.1%\nÃ—\nÃ—\nTable 4 Results of unbounded functions with different growth rates on ViT-Base. Point-wise functions have a growth\nrate upper bound, with logquad(ğ‘¥) being the fastest function that still converges. â€œÃ—â€ indicates training failure.\n3.3 Center Sensitivity\nWe use center sensitivity to characterize how quickly a point-wise function becomes responsive to input\nvariations around zero. Without center sensitivity, a function is locally flat around the origin, returning zero\nor near-zero over a finite interval. The region around zero is particularly important, as most activations tend\nto concentrate near the origin during training. Consequently, the responsiveness of a function in this area\ndirectly influences how effectively small signals can propagate through the network.\n5"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 6, "text": "Setup. Since center sensitivity is difficult to isolate independently, we approximate it using a controllable\nnear-zero inactive region. Under the same ViT setup, we modify each base function to incorporate a symmetric\nflat region around the origin with a sensitivity scale ğœ†> 0 to control the extent of this region. Specifically,\nfor inputs in the range ğ‘¥âˆˆ[âˆ’ğœ†, ğœ†], we enforce ğ‘“(ğ‘¥) = 0 and smoothly shift the positive and negative parts\noutward for |ğ‘¥| > ğœ†to ensure continuity at the boundaries. A smaller ğœ†results in a narrower flat region and\nhigher sensitivity near zero, while a larger ğœ†leads to lower sensitivity. We vary ğœ†over {0.1, 0.5, 1.0, 2.0, 3.0}\nacross three base functions.\nResults. As shown in Table 5, the best performance is achieved at ğœ†= 0. As ğœ†increases, the performance\nconsistently degrades. This trend is not very clear when ğœ†â‰¤0.5, but once ğœ†exceeds 1.0, the degradation\nbecomes much more obvious. Finally, when ğœ†â‰¥3.0, the training process diverges at an early stage.\nfunction\nğœ†= 0\n0.1\n0.5\n1.0\n2.0\n3.0\nerf(ğ‘¥)\n82.6%\n82.5%\n82.5%\n82.1%\n81.3%\nÃ—\ntanh(ğ‘¥)\n82.5%\n82.5%\n82.4%\n82.1%\n81.1%\nÃ—\narctan(ğ‘¥)\n82.3%\n82.3%\n82.1%\n81.8%\n80.9%\nÃ—\nTable 5 Results of center sensitivity (ğœ†) on ViT-Base. â€œÃ—â€ indicates training failure. The best performance is achieved\nwhen no flat region is given, showing the importance of center sensitivity.\n3.4 Monotonicity\nMonotonicity ensures a functionâ€™s output consistently increases (or decreases) as the input increases, preserving\nthe relative order of inputs throughout the transformation. Non-monotonic functions may disrupt the relative\nordering of activations. Furthermore, since a non-monotonic function necessarily has regions where its\nderivative changes sign, it may also produce flipped gradient signals during training.\nSetup.\nEach base function selected can serve as the monotonically increasing case, while its negated\ncounterpart is defined as ğ‘“neg(ğ‘¥) = âˆ’ğ‘“(ğ‘¥), representing the monotonically decreasing variant. As non-\nmonotonic comparisons, we include hump-shaped functions and oscillatory functions (see Figure 4) to examine\nhow violations of monotonicity influence the training performance. To control potential confounding factors,\nwe rescale each function so that its output range matches that of the monotonic functions. After rescaling, all\nfunctions are aligned in terms of zero-centeredness, boundedness, and center sensitivity.\n-0.5\n1.0\n0.5\n0.0\n-1.0\n8\n-4\n0\n4\n-8\nFigure 4 Visualization of point-wise functions\nwith different monotonicity behaviors. erf(ğ‘¥) and\nsin(ğ‘¥) refer to their standard form. The remain-\ning functions are defined as negerf(ğ‘¥) = âˆ’erf(ğ‘¥),\ndampx(ğ‘¥) =\n2ğ‘¥\n1+ğ‘¥2 , dampexp(ğ‘¥) = 2.72ğ‘¥Â· ğ‘’âˆ’|ğ‘¥|\nfunction\nğ‘“(ğ‘¥)\nğ‘“neg(ğ‘¥)\nerf(ğ‘¥)\n82.6%\n82.5%\ntanh(ğ‘¥)\n82.5%\n82.5%\narctan(ğ‘¥)\n82.3%\n82.2%\n(a) Monotonic\nfunction\nğ‘“(ğ‘¥)\nsin(ğ‘¥)\n81.6%\ndampx(ğ‘¥)\n80.7%\ndampexp(ğ‘¥)\n81.2%\n(b) Non-monotonic\nTable 6 Results of monotonicity on ViT-Base. Monotonic func-\ntions consistently achieve better performance than their negated\nversions and other non-monotonic functions, whether hump-\nshaped or oscillatory.\nThis identifies monotonicity as a key\nproperty for effective learning.\nResults. As shown in Table 6, both increasing and decreasing monotonic functions train stably and achieve\nhigh accuracy. In contrast, non-monotonic functions, whether hump-shaped or oscillatory, consistently perform\nworse than monotonic functions and lead to a clear drop in final accuracy. These results highlight monotonicity\nas a key property for point-wise functions to ensure effective learning.\n6"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 7, "text": "function\nalias\ntop-1 acc â†‘\nFID â†“\nViT-Base\nDiT-B/4\nDiT-L/4\nâ€“\nLayerNorm\n82.3%\n64.93\n45.91\n2ğœ‹âˆ’1/2âˆ«ï¸€ğ‘¥\n0 ğ‘’âˆ’ğ‘¡2 ğ‘‘ğ‘¡\nerf(ğ‘¥)\n82.8%\n63.23\n43.94\n(ğ‘’ğ‘¥âˆ’ğ‘’âˆ’ğ‘¥)(ğ‘’ğ‘¥+ ğ‘’âˆ’ğ‘¥)âˆ’1\ntanh(ğ‘¥)\n82.6%\n63.71\n45.48\nsin(clip(ğ‘¥, âˆ’ğœ‹\n2 , ğœ‹\n2 ))\nsatursin(ğ‘¥)\n82.6%\n63.90\n44.83\nclip (ln(ğ‘¥+\nâˆš\nğ‘¥2 + 1), âˆ’1, 1)\narcsinhclip(ğ‘¥)\n82.5%\n64.72\n45.48\nğ‘¥(ğ‘¥2 + 1)âˆ’1/2\nisru(ğ‘¥)\n82.3%\n65.72\n45.93\nsign(ğ‘¥) ((1 âˆ’ğ‘’âˆ’âˆš\n|ğ‘¥|))\nexproot(ğ‘¥)\n82.4%\n65.20\n46.91\nclip(ğ‘¥, âˆ’1, 1)\nlinearclip(ğ‘¥)\n82.3%\n66.08\n45.49\nâˆ’sign(ğ‘¥) ((ğ‘’âˆ’|ğ‘¥| âˆ’1))\nexpsign(ğ‘¥)\n82.2%\n64.85\n45.82\nclip (sign(ğ‘¥) ln(|ğ‘¥| + 1), âˆ’1, 1)\nlogsignclip(ğ‘¥)\n82.4%\n65.59\n46.34\nğ‘¥(\nâˆš\nğ‘¥2 + 1 + 1)âˆ’1\nrelsign(ğ‘¥)\n82.3%\n68.42\n48.33\narctan(ğ‘¥)\narctan(ğ‘¥)\n82.4%\n67.07\n46.62\nğ‘¥(1 + |ğ‘¥|)âˆ’1\nsmoothsign(ğ‘¥)\n82.4%\n68.84\n47.29\nclip (sign(ğ‘¥) ln(ğ‘¥2 + 1), âˆ’1, 1)\nlogquadclip(ğ‘¥)\n82.2%\n65.92\n47.12\nclip (sign(ğ‘¥) |ğ‘¥|2/3, âˆ’1, 1)\npower23clip(ğ‘¥)\n82.1%\n66.11\n46.47\nsign(ğ‘¥) ln(|ğ‘¥| + 1) (ln(|ğ‘¥| + 1) + 1)âˆ’1\nsaturlog(ğ‘¥)\n81.8%\n68.23\n47.44\nğ‘¥3(|ğ‘¥|3 + 1)âˆ’1\ncubsign(ğ‘¥)\n81.4%\n70.22\n49.16\nTable 7 Top-1 accuracy on ViT-Base and image generation quality (FID) on DiT-B/4 and DiT-L/4. Different\nfunctions show noticeable differences in performance. Among all the point-wise functions and LayerNorm, erf(ğ‘¥) shows\nthe best performance in both top-1 accuracy and FID. Visualization of each function is included in Appendix B.\n4 Function Search\nFrom the previous section, we observe that functions that are near zero-centered, bounded, center-sensitive\n(responsive to input variations around zero), and monotonic (increasing or decreasing) tend to yield better\noptimization performance. Building upon these insights, we start to construct our function set from widely used\nscalar functions and cumulative distribution functions (CDFs), including polynomial, rational, exponential,\nlogarithmic, and trigonometric forms. We then generate variants via simple transformations such as translation,\nscaling, mirroring, rotation, and clipping. Functions that satisfy our four function properties after these\ntransformations are retained as the candidate subset used in the search. For example, we transform the\nunbounded function arcsinh(ğ‘¥) by clipping it to the range [âˆ’1, 1], limiting it to a finite range and conforming\nto all four principles. In Appendix B, we provide further details about how we obtain these candidate functions.\nWithin this set, we evaluate their performance, and Derf emerges as the most effective function.\nSetup. We conduct an empirical search on two representative vision architectures: Vision Transformer\n(ViT-Base) (Dosovitskiy, 2021) and Diffusion Transformer (DiT-B/4 and DiT-L/4) (Peebles and Xie, 2023).\nModels are trained on ImageNet-1K (Deng et al., 2009) under their default training settings. For ViT, model\nperformance is measured using top-1 accuracy on the ImageNet-1K validation set. For DiT, we follow the\nstandard ImageNet reference batch evaluation and report the FrÃ©chet Inception Distance (FID) as the metric.\nFormulation. We quantitatively evaluate a set of functions under the constraint of our function properties,\nas illustrated in Figure 5. Each point-wise function is instantiated in a unified form in Equation 8, where\nğ‘“(Â·) denotes a candidate point-wise function, with learnable parameter ğ‘ and ğ›¼recentering and rescaling the\ninput. The parameters ğ›¾and ğ›½follow the same role as in standard normalization layers. We introduce a\nlearnable shift parameter ğ‘ , as it improves the final performance to varying degrees across different functions.\nDetailed ablation results on the effect of ğ‘ are provided in Section 7.1.\nğ‘¦= ğ›¾* ğ‘“(ğ›¼ğ‘¥+ ğ‘ ) + ğ›½,\n(8)\n7"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 8, "text": "0.5\n1\n1.5\n2\n2.5\n3\n0.25\n0.5\n0.75\n1\n0\nsatursin(x)\nerf(x)\ntanh(x)\nisru(x)\nexpsign(x)\narctan(x)\nsmoothsign(x)\nrelsign(x)\n0.5\n1\n1.5\n2\n2.5\n3\n0.25\n0.5\n0.75\n1\n0\npower23clip(x)\nlinearclip(x)\narcsinhclip(x)\nlogquadclip(x)\nlogsignclip(x)\ncubsign(x)\nexproot(x)\nsaturlog(x)\nFigure 5 Visualization of candidate point-wise functions on the positive half-axis. All functions are self-symmetric\nwith respect to the origin.\nQuantitative evaluation. As shown in Table 7, even though these S-shaped functions appear highly similar\nin form, their empirical training results show noticeable differences in final performance. Among all the\npoint-wise functions, erf(ğ‘¥) with the introduced transformations stands out as the best-performing function,\nconsistently surpassing all other candidates and the baseline normalization layers.\n5 Dynamic erf (Derf)\nFrom the search, we identify erf(ğ‘¥) as the most performant point-wise function. The error function erf(Â·) is\nclosely related to the cumulative distribution function (CDF) of a standard Gaussian distribution. Specifically,\nerf(ğ‘¥) can be defined by Equation 9. In our setup, erf(ğ‘¥) is in the form augmented with learnable parameters,\nwhich we introduce as Derf, Dynamic erf. Given an input tensor ğ‘¥, a Derf layer is defined in Equation 10,\nwhere both the shift ğ‘ and the scale ğ›¼are learnable scalars. ğ›¾and ğ›½are learnable per-channel vectors. To\nintegrate Derf into a transformer-based architecture, we replace each normalization layer with a corresponding\nDerf layer. In particular, the pre-attention, the pre-FFN, and the final normalization layers are all substituted\nin a one-to-one manner, ensuring consistent incorporation of Derf across the entire model.\nerf(ğ‘¥) =\n2\nâˆšğœ‹\nâˆ«ï¸ğ‘¥\n0\nğ‘’âˆ’ğ‘¡2ğ‘‘ğ‘¡\n(9)\nDerf(ğ‘¥) = ğ›¾erf(ğ›¼ğ‘¥+ ğ‘ ) + ğ›½\n(10)\nParameter initialization. We initialize ğ›¾to an all-one vector and ğ›½to an all-zero vector following the same\nstrategy as in standard normalization layers. For the additional scalar parameters introduced by Derf, the\nscaling parameter ğ›¼is initialized to 0.5, while the shift parameter ğ‘ is initialized to 0. Unless otherwise\nspecified, these initialization settings are adopted throughout all experiments.\n6 Experiments\nWe evaluate the effectiveness of Derf across various transformer-based and a few other modern architectures.\nFor each model, we replace the original normalization layers with DyT and Derf, following the standard\ntraining and evaluation protocols, as detailed in Appendix C. Across all tested architectures, Derf consistently\nachieves stronger performance over the baseline normalization methods and DyT. Besides each modelâ€™s default\nnormalization, we also report results with other common normalization methods in Appendix D.\nVision Transformers. We train ViT-Base and ViT-Large models (Dosovitskiy, 2021) on ImageNet-1K (Deng\net al., 2009) using LayerNorm (LN), DyT, and Derf for comparison. Table 8 reports the top-1 classification\naccuracy. Compared to LN and DyT, Derf achieves clearly higher top-1 accuracy.\n8"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 9, "text": "model\nLN\nDyT\nDerf\nÎ”LN\nÎ”DyT\nViT-B\n82.3%\n82.5%\n82.8%\nâ†‘0.5%\nâ†‘0.3%\nViT-L\n83.1%\n83.6%\n83.8%\nâ†‘0.7%\nâ†‘0.2%\nTable 8 Supervised classification accuracy on ImageNet-1K. Derf achieves higher top-1 accuracy than both LN and\nDyT on different model sizes, demonstrating its effectiveness in vision transformer architectures.\nDiffusion Transformers. We train three Diffusion Transformer (DiT) (Peebles and Xie, 2023) models on\nImageNet-1K (Deng et al., 2009). Consistent with the original DiT setup, the affine parameters in the\nnormalization layers are retained for class conditioning across LN, DyT, and Derf. After training, we evaluate\nthe FID scores using the standard ImageNet â€œreference batchâ€ to measure image generation quality, as\nreported in Table 9. Derf achieves a clear improvement in FID compared to both LayerNorm and DyT.\nmodel\nLN\nDyT\nDerf\nÎ”LN\nÎ”DyT\nDiT-B/4\n64.93\n63.94\n63.23\nâ†“1.70\nâ†“0.71\nDiT-L/4\n45.91\n45.66\n43.94\nâ†“1.97\nâ†“1.72\nDiT-XL/2\n19.94\n20.83\n18.92\nâ†“1.02\nâ†“1.91\nTable 9 Image generation quality (FID) on ImageNet. Lower FID indicates better image generation quality. Derf\nachieves lower FID scores than both LN and DyT across all DiT model sizes.\nSpeech models. We train two wav2vec 2.0 Transformer models (Baevski et al., 2020) on the LibriSpeech\ndataset (Panayotov et al., 2015) for speech representation learning. We report the final validation loss in\nTable 10. Compared to LayerNorm and DyT, Derf yields lower validation loss on different model sizes.\nmodel\nLN\nDyT\nDerf\nÎ”LN\nÎ”DyT\nwav2vec 2.0 Base\n1.95\n1.95\n1.93\nâ†“0.02\nâ†“0.02\nwav2vec 2.0 Large\n1.92\n1.91\n1.90\nâ†“0.02\nâ†“0.01\nTable 10 Speech pretraining validation loss on the LibriSpeech dataset. Derf achieves lower validation loss than both\nLN and DyT across two wav2vec 2.0 models, indicating its better representation quality.\nDNA models. For the long-range DNA sequence modeling task, we pretrain the HyenaDNA model (Nguyen\net al., 2023) and the Caduceus model (Schiff et al., 2024) using the human reference genome from (GRCh38,\n2013). Model evaluation is conducted on the GenomicBenchmarks dataset (GreÅ¡ovÃ¡ et al., 2023). We report\nthe averaged accuracy over all subtasks. As shown in Table 11, Derf surpasses both normalization layers and\nDyT in performance, demonstrating its robustness in genomic sequence modeling.\nmodel\nNorm\nDyT\nDerf\nÎ”Norm\nÎ”DyT\nHyena\n85.2%\n85.2%\n85.7%\nâ†‘0.5%\nâ†‘0.5%\nCaduceus\n86.9%\n86.9%\n87.3%\nâ†‘0.4%\nâ†‘0.4%\nTable 11 DNA classification accuracy on the GenomicBenchmarks dataset, averaged over each subtask. Each model\nis evaluated with its default normalization layer (LN for Heyna, RMSNorm for Caduceus). Derf consistently achieves\nhigher accuracy than both normalization layers and DyT, indicating its effectiveness in DNA model.\n9"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 10, "text": "Language models. We pretrain a GPT-2 (124M) model on the OpenWebText dataset and report the validation\nloss in Table 12. For DyT and Derf, we additionally finetune the initialization of the learnable parameter ğ›¼.\nWe observe that Derf achieves comparable performance to LN, while clearly outperforming DyT.\nmodel\nLN\nDyT\nDerf\nÎ”LN\nÎ”DyT\nGPT-2\n2.94\n2.97\n2.94\n0.00\nâ†“0.03\nTable 12 GPT-2 validation loss on the OpenWebText dataset. Derf matches the performance of LN while achieving\nlower validation loss than DyT.\n6.1 Stronger Generalization or Better Fitting?\nGiven Derfâ€™s superior performance, we aim to determine whether the gains arise from improved fitting capacity\nor stronger generalization. To this end, we compare the training loss of models respectively trained with\nnormalization layers, DyT, and Derf. Since lower training loss indicates stronger fitting ability, this comparison\nhelps us assess whether Derf improves optimization or enhances generalization.\nSetup. We compute training losses across diverse architectures and scales. To measure fitting capacity fairly,\nwe do not use the loss during optimization, which is confounded by stochastic regularization (e.g., stochastic\ndepth (Huang et al., 2016a)) and train-time augmentations. Instead, after training, we switch to evaluation\nmode, disable stochastic depth (when present), adopt the test-time preprocessing pipeline, and compute the\nloss on the training set. This yields a fair estimate of each modelâ€™s fitting capacity. In Appendix E, we provide\nthe detailed procedure for computing the evaluation-mode training loss for each model.\nResults. Across all architectures and scales, both Derf and DyT result in higher training loss than normalization-\nbased models, with Derf generally yielding slightly lower training loss than DyT, as shown in Table 13. This\nconsistent pattern indicates that neither Derf nor DyT improves fitting capacity over normalization layers.\nmodel\nNorm\nDerf\nDyT\nViT-B\n0.2623\n0.2681\n0.2714\nViT-L\n0.2034\n0.2066\n0.2083\nDiT-B\n0.1531\n0.1533\n0.1535\nDiT-L\n0.1501\n0.1510\n0.1518\nDiT-XL\n0.1432\n0.1436\n0.1440\nwav2vec 2.0 B\n1.8509\n1.8821\n1.8946\nwav2vec 2.0 L\n1.8241\n1.8563\n1.8641\nHyena\n1.1297\n1.1526\n1.1631\nCaduceus\n0.8917\n0.9129\n0.9203\nGPT-2\n2.9478\n2.9702\n2.9822\nTable 13 Evaluation-mode training loss of normalization layers (Norm), Derf, and DyT after optimization. Bolded\nindicates the lowest loss, and underlined means the second-lowest loss. Across all model architectures, the training loss\nfollows the relation: Norm < Derf < DyT. Both DyT and Derf exhibit higher training loss than normalization layers,\nwhile Derf achieves slightly lower loss than DyT.\nDiscussion. Despite the reduced fitting capacity, Derf delivers consistent performance gains across all evaluated\ntasks. We hypothesize that these gains arise primarily from both better generalization than normalization\nlayers and stronger fitting capacity than DyT.\nFirstly, point-wise functions promote stronger generalization. Although Derf yields higher training loss, it\nachieves superior downstream performance, indicating that its benefits stem not from improved fitting but\nfrom enhanced generalization. This difference likely originates from the contrasting operational principles\nbetween normalization layers and point-wise functions. Normalization layers adapt their transformation\n10"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 11, "text": "based on training statistics, allowing them to dynamically fit activation distributions throughout training.\nIn contrast, point-wise functions are controlled by only a small set of learnable scalar parameters (e.g., ğ›¼\nfor DyT and ğ›¼, ğ‘ for Derf) that do not adapt to activation statistics after training. They apply the same\ntransformation regardless of activation distribution. This limited adaptability constrains overfitting and\neffectively serves as an implicit regularizer, leading to improved generalization.\nSecondly, Derf exhibits stronger fitting power than DyT. It achieves lower training loss while retaining the\nimplicit regularization of point-wise functions, combining higher fitting capacity with strong generalization to\noutperform both DyT and normalization-based models.\n7 Analysis\nIn this section, we begin with two ablation studies examining the influence of the learnable shift parameter ğ‘ \non the training results, followed by an analysis of an approximation of Derf.\n7.1 Effect of s\nRemoving ğ‘ . We investigate the effect of the learnable scalar parameter ğ‘ by removing it from the point-wise\nfunction. As shown in Table 14, introducing this learnable shift consistently improves the overall training\nperformance, and the degree of improvement varies across different functions. The stronger results of erf(ğ‘¥)\nover tanh(ğ‘¥) indicate that Derf surpasses DyT not only because of the shift ğ‘ .\ntop-1 acc â†‘\nFID â†“\nfunction\nwithout ğ‘ \nwith ğ‘ \nwithout ğ‘ \nwith ğ‘ \nerf(ğ‘¥)\n82.6%\n82.8%\n63.39\n63.23\ntanh(ğ‘¥)\n82.5%\n82.6%\n63.94\n63.71\nsatursin(ğ‘¥)\n82.4%\n82.6%\n65.28\n63.90\nisru(ğ‘¥)\n82.2%\n82.3%\n66.14\n65.72\narctan(ğ‘¥)\n82.3%\n82.4%\n67.41\n67.07\narcsinhclip(ğ‘¥)\n82.4%\n82.5%\n65.19\n64.72\nTable 14 Ablation study of ğ‘ . Top-1 accuracy on ViT-Base and FID score on DiT-B/4, comparing models with and\nwithout ğ‘ . ğ‘ improves the overall training performance, while its effect varies across different point-wise functions.\nScalar vs. vector ğ‘ . We further examine whether using a per-channel vector parameter instead of a scalar ğ‘ \nleads to any performance improvement. As shown in Table 15, across all three point-wise functions, the choice\nbetween a scalar and a per-channel vector shows no significant impact on the final performance. Therefore,\nwe adopt the scalar form of ğ‘ for efficiency and simplicity during training.\nfunction\nvector\nscalar\nerf(ğ‘¥)\n82.8%\n82.8%\narctan(ğ‘¥)\n82.5%\n82.4%\narcsinhclip(ğ‘¥)\n82.5%\n82.5%\nTable 15 Top-1 accuracy of scalar vs. vector ğ‘ on\nViT-Base. Using either a scalar or a per-channel vector\nfor the parameter ğ‘ yields nearly identical performance.\nfunction\nViT-B\nViT-L\nDiT-B\nDiT-L\ntanh(ğ‘¥)\n82.6%\n83.6%\n63.71\n45.48\ntanh(ğœ€ğ‘¥)\n82.7%\n83.7%\n63.88\n45.13\nerf(ğ‘¥)\n82.8%\n83.8%\n63.23\n43.94\nTable 16 Top-1 accuracy of tanh(ğœ€ğ‘¥) on ViT and DiT.\ntanh(ğœ€ğ‘¥) yields a comparable or slightly improved perfor-\nmance over tanh(ğ‘¥) but still remains below erf(ğ‘¥).\n11"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 12, "text": "7.2 Approximating Derf\nGiven the superior performance of erf(ğ‘¥) over tanh(ğ‘¥), we approximate erf(ğ‘¥) by scaling tanh(ğ‘¥) and examine\nwhether this modification can lead to performance improvement. We introduce a fixed coefficient ğœ€and use\ntanh(ğœ€ğ‘¥), where ğœ€is obtained by minimizing the following objective:\nmin\nğœ€\nâˆ«ï¸+âˆ\nâˆ’âˆ\nâƒ’âƒ’tanh(ğœ€ğ‘¥) âˆ’erf(ğ‘¥)\nâƒ’âƒ’ğ‘‘ğ‘¥.\n(11)\nThe optimal value is found to be ğœ€â‰ˆ1.205. As shown in Table 16, tanh(ğœ€ğ‘¥) achieves a comparable or slightly\nimproved performance over the original tanh(ğ‘¥), while still performing worse than erf(ğ‘¥). This indicates that\nsimply scaling tanh(ğ‘¥) is insufficient to match the behavior or performance of erf(ğ‘¥).\n8 Related Work\nNormalization layers. Since the introduction of Batch Normalization (BN) (Ioffe and Szegedy, 2015), various\nnormalization methods have been proposed to better stabilize training. To address BNâ€™s limitations with\nsmall batches, several alternatives (Salimans and Kingma, 2016; Wu and He, 2018; Yan et al., 2020; Shen\net al., 2020; Singh and Krishnan, 2020) have been explored. In parallel, LayerNorm (Ba et al., 2016; Nguyen\nand Salazar, 2019; Xu et al., 2019; Xiong et al., 2020) and RMSNorm (Zhang and Sennrich, 2019) were\ndesigned for RNN (Hochreiter and Schmidhuber, 1997) and Transformer architectures (Vaswani et al., 2017).\nTask-specific variants (Ulyanov et al., 2016; Wu and He, 2018; Shen et al., 2020) further adapt normalization\nto applications such as object detection and style transfer.\nMechanisms of normalization. A series of studies has investigated how normalization layers contribute to\nmodel convergence. From an optimization perspective, normalization stabilizes gradient flow (Balduzzi et al.,\n2017; Daneshmand et al., 2020; Lubana et al., 2021), reduces sensitivity to initialization (Zhang et al., 2019;\nDe and Smith, 2020; Shao et al., 2020), and implicitly tunes learning rates (Arora et al., 2019; Tanaka and\nKunin, 2021). It has also been shown to smooth the loss landscape (Santurkar et al., 2018; Bjorck et al., 2018;\nKarakida et al., 2019) and reduce sharpness (Lyu et al., 2022; Dai et al., 2023; Mueller et al., 2023), promoting\nmore stable optimization dynamics. Understanding these underlying functionalities provides valuable guidance\nfor designing normalization-free training methods.\nNormalization-free methods. Building on this understanding of normalization, recent work explores how to\nachieve stable convergence without normalization. One line of work operates at the parameter and optimization\nlevel, using tailored initialization schemes (Bachlechner et al., 2021; De and Smith, 2020; Zhang et al., 2019),\nself-normalizing activations (Klambauer et al., 2017), weight normalization (Salimans and Kingma, 2016; Brock\net al., 2021a), or adaptive gradient clipping (Brock et al., 2021b) to maintain stable gradient propagation.\nAnother line of work modifies the architecture through structural simplifications (He and Hofmann, 2024) and\nSoftmax-only formulations (Jha and Reagen, 2024). More recently, point-wise functions such as Dynamic Tanh\n(Zhu et al., 2025) have been proposed, with theoretical analyses revealing their similarity to normalization\noperations (Stollenwerk, 2025). Unlike previous methods that aim to match the performance of normalization\nlayers, Derf consistently delivers stronger performance across diverse models.\n9 Conclusion\nIn this work, we demonstrate that well-designed point-wise functions do not merely match the performance of\nnormalization layers, but can surpass them. By revisiting the design space of point-wise functions, we identify\nzero-centeredness, boundedness, center sensitivity, and monotonicity as four key properties that enable strong\nperformance in Transformer-based models. Among the functions satisfying these properties, Derf stands out\nas the most effective design: it consistently outperforms normalization-based methods and another notable\npoint-wise function, DyT, across a wide range of modalities and tasks. Its simplicity and strong empirical\nperformance make Derf a compelling replacement for normalization layers in many Transformer architectures.\n12"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 13, "text": "Acknowledgments\nWe gratefully acknowledge the use of the Neuronic GPU computing cluster maintained by the Department of\nComputer Science at Princeton University. This work was substantially performed using Princeton Research\nComputing resources, a consortium led by the Princeton Institute for Computational Science and Engineering\n(PICSciE) and Research Computing at Princeton University. This work is also supported by the computational\nresources generously provided by Googleâ€™s TPU Research Cloud program.\nReferences\nSanjeev Arora, Zhiyuan Li, and Kaifeng Lyu. Theoretical analysis of auto rate-tuning by batch normalization.\nICLR, 2019.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\nLayer normalization.\narXiv preprint\narXiv:1607.06450, 2016.\nThomas Bachlechner, Bodhisattwa Prasad Majumder, Henry Mao, Gary Cottrell, and Julian McAuley. Rezero\nis all you need: Fast convergence at large depth. In UAI, 2021.\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for\nself-supervised learning of speech representations. In NeurIPS, 2020.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.\nDavid Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The\nshattered gradients problem: If resnets are the answer, then what is the question? In ICML, 2017.\nNils Bjorck, Carla P Gomes, Bart Selman, and Kilian Q Weinberger. Understanding batch normalization. In\nNeurIPS, 2018.\nAndrew Brock, Soham De, and Samuel L Smith. Characterizing signal propagation to close the performance\ngap in unnormalized resnets. ICLR, 2021a.\nAndrew Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-scale image\nrecognition without normalization. In ICML, 2021b.\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\nEnd-to-end object detection with transformers. In ECCV, 2020.\nZhaodong Chen, Lei Deng, Guoqi Li, Jiawei Sun, Xing Hu, Ling Liang, Yufei Ding, and Yuan Xie. Effective\nand efficient batch normalization using a few uncorrelated data for statistics estimation. IEEE Transactions\non Neural Networks and Learning Systems, 2020.\nYan Dai, Kwangjun Ahn, and Suvrit Sra. The crucial role of normalization in sharpness-aware minimization.\nIn NeurIPS, 2023.\nHadi Daneshmand, Jonas Kohler, Francis Bach, Thomas Hofmann, and Aurelien Lucchi. Batch normalization\nprovably avoids ranks collapse for randomly initialised deep networks. In NeurIPS, 2020.\nSoham De and Sam Smith. Batch normalization biases residual blocks towards the identity function in deep\nnetworks. In NeurIPS, 2020.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In CVPR, 2009.\nAlexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\n13"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 14, "text": "Ensembl GRCh38. p13 (genome reference consortium human build 38), insdc assembly, 2013.\nKatarÃ­na GreÅ¡ovÃ¡, Vlastimil Martinek, David ÄŒechÃ¡k, Petr Å imeÄek, and Panagiotis Alexiou. Genomic\nbenchmarks: a collection of datasets for genomic sequence classification. BMC Genomic Data, 2023.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma,\nPeiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement\nlearning. arXiv preprint arXiv:2501.12948, 2025.\nBobby He and Thomas Hofmann. Simplifying transformer blocks. ICLR, 2024.\nStefan Heimersheim. You can remove gpt2â€™s layernorm by fine-tuning. arXiv preprint arXiv:2409.13710, 2024.\nSepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural Computation, 1997.\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Weinberger. Deep networks with stochastic depth,\n2016a.\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic\ndepth. In ECCV, 2016b.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. In ICML, 2015.\nNandan Kumar Jha and Brandon Reagen. Aero: Softmax-only llms for efficient private inference. arXiv\npreprint arXiv:2410.13060, 2024.\nRyo Karakida, Shotaro Akaho, and Shun-ichi Amari. The normalization method for alleviating pathological\nsharpness in wide neural networks. In NeurIPS, 2019.\nGÃ¼nter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural\nnetworks. In NeurIPS, 2017.\nXiangru Lian and Ji Liu. Revisit batch normalization: New understanding and refinement via composition\noptimization. In AISTATS, 2019.\nAixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan,\nDamai Dai, Daya Guo, et al. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language\nmodel. arXiv preprint arXiv:2405.04434, 2024.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021.\nZhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet\nfor the 2020s. In CVPR, 2022.\nEkdeep S Lubana, Robert Dick, and Hidenori Tanaka. Beyond batchnorm: Towards a unified understanding\nof normalization in deep learning. In NeurIPS, 2021.\nKaifeng Lyu, Zhiyuan Li, and Sanjeev Arora. Understanding the generalization benefit of normalization\nlayers: Sharpness reduction. In NeurIPS, 2022.\nMaximilian Mueller, Tiffany Vlaar, David Rolnick, and Matthias Hein. Normalization layers are all that\nsharpness-aware minimization needs. In NeurIPS, 2023.\nEric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Michael Wornow, Callum Birch-Sykes, Stefano\nMassaroli, Aman Patel, Clayton Rabideau, Yoshua Bengio, et al. Hyenadna: Long-range genomic sequence\nmodeling at single nucleotide resolution. In NeurIPS, 2023.\nToan Q Nguyen and Julian Salazar. Transformers without tears: Improving the normalization of self-attention.\nIWSLT, 2019.\n14"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 15, "text": "Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on\npublic domain audio books. In ICASSP, 2015.\nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\nJMLR, 2020.\nTim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate training\nof deep neural networks. In NeurIPS, 2016.\nShibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normalization\nhelp optimization? In NeurIPS, 2018.\nYair Schiff, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. Caduceus:\nBi-directional equivariant long-range dna sequence modeling. In ICML, 2024.\nJie Shao, Kai Hu, Changhu Wang, Xiangyang Xue, and Bhiksha Raj. Is normalization indispensable for\ntraining deep neural network? In NeurIPS, 2020.\nSheng Shen, Zhewei Yao, Amir Gholami, Michael Mahoney, and Kurt Keutzer. Powernorm: Rethinking batch\nnormalization in transformers. In ICML, 2020.\nSaurabh Singh and Shankar Krishnan. Filter response normalization layer: Eliminating batch dependence in\nthe training of deep neural networks. In CVPR, 2020.\nFelix Stollenwerk.\nThe mathematical relationship between layer normalization and dynamic activation\nfunctions. arXiv preprint arXiv:2503.21708, 2025.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the\ninception architecture for computer vision. In CVPR, 2016.\nHidenori Tanaka and Daniel Kunin. Noetherâ€™s learning dynamics: Role of symmetry breaking in neural\nnetworks. In NeurIPS, 2021.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix,\nBaptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and\nfine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\nDmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for\nfast stylization. arXiv preprint arXiv:1607.08022, 2016.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\nYuxin Wu and Kaiming He. Group normalization. In ECCV, 2018.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan,\nLiwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In ICML, 2020.\nJingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin. Understanding and improving\nlayer normalization. In NeurIPS, 2019.\nJunjie Yan, Ruosi Wan, Xiangyu Zhang, Wei Zhang, Yichen Wei, and Jian Sun. Towards stabilizing batch\nstatistics in backward propagation of batch normalization. ICLR, 2020.\n15"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 16, "text": "An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li,\nDayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024.\nQiming Yang, Kai Zhang, Chaoxiang Lan, Zhi Yang, Zheyang Li, Wenming Tan, Jun Xiao, and Shiliang Pu.\nUnified normalization for accelerating and stabilizing transformers. In ACM MM, 2022.\nSangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix:\nRegularization strategy to train strong classifiers with localizable features. In ICCV, 2019.\nBiao Zhang and Rico Sennrich. Root mean square layer normalization. In NeurIPS, 2019.\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk\nminimization. 2018.\nHongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without normalization.\nICLR, 2019.\nZhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In\nAAAI, 2020.\nJiachen Zhu, Xinlei Chen, Kaiming He, Yann LeCun, and Zhuang Liu. Transformers without normalization.\nIn CVPR, 2025.\nAppendix\nA Property Analysis Details\nIn this section, we provided detailed explanation and visualization on how different function properties affect\nmodel training.\nA.1 Zero-centeredness\nWe plot the training curves for ğœ†horiz and ğœ†vert with values {0, 0.1, 1} in Figure 6. The trends are consistent\nwith those observed in top-1 accuracy on ImageNet-1K. For horizontal shifts, the training loss with ğœ†horiz = 0.1\nnearly overlaps with that of ğœ†horiz = 0, and even reaches a slightly lower loss. In contrast, vertical shifts\nexhibit a monotonic pattern: increasing ğœ†vert consistently raises the training loss, suggesting reduced fitting\ncapacity under larger vertical shift.\n0\n50\n100\n150\n200\n250\n300\nepoch\n3\n4\n5\n6\n7\nloss\nÎ» = 0\nÎ» = 0.1\nÎ» = 1\n(a) Horizontal shift\n0\n50\n100\n150\n200\n250\n300\nepoch\n3\n4\n5\n6\n7\nloss\nÎ» = 0\nÎ» = 0.1\nÎ» = 1\n(b) Vertical shift\nFigure 6 Training loss curve for horizontal and vertical shifts on the base point-wise function erf(ğ‘¥). The trends are\nconsistent with the patterns observed in top-1 accuracy on ImageNet-1K.\nA.2 Center Sensitivity\nWe visualize the training losses obtained as ğœ†varies over {0, 0.1, 0.5, 1.0, 2.0} on the base point-wise function\nerf(ğ‘¥). As shown in Figure 7, training loss shows a clear monotonic trend: larger ğœ†consistently leads to\nhigher loss, indicating that the width of the flat zone directly limits the modelâ€™s fitting capacity.\n16"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 17, "text": "0\n50\n100\n150\n200\n250\n300\nepoch\n3\n4\n5\n6\n7\nloss\nÎ» = 0\nÎ» = 0.1\nÎ» = 0.5\nÎ» = 1.0\nÎ» = 2.0\nFigure 7 Training loss curve for different center\nsensitivity (controlled by ğœ†). A larger ğœ†leads to\nhigher training loss and poorer fitting ability.\n0\n50\n100\n150\n200\n250\n300\nepoch\n3\n4\n5\n6\n7\nloss\nerf(x)\nsin(x)\nnegerf(x)\ndampx(x)\nFigure 8 Training loss curve for different monotonicity. Mono-\ntonic functions consistently achieve lower training loss than\nnon-monotonic functions.\nA.3 Monotonicity\nWe plot the training losses of four functions with distinct monotonicity patterns: the monotonically increasing\nerf(ğ‘¥), the monotonically decreasing negerf(ğ‘¥), the hump-shaped dampx(ğ‘¥), and the oscillatory sin(ğ‘¥). As\nshown in Figure 8, both increasing and decreasing monotonic functions achieve clearly lower training loss,\nindicating stronger fitting capacity. In contrast, the non-monotonic functions exhibit higher training loss.\nThis behavior aligns closely with the top-1 accuracy trends observed on ImageNet-1K.\nB Function Search Details\nIn function search, a wide variety of common functional forms are systematically explored under the constraint\nof our function properties. The candidates range from polynomial and rational functions to the trigonometric\nand hyperbolic families, as well as various cumulative distribution functions. Beyond these common functional\nforms, we also experiment with their variants through translation, scaling, concatenation, and clipping.\nWe categorize all candidate functions (see Table 7) into four groups: natural functions, transformed basic\nfunctions, clipped unbounded functions, and canonical ratio functions, and present detailed descriptions and\nvisualizations of how each group is constructed.\nNatural functions. This category consists of three functions: erf(ğ‘¥), tanh(ğ‘¥), and arctan(ğ‘¥). As shown in\nFigure 9, these functions naturally satisfy all the function properties, including zero-centeredness, boundedness,\ncenter sensitivity, and monotonicity. Among them, only arctan(ğ‘¥) is rescaled so that all three functions have\ntheir ranges unified to [âˆ’1, 1].\nTransformed basic functions. This category consists of six functions: satursin(ğ‘¥), expsign(ğ‘¥), exproot(ğ‘¥),\nrelsign(ğ‘¥), isru(ğ‘¥), and cubsign(ğ‘¥). These functions are constructed by starting from simple and commonly\nused primitives, such as power functions and polynomial forms. Through transformations including translation,\nscaling, and rotation, we reshape their original structures so that they satisfy all four function properties\nwhile preserving the qualitative behavior of the underlying base functions, as shown in Figure 10.\nClipped unbounded functions. This category consists of five functions: logsign(ğ‘¥), logquad(ğ‘¥), arcsinh(ğ‘¥),\npower23(ğ‘¥), and linear(ğ‘¥). These functions inherently satisfy zero-centeredness and center sensitivity. For\nlogsignclip(ğ‘¥), logquad(ğ‘¥), and power23clip(ğ‘¥), either due to domain asymmetry or because the original form\nis not monotonic, we construct the negative branch by mirroring the positive side around the origin to ensure\nmonotonicity, as shown in Figure 11. To additionally enforce boundedness, we clip their outputs to the interval\n[âˆ’1, 1], which leads to improved performance in practice.\nCanonical ratio functions. This category consists of two functions: saturlog(ğ‘¥) and smoothsign(ğ‘¥). Both\nfunctions are constructed using the canonical ratio form\nğ‘“(ğ‘¥)\n|ğ‘“(ğ‘¥)|+1, which naturally enforces boundedness\nand monotonicity. By selecting ğ‘“(ğ‘¥) to be an odd, zero-centered base function, the resulting ratio form\nautomatically satisfies zero-centeredness and center sensitivity as well. As shown in Figure 12, this construction\nyields smooth saturating behaviors that remain stable across a wide input range.\n17"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 18, "text": "-3\n-2\n-1\n0\n1\n2\n3\n-1\n-0.5\n0\n0.5\n1\nerf(x)\ntanh(x)\narctan(x)\nFigure 9 Visualization of natural functions.\n4\n2\n0\n2\n4\n1.0\n0.5\n0.0\n0.5\n1.0\nOriginal Function\nsin(x)\n4\n2\n0\n2\n4\n1.0\n0.5\n0.0\n0.5\n1.0\nAfter Transformation\nsatursin(x)\n2\n0\n2\n4\n0\n1\n2\n3\n4\n5\neâˆ’x\n4\n2\n0\n2\n4\n1.0\n0.5\n0.0\n0.5\n1.0\nexpsign(x)\n0\n2\n4\n6\n8\n0.00\n0.25\n0.50\n0.75\n1.00\neâˆ’\npx\n4\n2\n0\n2\n4\n1.0\n0.5\n0.0\n0.5\n1.0\nexproot(x)\n4\n2\n0\n2\n4\n1\n0\n1\n2\n3\n4\np\n1 + x2\n4\n2\n0\n2\n4\n1.0\n0.5\n0.0\n0.5\n1.0\nrelsign(x)\n0\n1\n2\n3\n4\n0.0\n0.5\n1.0\n1.5\n(1 + x2)âˆ’1\n4\n2\n0\n2\n4\n1.0\n0.5\n0.0\n0.5\n1.0\nisru(x)\n4\n2\n0\n2\n4\n20\n10\n0\n10\n20\nxâˆ’3\n4\n2\n0\n2\n4\n1.0\n0.5\n0.0\n0.5\n1.0\ncubsign(x)\nFigure 10 Visualization of transformed basic functions.\n0\n1\n2\n3\n4\n0\n1\n2\n3\nOriginal Function\nln(x + 1)\n4\n2\n0\n2\n4\n1.0\n0.5\n0.0\n0.5\n1.0\nAfter Transformation\nlogsignclip(x)\n0\n1\n2\n3\n4\n0\n1\n2\n3\nln(x2 + 1)\n4\n2\n0\n2\n4\n1.0\n0.5\n0.0\n0.5\n1.0\nlogquadclip(x)\n4\n2\n0\n2\n4\n2\n1\n0\n1\n2\narcsinh(x)\n4\n2\n0\n2\n4\n1.0\n0.5\n0.0\n0.5\n1.0\narcsinhclip(x)\n0\n1\n2\n3\n4\n0\n1\n2\n3\nx2/3\n4\n2\n0\n2\n4\n1.0\n0.5\n0.0\n0.5\n1.0\npower23clip(x)\n4\n2\n0\n2\n4\n4\n2\n0\n2\n4\nx\n4\n2\n0\n2\n4\n1.0\n0.5\n0.0\n0.5\n1.0\nlinearclip(x)\nFigure 11 Visualization of clipped unbounded functions.\n4\n2\n0\n2\n4\n4\n2\n0\n2\n4\nOriginal Function\nx\n4\n2\n0\n2\n4\n1.0\n0.5\n0.0\n0.5\n1.0\nAfter Transformation\nsmoothsign(x)\n8\n4\n0\n4\n8\n2\n1\n0\n1\n2\nsign(x) Â· ln(|x| + 1)\n4\n2\n0\n2\n4\n1.0\n0.5\n0.0\n0.5\n1.0\nsaturlog(x)\nFigure 12 Visualization of canonical ratio functions.\n18"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 19, "text": "C Experimental Settings\nVision Transformers. For all supervised classification experiments on ImageNet-1K, we adopt the training\nconfigurations summarized in Table 17. ViT-B and ViT-L share the same hyperparameters, except that ViT-L\nemploys a modified AdamW momentum setting with (ğ›½1=0.9, ğ›½2=0.95) and a higher stochastic depth rate of\n0.5.\nconfig\nvalue\noptimizer\nAdamW\nbase learning rate\n4e-3\nweight decay\n0.05\noptimizer momentum\nğ›½1=0.9, ğ›½2=0.999 (ğµ), 0.95 (ğ¿)\neffective batch size\n4096\nlearning rate schedule\ncosine decay\nwarmup epochs\n20\ntraining epochs\n300\naugmentation\nrand-m9-mstd0.5-inc1\nlabel smoothing (Szegedy et al., 2016)\n0.1\nmixup (Zhang et al., 2018)\n0.8\ncutmix (Yun et al., 2019)\n1.0\nrandom erase (Zhong et al., 2020)\n0.25\ndrop path (Huang et al., 2016b)\n0.15 (B), 0.5 (L)\nexp. moving average (EMA)\n0.9999\nTable 17 Training Configurations of ViT.\nDiffusion Transformers. We use the official implementation (Peebles and Xie, 2023) to train all DiT model\nsizes as shown in Table 18. We observe that the default learning rate is suboptimal for the models in this\nwork. For both the search function experiments and the final evaluation of Derf, we go through three learning\nrates, 1 Ã— 10âˆ’4, 2 Ã— 10âˆ’4, and 4 Ã— 10âˆ’4, for all models, whether they use LayerNorm or a point-wise function,\nand report the best result. We also observe that the zero initialization negatively affects the performance of\nDerf models and other point-wise function models. Therefore, we retain the zero initialization for LN models\nbut remove it for the other models.\nconfig\nvalue\noptimizer\nAdamW\nbase learning rate\n{1e-4, 2e-4, 4e-4}\nweight decay\n0\noptimizer momentum\nğ›½1=0.9, ğ›½2=0.999\neffective batch size\n256\nlearning rate schedule\nconstant\ntraining epochs\n80\nexp. moving average (EMA)\n0.9999\nTable 18 Training Configurations of DiT.\nSpeech models. For both wav2vec 2.0 models, we retain the first GroupNorm layer and the LayerNorm\nlocated after the convolutional feature extractor, since both primarily serve as data normalization to handle\nthe unnormalized input data. We use the official implementation (Baevski et al., 2020) for both the Base and\nLarge models, keeping all hyperparameters identical to the original setup, as shown in Table 19. The only\nchange we make is running all modelsâ€”whether normalization-based or point-wise-function-basedâ€”in fp32\nprecision instead of the default bf16. We report the final validation loss.\nDNA models. For both the HyenaDNA model (Nguyen et al., 2023) and the Caduceus model (Schiff et al.,\n2024), we directly follow their official implementations without modifying hyperparameters, as shown in\nTable 20. In particular, Hyena uses LayerNorm and Caduceus uses RMSNorm. For our evaluation, we replace\neach modelâ€™s original normalization layer with Derf and report the average accuracy across all tasks.\nLanguage models. For the GPT-2 (124M) model, we follow the hyperparameters as shown in Table 21.\nFor Derf and DyT, we configure the ğ›¼initialization separately for the point-wise function layer following\n19"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 20, "text": "config\nvalue\noptimizer\nAdam\nlearning rate\n5e-4 (B), 3e-4 (L)\nweight decay\n0.01\noptimizer momentum\nğ›½1=0.9, ğ›½2=0.98\nmax tokens\n1400000 (B), 1200000 (L)\nlearning rate schedule\npolynomial decay\nwarmup updates\n32000 (B), 20000 (L)\nmax updates\n400000 (B), 250000 (L)\ndropout (input to encoder)\n0.1\ndropout (target features)\n0.1\ndropout (transformer)\n0.0 (B), 0.1 (L)\nlayer dropout\n0.05 (B), 0.2 (L)\nfeature grad mult\n0.1\nlatent temp\n[2,0.5,0.999995] (B), [2.0,0.1,0.999995] (L)\nmax sample size\n250000 (B), 320000 (L)\nTable 19 Training Configurations of wav2vec 2.0.\nconfig\nvalue\noptimizer\nAdamW\nlearning rate\n6e-4 (H), 8e-3 (C)\nsequence length\n1024 (H), 131072 (C)\neffective batch size\n1024 (H), 8 (C)\ntraining steps\n10000 (H), 50000 (C)\nRC augmentation\ntrue (H), false (C)\nMLM probability\n0.0 (H), 0.15 (C)\nbidirectional\nfalse (H), true (C)\nTable 20 Training Configurations of HyenaDNA and Caduceus. H denotes HyenaDNA, C denotes Caduceus.\nthe attention layer and for the other point-wise function layers. We try multiple combinations of these\ninitialization settings and report the best validation loss.\nconfig\nvalue\noptimizer\nAdamW\nbase learning rate\n6e-4\nweight decay\n0.1\noptimizer momentum\nğ›½1=0.9, ğ›½2=0.95\ngradient clipping\n1.0\nblock size\n1024\ngradient accumulation steps\n40\neffective batch size\n491,520\nlearning rate schedule\ncosine decay\nwarmup iterations\n2,000\ntraining iterations\n300,000\ndropout\n0.0\nmixed precision\nbf16\nTable 21 Training Configurations of GPT-2 (124M).\nD Additional Results\nBeyond evaluating each model with its default normalization layer (typically LN), we additionally test\nRMSNorm and GroupNorm (GN) to enable a more complete comparison. RMSNorm is widely used in modern\nlarge language models, including T5 (Raffel et al., 2020), LLaMA (Touvron et al., 2023a,b; Dubey et al.,\n2024), Qwen (Bai et al., 2023; Yang et al., 2024), and DeepSeek (Liu et al., 2024; Guo et al., 2025), while\nGN is employed in several vision architectures, including ConvNeXt (Liu et al., 2022), DETR (Carion et al.,\n2020), and Swin Transformer (Liu et al., 2021).\nAll evaluations follow the same experimental settings described in the previous section. These additional\nresults show that Derf not only surpasses the default choices used in each model, but also outperforms the\n20"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 21, "text": "other normalization alternatives we evaluate.\nVision Transformers. For both ViT-Base and ViT-Large (Dosovitskiy, 2021), the default normalization layer\nis LayerNorm. To complement the results, we also evaluate RMSNorm (Zhang and Sennrich, 2019) and GN\n(Wu and He, 2018) as additional replacements in Table 22. Compared to all other methods, Derf achieves\nclearly higher top-1 accuracy, demonstrating its effectiveness in vision transformer architectures.\nmodel\nLN\nDyT\nDerf\nRMSNorm\nGN\nViT-B\n82.3%\n82.5%\n82.8%\n82.4%\n82.5%\nViT-L\n83.1%\n83.6%\n83.8%\n83.0%\n83.1%\nTable 22 Supervised classification accuracy on ImageNet-1K. Derf achieves higher top-1 accuracy than all other\nmethods on different model sizes.\nDiffusion Transformers. For DiT models (Peebles and Xie, 2023), we additionally evaluate RMSNorm (Zhang\nand Sennrich, 2019) as an alternative normalization layer and compare its performance with LN, DyT, and\nDerf. As shown in Table 23, Derf achieves a clear improvement in FID compared to all other methods.\nmodel\nLN\nDyT\nDerf\nRMSNorm\nDiT-B/4\n64.93\n63.94\n63.23\n65.08\nDiT-L/4\n45.91\n45.66\n43.94\n45.02\nDiT-XL/2\n19.94\n20.83\n18.92\n20.76\nTable 23 Image generation quality (FID) on ImageNet. Lower FID indicates better image generation quality. Derf\nachieves lower FID scores than all other methods across different DiT models.\nSpeech models. For two wav2vec 2.0 Transformer models (Baevski et al., 2020), we additionally evaluate\nRMSNorm (Zhang and Sennrich, 2019) as an alternative normalization layer and compare its performance\nwith LN, DyT, and Derf in Table 24. Compared to other methods, Derf yields lower validation loss on different\nmodel sizes\nmodel\nLN\nDyT\nDerf\nRMSNorm\nwav2vec 2.0 Base\n1.95\n1.95\n1.93\n1.95\nwav2vec 2.0 Large\n1.92\n1.91\n1.90\n1.93\nTable 24 Speech pretraining validation loss on the LibriSpeech dataset. Derf achieves lower validation loss than all\nother methods across two wav2vec 2.0 models.\nDNA models. For the HyenaDNA model (Nguyen et al., 2023) and the Caduceus model (Schiff et al., 2024),\nwe additionally evaluate both LayerNorm and RMSNorm for each architecture, regardless of their default\nchoices, and compare their performance with DyT and Derf in Table 25.\nmodel\nLN\nDyT\nDerf\nRMSNorm\nHyena\n85.2%\n85.2%\n85.7%\n85.2%\nCaduceus\n87.0%\n86.9%\n87.3%\n86.9%\nTable 25 DNA classification accuracy on the GenomicBenchmarks dataset, averaged over each subtask.\nDerf\nconsistently outperforms other methods across two different DNA models.\nLanguage models. For the GPT-2 (124M) model, we additionally evaluate RMSNorm (Zhang and Sennrich,\n2019) for a more complete comparison of normalization choices.\nAs shown in Table 26, Derf achieves\ncomparable performance to both LN and RMSNorm, while clearly outperforming DyT.\n21"}
{"pdf_id": "arxiv_251210938_stronger_normalization_free", "page": 22, "text": "model\nLN\nDyT\nDerf\nRMSNorm\nGPT-2\n2.94\n2.97\n2.94\n2.95\nTable 26 GPT-2 validation loss on the OpenWebText dataset. Derf matches the performances of both LayerNorm\nand RMSNorm while achieving lower validation loss than DyT.\nE Loss Calculation Details\nVision Transformers. For ViT models, we measure fitting capacity under a deterministic evaluation setup.\nWe switch the model to evaluation mode, disable drop-path, mixup, cutmix, label smoothing, and all data\naugmentations, and apply only the standard test-time preprocessing (center crop and normalize). The\ncross-entropy loss is then computed on the training set and averaged over all samples.\nDiffusion Transformers. For DiT models, we evaluate fitting capacity by switching the model to evaluation\nmode. We apply the standard test-time preprocessing (center crop, random horizontal flip, and normalize).\nSince DiT does not employ drop-path, no stochastic regularization needs to be disabled. We then compute\nthe diffusion MSE loss over the first 100 training batches and report the average.\nOther models. For all other models, wav2vec 2.0, HyenaDNA, Caduceus, and GPT2, we simply apply the\nsame procedure: use the standard test-time preprocessing, disable drop-path or dropout when present, and\ncompute the training loss over the full training set, reporting the average.\n22"}
