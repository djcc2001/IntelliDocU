{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 1, "text": "Bidirectional Normalizing Flow: From Data to Noise and Back\nYiyang Lu1,2,∗,†,‡ Qiao Sun1,∗,† Xianbang Wang1,∗Zhicheng Jiang1 Hanhong Zhao1 Kaiming He1\n∗Equal technical contribution\n†Project lead\n1MIT\n2Tsinghua University\nAbstract\nNormalizing Flows (NFs) have been established as a prin-\ncipled framework for generative modeling. Standard NFs\nconsist of a forward process and a reverse process: the for-\nward process maps data to noise, while the reverse process\ngenerates samples by inverting it. Typical NF forward trans-\nformations are constrained by explicit invertibility, ensuring\nthat the reverse process can serve as their exact analytic\ninverse. Recent developments in TARFlow and its variants\nhave revitalized NF methods by combining Transformers and\nautoregressive flows, but have also exposed causal decoding\nas a major bottleneck. In this work, we introduce Bidirec-\ntional Normalizing Flow (BiFlow), a framework that re-\nmoves the need for an exact analytic inverse. BiFlow learns\na reverse model that approximates the underlying noise-to-\ndata inverse mapping, enabling more flexible loss functions\nand architectures. Experiments on ImageNet demonstrate\nthat BiFlow, compared to its causal decoding counterpart,\nimproves generation quality while accelerating sampling\nby up to two orders of magnitude. BiFlow yields state-of-\nthe-art results among NF-based methods and competitive\nperformance among single-evaluation (“1-NFE”) methods.\nFollowing recent encouraging progress on NFs, we hope our\nwork will draw further attention to this classical paradigm.\n1. Introduction\nNormalizing Flows (NFs) are a long-standing family\nof generative models [45, 10, 30]. They contain two pro-\ncesses: a forward process that learns to transform data into\nnoise, and a reverse process that generates samples by in-\nverting this transformation. A notable property of NFs is\nthat the underlying flow trajectories from data to noise are\nlearned rather than imposed. This differs from their modern\ncontinuous-time counterparts [7], such as Flow Matching\n(FM) [36, 37, 1], whose ground-truth trajectories are pre-\ndetermined via time-scheduling. However, this advantage\nof NFs comes at the cost of increased learning difficulty,\ntypically leading to more demanding constraints on forward\narchitectures and objective formulations.\n‡Work done as an intern at MIT.\n(a) Standard Normalizing Flow:\nexplicit inverse.\n(b) Bidirectional Normalizing Flow:\nlearned inverse.\nFigure 1. Conceptual comparison between standard Normalizing\nFlows and our proposed Bidirectional Normalizing Flow (BiFlow).\nInstead of constraining the forward model F to be explicitly in-\nvertible and using its exact analytic inverse for generation, BiFlow\nintroduces a learnable reverse model G that approximates this in-\nverse through our hidden alignment objective. This design frees\nBiFlow from architectural constraints and enables flexible loss de-\nsign, allowing for efficient generation with improved quality in a\nsingle forward pass.\nThe standard NF paradigm [45, 10] requires the reverse\nprocess to be the exact analytic inverse of the forward pro-\ncess (Fig. 1a). This requirement restricts the range of forward\nmodel architectures that can be employed, as the model must\nbe explicitly invertible and its Jacobian determinant must be\ncomputable, tractable, and differentiable. Existing work on\nNFs [45, 10, 57, 30, 41, 29] have largely focused on design-\ning compound forward functions that satisfy these require-\nments. Despite these diverse attempts, NF-based methods re-\nmain limited in their ability to use powerful, general-purpose\narchitectures (e.g., U-Nets [47] or Vision Transformers [12]),\nin contrast to many modern generative model families.\nRecently, the gap between NFs and other generative mod-\nels has been largely closed by TARFlow [65] and its exten-\nsions [21]. TARFlow has effectively integrated Transformers\n[58] with autoregressive flows [30, 41] into the NF paradigm.\nThis design allows NF methods to benefit from the power-\nful Transformers, substantially mitigating a major limitation\nof traditional NFs. However, to maintain computable and\ntractable Jacobian determinants, TARFlow decomposes the\nforward process into a long chain (e.g., thousands of steps)\n1\narXiv:2512.10953v1  [cs.LG]  11 Dec 2025"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 2, "text": "0.001\n0.01\n0.1\n0.5\nReverse Process Time (s, log-scale)\n0\n2\n4\n6\n8\nFID-50K (w/ CFG)\n2.39\n4.54\n6.83\n2.1 points\n better\n697× Faster\n4.4 points\n better\n224× Faster\nimproved TARFlow\nBiFlow\nFigure 2. BiFlow surpasses our improved TARFlow baseline by\na wide margin in generation quality, despite using a base-size\nmodel versus an extra-large model, and it achieves markedly faster\nsampling as well. The x-axis denotes the wall-clock time (log scale)\nfor generating one image on 8 v4 TPU cores. VAE decoding is\nomitted from this figure; comprehensive inference cost comparison\nappears in Tab. 3.\nof autoregressive operations. The resulting explicit inverse\ntherefore requires a large number of causal steps at inference\ntime, which is difficult to parallelize. This design not only\nslows down sampling, but also retains the undesirable archi-\ntectural constraints during inference, e.g., the reverse model\ncannot perform feedforward, non-causal attention.\nIn this work, we introduce Bidirectional Normalizing\nFlow (BiFlow), a framework in which both the forward and\nreverse processes are learned. In our framework, the designs\nof the forward and reverse processes are decoupled: the\nforward process can be any NF model Fθ that is computable,\ntractable, and easy to learn (e.g., an improved TARFlow),\nwhile the reverse process learns a separate model Gϕ to\napproximate its inverse (Fig. 1b). In contrast to the explicit\ninverse, our reverse model is highly flexible: it can be a\nfeedforward, non-causal Transformer that is both expressive\nand efficient to run, naturally enabling high-quality, single\nfunction evaluation (1-NFE) generation.\nLearning the reverse model Gϕ is not merely a form of dis-\ntillation, even though we use a pre-trained forward model Fθ:\nin fact, our learned reverse model Gϕ can outperform the ex-\nplicit inverse of Fθ. Compared to distilling the noise-to-data\ntrajectories, we find that aligning the intermediate hidden\nstates yields results even better than the explicit inverse. In\naddition, our learnable reverse model can naturally eliminate\nthe extra step of score-based denoising in TARFlow, sim-\nplifying and accelerating inference while improving quality.\nSuch a “what-you-see-is-what-you-get” property further en-\nables the use of perceptual loss [68], which is impossible or\ndifficult to leverage with an explicit inverse. Putting these\nfactors together, our learned reverse model can substantially\noutperform its explicit-inverse counterpart.\nWe report competitive results on the ImageNet 256×256\ngeneration. Comparing with an improved TARFlow (which\nis also the forward model for BiFlow), BiFlow achieves an\nFID of 2.39 using a DiT-B size [42] model, while being two\norders of magnitude faster (see Fig. 2; detailed in Tab. 3).\nThis not only sets a new state-of-the-art result among NF-\nbased methods, but also represents a strong 1-NFE result in\ncomparison with other generative model families.\nFollowing the progress established by TARFlow and ex-\ntensions, our work on BiFlow further unleashes the potential\nof NFs as a strong competitor among modern generative\nmodel families. Our findings indicate that the NF principle of\nlearning the forward trajectories, rather than pre-scheduling\nthem, can be advantageous and need not introduce inference-\ntime limitations. Considering that modern Flow Matching\nmethods are continuous-time NFs with pre-scheduled tra-\njectories, we hope our study will shed light on the potential\nsynergy among these related methods.\n2. Related Work\nNormalizing Flows. Normalizing Flows (NFs) have long\nserved as a principled framework for probabilistic genera-\ntive modeling. Over the past decade, extensive research has\nfocused on enhancing the expressivity and scalability of NFs\nunder the constraint of invertible transformations. Planar\nflows [45] and NICE [10] pioneered the use of simple re-\nversible mappings to construct deep generative models. Real\nNVP [11] and Glow [29] extended this framework with non-\nvolume-preserving transformations and convolutional archi-\ntectures. IAF [30] and MAF [41] introduced autoregressive\nflows to improve expressivity while maintaining tractable\nlikelihoods. TARFlow [65] and STARFlow [21] further re-\nvitalized the NF family by incorporating Transformer into\nautoregressive flows. They demonstrated significant gains\nin generation quality and scalability, reaffirming NFs as a\ncompetitive paradigm in modern generative modeling.\nDespite these advances, standard NFs still inherit limita-\ntions from their invertibility requirement. In particular, au-\ntoregressive flow formulations impose strict causal ordering\nand sequential dependencies, which constrain architectural\ndesign and lead to slow inference.\nContinuous Normalizing Flows. Continuous Normalizing\nFlows (CNFs) [20, 14, 19] generalize discrete flows by mod-\neling transformations as continuous-time dynamics governed\nby ordinary differential equations (ODEs) [7]. CNFs enable\nmore flexible architectures and tractable likelihood compu-\ntation via numerical ODE simulations. FM [36, 37, 13]\nreformulates the explicit maximum-likelihood training ob-\njective into an equivalent implicit objective. Diffusion mod-\nels [25, 51, 9] can be interpreted as a special case of Flow\nMatching with stochastic dynamics, achieving impressive\nfidelity and scalability. Despite their empirical success, the\n2"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 3, "text": "TARFlow Block\nTARFlow Block\nFigure 3. Illustration on the autoregressive inference process of\nTARFlow. In each block, each token is transformed one by one,\ndepending on previous tokens. This is repeated for a sequence\nwith length 256 for a 32×32 input with patch size 2, and is further\nrepeated for all blocks (e.g., 8 blocks). Altogether, TARFlow infer-\nence requires 8×256 sequential function evaluations.\nimplicit formulation of FM and diffusion models sacrifices\nthe learnable bidirectional mapping that characterizes NFs.\n3. Background: Normalizing Flows\nNormalizing Flows (NFs) are a class of generative models\nthat establish a bijective transformation between a Gaussian\nprior distribution p0 and a complex data distribution pdata.\nAn NF consists of a forward process and a reverse process.\nGiven a data sample x ∈RD ∼pdata, the forward process\nF maps it into the Gaussian prior space z = F(x). The\nmodel assigns the data likelihood p(x) through the change-\nof-variables formula. Training is performed by optimizing F\nto maximize the log-likelihood log p(x) over data samples.\nClassical NF requires the forward process F to be explic-\nitly invertible for exact likelihood computation and efficient\nsampling. Once trained, its exact inverse, F−1, can be used\nfor generation by transforming Gaussian noise back to the\ndata space, i.e., x = F−1(z) where z ∼p0.\nIn practice, to enhance expressiveness, the forward pro-\ncess is commonly constructed as a composition of multiple\nsimpler bijective transformations F := fB−1 ◦· · · ◦f1 ◦f0\n(◦denotes function composition). Under this formulation,\nthe log-likelihood objective becomes\nlog p(x) = log p0(z) +\nX\ni\nlog\n\f\f\f\fdet ∂fi(xi)\n∂xi\n\f\f\f\f ,\n(1)\nwith x0 = x and xi+1 = fi(xi). Here, det(·) denotes\nthe determinant operator. Designing transformations that\nyield computable and differentiable determinant has been a\nkey consideration in prior NF formulations. This require-\nment motivates specialized designs such as affine coupling\n[10, 11] and autoregressive flows [30, 41], which preserve\ntractable Jacobians.\nImportantly, while the log-determinant term in Eq. (1)\nrequires the forward process F to be invertible, it does not\nnecessitate an explicitly invertible formulation. The explicit\ninverse is only required at inference time, where we need to\nmap samples from prior back to the data space.\nTARFlow. TARFlow [65] integrates Transformer architec-\ntures into autoregressive flows (AF), substantially improv-\ning their expressiveness and scalability. The core idea in\nAF is to further decompose each sub-transformation fi, pa-\nrameterized by a block, into T steps, where T denotes the\nsequence length of the input tokens. Each step transforms\nthe i-th token only conditioned on its predecessors, which\ncan naturally be realized through Transformer layers with\ncausal masks. To capture bidirectional context, AF flips\nthe sequence order in alternating blocks. By combining ex-\npressive Transformer architectures with autoregressive flows,\nTARFlow successfully revives NF to remain competitive\nwith today’s state-of-the-art generative models.\nHowever, AF parameterization introduces asymmetry\nbetween training and sampling.\nSimilar to next-token-\nprediction language models, although likelihood evaluation\nand training can be parallelized efficiently, sampling must\nproceed sequentially due to the autoregressive nature, as\nillustrated in Fig. 3. In practice, this requires performing,\ne.g., thousands of (8×256) inverse transformations one after\nanother, resulting in substantial inference latency.\n4. Bidirectional Normalizing Flow\nWe propose a Bidirectional Normalizing Flow (BiFlow)\nframework, which has: (i) a forward model Fθ that trans-\nforms data samples into pure noise, and (ii) a learnable,\nseparate reverse model Gϕ that approximates its inverse,\nmapping noise back to the data space. Training is performed\nin two stages: first, similar to classical NF, we train the\nforward model using maximum likelihood estimation; then,\nkeeping the forward model fixed, we train the reverse model\nto approximate its inverse mapping.\nNotably, our reverse model Gϕ is not constrained by ex-\nplicit invertibility. As a result, this allows us to design the\nreverse model with arbitrary architectures (e.g., bidirectional\nattention-based Transformers) and training objectives. Next,\nwe discuss the formulation, objectives, and learning dynam-\nics of the reverse process.\n4.1. Learning to Approximate the Inverse\nGiven a pre-trained forward model Fθ, our goal is to\noptimize a reverse model Gϕ that approximates its inverse.\nWe consider three strategies: (i) naive distillation; (ii) hidden\ndistillation; (iii) hidden alignment, as approaches to learning\nthe reverse model. Fig. 4 illustrates the differences among\nthese methods, as we describe next.\nNaive Distillation. A straightforward strategy is to impose\na direct distillation loss:\nLnaive(x) = D\n\u0000x, x′\u0001\n,\n3"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 4, "text": "(a) Naive Distillation.\n(b) Hidden Distillation.\n(c) Hidden Alignment.\nFigure 4. Comparison of three approaches for learning the reverse process. Each ◦marks a position where the model returns to the same\ndimension as input x. Blue arrows with D refer to distance loss terms. Our hidden alignment strategy (Fig. 4c) combines the strengths of\nFig. 4a and Fig. 4b, leveraging the entire trajectory for supervision without repeatedly returning to input space.\nwhere x is a data sample, x′ = Gϕ(Fθ(x)) is the recon-\nstructed data, and D denotes a distance metric (e.g., L2\ndistance). The reverse model is trained to minimize the\nreconstruction error on data samples (see Fig. 4a).\nThis simple approach provides supervision only at the fi-\nnal output, which may be insufficient for effectively training\nthe reverse model. Directly mapping pure noise to data in\none step is highly under-constrained, making it difficult for\nthe reverse network to learn a reliable inverse from a single\nreconstruction loss.\nHidden Distillation. A typical NF is composed of a se-\nquence of simple sub-transformations, i.e., Fθ = fB−1 ◦\n· · · ◦f1 ◦f0, where each fi is a transformation block and B\ndenotes the total number of blocks. We can strengthen the\ntraining signal by leveraging the full sequence of intermedi-\nate states generated along the forward trajectory.\nAs illustrated in Fig. 4b, starting from x ∼pdata, the\nforward model produces a trajectory of intermediate hidden\nstates {xi} with z = Fθ(x) as the final output prior. Analo-\ngously, we also design the reverse model to be composed of\nB blocks, generating a reverse trajectory {hi} from z. We\ndistill the reverse model by enforcing the two trajectories to\nbe close. Formally, the loss is defined as:\nLhidden(x) =\nX\ni\nD\n\u0000xi, hi\u0001\n,\nwhere h0 corresponds to the reconstructed output x′. Option-\nally, each term can be assigned a distinct weighting factor.\nThis formulation encourages the reverse model to invert each\nsub-transformation individually, which could help guide the\nreverse model to invert the mapping Fθ step by step. The in-\ntermediate hidden states {xi} serve as auxiliary supervision\nfor learning the correspondence between x and z.\nAlthough this hidden distillation strategy provides more\nsupervision than naive distillation, it introduces structural\nconstraints on model design. Since each intermediate state\nxi has the same dimensionality as the input, the reverse\nmodel is forced to repeatedly project features down to the\ninput space and then back up into the hidden space. This\nrigid requirement restricts architectural flexibility, ultimately\nlimiting the model’s effectiveness.\nHidden Alignment. We propose a more flexible strategy,\ntermed hidden alignment. Crucially, it leverages the full for-\nward trajectory for supervision while relaxing the restrictive\nrequirement in hidden distillation that intermediate hidden\nstates must lie in the input space.\nAs shown in Fig. 4c, we extract intermediate hidden states\n{hi} from the reverse model Gϕ. Unlike hidden distillation,\nwhich enforces each hi to directly match its input-space\ncounterpart xi, we introduce a set of learnable projection\nheads {φi} to align the projected representations φi(hi) with\nthe corresponding forward states xi. The training objective\nthen becomes:\nLalign(x) =\nX\ni\nD\n\u0000xi, φi(hi)\n\u0001\n,\n(2)\nwhere h0 = x′ and φ0 is the identity mapping.\nThis simple modification allows the reverse model to\nbenefit from full trajectory supervision while maintaining\narchitectural and representational flexibility. By decoupling\nthe representation space from the input token space, hidden\nalignment avoids the potential semantic distortion caused by\nrepeated projections.\n4.2. Eliminating Score-based Denoising\nExisting state-of-the-art NFs such as TARFlow [65] devi-\nate from standard flow-based modeling in that they learn\na noise-perturbed distribution and then denoise the out-\nput. Specifically, during training, TARFlow takes a noise-\nperturbed input ˜x = x + σϵ, where ϵ ∼N(0, I), and during\ninference, TARFlow first generates ˜x = F−1\nθ (z), then per-\nforms an additional score-based denoising step:\nx ←˜x + σ2 ∇˜x log p(˜x),\n(3)\nas illustrated in Fig. 5a, where the score term is computed\nvia a forward-backward pass. This post-processing almost\ndoubles the inference cost, becoming a clear computational\nbottleneck for efficient generation.\nLearned Denoising. We eliminate the explicit score-based\ndenoising step by integrating denoising directly into the re-\nverse model. As illustrated in Fig. 5b, we extend the forward\ntrajectory from ˜x to z by appending the clean data x at its\nstart, and extend the reverse model with one additional block\nh0 →x′ that learns denoising jointly with the inverse. The\n4"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 5, "text": "(a) TARFlow: explicit denoising.\n(b) BiFlow: learned denoising.\nFigure 5. Incorporating the denoising step into our hidden align-\nment framework. The reverse model is extended with an additional\nblock dedicated to denoising. Our learned denoising eliminates the\nneed for calculating the score function through a whole forward-\nbackward pass, incurring only a single additional block forward.\nresulting reverse network, with one extra block for denoising,\nmaps z to a clean sample x′ in a single pass. As such, our\nreverse model directly learns the correspondence between z\nand the clean data x directly, rather than the noisy data ˜x.\nThe training process follows the same objective as Eq. (2),\nwith a reconstruction loss on (x, x′) and hidden alignment\nlosses on intermediate states. By integrating denoising into\nthe reverse process itself, BiFlow achieves a unified learned\nformulation for generation, where inverse and denoising are\nseamlessly coupled within a single direct generative model,\neliminating the need for any extra refinement step.\n4.3. Distance Metric\nBiFlow provides a flexible supervised-learning frame-\nwork for tackling the generation problem. This flexibility\nstems from two key properties of BiFlow: (i) 1-NFE genera-\ntion — the learned reverse model produces a sample x′ in a\nsingle forward pass, so generated samples are directly acces-\nsible during training; and (ii) explicit pairing — the forward\nprocess establishes a direct correspondence between data x\nand noise z, serving as training pairs for the reverse model.\nTogether, these properties realize a what-you-see-is-what-\nyou-get training regime: generated samples are available for\nimmediate loss evaluation and backpropagation, enabling\nrich semantic supervision signals.\nOur framework is highly flexible in the choice of loss\nfunctions: almost any distance metric can be used, and mul-\ntiple metrics can be combined. Our default choice for the\ndistance metric D in Eq. (2) is simply mean squared error\n(MSE). To enhance realism, we further apply perceptual loss\nat the final VAE-decoded image, while intermediate hidden\nstates remain aligned by MSE. In this work, we adopt both\nVGG [50] and ConvNeXt V2 [61] feature spaces for per-\nceptual loss (our implementation for VGG features follows\nLPIPS [68]). As in prior work [16, 52, 17], all loss terms\ncan be adaptively re-weighted during training. Details are\nprovided in Appendix B.3.\n4.4. Norm Control\nThe intermediate states produced by the forward model\nare unconstrained under the NF formulation, often exhibiting\nlarge norm fluctuations across blocks (see Fig. 8a). These\nvariations can lead to imbalanced supervision when using\nmagnitude-sensitive losses such as MSE for reverse-model\ntraining. To mitigate this issue, we introduce two comple-\nmentary norm-control strategies applied to the forward and\nreverse models to ensure stable and consistent supervision\nstrength (details in Appendix B.3).\nOn the forward model, we clip the output parameters of\neach transformation fi within a fixed range [−c, c], limiting\nexcessive scaling and stabilizing intermediate state norms\nwithout compromising expressiveness. On the reverse model,\nwe normalize each intermediate state before performing hid-\nden alignment, which equalizes the contribution across tra-\njectory depth and promotes scale-invariant learning.\n4.5. BiFlow with Guidance\nClassifier-free guidance (CFG) [24] was originally pro-\nposed for diffusion models to control the trade-off between\nsample diversity and fidelity. Due to its effectiveness, it has\nbeen widely adopted in diffusion-based generative models.\nFollowing this success, recent Normalizing Flows [65, 21]\nand autoregressive models [56, 35] also incorporate CFG to\nfurther improve generation quality.\nCFG can be seamlessly integrated into BiFlow’s infer-\nence process by extrapolating conditional and unconditional\npredictions of Gϕ at each hidden state hi, i.e.,\nhi+1 = (1 + wi) Gi\nϕ(hi | c) −wi Gi\nϕ(hi),\n(4)\nwhere c is the class condition and wi is the guidance scale\n(our w definition follows the original CFG formulation [24],\ni.e., w = 0 is w/o CFG). The subscript i indicates that wi can\ndiffer among blocks, supporting CFG interval [31]. More\nresults are provided in Appendix C.2.\nDirectly applying CFG doubles the computational cost\nduring inference, since each guided block requires two for-\nward passes. To alleviate this, following [5, 55], we incor-\nporate CFG into the training stage, enabling inference with\nonly one function evaluation (1-NFE) while preserving the\nbenefits of guidance. Additionally, to retain the flexibility\nof adjusting guidance scales at inference time, we allow the\nreverse model to leverage CFG scale as condition [39, 18].\nBy training the model with a range of guidance scales, Bi-\nFlow can generate outputs corresponding to various guidance\nstrengths within a single forward pass. Further details are\nprovided in Appendix B.2.\n5"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 6, "text": "5. Experiments\nExperiment Settings. Our experiments are conducted on\nclass-conditional ImageNet [8] generation at 256×256 reso-\nlution. We evaluate Fr´echet Inception Distance (FID) [23]\nand Inception Score (IS) [48] on 50000 generated images.\nFollowing [46, 13, 21], we implement our models on the\nlatent space of a pre-trained VAE tokenizer. For ImageNet\n256×256, the tokenizer maps images to a 32×32×4 latent\nrepresentation, serving as the input and output domain of our\nmodels.\nImproved TARFlow as Baseline. Our BiFlow framework\nbuilds upon TARFlow [65] as our forward model. We in-\ntroduce several modifications to the original TARFlow to\nenhance stability and performance. Specifically, we replace\nadditive conditioning with in-context conditioning [42] and\napply the norm control strategy in Sec. 4.4, while omitting\nSTARFlow-specific components such as deep-shallow de-\nsign, decoder finetuning, and customized CFG. We denote\nthis enhanced version as improved TARFlow (iTARFlow).\nAs shown in the table below, it achieves substantial gains\nover the original TARFlow, both with or without CFG, es-\ntablishing a strong baseline for BiFlow.\nMethod\nFID (↓)\n# Params\nw/o CFG\nw/ CFG\nlatent TARFlow-B/2 *\n59.43\n10.89\n118M\n+ in-context conditioning\n53.87\n8.25\n120M\n+ 160 epochs →960 epochs\n45.48\n7.05\n120M\n+ norm control (iTARFlow)\n44.46\n6.83\n120M\nConfigurations. Our reverse model adopts a ViT backbone\nwith modern Transformer components [53, 66] and multi-\ntoken in-context conditioning [18]. We name our model as\nBiFlow-B/2, where B/2 indicates a base-sized model with\npatch size 2, resulting in a sequence length of 256. In our\nablation studies, we choose an iTARFlow as our forward\nmodel and train the reverse model with the forward model\nfixed. Unless otherwise specified, our ablations employ the\nadaptive-weighted MSE, while final comparisons in Tab. 4\nincorporate perceptual distance mentioned in Sec. 4.3 for\noptimal performance. Details are provided in Appendix A.\n5.1. Ablation: Learning to Approximate the Inverse\nWe evaluate three strategies for learning the reverse\nmodel, as described in Sec. 4.1, and report generation qual-\nity (FID in Tab. 1) as well as reconstruction error (see Ap-\npendix C.1).\nThe naive distillation approach, trained with a simple\nMSE objective, already outperforms the exact inverse base-\nline, indicating that a learned reverse model is a practical\nand competitive alternative to the analytic inverse.\nHidden distillation supervises the reverse model using\nthe entire forward trajectory. However, repeated projections\n*The latent TARFlow-B/2 is our TARFlow reproduction in VAE latent.\nFID (↓)\nattention\nexact inverse\n44.46\ncausal\nnaive distillation\n43.41 −1.05\nbidirect\nhidden distillation\n55.00 +10.54\nbidirect\nhidden alignment\n36.93 −7.53\nbidirect\nTable 1. Reverse learning method. Naive distillation can exceed\nthe exact inverse with a simple MSE objective. Our hidden align-\nment yields the best result among the three strategies. (Settings:\nBiFlow-B/2, 160 epochs, adaptive weighted MSE loss, w/o CFG)\nbetween representation and input spaces cause information\nloss and limit architectural expressiveness. This results in\ndegraded performance compared to the naive distillation.\nOur proposed hidden alignment method removes the re-\npeated projections inherent in hidden distillation while re-\ntaining full trajectory-level supervision, thereby preserving\nboth architectural flexibility and representational richness. It\nachieves the best performance among the three strategies and\nsurpasses the exact inverse by a clear margin in generation\nquality. These results collectively demonstrate that hidden\nalignment is an effective and robust strategy for learning an\napproximate inverse in BiFlow.\n5.2. Other Ablations\nWe ablate several key design choices in BiFlow and ana-\nlyze their impact on performance in Tab. 2.\nBiFlow with Guidance. BiFlow is conditioned on the CFG\nscale and learns across a range of CFG scales during training.\nThis enables 1-NFE inference while preserving the benefits\nand flexibility of guidance. As shown in Tab. 2a, compared to\nstandard CFG approach, our training-time CFG mechanism\nreduces inference cost by half while achieving better FID.\nLearned Denoising. Tab. 2b demonstrates the effective-\nness of our learned denoising strategy. By jointly training\ndenoising with the inverse, our learned one-block denoiser\nimproves generation quality over the score-based denoising\nused in TARFlow. Moreover, our approach introduces only a\nsingle additional block, whereas TARFlow’s score-based de-\nnoising requires an extra forward-backward pass (incurring\n15.8× flops). This substantially reduces inference overhead.\nNorm Control. We introduce two norm control strategies in\nSec. 4.4 and evaluate their effectiveness in Tab. 2c. Apply-\ning either strategy alleviates imbalance in MSE loss across\nblocks, thereby enhancing performance. We provide visual-\nizations of the norm statistics in Appendix C.4.\nDistance Metric. Our framework supports various distance\nmetric designs. As shown in Tab. 2d, incorporating percep-\ntual distance [68, 61] at the image end can largely improve\ngeneration quality. Notably, when both VGG and ConvNeXt\nfeatures are used for the perceptual loss, the optimal guid-\nance scale in Eq. (4) for this model is close to 0.0, resulting\nin performance similar to no-CFG setting. This suggests\nthese features already provide strong class-discriminative\ninformation. More results are provided in Appendix C.3.\n6"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 7, "text": "FID, w/o CFG\nFID, w/ CFG\ninference-time CFG\n36.93 (NFE=1)\n6.90 (NFE=2)\n→training-time CFG\n31.88 (NFE=1)\n6.79 (NFE=1)\n(a) BiFlow with guidance. Conditioning on the CFG scale during training\nimproves FID both w/ and w/o CFG while preserving flexible 1-NFE infer-\nence. The baseline w/o CFG is final results in Tab. 1.\nFID, w/o CFG\nFID, w/ CFG\nlearned denoise\n31.88\n6.79\n→no denoise\n100.51\n26.20\n→score-based denoise\n42.62\n10.98\n(b) Learned denoising. Our learned denoising scheme is effective. Com-\npared to score-based denoising in TARFlow, it eliminates an extra forward-\nbackward calculation, and unifies the denoising step into our framework.\nFID, w/o CFG\nFID, w/ CFG\nnorm control: clip\n31.88\n6.79\nnorm control: none\n45.54\n12.33\nnorm control: traj.\n34.88\n8.03\n(c) Norm control. Either clipping the forward model’s output or normaliz-\ning the forward trajectory improves generation quality by ensuring balanced\nsupervision strength across blocks.\nFID, w/o CFG\nFID, w/ CFG\nMSE\n31.88\n6.79\n+ LPIPS\n14.15\n4.91\n+ LPIPS + ConvNeXt\n2.46\n2.46\n(d) Distance metric. Our framework enables a flexible design of distance\nmetrics. Incorporating perceptual distance improves generation quality.\nTable 2. Ablation study on ImageNet 256×256 generation. FID-\n50K with 1-NFE is reported by default. (Settings: BiFlow-B/2, 160\nepochs. By default: adaptive weighted MSE loss without perceptual\nloss, training-time CFG.)\nScaling Behavior. We investigate the scaling behavior of\nBiFlow under different distance metrics, using iTARFlow\nof corresponding size as forward models. We summarize\npreliminary results in the table below.\nFID, w/ CFG\nB\nXL\nMSE\n6.79\n4.61\n+ LPIPS\n4.91\n3.36\n+ LPIPS + ConvNeXt\n2.46\n2.57\nOverall, BiFlow exhibits clear gains from increased\nmodel capacity when trained without the ConvNeXt-based\nperceptual loss. However, after incorporating ConvNeXt\nfeatures, further scaling yields diminishing returns, with FID\nimprovements gradually saturating. We hypothesize this\nbehavior may be related to overfitting, as evidenced by an\nincrease in FID during training. A comprehensive investiga-\ntion of BiFlow’s scaling behavior is left for future work.\n5.3. BiFlow vs. improved TARFlow\nWe compare our learned reverse model (BiFlow) with\nthe exact analytic inverse baseline (improved TARFlow) of\nthe forward process. In Tab. 3, we benchmark in terms of\ngeneration quality (FID score) and inference efficiency (flops\nand wall-clock time for generating a single image). Details\nof our benchmarking setup are provided in Appendix A.\nBiFlow\nimproved TARFlow\nB/2\nB/2\nM/2\nL/2\nXL/2\nFID\n2.39\n6.83\n5.22\n4.82\n4.54\n# Params\n133M\n120M\n296M\n448M\n690M\nGflops\n38\n152\n363\n552\n836\nWall-clock time (ms)\nTPU\n0.29+1.3\n65+1.3\n85+1.3\n165+1.3\n202+1.3\nGPU\n2.15+2.7\n129+2.7\n208+2.7\n349+2.7\n400+2.7\nCPU\n80+240\n9040+240\n16200+240\n20400+240\n26300+240\nWall-clock speedup, BiFlow-B/2 vs. iTARFlow: (VAE excluded, see also Fig. 2)\nTPU\n-\n224×\n293×\n569×\n697×\nGPU\n-\n60×\n97×\n162×\n186×\nCPU\n-\n113×\n203×\n255×\n329×\nWall-clock speedup, BiFlow-B/2 vs. iTARFlow: (VAE included)\nTPU\n-\n42×\n54×\n105×\n128×\nGPU\n-\n27×\n43×\n73×\n83×\nCPU\n-\n29×\n51×\n65×\n83×\nTable 3. Comparison between BiFlow and iTARFlow baseline.\nWe report both generation quality (FID-50K) and inference cost\nper image. All wall-clock time measurements are reported as\n“generator + VAE decoding”. Compared to iTARFlow, BiFlow\nachieves one to two orders of magnitude faster sampling on TPU,\nGPU, and CPU, while attaining superior generation quality. (The\nVAE decoder contains 49M parameters and requires 308 Gflops.)\nExperiments show that our BiFlow-B/2 surpasses the\nexact inverse of the improved TARFlow-XL/2 baseline in\ngeneration quality. Remarkably, BiFlow requires only a\nsingle function evaluation (1-NFE), compared to 256×2\nsequential decoding steps for the autoregressive inference\nof the exact analytic inverse — resulting in up to a 42×\nspeedup for models of similar size on TPU.\nWhy can a learned inverse outperform the exact inverse?\nOur reverse model Gϕ is trained to reconstruct real images\ndirectly, rather than to replicate synthetic samples produced\nby the exact inverse as in conventional distillation. This\nencourages its predictions to align more closely with the true\ndata distribution. In addition, Gϕ is optimized end-to-end\nwith the forward map fixed, learning to directly transform\nnoise into clean data. This joint optimization can help the\nmodel to learn a stable and globally consistent mapping.\nWhy is a learned inverse significantly faster than the exact\ninverse? From an algorithmic perspective, two key improve-\nments reduce the computational cost of BiFlow. First, Bi-\nFlow eliminates the score-based denoising step required\nby the exact inverse of TARFlow, removing a major com-\nputational bottleneck. Second, we integrate CFG into the\ntraining stage, effectively halving the inference cost com-\npared to applying CFG during sampling. Together, these two\nimprovements reduce the flops by roughly 4×.\nFrom an architectural perspective, the autoregressive de-\nsign of TARFlow imposes inherent limitations on parallelism\nduring inference. Our bidirectional attention Transformer\ndesign allows for fully parallelized computation across the\nsequence dimension, which leads to significant speedups\non modern accelerators. Notably, due to the efficiency of\nBiFlow, the VAE decoder has become a dominant computa-\ntional overhead, which is outside the scope of this work.\n7"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 8, "text": "Method\n# Params\nNFE\nFID(↓)\nIS(↑)\nAutoregressive Normalizing Flow\nTARFlow-XL/8@pix [65]\n1.3B\n⋆\n5.56\n-\nSTARFlow-XL/1 [21]\n1.4B\n⋆\n2.40\n-\nAutoregressive Normalizing Flow (our impl.)\niTARFlow-B/2\n120M\n⋆\n6.83\n226.2\niTARFlow-M/2\n296M\n⋆\n5.22\n255.5\niTARFlow-L/2\n448M\n⋆\n4.82\n254.8\niTARFlow-XL/2\n690M\n⋆\n4.54\n259.3\n1-NFE Normalizing Flow\nBiFlow-B/2 (Ours)\n133M\n1\n2.39\n303.0\nMethod\n# Params\nNFE\nFID(↓)\nIS(↑)\nGANs\nBigGAN-deep [3]\n112M\n1\n6.95\n202.6\nGigaGAN [26]\n569M\n1\n3.45\n225.5\nStyleGAN-XL [49]\n166M\n1\n2.30\n265.1\n1-NFE diffusion/flow matching from scratch\niCT-XL/2 [52]\n675M\n1\n34.24\n-\nShortcut-XL/2 [15]\n675M\n1\n10.60\n-\nMeanFlow-XL/2 [17]\n676M\n1\n3.43\n247.5\nTiM-XL/2 [60]\n664M\n1\n3.26\n210.3\nα-Flow-XL/2+ [67]\n676M\n1\n2.58\n-\niMF-XL/2 [18]\n610M\n1\n1.72\n282.0\n1-NFE diffusion/flow matching (distillation)\nπ-Flow-XL/2 [6]\n675M\n1\n2.85\n-\nDMF-XL/2+ [32]\n675M\n1\n2.16\n-\nFACM-XL/2 [43]\n675M\n1\n1.76\n290.0\nMethod\n# Params\nNFE\nFID(↓)\nIS(↑)\nautoregressive/masking\nMaskGIT [4]\n227M\n⋆\n6.18\n182.1\nRCG, conditional [34]\n512M\n⋆\n2.12\n267.7\nVAR-d30 [56]\n2.0B\n⋆\n1.92\n323.1\nMAR-H [35]\n943M\n⋆\n1.55\n303.7\nRAR-XXL [63]\n1.5B\n⋆\n1.48\n326.0\nxAR-H [44]\n1.1B\n⋆\n1.24\n301.6\nMulti-NFE diffusion/flow matching\nADM-G [9]\n554M\n250×2\n4.59\n-\nLDM-4-G [46]\n400M\n250×2\n3.60\n247.7\nDiT-XL/2 [42]\n675M\n250×2\n2.27\n278.2\nSiT-XL/2 [38]\n675M\n250×2\n2.06\n252.2\nJiT-G/16 [33]\n2B\n100×2\n1.82\n292.6\nSiT-XL/2 + REPA [64]\n675M\n250×2\n1.42\n305.7\nLightningDiT-XL/1 [62]\n675M\n250×2\n1.35\n295.3\nDDT-XL/2 [59]\n675M\n250×2\n1.26\n310.6\nDiTDH-XL + RAE [69]\n839M\n50×2\n1.13\n262.6\nTable 4. System-level comparison on ImageNet 256×256 class-conditional generation. All results are reported with CFG if applicable.\nLeft: Comparison with Normalizing Flow models. Middle: Other 1-NFE generative models, including GANs and diffusion/flow matching-\nbased models. Right: Other families of generative models. Our BiFlow model is trained for 350 epochs with perceptual distance. In all\ntables, ×2 indicates the use of CFG incurs double NFEs. ⋆: All AR-based methods, including AR Normalizing Flow (left) and other AR\nmodels (right), involve a large number of forward evaluations, yet each evaluation is on one or a very few tokens. For example, for standard\nleft-to-right order AR, the average NFE of the entire AR process is roughly 1 (or 2× w/ CFG), that is, K evaluations with a\n1\nK fraction of\ntokens each. In addition, TARFlow/iTARFlow has an extra NFE of 2 due to the score-based denoising post-processing. Results of [52] is collected\nfrom [70]; results of [65] is collected from [21]; TARFlow-XL/8@pix denotes TARFlow on pixel-space with patch size 8.\n5.4. Comparison with Prior Works\nIn Tab. 4, we provide system-level comparisons with\nprevious methods on class-conditional ImageNet 256×256\ngeneration. We categorize prior works into three groups:\nNormalizing Flows (Tab. 4, left), 1-NFE generative models\n(Tab. 4, middle), and other families of generative models\n(Tab. 4, right). All our models are trained to convergence.\nComparison with Normalizing Flows. Tab. 4 (left) com-\npares BiFlow with previous state-of-the-art Normalizing\nFlows models. Our BiFlow-B/2, with only 133 million pa-\nrameters, achieves an FID of 2.39 in a single function eval-\nuation (1-NFE), establishing a new state-of-the-art among\nNormalizing Flows. In contrast, STARFlow uses thousands\nof sequential decoding steps due to their autoregressive sam-\npling process. It yields a similar FID score with about 10×\nparameters and more than 400× inference wall-clock time\n(see Tab. 6 for details).\nMore broadly, BiFlow represents a significant advance-\nment in Normalizing Flows, demonstrating that direct and\nefficient generation can coexist with high fidelity.\nComparison with Other Generative Models. We compare\nBiFlow with other generative model families, especially\n1-NFE methods. As shown in Tab. 4, BiFlow offers an\nexcellent balance between generation quality and sampling\nefficiency. These results demonstrate that BiFlow achieves\nperformance on par with leading 1-NFE generative models.\n6. Conclusion\nThis work revisits one of the oldest, yet most principled,\nfoundations of generative modeling — Normalizing Flows —\nand redefines its boundaries. We challenge the conventional\nFigure 6. 1-NFE Generation Results. We show selected samples\ngenerated by our BiFlow-B/2 model with guidance scale 2.0 on\nImageNet 256×256. BiFlow achieves high-fidelity generation with\nonly a single function evaluation (1-NFE) from noise.\nwisdom that the reverse process must be the exact analytic\ninverse of the forward process, and demonstrate that the\nlong-held constraint is unnecessary. By introducing a learn-\nable reverse model, BiFlow pushes Normalizing Flows from\nanalytically invertible mappings to trainable bidirectional\nsystems, from autoregressive sampling to fully parallelized,\nefficient 1-NFE generation, and from an implicit genera-\ntive model towards a direct generative model. Experiments\ndemonstrate that BiFlow achieves competitive generation\nquality among Normalizing Flows, while delivering up to\ntwo orders of magnitude faster inference than its explicit\ninverse counterpart. We hope this work can serve as a step\ntoward rethinking and expanding the scope of Normaliz-\ning Flows, inspiring future research on direct, flexible, and\nefficient NF-based generation.\n8"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 9, "text": "config\nBiFlow\nimproved TARFlow\nB/2\nB/2\nM/2\nL/2\nXL/2\n# Params (M)\n133\n120\n296\n448\n690\nblock\n9\n8\n10\n12\n15\nlayer\n8\n8\n9\n9\n9\nhidden dim\n384\n384\n512\n576\n640\nattn heads\n6\n6\n8\n9\n10\npatch size\n2\n2\nclass tokens\n8\n—\nguidance tokens\n4† / 1\n—\nepochs\n160† / 350\n960\n640\n640\n480\nbatch size\n256\n256\nlearning rate\n4e-4\n4e-4\n4e-4\n2e-4\n1e-4\nlr schedule\nconstant\nconstant\nlr warmup\n10 epochs\n10 epochs\noptimizer\nAdam\nAdam [28]\nAdam (β1, β2)\n(0.9, 0.95)\n(0.9, 0.95)\nweight decay\n0.0\n0.0\ndropout\n0.0\n0.0\nema decay\n0.9999\n0.9999\nlabel drop\n0.1\n0.1\nadaptive weight p\n1\n—\nwVGG\n1.0† / 0.8\n—\nwConvNeXt\n0.4† / 0.6\n—\nclip range c\n—\n1.0\n1.0\n3.0\n3.0\nnoise level σ\n—\n0.3\nTable 5. Configurations and training hyperparameters on Ima-\ngeNet 256×256. † indicates the setting in the ablation study.\nA. Implementation Details\nWe implement all experiments using the JAX frame-\nwork [2] on Google TPU hardware. All reported results are\nobtained on TPU v4, v5p, and v6e cores. The configurations\nand training hyperparameters for improved TARFlow and\nBiFlow are provided in Tab. 5. For the MSE-only ablation in\nSec. 5, we employ adaptive weighting with exponent p = 2;\nfor all other experiments we use p = 1 (see Appendix C.3\nfor detailed ablations).\nFID Evaluation. For generative evaluation, we compute\nthe Fr´echet Inception Distance (FID) [23] between 50,000\ngenerated images and training images, without applying any\ndata augmentation. We use the Inception-V3 model [54] pro-\nvided by StyleGAN3 [27], converted into a JAX-compatible\nimplementation. We sample 50 images per class for all 1000\nImageNet classes, following the protocol in [69].\nInference Cost Evaluation. In Fig. 2 and Tab. 3, we report\ninference cost across three hardware configurations: GPU,\nTPU, and CPU. For all metrics, we report the average per-\nimage runtime in seconds, averaged over multiple runs to\nensure stability. All measurements include the overhead of\nCFG and VAE decoding time when applicable. We also\nprovide a comparison with prior Normalizing Flow mod-\nels [65, 21] in Tab. 6. All autoregressive models utilize\nKV-cache to accelerate inference, and Gflops in Tab. 3 is\nestimated using JAX’s cost analysis function.\nMethod\n# Params\nTime (ms)\nSpeed\nVAE?\nTARFlow-XL/8@pix [65]\n1.3B\n1192\n1×\n✗\nSTARFlow-XL/1 [21]\n1.4B\n677+1.3\n1.76×\n✓\niTARFlow-B/2\n120M\n65+1.3\n18.0×\n✓\niTARFlow-M/2\n296M\n85+1.3\n13.8×\n✓\niTARFlow-L/2\n446M\n165+1.3\n7.17×\n✓\niTARFlow-XL/2\n675M\n202+1.3\n5.86×\n✓\nBiFlow-B/2 (Ours)\n133M\n0.29+1.3\n750×\n✓\n(a) TPU inference time comparison, benchmarked on 8 TPU v4 cores with\na pre-compiled JAX sampling function.\nMethod\n# Params\nTime (ms)\nSpeed\nVAE?\nTARFlow-XL/8@pix [65]\n1.3B\n3452\n1×\n✗\nSTARFlow-XL/1 [21]\n1.4B\n2193+2.7\n1.57×\n✓\niTARFlow-B/2\n120M\n129+2.7\n26.2×\n✓\niTARFlow-M/2\n296M\n208+2.7\n16.4×\n✓\niTARFlow-L/2\n446M\n349+2.7\n9.82×\n✓\niTARFlow-XL/2\n675M\n400+2.7\n8.57×\n✓\nBiFlow-B/2 (Ours)\n133M\n2.15+2.7\n712×\n✓\n(b) GPU inference time comparison, benchmarked on 1 NVIDIA H200 core\nwith PyTorch and torch.compile optimization if beneficial.\nMethod\n# Params\nTime (ms)\nSpeed\nVAE?\nTARFlow-XL/8@pix [65]\n1.3B\n512000\n1×\n✗\nSTARFlow-XL/1 [21]\n1.4B\n276700+240\n1.85×\n✓\niTARFlow-B/2\n120M\n9040+240\n55.2×\n✓\niTARFlow-M/2\n296M\n16200+240\n31.1×\n✓\niTARFlow-L/2\n446M\n20400+240\n24.8×\n✓\niTARFlow-XL/2\n675M\n26300+240\n19.3×\n✓\nBiFlow-B/2 (Ours)\n133M\n80+240\n1600×\n✓\n(c) CPU inference time comparison, benchmarked on 1 AMD EPYC 7B12\nnode with 120 physical CPU cores and 400GB RAM. We reuse the PyTorch\nimplementations with torch.compile optimization if beneficial.\nTable 6. Comparison of NF models’ inference wall-clock time\non TPU, GPU, and CPU. The wall-clock time is evaluated per\nimage on average in milliseconds. All models include the overhead\nof CFG at inference time, as well as the VAE decoding time when\napplicable. All autoregressive models utilize KV-cache to acceler-\nate inference. See Appendix A for further details.\nFor TPU wall-clock time, all models are evaluated using\na pre-compiled JAX sampling function on 8 TPU v4 cores.\nReported times exclude compilation overhead. We use a\nlocal device batch size of 10 for model inference, and 200\nfor VAE decoding.\nFor GPU wall-clock time, all models are re-implemented\nin PyTorch and evaluated on a single NVIDIA H200 GPU\nwith a batch size of 128. The VAE decoding time is obtained\nwith torch.compile optimization.\nFor CPU wall-clock time, we reuse the PyTorch imple-\nmentation on a single AMD EPYC 7B12 node (120 physical\nCPU cores and 400 GB RAM). We use a smaller batch size\nof 64 for most models; however, TARFlow and STARFlow\nare restricted to a batch size of 4 due to efficiency concerns.\nWe observe that batch size has a negligible impact on per-\nimage CPU inference time. All other experimental settings\nremain consistent with the GPU evaluation.\n9"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 10, "text": "Algorithm 1 BiFlow: Training.\n# x: training batch, (N, H, W, C)\n# F: forward model (B blocks), frozen\n# G: reverse model (B + 1 blocks)\n# phi: projection heads\n# noise injection\ne = randn like(x)\nx_tilde = x + noise_level * e\n# get forward trajectory xs and prior z\nxs, z = F(x_tilde)\n# get reverse trajectory hs and\nreconstructed x’\nhs, x_prime = G(z)\n# project hidden into input space\nfor i in range(B):\nhs[i] = phi[i](hs[i])\n# compute loss\nloss_align = mse(xs, hs)\nloss_recon = metric(x, x_prime)\nloss = loss_align + loss_recon\nAlgorithm 2 BiFlow: 1-NFE Sampling.\ne = randn(x_shape)\n_, x = G(e)\nB. Method Details\nB.1. Pseudocode\nWe provide the pseudocode for training our BiFlow model\nwith hidden alignment in Alg. 1, as well as the 1-NFE sam-\npling procedure in Alg. 2.\nIn the algorithm, the forward model F produces the entire\nforward trajectory xs, i.e., ˜x, x1, x2, . . . , xB−1, along with\nthe prior z = xB. Similarly, the reverse model G outputs the\nsequence of intermediate hiddens states hs as reverse tra-\njectory: hB−1, hB−2, . . . , h0, along with the reconstructed\nclean input x prime.\nThe final loss function consists of alignment loss between\nforward and reverse hidden states in Eq. (2), and reconstruc-\ntion loss between the clean input x and reconstructed output\nx prime.\nB.2. BiFlow with Guidance\nTraining-time CFG. As discussed in Sec. 4.5, to enable\nguided sampling within a single forward pass (1-NFE), we\ndirectly train a guided reverse model Gcfg\nϕ defined as\nGi,cfg\nϕ\n(hi | c) = (1 + wi)Gi\nϕ(hi | c) −wiGi\nϕ(hi).\n0.0\n0.5\n1.0\n1.5\nCFG scale\n0\n10\n20\n30\n40\nFID-50K\ninference-time CFG (2-NFE)\ntraining-time CFG    (1-NFE)\nFigure 7. Comparison between training-time and inference-time\nCFG of BiFlow. Training-time CFG achieves similar or better\nperformance compared to inference-time CFG while requiring only\nhalf inference compute and retaining full post-hoc tuning flexibility.\n(Settings: BiFlow-B/2, 160 epochs, MSE-only baseline.)\nwhere wi is the guidance scale at block i. The unconditional\noutput of Gi,cfg\nϕ\nmatches that of the original Gi\nϕ. Therefore,\nthe unguided block output can be expressed as\nhi+1 =\nGi,cfg\nϕ\n(hi | c) + wiGi,cfg\nϕ\n(hi)\n1 + wi\n.\n(5)\nDuring training, we compute our hidden-alignment loss di-\nrectly on hi+1 from Eq. (5). At inference time, this for-\nmulation allows us to use Gi,cfg\nϕ\ndirectly, producing guided\nsamples with only a 1-NFE forward pass. We add stop gra-\ndient to the unconditional output to stabilize training.\nGuidance conditioning.\nTo retain the ability to adjust\nthe CFG scale at inference time, we explicitly condition\nthe reverse model on the guidance scale [39, 18], i.e.,\nGi,cfg\nϕ\n(hi | c, wi). During training, we sample wi from a\nuniform distribution U(0, wmax) and apply Eq. (5) to com-\npute the unguided output hi+1 for hidden alignment loss.\nWe compare this training-time CFG scheme with the\nmore conventional inference-time CFG in Fig. 7. Training-\ntime CFG achieves similar (even better) performance while\npreserving the 1-NFE efficiency and the flexibility to sweep\nCFG scales at inference time.\nB.3. Distance Metric\nAdaptive weighting. We adopt the adaptive loss reweighting\nstrategy from [16, 52, 17]. Given a prediction x and target y,\nthe adaptive-weighted distance is defined as:\nDp = sg(wp) · D(x, y),\nwp = (D(x, y) + c)−p,\nwhere c is a small constant and p ≥0 is adaptive weight.\nsg(·) denotes the stop-gradient operator. We apply adaptive\nweighting to all loss terms in our training objective.\nVGG feature. For the perceptual loss based on VGG fea-\ntures, we follow the LPIPS formulation [68]. Since our\nmodel operates in the latent space of a pre-trained VAE, we\ndecode the predicted latent x′ back into image space and\ncompute the LPIPS loss against the ground-truth image.\n10"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 11, "text": "MSE\nLPIPS\nnaive distillation\n0.115\n0.331\nhidden distillation\n0.156\n0.392\nhidden alignment\n0.111\n0.321\nTable 7. Reverse learning methods: reconstruction fidelity. We\nreport MSE and LPIPS between the original sample x and the\nreconstructed sample x′ produced by the learned reverse model.\nAmong the three strategies for approximating the inverse transfor-\nmation, the hidden alignment method achieves the most accurate\nreconstruction. The corresponding FIDs are shown in Tab. 1 (Set-\ntings: BiFlow-B/2, 160 epochs, no CFG, adaptive-weighted MSE\nloss only. All three rows share the same forward model.)\nConvNeXt feature. In addition to VGG features, we in-\ncorporate ConvNeXt V2 [61] (ImageNet-22K pre-trained,\nbase-size) as a complementary perceptual feature extractor.\nSimilar to the LPIPS, both the reconstructed image x′ and\nthe ground-truth image x are passed through the ConvNeXt\nnetwork after VAE decoding. The perceptual distance is\ncomputed using the extracted features, excluding the final\nclassification head.\nUsage of loss terms. In the ablation studies in Sec. 5, we\nuse only the adaptively weighted MSE loss unless otherwise\nnoted. In Tab. 4, we combine all three loss terms:\nL(x) =\nX\ni\nLMSE(xi, φi(hi))\n+ wVGGLVGG(x, x′) + wConvNeXtLConvNeXt(x, x′),\nwhere wVGG and wConvNeXt are tunable hyperparameters. We\nobserve that the final performance is particularly sensitive to\nwConvNeXt. Concrete weights are specified in Appendix A.\nNormalized Trajectory. As described in Sec. 4.4, the re-\nverse model is trained to align with a normalized forward\ntrajectory. Specifically, we pre-compute the squared norm\n∥xi∥2 of each trajectory point and average it over the entire\ndataset. During reverse model training, the intermediate\ntrajectory points are divided by\np\nE[∥xi∥2], ensuring scale\nconsistency across different blocks.\nWe do not use normalized trajectories in any experiments\nexcept the one in Tab. 2c, as we observe no significant differ-\nence when combined with iTARFlow. Nonetheless, normal-\nized trajectories are worth noting for scenarios where one\nwishes to use a pre-trained NF model without clipping.\nC. Additional Experiments\nC.1. Learning to Approximate the Inverse\nIn Tab. 1, we compare the empirical performance of the\nthree reverse learning approaches introduced in Sec. 4. Here,\nwe further provide quantitative results on their reconstruction\nfidelity in Tab. 7. Specifically, we evaluate the reconstruction\ndistance D(x, x′) using MSE and LPIPS (VGG-based) as\nmetrics.\nwd\\w\n0.5\n0.6\n0.7\n0.5\n10.51\n9.45\n8.77\n0.6\n10.21\n9.21\n8.59\n0.7\n9.93\n8.99\n8.41\n3.5\n7.05\n6.87\n6.89\n4.0\n6.94\n6.81\n6.87\n4.5\n6.88\n6.79\n6.87\n5.0\n6.86\n6.80\n6.89\nTable 8. Separate guidance scale for the denoising block. BiFlow\neliminates the score-based denoising step in TARFlow by learning\na dedicated denoising block, jointly trained with other blocks. This\ndenoising block serves a different purpose from the rest NF reverse\nprocess. Using a separate, larger guidance scale for the denoising\nblock improves sample quality. (Settings: BiFlow-B/2, 160 epochs,\nadaptively-weighted MSE only, training-time CFG. FID w/o CFG:\n31.88.)\nWe observe that the proposed hidden-alignment strategy\nachieves the lowest regression loss across both metrics. This\nindicates that hidden alignment provides a more accurate\nmapping between x and x′, leading to a better-behaved re-\nverse learning process.\nC.2. BiFlow with Guidance\nAs discussed in Sec. 4.2, the additional denoising block\nin our reverse model functions as a dedicated denoiser, while\nthe preceding B blocks focus on inverting the forward sub-\ntransformations. This structure naturally motivates applying\nCFG differently across these two components. We empiri-\ncally validate this design choice in Tab. 8.\nFor training-time CFG, we use a shared guidance scale\nacross all blocks, sampling w from a simple uniform prior\nU[0, 0.5]. In ablation studies that use MSE loss only (Sec. 5),\nwe decouple the guidance scales for the inverse blocks and\nthe denoising block, since the optimal pair (w, wd) typically\nsatisfies wd ≫w. In this case, we sample w ∼U[0, 1] and\nwd ∼U[0, 8].\nC.3. Distance Metric\nIn Sec. 4.3, we discuss different choices of distance met-\nrics for training BiFlow. We ablate the choice of perceptual\ndistance terms in Tab. 9. First, we compare different fea-\nture extractors, including a ResNet-101 [22] pre-trained for\nclassification and a DINOv2-B model [40], as reported in\nTab. 9a. For ResNet-101, we extract features by remov-\ning the final MLP head, following the same procedure as\nConvNeXt. Among all tested feature extractors, ConvNeXt\nachieves the best empirical performance. We further evaluate\nthe combination of VGG and ConvNeXt features in Tab. 9b.\nThe results indicate that using both features together yields\nbetter FID scores than using either one individually.\nFurthermore, we study the effect of adaptive weighting\nin the MSE loss in Tab. 10. MSE with adaptive weighting\nconsistently outperforms the naive MSE loss.\n11"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 12, "text": "feature model\nFID, w/o CFG\nFID, w/ CFG\nnone\n31.88\n6.79\nVGG + ResNet\n9.69\n4.34\nVGG + DINO\n9.33\n4.36\nConvNeXt + DINO\n3.19\n3.19\nVGG + ConvNeXt\n2.46\n2.46\n(a) Feature model ablation.\nwVGG\nwConvNeXt\nFID, w/o CFG\nFID, w/ CFG\n0.0\n0.0\n31.88\n6.79\n1.0\n0.0\n16.97\n5.31\n0.0\n0.4\n2.62\n2.62\n1.0\n0.4\n2.46\n2.46\n(b) VGG and ConvNeXt weight ablation.\nTable 9. Ablation on perceptual loss for BiFlow-B/2. FID-50K\nwith/without CFG are reported. (Settings: BiFlow-B/2, 160 epochs,\ntraining-time CFG, weight for two perceptual losses are 1.0 and 0.4\nby default.)\nadaptive weight p\nFID, w/o CFG\nFID, w/ CFG\n0.0\n38.23\n7.49\n0.5\n34.74\n7.08\n1.0\n34.43\n6.70\n2.0\n31.88\n6.79\nTable 10. Ablation on adaptive weighting. Adaptive weighted\nMSE loss works better than naive MSE (p = 0.0). (Settings: BiFlow-\nB/2, 160 epochs, adaptive weighted MSE only, training-time CFG.)\nC.4. Improved TARFlow Norm Control\nTo mitigate the imbalance in loss magnitudes across dif-\nferent blocks of BiFlow, we introduce a simple yet effective\nmodification to the original TARFlow [65] in Sec. 4.4: clip-\nping the parameters µ and α within a fixed range. This\nadjustment stabilizes training and improves final FID perfor-\nmance.\nIn Fig. 8, we visualize the norms of intermediate trajec-\ntory states during training of the improved TARFlow. With-\nout clipping, the norms across blocks diverge sharply and\ncontinue to grow as training progresses. With clipping, the\nnorms remain stable and well-controlled within a reasonable\nrange. Such normalization substantially benefits the training\nof the reverse model in the downstream.\nC.5. Improved TARFlow CFG\nFor completeness, we also examine classifier-free guid-\nance (CFG) designs for TARFlow, although this component\nis orthogonal to our main contributions. In the original\nTARFlow [65], the reverse update rule at block i is\nzi\nt,cfg = zi+1\nt\n⊙exp(αi\nt,cfg) + µi\nt,cfg,\nwhere guidance is applied to the predicted parameters by\nαi\nt,cfg = (1 + wt) αi\nt(· | c) −wt αi\nt(·),\nµi\nt,cfg = (1 + wt) µi\nt(· | c) −wt µi\nt(·),\n0\n1\n2\n3\n4\n5\nTraining Steps (×106)\n0\n10\n20\n30\n40\n50\n60\nSquared L2 Norm\n||x1||2\n||x2||2\n||x3||2\n||x4||2\n||x5||2\n||x6||2\n||x7||2\n(a) Improved TARFlow w/o clipping.\n0\n1\n2\n3\n4\n5\nTraining Steps (×106)\n0.2\n0.4\n0.6\n0.8\nSquared L2 Norm\n||x1||2\n||x2||2\n||x3||2\n||x4||2\n||x5||2\n||x6||2\n||x7||2\n(b) Improved TARFlow w/ clipping.\nFigure 8.\nComparison of intermediate states’ norm during\nTARFlow training between training with and without clipping.\nLeft: without clipping, the norms at different blocks diverge signif-\nicantly, and continue to increase as training proceeds. Right: with\nclipping, the norms are well controlled within a reasonable range,\nstabilizing training and improving final FID scores.\nLinear\nConst\nµ, α\nz\nµ, α\nz\nOnline\n6.83(2.8)\n6.82(2.8)\n7.26(1.3)\n7.24(1.3)\nOffline\n22.14(1.2)\n22.03(1.2)\n18.23(1.0)\n18.11(1.0)\nTable 11. Improved TARFlow CFG ablation. Online guidance\nsubstantially outperforms the offline variants, whereas the choice\nof guidance schedule (linear vs. constant) and the level at which\nguidance is applied (µ, α vs. z) has only minor impact. Numbers\nin gray parentheses denote the corresponding optimal CFG scale.\n(Settings: improved TARFlow-B/2, FID w/o CFG: 44.46.)\nwith a linearly increasing guidance schedule wt =\nt\nT −1w\nalong the token dimension. Here, subscript t denotes the\ntoken dimension, superscript i denotes the block dimension,\nand (· | c) and (·) represent the conditional and uncondi-\ntional counterparts, respectively.\nFollowing prior CFG studies in diffusion models, we\ndecompose the design space into three orthogonal choices:\nSchedule. We can replace the original linearly increasing\nwt with a constant guidance scale: wt = w.\nSpace for applying guidance.\nParameter-space CFG\n(µ, α) vs. pixel-space CFG applied directly to z:\nzi\nt,cfg = (1 + wt)zi\nt(· | c) −wtzi\nt(·).\nWe denote these two settings by “µ, α” and “z”, respectively.\n12"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 13, "text": "Figure 9. Inpainting. BiFlow enables efficient image inpaint-\ning by leveraging its bidirectional mapping between images and\nnoise. By resampling the masked part of the noise, BiFlow can\nperform training-free inpainting on various image masks. Each\ntriplet contains ground-truth image (left), masked image (middle),\nand reconstructed image (right).\nOnline vs. Offline. We distinguish between online and\noffline CFG strategies. The online approach (TARFlow’s\npractice) applies guidance at each generation step; the offline\napproach generates the entire conditional and unconditional\nsequences independently and performs extrapolation only\nonce on the final outputs. The difference lies only in how\nguidance interacts with intermediate states.\nWhile both approaches have similar runtimes, Tab. 11\nshows that online CFG significantly outperforms the offline\nvariant. Regarding other hyperparameters, a linear schedule\noffers a slight advantage over a constant one, while applying\nguidance in parameter space versus pixel space yields similar\nperformance. Overall, TARFlow’s original CFG formulation\nis close to optimal.\nBased on these results, we use the original TARFlow CFG\nformulation (Online, Linear, Parameter-space) as our base-\nline. It is important to note that this CFG setting only affects\nthe inference quality of the forward model; the training of\nour BiFlow reverse model always relies on the unguided\nforward trajectory.\nD. Training-free Image Editing\nBiFlow naturally supports several training-free image\nediting applications by explicitly modeling a bidirectional\nmapping between the data and noise domains. We showcase\ntwo representative applications: inpainting and class editing.\nFor brevity, we omit the VAE encoder/decoder in the follow-\ning descriptions, as they always serve as pre-/post-processing\nsteps in our experiments.\nInpainting. The forward model Fθ encodes an image x\ninto noise z = Fθ(x). We empirically observe that localized\nperturbations in z predominantly affect corresponding spatial\nregions in the reconstructed image.\negyptian cat\n−→\nkit fox\nhen\n−→\nflamingo\ntennis ball\n−→\ngolden retriever\ndaisy\n−→\ncustard apple\nFigure 10. Class Editing. BiFlow constructs an explicit bidirec-\ntional mapping between images and noise. With this property,\nBiFlow is able to conduct training-free class editing by modifying\nonly the label condition in the forward and reverse process.\nBased on this property, BiFlow enables inpainting with\nan arbitrary binary mask M ∈{0, 1}H×W . Given a masked\nimage xmask = M ⊙x, we first map it to the noise domain\nusing the forward model: zmask = Fθ(xmask). We then\nresample the masked portion of the prior as\nz′ = M ⊙zmask + (1 −M) ⊙ϵ,\nϵ ∼N(0, I).\nFinally, the modified noise z′ is mapped back to the im-\nage domain by the reverse model Gϕ. This procedure fills\nthe masked region with content coherent with the context.\nRepresentative examples are shown in Fig. 9.\nClass Editing. The reverse model Gϕ allows us to generate\nimages from noise z under different class conditions. For\na fixed z, changing the class label c primarily modifies the\nclass-dependent appearance while largely preserving the\nglobal spatial structure.\nConcretely, given an image x with label c, we obtain its\nprior variable z = Fθ(x | c), and reconstruct it using a\ndifferent label c′, writing x′ = Gϕ(z | c′). As illustrated\nin Fig. 10, BiFlow effectively alters class-specific attributes\nwhile maintaining the overall structure, enabling intuitive\nclass editing without retraining.\nEfficiency. Both inpainting and class editing require only a\nsingle forward pass from data to noise and a single reverse\npass from noise to data, making BiFlow a lightweight and\nefficient tool for training-free image manipulation.\nE. Visualizations\nWe provide uncurated 1-NFE generation results of\nBiFlow-B/2 in Fig. 11 to Fig. 13.\nAcknowledgements. We greatly thank Google TPU Re-\nsearch Cloud (TRC) for granting us access to TPUs. Q. Sun,\nX. Wang, Z. Jiang, and H. Zhao are supported by the MIT\nUndergraduate Research Opportunities Program (UROP).\nWe thank Zhengyang Geng, Tianhong Li, and our other\ngroup members for their helpful discussions and feedback\non the draft.\n13"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 14, "text": "References\n[1] Michael S. Albergo and Eric Vanden-Eijnden. Building nor-\nmalizing flows with stochastic interpolants. In ICLR, 2023.\n[2] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James\nJohnson, Chris Leary, Dougal Maclaurin, George Necula,\nAdam Paszke, Jake VanderPlas, Skye Wanderman-Milne,\nand Qiao Zhang.\nJAX: composable transformations of\nPython+NumPy programs, 2018.\n[3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\nscale gan training for high fidelity natural image synthesis. In\nICLR, 2018.\n[4] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T.\nFreeman. Maskgit: Masked generative image transformer. In\nCVPR, 2022.\n[5] Huayu Chen, Kai Jiang, Kaiwen Zheng, Jianfei Chen, Hang\nSu, and Jun Zhu. Visual generation without guidance. In\nICML, 2025.\n[6] Hansheng Chen, Kai Zhang, Hao Tan, Leonidas Guibas,\nGordon Wetzstein, and Sai Bi. pi-flow: Policy-based few-\nstep generation via imitation distillation.\narXiv preprint\narXiv:2510.14974, 2025.\n[7] Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and\nDavid Duvenaud. Neural ordinary differential equations. In\nNeurIPS, 2018.\n[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\nFei-Fei. Imagenet: A large-scale hierarchical image database.\nIn CVPR, 2009.\n[9] Prafulla Dhariwal and Alex Nichol. Diffusion models beat\ngans on image synthesis. In NeurIPS, 2021.\n[10] Laurent Dinh, David Krueger, and Yoshua Bengio. Nice:\nNon-linear independent components estimation. In ICLR\nWorkshop, 2015.\n[11] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Den-\nsity estimation using real nvp. In ICLR, 2017.\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In ICLR, 2021.\n[13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim En-\ntezari, Jonas M¨uller, Harry Saini, Yam Levi, Dominik Lorenz,\nAxel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn,\nZion English, Kyle Lacey, Alex Goodwin, Yannik Marek,\nand Robin Rombach. Scaling rectified flow transformers for\nhigh-resolution image synthesis. In ICML, 2024.\n[14] Chris Finlay, J¨orn-Henrik Jacobsen, Levon Nurbekyan, and\nAdam M Oberman. How to train your neural ode: the world\nof jacobian and kinetic regularization. In ICML, 2020.\n[15] Kevin Frans, Danijar Hafner, Sergey Levine, and Pieter\nAbbeel. One step diffusion via shortcut models. In ICLR,\n2024.\n[16] Zhengyang Geng, Ashwini Pokle, Weijian Luo, Justin Lin,\nand J. Zico Kolter. Consistency models made easy. In ICLR,\n2024.\n[17] Zhengyang Geng, Mingyang Deng, Xingjian Bai, J. Zico\nKolter, and Kaiming He. Mean flows for one-step generative\nmodeling. In NeurIPS, 2025.\n[18] Zhengyang Geng, Yiyang Lu, Zongze Wu, Eli Shechtman,\nJ. Zico Kolter, and Kaiming He. Improved mean flows: On the\nchallenges of fastforward generative models. arXiv preprint\narXiv:2512.02012, 2025.\n[19] Arnab Ghosh, Harkirat Singh Behl, Emilien Dupont, Philip\nH. S. Torr, and Vinay Namboodiri. Steer: Simple temporal\nregularization for neural ode. In NeurIPS, 2020.\n[20] Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, Ilya\nSutskever, and David Duvenaud. Ffjord: Free-form continu-\nous dynamics for scalable reversible generative models. In\nICLR, 2018.\n[21] Jiatao Gu, Tianrong Chen, David Berthelot, Huangjie Zheng,\nYuyang Wang, Ruixiang Zhang, Laurent Dinh, Miguel An-\ngel Bautista, Josh Susskind, and Shuangfei Zhai. Starflow:\nScaling latent normalizing flows for high-resolution image\nsynthesis. In NeurIPS, 2025.\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR, 2016.\n[23] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-\nhard Nessler, and Sepp Hochreiter. Gans trained by a two\ntime-scale update rule converge to a local nash equilibrium.\nIn NeurIPS, 2017.\n[24] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. In NeurIPS Workshop, 2021.\n[25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In NeurIPS, 2020.\n[26] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli\nShechtman, Sylvain Paris, and Taesung Park. Scaling up gans\nfor text-to-image synthesis. In CVPR, 2023.\n[27] Tero Karras, Miika Aittala, Samuli Laine, Erik H¨ark¨onen,\nJanne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free\ngenerative adversarial networks. In NeurIPS, 2021.\n[28] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In ICLR, 2015.\n[29] Diederik P. Kingma and Prafulla Dhariwal. Glow: Generative\nflow with invertible 1x1 convolutions. In NeurIPS, 2018.\n[30] Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi\nChen, Ilya Sutskever, and Max Welling. Improved variational\ninference with inverse autoregressive flow. In NeurIPS, 2016.\n[31] Tuomas Kynk¨a¨anniemi, Miika Aittala, Tero Karras, Samuli\nLaine, Timo Aila, and Jaakko Lehtinen. Applying guidance\nin a limited interval improves sample and distribution quality\nin diffusion models. In NeurIPS, 2024.\n[32] Kyungmin Lee, Sihyun Yu, and Jinwoo Shin. Decoupled\nmeanflow: Turning flow models into flow maps for acceler-\nated sampling. arXiv preprint arXiv:2510.24474, 2025.\n[33] Tianhong Li and Kaiming He. Back to basics: Let denoising\ngenerative models denoise. arXiv preprint arXiv:2511.13720,\n2025.\n[34] Tianhong Li, Dina Katabi, and Kaiming He. Return of uncon-\nditional generation: A self-supervised representation genera-\ntion method. In NeurIPS, 2024.\n[35] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and\nKaiming He. Autoregressive image generation without vector\nquantization. In NeurIPS, 2024.\n14"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 15, "text": "[36] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maxi-\nmilian Nickel, and Matt Le. Flow matching for generative\nmodeling. In ICLR, 2023.\n[37] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight\nand fast: Learning to generate and transfer data with rectified\nflow. In ICLR, 2023.\n[38] Nanye Ma, Mark Goldstein, Michael S. Albergo, Nicholas M.\nBoffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring\nflow and diffusion-based generative models with scalable\ninterpolant transformers. In ECCV, 2024.\n[39] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik P.\nKingma, Stefano Ermon, Jonathan Ho, and Tim Salimans.\nOn distillation of guided diffusion models. In CVPR, 2023.\n[40] Maxime Oquab, Timoth´ee Darcet, Th´eo Moutakanni, Huy Vo,\nMarc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel\nHaziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud\nAssran, Nicolas Ballas, Wojciech Galuba, Russell Howes,\nPo-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat,\nVasu Sharma, Gabriel Synnaeve, Hu Xu, Herv´e Jegou, Julien\nMairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski.\nDinov2: Learning robust visual features without supervision.\nTMLR, 2024.\n[41] George Papamakarios, Theo Pavlakou, and Iain Murray.\nMasked autoregressive flow for density estimation.\nIn\nNeurIPS, 2017.\n[42] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In ICCV, 2023.\n[43] Yansong Peng, Kai Zhu, Yu Liu, Pingyu Wu, Hebei Li, Xi-\naoyan Sun, and Feng Wu. Flow-anchored consistency models.\narXiv preprint arXiv:2507.03738, 2025.\n[44] Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan L Yuille,\nand Liang-Chieh Chen. Beyond next-token: Next-x prediction\nfor autoregressive visual generation. In ICCV, 2025.\n[45] Danilo Jimenez Rezende and Shakir Mohamed. Variational\ninference with normalizing flows. In ICML, 2015.\n[46] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¨orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In CVPR, 2022.\n[47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentation.\nIn MICCAI, 2015.\n[48] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\nCheung, Alec Radford, and Xi Chen. Improved techniques\nfor training gans. In NeurIPS, 2016.\n[49] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl:\nScaling stylegan to large diverse datasets. In SIGGRAPH,\n2022.\n[50] Karen Simonyan and Andrew Zisserman. Very deep convolu-\ntional networks for large-scale image recognition. In ICLR,\n2015.\n[51] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising\ndiffusion implicit models. In ICLR, 2020.\n[52] Yang Song and Prafulla Dhariwal. Improved techniques for\ntraining consistency models. In ICLR, 2023.\n[53] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo\nWen, and Yunfeng Liu. Roformer: Enhanced transformer\nwith rotary position embedding. Neurocomputing, 2024.\n[54] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the inception archi-\ntecture for computer vision. In CVPR, 2016.\n[55] Zhicong Tang, Jianmin Bao, Dong Chen, and Baining Guo.\nDiffusion models without classifier-free guidance.\narXiv\npreprint arXiv:2502.12154, 2025.\n[56] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Li-\nwei Wang. Visual autoregressive modeling: scalable image\ngeneration via next-scale prediction. In NeurIPS, 2024.\n[57] Jakub M. Tomczak and Max Welling.\nImproving varia-\ntional auto-encoders using householder flow. arXiv preprint\narXiv:1611.09630, 2016.\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017.\n[59] Shuai Wang, Zhi Tian, Weilin Huang, and Limin Wang.\nDdt:\nDecoupled diffusion transformer.\narXiv preprint\narXiv:2504.05741, 2025.\n[60] Zidong Wang, Yiyuan Zhang, Xiaoyu Yue, Xiangyu Yue,\nYangguang Li, Wanli Ouyang, and Lei Bai. Transition models:\nRethinking the generative learning objective. arXiv preprint\narXiv:2509.04394, 2025.\n[61] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei\nChen, Zhuang Liu, In So Kweon, and Saining Xie. Con-\nvnext v2: Co-designing and scaling convnets with masked\nautoencoders. In CVPR, 2023.\n[62] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruc-\ntion vs. generation: Taming optimization dilemma in latent\ndiffusion models. In CVPR, 2025.\n[63] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-\nChieh Chen. Randomized autoregressive visual generation.\nIn ICCV, 2025.\n[64] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong,\nJonathan Huang, Jinwoo Shin, and Saining Xie. Representa-\ntion alignment for generation: Training diffusion transformers\nis easier than you think. In ICLR, 2025.\n[65] Shuangfei Zhai, Ruixiang Zhang, Preetum Nakkiran, David\nBerthelot, Jiatao Gu, Huangjie Zheng, Tianrong Chen,\nMiguel Angel Bautista, Navdeep Jaitly, and Joshua Susskind.\nNormalizing flows are capable generative models. In ICML,\n2024.\n[66] Biao Zhang and Rico Sennrich. Root mean square layer\nnormalization. In NeurIPS, 2019.\n[67] Huijie Zhang, Aliaksandr Siarohin, Willi Menapace, Michael\nVasilkovsky, Sergey Tulyakov, Qing Qu, and Ivan Sko-\nrokhodov. Alphaflow: Understanding and improving mean-\nflow models. arXiv preprint arXiv:2510.20771, 2025.\n[68] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR, 2018.\n[69] Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining\nXie. Diffusion transformers with representation autoencoders.\narXiv preprint arXiv:2510.11690, 2025.\n[70] Linqi Zhou, Stefano Ermon, and Jiaming Song. Inductive\nmoment matching. In ICML, 2025.\n15"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 16, "text": "class 12: house finch, linnet, Carpodacus mexicanus\nclass 81: ptarmigan\nclass 207: golden retriever\nclass 279: Arctic fox, white fox, Alopex lagopus\nclass 309: bee\nclass 323: monarch, monarch butterfly, milkweed butterfly, Danaus plexippus\nclass 327: starfish, sea star\nclass 417: balloon\nFigure 11. Uncurated 1-NFE class-conditional generation samples of BiFlow-B/2 on ImageNet 256×256. CFG scale: 2.0\n16"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 17, "text": "class 425: barn\nclass 437: beacon, lighthouse, beacon light, pharos\nclass 449: boathouse\nclass 497: church, church building\nclass 538: dome\nclass 554: fireboat\nclass 562: fountain\nclass 616: knot\nFigure 12. Uncurated 1-NFE class-conditional generation samples of BiFlow-B/2 on ImageNet 256×256. CFG scale: 2.0\n17"}
{"pdf_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 18, "text": "class 646: maze, labyrinth\nclass 649: megalith, megalithic structure\nclass 888: viaduct\nclass 952: fig\nclass 970: alp\nclass 973: coral reef\nclass 975: lakeside, lakeshore\nclass 985: daisy\nFigure 13. Uncurated 1-NFE class-conditional generation samples of BiFlow-B/2 on ImageNet 256×256. CFG scale: 2.0\n18"}
