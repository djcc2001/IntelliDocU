{"pdf_id": "arxiv_251210954_group_diffusion", "page": 1, "text": "Group Diffusion: Enhancing Image Generation\nby Unlocking Cross-Sample Collaboration\nSicheng Mo1 Thao Nguyen2 Richard Zhang3 Nick Kolkin3 Siddharth Srinivasan Iyer3\nEli Shechtman3 Krishna Kumar Singh3 Yong Jae Lee3 Bolei Zhou1 Yuheng Li3\n1University of California, Los Angeles\n2University of Wisconsin‚ÄìMadison\n3Adobe Research\nhttps://sichengmo.github.io/GroupDiff/\nAbstract\nIn this work, we explore an untapped signal in diffusion\nmodel inference. While all previous methods generate im-\nages independently at inference, we instead ask if samples\ncan be generated collaboratively. We propose Group Diffu-\nsion, unlocking the attention mechanism to be shared across\nimages, rather than limited to just the patches within an im-\nage. This enables images to be jointly denoised at inference\ntime, learning both intra and inter-image correspondence.\nWe observe a clear scaling effect ‚Äì larger group sizes yield\nstronger cross-sample attention and better generation qual-\nity.\nFurthermore, we introduce a qualitative measure to\ncapture this behavior and show that its strength closely cor-\nrelates with FID. Built on standard diffusion transformers,\nour GroupDiff achieves up to 32.2% FID improvement on\nImageNet-256√ó256. Our work reveals cross-sample infer-\nence as an effective, previously unexplored mechanism for\ngenerative modeling.\n1. Introduction\n‚ÄúAlone we can do so little;\ntogether we can do so much.‚Äù\nHelen Keller\nDuring generative model training, network weights are\noptimized using batches of images to learn an underlying\nimage distribution [6, 12, 16, 22, 36, 44]. However, at in-\nference time, images are typically generated independently.\nWhile patches within an image can interact to produce a co-\nherent output, patches across different images are processed\nseparately. This raises an intriguing, unexplored question ‚Äì\ncan images and patches across a batch collaborate to en-\nhance generation quality collectively?\nFollowing our inquiry, we introduce Group Diffusion,\nwhich jointly denoises a group of samples with the same\nExample \ngeneration\nCo-generations \nwithin group\nGroup Diffusion\n(Ours)\n1\nFID: 3.50\nFID: 2.92\nFID: 2.42\nFID: 2.14\nIncreasing \ngroup size\n2\n4\n8\nIndividual Diffusion\n(baseline)\nFigure 1. In standard diffusion (top row), samples are generated\nindependently. Our GroupDiff uses cross-sample attention, en-\nabling samples within a batch to collaborate on a generation. We\nshow selected examples of class-conditional ImageNet generation,\nusing group sizes {1, 2, 4, 8}. We find that the average generation\nquality improves with larger group size.\nconditioning. This is enabled using bidirectional attention\nacross samples. During training, we construct each group\nby querying semantically or visually similar samples from\nthe training dataset, allowing the attention mechanism to\nsee all patches from within the group. Then, at test time,\nwe generate images in a batch, allowing images within the\nbatch to aid one another in the diffusion process.\nWe observe a clear scaling effect, where increasing the\ngroup size strengthens cross-sample attention and consis-\ntently improves generation quality, as illustrated in Figure 1.\nWe further analyze the attention patterns across images. As\nshown in Figure 2, the group-wise denoising enables each\npatch to attend to others within the group, allowing the\n1\narXiv:2512.10954v1  [cs.CV]  11 Dec 2025"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 2, "text": "Class 555: fire engine, fire truck\nClass 980: volcano\nClass 88: macaw\nClass 208: golden retriever\nFigure 2. Attention map visualization. We show the attention map, using the query point starred on the left, across samples (group size\n4), from the second layer. The star refers to the anchor patch. High attention score patches are denoted in red. During the generation\nprocess, each image patch is encouraged to attend to similar patches from other images, which enhances the generation quality.\nmodel to learn both intra and inter-image correspondence.\nInterestingly, we show that generation quality is largely de-\ntermined by how attention is distributed across samples,\nwith the model assigning higher weights to semantically\nrelevant samples that exert a stronger influence on the fi-\nnal output. We additionally identify a qualitative measure\nof cross-sample attention whose strength correlates closely\nwith generation quality, providing deeper insight into how\ngroup-wise interaction governs the generation process.\nWe summarize our contribution as follows:\n(1) We\npresent GroupDiff, a simple yet effective framework that\njointly denoise a group of samples with the same condi-\ntion rather than individual images, enabling cross-sample\ninteraction through attention.\n(2) A systematic study on\nGroupDiff training and inference behavior, offering insights\nfor better leveraging inter-sample correspondence in image\ngeneration. (3) Our framework improves generation qual-\nity and flexibility over traditional systems; e.g., integrat-\ning GroupDiff with SiT yields 20.9% and 32.2% better FID\nwhen trained from scratch and resumed from a pre-trained\ncheckpoint, respectively.\n2. Related Work\nDiffusion models. Powered by their ability to model com-\nplex distributions via iterative denoising, diffusion mod-\nels have become the leading paradigm for high-fidelity im-\nage [9, 16, 42, 44, 45], video [17, 24, 32, 37, 58, 66, 71]\nand multi-modal concept [34, 47, 72] generation.\nBe-\nsides relying solely on the diffusion objective, recent litera-\nture [26, 62, 67, 70] explores the alignment between gener-\native modeling and representation learning. REPA [67] ac-\ncelerates diffusion model training by aligning its represen-\ntation with the pretrained SSL models. REPA-E [26] further\nleverages the pretrained model‚Äôs knowledge with additional\nlearnable parameters from the latent encoder.\nMeanwhile, another line of work addresses this potential\nlimitation from the pre-trained vision encoder by aligning\ncross-layer features to each other (SRA [20]) or explicitly\napplying SSL object function on generative model represen-\ntation (Dispersive Loss [59]). In contrast, GroupDiff learns\na stronger representation implicitly by allowing group at-\ntention to learn both inter and intra-image correspondence.\nThis novel approach offers a fresh perspective on integrat-\ning diffusion modeling with representation learning.\nSemantic correspondence in diffusion models. Semantic\ncorrespondence maps semantically related regions across\nimages, enabling alignment despite changes in appearance\nor pose. In addition to its state-of-the-art generation ca-\npability, a large-scale pre-trained text-to-image diffusion\nmodel [9, 43, 45] naturally captures such semantic cor-\nrespondence robustly, which unlocks promising applica-\ntions in classification [27] and segmentation [51, 54, 63]\nwith such features. Meanwhile, a line of works [30, 69]\nextract high-quality representation from the denoiser by\nadding different levels of noise and enabling robust cross-\nimage point matching. Follow-up work leverages the global\nlevel dense semantic correspondence for image-to-image\ntranslation [8, 29, 33, 35, 56] method without additional\ntraining. Furthermore, there is another line of work that\ngoes beyond single-image generation to multi-view genera-\ntion [18], style-controlled group generation [48], and video\ngeneration [21], by modeling inter-image correspondence\nwith mutual attention. Different from the aforementioned\nliterature, our method explicitly leverages cross-sample re-\nlationships to enhance individual sample‚Äôs quality by jointly\ndenoising all images within a group together, instead of im-\nplicitly learning it from individual samples.\nUnified transformer models.\nTransformer models [57]\nhave unified domain-specific architecture design across lan-\nguage, vision, and audio. It first showcased its strong capa-\nbility on encoder-decoder and later decoder-only language\nmodels in the language domain. ViT [7] proposed to con-\n2"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 3, "text": "Individual\ndiffusion\n(baseline)\nGroup\nDiffusion\n(ours)\nReshape\nMulti-Head \nSelf-Attention\nPointwise \nFeedforward\nùëê\nInput\ntokens\nConditioning\npatches\nbatch\nReshape\nGroup Attention Operation\nMLP\nOutput\ntokens\nFigure 3. Approach. (Left) Previous approaches generate images independently. We explore Group Diffusion, which allows a set of\nimages to collaborate together during inference time. (Right) Group attention can be implemented simply by reshaping the tokens within\na batch, before and after the attention operation.\nvert images to a series of smaller patches to adapt the trans-\nformer model to the vision field and find its remarkable\nscaling capabilities under increasing data, training compute,\nand data. In the image generative model field, Diffusion\nTransformer [40] firstly verified the outstanding scalability\nof such an architecture, and a similar model design has been\nfurther extended to video diffusion models in [2, 24, 58].\nMoreover, multi-modal models [34, 47, 53, 72] with unified\ntransformer again verified the generaizability of such archi-\ntecture. GroupDiff benefits from the flexibility of the uni-\nfied transformer model design by adding multi-image gen-\neration capability to the image generative model.\n3. Group Diffusion\n3.1. Preliminary\nDiffusion\nmodels\ngradually\nreverse\nthe\nprocess\nof\nadding noise to an image, starting from a noise vec-\ntor xT and progressively generating less noisy samples\nxT ‚àí1, xT ‚àí2, ..., x0 with learned denoising function eŒ∏.\nThe training objective aims to minimize the difference\nbetween the predicted and true noise. Specifically, for each\ntime step t, the objective is to solve the following denoising\nproblem on the image data x:\nLDM = Ex,œµ‚àºN(0,I),t\n\u0002\n‚à•œµ ‚àíeŒ∏(xt; t, c)‚à•2\n2\n\u0003\n,\n(1)\nwhere xt is the noisy image at time step t, uniformly sam-\npled from {1, . . . , T}, and eŒ∏(xt, t, c) is the denoising func-\ntion that predicts the noise added to xt conditioned on the\ntime step t and context c (often a text prompt or class label).\nClassifier-free diffusion guidance [15] enables control-\nling the trade-off between sample quality and diversity in\ndiffusion models. It shifts pŒ∏(c|xt) to assign a higher likeli-\nhood to the condition c without additional classifier. This is\nimplemented by training the diffusion model for both con-\nditional and unconditional denoising and combining the two\nscore estimates at inference time. Specifically, at inference\ntime, the modified score estimate ÀúeŒ∏(xt, c) is extrapolated\nin the direction towards the conditional eŒ∏(xt, c) and away\nfrom the unconditional eŒ∏(xt, ‚àÖ).\nÀúeŒ∏(xt; t, c) = eŒ∏(xt; t, c) + s ¬∑\n\u0000eŒ∏(xt; t, c) ‚àíeŒ∏(x;t, ‚àÖ)\n\u0001\n(2)\n3.2. Approach\nAt the core of our method is the idea of generating multi-\nple images together, so each sample can enhance its gener-\nation by selectively learning from other samples, as illus-\ntrated in Figure 2. In our GroupDiff, we construct a group\nwith related image data, thus allowing the diffusion model\nto learn a better representation that can be aided by other\nsamples. At test time, we generate multiple images, condi-\ntioned on the same conditioning c, a setup that aligns well\nwith modern applications, where users typically expect sev-\neral outputs under the same condition. We follow best prac-\ntices, adopting the Diffusion Transformer (DiT [40]) model\narchitecture, which uses an attention mechanism between\npatches within an image. We simply modify the attention\nby concatenating the group of image patches together, so\nthat each patch can take other samples into consideration.\nTo ensure that the diffusion model can recognize different\nimage samples, we add the same learnable sample embed-\nding to all patches from a given image. We formally define\nthe GroupDiff method as follows.\nQuery method. Our hypothesis for GroupDiff is that im-\nages in the same group are related either semantically or\nvisually, and can be used to aid in the denoising process.\n3"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 4, "text": "Thus, we must construct sets of images that are related dur-\ning training time. Given the image x ‚ààRH√óW √ó3 and the\nentire image dataset D, we define the query function q(x)\nas the following:\nq(x; D; œÑimg) = {xi ‚ààD | sim(x, xi) ‚â•œÑimg} ,\n(3)\nwhere sim(¬∑) returns the image similarity between two im-\nages, and œÑimg is a similarity threshold. In practice, we com-\npute the sim(¬∑) by cosine similarity between image embed-\ndings from pre-trained models like CLIP [41] or DINO [38].\nGroupDiff training. At each training step, we first con-\nstruct a group of related images X ‚ààRN√óH√óW √ó3, includ-\ning the original image x, by randomly sampling N ‚àí1 im-\nages from the images returned by query function q(x; D; œÑ).\nWe use threshold œÑimg = 0.7 in our experiments, which\nretrieves a sufficient number of related samples. For such\nimage group, we first extract their latent with a pre-trained\nVAE from Stable Diffusion [45]. To obtain the noisy la-\ntent, we sample the timestep independently for each sam-\nple but ensure that the variance of the timestep within each\ngroup is under the threshold of timestep variation œÉtv. To\ncompute the group attention, we first extract the hidden\nstates h from the input X, and then reshape them from\nRN√óL√óC ‚ÜíR1√ó(NL)√óC, where L is the image patch se-\nquence length and C is the channel. After the Attention(¬∑)\noperation, we reshape the hidden states back.\nIn particular, GroupDiff enables generating multiple\nsamples in a group by using LGroup as the loss function as\nfollows:\nLGroup = EX,E‚àºN(0,I),t\n\" N\nX\ni=1\n‚à•œµi ‚àíeŒ∏(X; t, c)i‚à•2\n2\n#\n, (4)\nwhere c is the condition and t is the denoising timestep.\nGroupDiff inference. GroupDiff enables generating N de-\npendent images following the condition c together at the in-\nference time, instead of N independent image as in previous\nsystems [40]. At each timestep, the denoiser predicts two\nscores: conditional and unconditional. We introduce two\nvariations of our method, GroupDiff-f and GroupDiff-l,\nby flexibly deciding whether to predict the conditional score\nwith group attention or not. For GroupDiff-f, we obtain both\nscores from group attention and apply the CFG guidance to\ncombine those scores as follows:\nÀúeŒ∏(Xt; t, c) = eŒ∏(Xt; t, c)\n+ s ¬∑ (eŒ∏(Xt; t, c) ‚àíeŒ∏(Xt; t, ‚àÖ)).\n(5)\nFor GroupDiff-l, only the unconditional score is predicted\nfrom group attention. In this case, we obtain ÀúeŒ∏ as follows:\nÀúeŒ∏(Xt; t, c) = {eŒ∏(Xi\nt; t, c)}n\ni=1\n+ s ¬∑ ({eŒ∏(Xi\nt; t, c)}n\ni=1 ‚àíeŒ∏(Xt; t, ‚àÖ)),\n(6)\nwhere Xi\nt is the ith element in group X.\nBy convention, only 10% of the data is used to train the\nunconditional model for generation with CFG [15]. Since\nGroupDiff-l applies the large group size only to this uncon-\nditional model, the remaining 90% is trained with a group\nsize of one. Thus, most of the training remains identical\nto standard diffusion, making GroupDiff-l computationally\nlightweight compared to GroupDiff-f and close to baseline\nsystems [31, 40].\nEmpirically, we find that GroupDiff-\nl strikes a good balance between generation quality and\ncomputational cost.\nThroughout the paper, we refer to\nGroupDiff as GroupDiff-l unless otherwise specified.\n4. Experiments\nWe now analyze our proposed GroupDiff, beginning with\nthe introduction of the experiment setup and a series of ab-\nlation studies on the group settings, followed by observa-\ntions of the intriguing property and behavior of GroupDiff.\nLastly, we benchmark with previous leading systems.\n4.1. Setup\nImplementation Details. We strictly follow the DiT [40]\nand SiT [31] model architecture/configuration and data pro-\ncess.\nWe train the GroupDiff with AdamW optimizer,\na constant learning rate of 1 √ó 10‚àí4, and weight decay\n0.01 on A100 GPUs.\nSampling is performed using the\nSDE Euler-Maruyama sampler and the iDDPM [36] sam-\npler with NFE = 250 when SiT [31] and DiT [40] are se-\nlected as the baseline model, respectively. We consistently\nuse a global batch size of 256 when adjusting the group size\nto ensure a fair comparison across variations and baseline\nmethods. Additional implementation details and baseline\nintroduction are provided in the Supplementary.\nDatasets and metrics. Following DiT [40], we conduct\nexperiments on ImageNet [5] and use a pretrained Stable\nDiffusion VAE with a compression ratio of 8 to encode each\n256 √ó 256 image into a compressed vector x ‚ààR32√ó32√ó4.\nAnd we report the FID [14], Inception Score [46], Precision\nand Recall [25] for measuring the generation quality.\n4.2. Main Properties\nAs shown in Table 1, we discover that GroupDiff consis-\ntently provides a substantially improved generation perfor-\nmance across various design choices, achieving a much bet-\nter FID score than the vanilla model. Below, we provide a\ndetailed analysis of the impact of each component.\nGroup model. The leading diffusion systems usually ben-\nefit from Classifier-Free Guidance [15], which takes the\njoint effect with the conditional model and unconditional\nmodel. In practice, those two models usually share most\nmodel weights besides the condition embedding. We begin\n4"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 5, "text": "GroupDiff Settings\nw/o CFG\nw/ CFG\nCross-Sample\nLinear Prob\nIter\nModel\nQuery Method\nNoise Var.\nFID ‚Üì\nFID ‚Üì\ncfg-scale\nAttn. Score ‚Üë\nAcc. ‚Üë\n800K\nC = 1, UC = 1\n-\n0\n14.38\n3.50\n1.5\n-\n49.48\n800K\nC = 4, UC = 4\nClass\n0\n14.27\n3.08\n1.6\n-\n62.15\n800K\nC = 1, UC = 4\nClass\n0\n13.22\n2.81\n1.6\n-\n64.44\n800K\nC = 1, UC = 2\nCLIP-L\n0\n13.47\n2.92\n1.5\n0.00%\n55.33\n800K\nC = 1, UC = 4\nCLIP-L\n0\n13.93\n2.42\n2.0\n19.95%\n58.83\n800K\nC = 1, UC = 8\nCLIP-L\n0\n13.08\n2.14\n2.2\n51.13%\n67.93\n800K\nC = 1, UC = 16\nCLIP-L\n0\n13.84\n1.86\n2.5\n56.47%\n72.91\n800K\nC = 1, UC = 4\nRandom\n0\n13.28\n3.57\n1.5\n23.17%\n-\n800K\nC = 1, UC = 4\nClass\n0\n13.22\n2.81\n1.6\n22.51%\n64.44\n800K\nC = 1, UC = 4\nCLIP-B\n0\n13.47\n2.51\n1.9\n19.20%\n61.14\n800K\nC = 1, UC = 4\nCLIP-L\n0\n13.93\n2.42\n2.0\n19.95%\n58.83\n800K\nC = 1, UC = 4\nSigLIP\n0\n13.83\n2.45\n2.0\n19.98%\n63.32\n800K\nC = 1, UC = 4\nDINOv2-B\n0\n14.40\n2.51\n1.9\n18.45%\n63.32\n800K\nC = 1, UC = 4\nDINOv2-L\n0\n13.35\n2.51\n1.9\n22.85%\n59.16\n800K\nC = 1, UC = 4\nI-JEPA\n0\n13.08\n2.44\n1.8\n18.50%\n60.50\n800K\nC = 1, UC = 4\nCLIP-L\n0\n13.93\n2.42\n2.0\n19.95%\n58.83\n800K\nC = 1, UC = 4\nCLIP-L\n20\n13.50\n2.42\n2.0\n21.37%\n60.48\n800K\nC = 1, UC = 4\nCLIP-L\n50\n12.81\n2.34\n2.0\n26.23%\n68.91\n800K\nC = 1, UC = 4\nCLIP-L\n100\n12.78\n2.32\n1.9\n23.33%\n62.82\n800K\nC = 1, UC = 4\nCLIP-L\n150\n13.70\n2.25\n2.0\n24.31%\n63.74\n800K\nC = 1, UC = 4\nCLIP-L\n200\n13.26\n2.32\n1.8\n24.46%\n60.03\nTable 1. Component-wise analysis on ImageNet 256 √ó 256 with DiT-XL/2 [40] trained for 800K iterations. All metrics except accuracy\n(Acc.) are measured with the iDDPM [36] sampler with NFE= 250. For generation results with Classifier-Free Guidance, we search for\nthe optimal guidance scale using an interval of 0.1 and report the one with the optimal FID score. ‚Üëand ‚Üìindicate whether higher or lower\nvalues are better, respectively. C and UC referring to the conditional model and unconditional model,respectively.\nthe ablation by analyzing the model behavior when apply-\ning the Group Attention operation on one or both models.\nIn this analysis, we use the ImageNet [5] class label as\nthe query method to build each group. We observe that\nGroupDiff consistently outperforms the individual diffusion\nbaseline. Notably, when only running the UC model in the\nGroupDiff mode, our system further achieves higher gener-\nation quality when both the CFG is disabled or enabled, re-\nflected by lower FID. Under this setting, we observe that the\ncondition model‚Äôs generation capability has also improved\nwhen we train only the unconditional model with group at-\ntention. We hypothesize that the stronger representation in\nthe UC model implicitly enhances the C model via weight\nsharing. In later experiments, we set C=1, UC=N as the\ndefault choice to balance training and inference.\nGroup size. We also study the impact of group size in\nGroupDiff.\nLarger groups generally yield better genera-\ntion results, as reflected by consistent improvements in FID\nand feature quality. We hypothesize that larger groups of-\nfer greater flexibility for finding better patch-level matches,\nthereby enhancing generation and internal representations.\nDetailed pattern analysis is provided in Sec. 4.3. In the fol-\nlowing experiments, we choose 4 as the group size for fair\ncomparison with baseline methods.\nGroup construction method. We then investigate the im-\nClass Label\nCLIP-L\nDINOv2-L\nQuery Image\nFigure 4.\nComparison of group candidates from different\nquery methods. The difference in pretraining settings lead each\nquery method to form distinct groups. We show nearest samples\nfrom the ImageNet [5] training split, with the class label row show-\ning random same-class samples.\npact of different group construction methods, including ran-\ndom sampling, class-based grouping, and similarity-based\nretrieval via pre-trained vision encoders.\nQuantitatively,\nsimilarity-based grouping yields the best generation quality,\nfollowed by class-based grouping, while random sampling\nperforms the worst (on par with the baseline). This indicates\nthat group attention does not degrade the baseline diffusion\nmodel‚Äôs performance, even without any bells and whistles.\nMeanwhile, we hypothesize that image similarity within a\n5"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 6, "text": "FID\nCross-Sample Score (%)\nCross Sample Attention Weight\nLayer Index\nFigure 5. Cross-Sample Attention in GroupDiff. (a) FID vs Cross-Sample Score (left). Our GroupDiff shows a strong correlation\n(0.95) between cross-attention to other samples and generation quality. (b) Cross-Sample Attention Visualization (right).\ngroup is crucial for strengthening cross-sample interaction.\nRandom groups often contain unrelated samples and thus\nlack meaningful mutual information, whereas similarity-\nbased retrieval retrieves semantically coherent images, re-\nducing the FID (with CFG) from 3.57 to around 2.4.\nInterestingly, Figure 4 shows that different pre-trained\nencoders form visually distinct groups. For instance, CLIP-\nL [41] tends to cluster semantically similar samples, while\nDINOv2-B [38] captures alternative aspects of visual sim-\nilarity. Nevertheless, their resulting generation quality re-\nmains comparable, suggesting that the benefit primarily\narises from semantic consistency rather than the specific en-\ncoder style. Overall, GroupDiff demonstrates strong flex-\nibility and generalization, showing that the quality of the\npre-trained encoders does not limit its performance.\nGroup noise-level variation. Lastly, we explore the ef-\nfect of introducing noise-level variance within each group.\nInstead of applying the same noise level to the entire group,\nwe restrict the noise levels of the other samples to differ\nfrom that of the first sample by up to a specified range, e.g.\n50 or 200. Prior works [4, 65] verified that adding different\nlevel of noise could be an effective augmentation method for\nimproving representations learning and generation quality.\nIn our setting, we hypothesize that noisier samples benefit\nfrom cleaner ones within the same group, further encourag-\ning cross-sample attention. We find that setting the noise-\nlevel variation in the range of 50 to 200 yields the best per-\nformance, improving both FID and linear probe accuracy\nwhile strengthening cross-sample attention.\n4.3. GroupDiff Generation Pattern Analysis\nAfter validating the effectiveness of different group settings,\nwe now analyze why and how GroupDiff improves genera-\ntion quality and investigate its unique generation patterns.\nCross-Sample Attention. To understand why GroupDiff\nimproves generation, we examine how cross-sample inter-\naction influences the diffusion process.\nAt the core of\nGroupDiff is cross-sample attention, enabling each patch\n0.2\n0.4\n0.8\n0.6\nFigure 6.\nControlling GroupDiff denoising steps.\nWe show\ngenerated sample examples when GroupDiff is turned off after\ndifferent denoising stages.\nStable quality after denoising with\nGroupDiff at early steps.\nto establish intra-image and inter-image correspondence\nacross the group. Figure 2 shows that a patch corresponding\nto a ‚Äúdog‚Äôs ear‚Äù attends to both the same region of its own\ninstance and to similar ‚Äúear‚Äù regions in other dog images.\nTo quantitatively measure the cross-sample attention, we\ndefine the image-level self-attention as attention assigned to\nits own patches, and cross-attention as attention assigned to\npatches from other images in the group. Formally, let image\nxi contain patch indices Ii. For a query patch q ‚ààIi and\nany key patch k, let the attention weight be Œ±qk. We define\nthe image-level cross-attention weight for image xi as\nP xi\ncross =\n\b\npxi‚Üíxj \f\f j Ã∏= i\n\t\n,\nwhere pxi‚Üíxj =\nX\nq‚ààIi\nX\nk‚ààIj\nŒ±qk.\n(7)\nFurthermore, we introduce the mean cross-attention\nscore and the max cross-attention score of image xi by tak-\n6"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 7, "text": "ing the mean and maximum over P xi\ncross:\nP xi\ncross-mean = mean(P xi\ncross) ,\nP xi\ncross-max = max(P xi\ncross) .\nAttention over denoising steps. To further quantify this\neffect, we measure cross-sample attention across different\ndenoising steps using the image-level cross-attention score,\nP xi\ncross. For each image, we compute its mean and maxi-\nmum cross-attention scores, P xi\ncross-mean and P xi\ncross-max, and\naverage these statistics over all images in the group. As\nshown in Figure 5 (right), both the mean and maximum\ncross-attention scores gradually decrease as the noise level\nreduces, indicating that inter-sample information exchange\nis most active at the early stages of denoising when global\nstructure and semantics are being formed.\nTo validate this observation, we conduct an intervention\nexperiment by turning off GroupDiff after a certain num-\nber of denoising steps and continuing the process using the\nbaseline DiT model. As illustrated in Figure 6, disabling\nGroupDiff in the middle or late stages yields little qual-\nity degradation, confirming our aforementioned hypothesis.\nTable 2 shows that GroupDiff could be faster without de-\ngraded quality by only applying group attention in the early\nand middle stages.\nAttention over denoiser layers.\nWe also exam-\nine the layer-wise distribution of cross-sample attention.\nGroupDiff shows stronger cross-sample attention in the\nearly and final layers, suggesting that it uses other samples\nto form global context and later refine details. Table 3 shows\nthat early layers are essential, while late layers have much\nless impact on GroupDiff. These results indicate GroupDiff\nstrengthens cross-sample interaction in the early timesteps\nand shallow layers, leading to improved generation quality.\nMethods\nFID-10K\nBaseline\n4.21\nw/t 0.0-0.2\n4.04\nw/t 0.0-0.4\n3.92\nw/t 0.0-0.6\n4.63\nTable 2.\nAblation on\ngroup attention timestep.\nMethods\nFID-10K\nBaseline\n4.21\nw/o layer 1-9\n294.38\nw/o layer 10-19\n5.49\nw/o layer 20-27\n4.49\nTable 3.\nAblation on\ngroup attention layers.\nCross-sample attention score.\nUnder a setting that en-\ncourages cross-image attention, we hypothesize two possi-\nble operating modes: (i) an evenly distributed mode, where\nan image spreads attention across all others, and (ii) a\nneighbor-focused mode, where it primarily attends to its\nmost similar counterpart. We focus on the latter behavior\nand quantify its strength using an image-level cross-sample\nattention score defined as\nScross = Pcross-max ‚àíPcross-mean\nPcross-max\n,\n(8)\nwhere Pcross-max and Pcross-mean denote the maximum and\nmean cross-sample attention from one image to the others\nReference\nReplace 2nd\nReplace 3rd\nReplace 4th\nLower attention weights to the first sample  \nFigure 7. Controlling conditions. The reference group uses class\n89, and in each row, one sample‚Äôs (red) condition is changed to\nclass 360.\nin the group. Intuitively, this score measures how strongly\nthe attention distribution concentrates on the most similar\nimage, normalized by the overall attention magnitude. A\nscore close to 0 indicates a uniform, distributed attention\npattern, while a score close to 1 reflects a highly peaked,\nneighbor-focused attention on a single image.\nBy varying the query method, noise range, and group\nsize across GroupDiff variants, we compare their cross-\nsample attention scores with their FID. We observe a strong\ncorrelation (r = 0.95; Fig. 5 left), showing that more\nneighbor-focused cross-sample attention leads to higher\ngeneration quality. Upon closer inspection, several distinct\nclusters emerge in the plot, primarily corresponding to dif-\nferent group sizes. We find that increasing the group size ef-\nfectively encourages stronger cross-sample attention behav-\nior, further improving generation quality. Moreover, even\nwithin each cluster, higher cross-sample attention scores\nstill correlate with lower FID, showing that this interaction\nreliably reflects generation quality.\nCross-condition generation. To further validate the role of\ncross-sample attention, we conduct a controlled experiment\nby replacing one image in the group with a sample from a\ndifferent class while keeping the latent variables fixed. We\nfirst generate a group of reference images and rank them\nby their cross-attention weights to the first sample, pxi‚Üíx1.\nThen, we gradually replace the condition of one sample\nwith another class during the entire denoising process and\nshow the results in Figure 7. We observed that the genera-\ntion of the reference (green box) image is highly sensitive\nto which sample is replaced. When we replace a sample\nthat originally receives high attention weights, the refer-\nence image changes significantly. In contrast, replacing a\nlow-attention sample results in almost no visual difference.\n7"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 8, "text": "Figure 8. Qualitative Results. Examples of class-conditional generation on ImageNet 256√ó256 using GroupDiff-4 with SiT-XL/2.\nMethod\nEpoch\nw/ CFG\nFID\nIS\nPre.\nRec.\nWith semantic feature distillation\nDDT-XL [61]\n800\n1.26\n310.6\n0.79\n0.65\nSiT-XL/2 + REPA-E [26]\n800\n1.26\n314.9\n0.79\n0.66\nSiT-XL/2 + REPA [67]\n800\n1.42\n305.7\n0.80\n0.64\n+ our GroupDiff-4*\n800 + 100\n1.14\n315.3\n0.77\n0.66\nWithout semantic feature distillation\nADM [6]\n400\n4.59\n186.7\n0.82\n0.52\nVDM++ [23]\n-\n2.12\n267.7\n-\n-\nLDM-4 [43]\n1400\n3.60\n247.7\n0.87\n0.48\nMDTv2-XL/2 [11]\n900\n1.58\n314.7\n0.79\n0.65\nVAR-d30 [55]\n350\n1.92\n323.1\n0.82\n0.59\nLlamaGen-3B [50]\n-\n2.18\n263.3\n0.81\n0.58\nRandAR-XXL [39]\n300\n2.15\n322.0\n0.79\n0.62\nMaskDiT [70]\n1600\n2.28\n276.6\n0.89\n0.61\nDiT-XL/2 [40]\n1400\n2.27\n278.2\n0.83\n0.57\n+ our GroupDiff-4\n800\n1.66\n279.4\n0.83\n0.57\n+ our GroupDiff-4‚àó\n1400 + 100\n1.55\n285.4\n0.80\n0.63\nSiT-XL/2 [31]\n1400\n2.06\n270.3\n0.82\n0.59\n+ SRA [20]\n800\n1.58\n311.4\n0.80\n0.63\n+ Dispersive Loss [59]\n1200\n1.97\n-\n-\n-\n+ our GroupDiff-4\n800\n1.63\n283.2\n0.81\n0.64\n+ our GroupDiff-4‚àó\n1400 + 100\n1.40\n290.7\n0.79\n0.64\nTable 4. System-Level performance comparison on ImageNet\n256 √ó 256. Our GroupDiff enables the DiT/SiT model to achieve\nstate-of-the-art performance both with/without semantic feature\ndistillation.\n‚àó: continue training from pre-trained checkpoint for\nan additional 100 epochs.\nThis indicates that cross-sample attention controls the inter-\nimage correspondence within the group, with high-attention\nsamples contributing more to the final generation, consis-\ntent with our earlier observations of cross-sample attention\npatterns. Furthermore, we believe this property points to a\npromising future direction. When the group size is suffi-\nciently large, the generation process of GroupDiff could be\nextended to handle diverse or cross-conditioned inputs, en-\nabling more flexible inter-image correspondence within the\ngeneration process.\n4.4. Benchmarking with Previous Systems\nWe compare against leading generative systems in Table 4.\nFor this experiment, we train GroupDiff in two settings:\nfrom scratch and from the pre-trained weights, denoted as\nGroupDiff-4 and GroupDiff-4‚àó, respectively. When train-\ning from scratch, GroupDiff improves DiT-XL/2 with 29%\nlower FID and SiT-XL/2 with 30% lower FID while only\nusing 57% of original training iterations.\nFor the second setting, we only use the Lgroup as the\ntraining objective, no matter if other objectives, e.g. Lrepa\nfrom REPA [67], exist in the previous stages.\nNotably,\nGroupDiff-4 with DiT-XL/2 achieves an FID of 1.55 (from\n2.27) and GroupDiff-4 with SiT-XL/2 further improves to\n1.40 (from 2.06) with only 100 additional training epochs,\noutperforming all other state-of-the-art methods when no\nsemantic feature distillation has been applied. Moreover,\nwhen using pre-trained weights from the semantic feature\ndistillation method, GroupDiff again obtains a significant\nimprovement, achieving an FID of 1.14 (down from 1.42).\nQualitative samples are provided in Figure 8.\n5. Discussion and Conclusion\nLimitations.\nWhile GroupDiff demonstrates strong im-\nprovements in generation quality, its increased training cost\nremains a challenge. When the group size is n, GroupDiff-\nf and GroupDiff-l require approximately (n ‚àí1)√ó and\n(0.1n)√ó longer training time in every iteration, and (n ‚àí\n1)√ó and 0.5(n ‚àí1)√ó longer inference time, respectively.\nNevertheless, (a) this design opens a new avenue for explor-\ning the trade-off between computational cost and generation\nquality, and (b) a high-quality model can serve as a teacher\nto distill faster and lighter students. We leave the study for\na more efficient method for future exploration.\nConclusion.\nWe introduce Group Diffusion, a simple\nyet effective framework that reshapes diffusion training\ninto a group-wise denoising process. By enabling cross-\nsample attention among related instances, the model im-\nplicitly learns relational structures that enhance represen-\ntation quality and generation fidelity. Experiments on Ima-\ngeNet demonstrate consistent FID improvements across ar-\nchitectures with minimal computational overhead. Beyond\nboosting performance, Group Diffusion provides a new lens\nconnecting representation learning and generative model-\ning, suggesting that cross-sample interactions can serve as\nan implicit form of supervision for stronger and more gen-\neralizable diffusion models.\n8"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 9, "text": "References\n[1] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li,\nHang Su, and Jun Zhu. All are worth words: A vit backbone\nfor diffusion models. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n22669‚Äì22679, 2023. 21\n[2] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue,\nYufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luh-\nman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya\nRamesh.\nVideo generation models as world simulators.\n2024. 3\n[3] Shoufa Chen, Chongjian Ge, Shilong Zhang, Peize Sun, and\nPing Luo.\nPixelflow: Pixel-space generative models with\nflow. arXiv preprint arXiv:2504.07963, 2025. 13\n[4] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In International conference on ma-\nchine learning, pages 1597‚Äì1607. PmLR, 2020. 6\n[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei.\nImagenet: A large-scale hierarchical im-\nage database. In Computer Vision and Pattern Recognition,\n2009. CVPR 2009. IEEE Conference on, pages 248‚Äì255.\nIEEE, 2009. 4, 5\n[6] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780‚Äì8794, 2021. 1, 8, 12, 13\n[7] Alexey Dosovitskiy.\nAn image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 2\n[8] Dave Epstein, Allan Jabri, Ben Poole, Alexei Efros, and\nAleksander Holynski. Diffusion self-guidance for control-\nlable image generation.\nAdvances in Neural Information\nProcessing Systems, 36:16222‚Äì16239, 2023. 2\n[9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim\nEntezari, Jonas M¬®uller, Harry Saini, Yam Levi, Dominik\nLorenz, Axel Sauer, Frederic Boesel, et al. Scaling recti-\nfied flow transformers for high-resolution image synthesis.\nIn Forty-first international conference on machine learning,\n2024. 2, 21\n[10] Wan-Cyuan Fan, Yen-Chun Chen, DongDong Chen, Yu\nCheng, Lu Yuan, and Yu-Chiang Frank Wang. Frido: Fea-\nture pyramid diffusion for complex scene image synthesis.\nIn Proceedings of the AAAI conference on artificial intelli-\ngence, pages 579‚Äì587, 2023. 21\n[11] Shanghua\nGao,\nPan\nZhou,\nMing-Ming\nCheng,\nand\nShuicheng Yan. Mdtv2: Masked diffusion transformer is a\nstrong image synthesizer. arXiv preprint arXiv:2303.14389,\n2023. 8, 12\n[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial networks. Commu-\nnications of the ACM, 63(11):139‚Äì144, 2020. 1\n[13] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo\nZhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-\ntor quantized diffusion model for text-to-image synthesis. In\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 10696‚Äì10706, 2022. 21\n[14] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017. 4, 12\n[15] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 3, 4\n[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840‚Äì6851, 2020. 1, 2\n[17] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu,\nand Jie Tang.\nCogvideo:\nLarge-scale pretraining for\ntext-to-video generation via transformers.\narXiv preprint\narXiv:2205.15868, 2022. 2\n[18] Zehuan Huang, Yuan-Chen Guo, Haoran Wang, Ran Yi,\nLizhuang Ma, Yan-Pei Cao, and Lu Sheng.\nMv-adapter:\nMulti-view consistent image generation made easy.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 16377‚Äì16387, 2025. 2\n[19] Allan Jabri, David Fleet, and Ting Chen.\nScalable adap-\ntive computation for iterative generation.\narXiv preprint\narXiv:2212.11972, 2022. 13\n[20] Dengyang Jiang, Mengmeng Wang, Liuzhuozheng Li, Lei\nZhang, Haoyu Wang, Wei Wei, Guang Dai, Yanning Zhang,\nand Jingdong Wang. No other representation component is\nneeded: Diffusion transformers can provide representation\nguidance by themselves. arXiv preprint arXiv:2505.02831,\n2025. 2, 8, 12\n[21] Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James M\nRehg, and Pinar Yanardag. Rave: Randomized noise shuf-\nfling for fast and consistent video editing with diffusion mod-\nels. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 6507‚Äì6516,\n2024. 2\n[22] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks,\n2019. 1\n[23] Diederik Kingma and Ruiqi Gao.\nUnderstanding diffu-\nsion objectives as the elbo with simple data augmentation.\nAdvances in Neural Information Processing Systems, 36:\n65484‚Äì65516, 2023. 8\n[24] W Kong, Q Tian, Z Zhang, R Min, Z Dai, J Zhou, J Xiong,\nX Li, B Wu, J Zhang, et al. Hunyuanvideo: A systematic\nframework for large video generative models, 2025. URL\nhttps://arxiv. org/abs/2412.03603. 2, 3\n[25] Tuomas Kynk¬®a¬®anniemi, Tero Karras, Samuli Laine, Jaakko\nLehtinen, and Timo Aila. Improved precision and recall met-\nric for assessing generative models. Advances in neural in-\nformation processing systems, 32, 2019. 4, 12\n[26] Xingjian Leng, Jaskirat Singh, Yunzhong Hou, Zhenchang\nXing, Saining Xie, and Liang Zheng. Repa-e: Unlocking\nvae for end-to-end tuning with latent diffusion transformers.\narXiv preprint arXiv:2504.10483, 2025. 2, 8, 12\n[27] Alexander C Li, Mihir Prabhudesai, Shivam Duggal, Ellis\nBrown, and Deepak Pathak. Your diffusion model is secretly\na zero-shot classifier. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 2206‚Äì2217,\n2023. 2\n9"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 10, "text": "[28] Tianhong Li and Kaiming He.\nBack to basics:\nLet\ndenoising generative models denoise.\narXiv preprint\narXiv:2511.13720, 2025. 13\n[29] Kuan Heng Lin, Sicheng Mo, Ben Klingher, Fangzhou\nMu, and Bolei Zhou.\nCtrl-x: Controlling structure and\nappearance for text-to-image generation without guidance.\nAdvances in Neural Information Processing Systems, 37:\n128911‚Äì128939, 2024. 2\n[30] Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holyn-\nski, and Trevor Darrell. Diffusion hyperfeatures: Search-\ning through time and space for semantic correspondence.\nAdvances in Neural Information Processing Systems, 36:\n47500‚Äì47510, 2023. 2\n[31] Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M\nBoffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Explor-\ning flow and diffusion-based generative models with scalable\ninterpolant transformers. In European Conference on Com-\nputer Vision, pages 23‚Äì40. Springer, 2024. 4, 8, 12\n[32] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Zi-\nwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte:\nLatent diffusion transformer for video generation.\narXiv\npreprint arXiv:2401.03048, 2024. 2\n[33] Sicheng Mo, Fangzhou Mu, Kuan Heng Lin, Yanli Liu,\nBochen Guan, Yin Li, and Bolei Zhou.\nFreecontrol:\nTraining-free spatial control of any text-to-image diffusion\nmodel with any condition. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 7465‚Äì7475, 2024. 2\n[34] Sicheng Mo, Thao Nguyen, Xun Huang, Siddharth Srini-\nvasan Iyer, Yijun Li, Yuchen Liu, Abhishek Tandon, Eli\nShechtman, Krishna Kumar Singh, Yong Jae Lee, et al. X-\nfusion: Introducing new modality to frozen large language\nmodels. arXiv preprint arXiv:2504.20996, 2025. 2, 3\n[35] Thao Nguyen, Yuheng Li, Utkarsh Ojha, and Yong Jae\nLee. Visual instruction inversion: Image editing via image\nprompting. Advances in Neural Information Processing Sys-\ntems, 36:9598‚Äì9613, 2023. 2\n[36] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models.\nIn International\nconference on machine learning, pages 8162‚Äì8171. PMLR,\n2021. 1, 4, 5\n[37] OpenAI.\nSora: Creating video from text ‚Äî openai.com.\nhttps://openai.com/index/sora/, 2025.\n[Ac-\ncessed 30-04-2025]. 2\n[38] Maxime Oquab,\nTimoth¬¥ee Darcet,\nTh¬¥eo Moutakanni,\nHuy Q. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernan-\ndez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby,\nMahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russ\nHowes, Po-Yao (Bernie) Huang, Shang-Wen Li, Ishan Misra,\nMichael G. Rabbat, Vasu Sharma, Gabriel Synnaeve, Huijiao\nXu, Herv¬¥e J¬¥egou, Julien Mairal, Patrick Labatut, Armand\nJoulin, and Piotr Bojanowski. Dinov2: Learning robust vi-\nsual features without supervision. ArXiv, abs/2304.07193,\n2023. 4, 6\n[39] Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao\nTan, Kai Zhang, William T Freeman, and Yu-Xiong Wang.\nRandar: Decoder-only autoregressive visual generation in\nrandom orders. In Proceedings of the Computer Vision and\nPattern Recognition Conference, pages 45‚Äì55, 2025. 8, 12\n[40] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision, pages 4195‚Äì4205,\n2023. 3, 4, 5, 8, 12, 21\n[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. arXiv preprint arXiv:2103.00020, 2021. 4, 6\n[42] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 2\n[43] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick\nEsser, and Bj¬®orn Ommer. High-resolution image synthesis\nwith latent diffusion models. CVPR, 2022. 2, 8, 12\n[44] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¬®orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684‚Äì10695, 2022. 1, 2\n[45] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¬®orn Ommer. High-resolution image syn-\nthesis with latent diffusion models, 2022. 2, 4\n[46] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\nCheung, Alec Radford, and Xi Chen. Improved techniques\nfor training gans. Advances in neural information processing\nsystems, 29, 2016. 4, 12\n[47] Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang,\nXi Victoria Lin, Luke Zettlemoyer, and Lili Yu. Llamafu-\nsion: Adapting pretrained language models for multimodal\ngeneration. arXiv preprint arXiv:2412.15188, 2024. 2, 3\n[48] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro\nChin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang,\nGlenn Entis, Yuanzhen Li, et al. Styledrop: Text-to-image\ngeneration in any style. arXiv preprint arXiv:2306.00983,\n2023. 2\n[49] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue\nPeng, Ping Luo, and Zehuan Yuan. Autoregressive model\nbeats diffusion: Llama for scalable image generation. arXiv\npreprint arXiv:2406.06525, 2024. 12\n[50] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue\nPeng, Ping Luo, and Zehuan Yuan. Autoregressive model\nbeats diffusion: Llama for scalable image generation. arXiv\npreprint arXiv:2406.06525, 2024. 8\n[51] Raphael Tang, Linqing Liu, Akshat Pandey, Zhiying Jiang,\nGefei Yang, Karun Kumar, Pontus Stenetorp, Jimmy Lin,\nand Ferhan Ture. What the daam: Interpreting stable diffu-\nsion using cross attention. arXiv preprint arXiv:2210.04885,\n2022. 2\n[52] Ming Tao, Hao Tang, Fei Wu, Xiao-Yuan Jing, Bing-Kun\nBao, and Changsheng Xu.\nDf-gan: A simple and effec-\ntive baseline for text-to-image synthesis. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 16515‚Äì16525, 2022. 21\n10"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 11, "text": "[53] Chameleon Team. Chameleon: Mixed-modal early-fusion\nfoundation models. arXiv preprint arXiv:2405.09818, 2024.\n3\n[54] Junjiao Tian, Lavisha Aggarwal, Andrea Colaco, Zsolt Kira,\nand Mar Gonzalez-Franco. Diffuse attend and segment: Un-\nsupervised zero-shot segmentation using stable diffusion. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 3554‚Äì3563, 2024. 2\n[55] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Li-\nwei Wang. Visual autoregressive modeling: Scalable image\ngeneration via next-scale prediction. Advances in neural in-\nformation processing systems, 37:84839‚Äì84865, 2024. 8, 12\n[56] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali\nDekel.\nPlug-and-play diffusion features for text-driven\nimage-to-image translation. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 1921‚Äì1930, 2023. 2\n[57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 2\n[58] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao,\nChen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao\nYang, et al. Wan: Open and advanced large-scale video gen-\nerative models. arXiv preprint arXiv:2503.20314, 2025. 2,\n3\n[59] Runqian Wang and Kaiming He. Diffuse and disperse: Im-\nage generation with representation regularization.\narXiv\npreprint arXiv:2506.09027, 2025. 2, 8, 12\n[60] Shuai Wang, Ziteng Gao, Chenhui Zhu, Weilin Huang, and\nLimin Wang. Pixnerd: Pixel neural field diffusion. arXiv\npreprint arXiv:2507.23268, 2025. 13\n[61] Shuai Wang, Zhi Tian, Weilin Huang, and Limin Wang.\nDdt:\nDecoupled diffusion transformer.\narXiv preprint\narXiv:2504.05741, 2025. 8, 12\n[62] Ge Wu, Shen Zhang, Ruijing Shi, Shanghua Gao, Zhenyuan\nChen, Lei Wang, Zhaowei Chen, Hongcheng Gao, Yao Tang,\nJian Yang, et al. Representation entanglement for genera-\ntion: Training diffusion transformers is much easier than you\nthink. arXiv preprint arXiv:2507.01467, 2025. 2\n[63] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiao-\nlong Wang, and Shalini De Mello. Open-vocabulary panop-\ntic segmentation with text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 2955‚Äì2966, 2023. 2\n[64] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang,\nZhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-\ngrained text to image generation with attentional generative\nadversarial networks. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 1316‚Äì\n1324, 2018. 21\n[65] Jiawei Yang, Tianhong Li, Lijie Fan, Yonglong Tian, and\nYue Wang. Latent denoising makes good visual tokenizers.\narXiv preprint arXiv:2507.15856, 2025. 6\n[66] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu\nHuang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-\nhan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video\ndiffusion models with an expert transformer. arXiv preprint\narXiv:2408.06072, 2024. 2\n[67] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon\nJeong, Jonathan Huang, Jinwoo Shin, and Saining Xie.\nRepresentation alignment for generation:\nTraining dif-\nfusion transformers is easier than you think.\nArXiv,\nabs/2410.06940, 2024. 2, 8, 12\n[68] Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and\nYinfei Yang. Cross-modal contrastive learning for text-to-\nimage generation.\nIn Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n833‚Äì842, 2021. 21\n[69] Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Pola-\nnia Cabrera, Varun Jampani, Deqing Sun, and Ming-Hsuan\nYang. A tale of two features: Stable diffusion complements\ndino for zero-shot semantic correspondence.\nAdvances in\nNeural Information Processing Systems, 36:45533‚Äì45547,\n2023. 2\n[70] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima\nAnandkumar. Fast training of diffusion models with masked\ntransformers. arXiv preprint arXiv:2306.09305, 2023. 2, 8,\n12\n[71] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen,\nShenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang\nYou. Open-sora: Democratizing efficient video production\nfor all. arXiv preprint arXiv:2412.20404, 2024. 2\n[72] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala,\nMichihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe\nMa, Luke Zettlemoyer, and Omer Levy. Transfusion: Pre-\ndict the next token and diffuse images with one multi-modal\nmodel. 2024. 2, 3\n[73] Mingyuan\nZhou,\nHuangjie\nZheng,\nZhendong\nWang,\nMingzhang Yin, and Hai Huang. Score identity distillation:\nExponentially fast distillation of pretrained diffusion models\nfor one-step generation. In Forty-first International Confer-\nence on Machine Learning, 2024. 13\n[74] Y Zhou, R Zhang, C Chen, C Li, C Tensmeyer, T Yu,\nJ Gu, J Xu, and T Sun.\nLafite: Towards language-free\ntraining for text-to-image generation. arxiv. arXiv preprint\narXiv:2111.13792, 3, 2022. 21\n[75] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. Dm-gan:\nDynamic memory generative adversarial networks for text-\nto-image synthesis. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n5802‚Äì5810, 2019. 21\n11"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 12, "text": "Supplementary\nA. Implementation Details\n12\nA.1. Baselines . . . . . . . . . . . . . . . . . . .\n12\nA.2. Evaluation Metric\n. . . . . . . . . . . . . .\n12\nA.3. Hyperparameter\n. . . . . . . . . . . . . . .\n12\nB. Experiment\n12\nB.1. Ablations . . . . . . . . . . . . . . . . . . .\n12\nB.2. Extending to Pixel Diffusion.\n. . . . . . . .\n13\nB.3. Additional Qualitative Results.\n. . . . . . .\n13\nB.4. Cross-Sample Score Visualization . . . . . .\n21\nB.5. Text-to-Image Generation\n. . . . . . . . . .\n21\nA. Implementation Details\nA.1. Baselines\nWe introduce the baselines of the leading generative sys-\ntems as follows:\n‚Ä¢ ADM [6] leverages classifier for guiding diffusion sam-\npling to improve generation.\n‚Ä¢ LDM [43] presents latent diffusion, enabling fast, high-\nresolution generation by training diffusion models in a\nlatent space.\n‚Ä¢ MDTv2 [11] combines masked token modeling with\ndiffusion transformers to learn visual representations.\n‚Ä¢ VAR [55] introduces next-scale prediction to autore-\ngressive generative models.\n‚Ä¢ LlamaGen [49] shows vanilla autoregressive models\ncould achieve strong generation performance at scale,\noutperforming diffusion baselines.\n‚Ä¢ RandAR [39] proposes a decoder-only autoregressive\nmodel that utilizes position instruction tokens to gener-\nate image tokens in arbitrary orders.\n‚Ä¢ MaskDiT [70] uses masked input patches and an\nasymmetric encoder-decoder to achieve faster diffusion\nmodel training.\n‚Ä¢ DiT [40] proposes a scalable transformer architecture\nbased on AdaIN-zero for diffusion model training.\n‚Ä¢ SiT [31] further improves the efficiency and scalability\non DiT by introducing flow matching.\n‚Ä¢ REPA [67] analyzes the alignment between feature\nquality and generation fidelity of diffusion backbone\nand accelerates diffusion model training by aligning dif-\nfusion feature with pre-trained vision encoders.\n‚Ä¢ REPA-E [26] enables representation learning inside\ndiffusion backbones by unlocking the latent encoder.\n‚Ä¢ DDT [61] proposes a diffusion architecture that sepa-\nrates semantic encoding from high-frequency decoding\nto accelerate convergence during training.\n‚Ä¢ SRA [20] introduces a simple approach to align cross-\nlayer diffusion backbone features to improve training\nefficiency without a pre-trained vision encoder.\n‚Ä¢ Dispersive Loss [59] introduces a simple regularization\nloss that encourages internal representations to disperse\nin the hidden space to improve diffusion model training.\nA.2. Evaluation Metric\nWe use the conventional evaluation pipelines for class-\nconditional generative models, following ADM [6]. Specif-\nically, we introduce the focusing concept of each metric:\n‚Ä¢ Fr¬¥echet Inception Distance (FID) [14] evaluates the\nfeature distance of generated images and the reference\nsamples. Lower FID usually suggests better generation\nfidelity and diversity.\n‚Ä¢ Inception Score (IS) [46] measures image quality and\ndiversity based on how confidently a classifier recog-\nnizes each image and how varied the generated classes\nare. A higher Inception Score indicates a more mean-\ningful image within each class.\n‚Ä¢ Precision and recall [25]. Precision captures the re-\nalism of generated images, while recall captures their\ndiversity relative to real data.\nA.3. Hyperparameter\nIn Table 5,we introduce the hyperparameter setting for mod-\nels reported at Table 3.\nB. Experiment\nB.1. Ablations\nGroupDiff-f: group size. We additionally investigate into\nthe group size in GroupDiff-f setting.\nFigure 9 shows\nthe which images shares the same group during infer-\nence. We compare the uncurated samples from GroupDiff-\nf-{1,2,3,4} in Figure 10 and Figure 11. Our observation\non GroupDiff-f aligns that of GroupDiff-l, where increas-\ning the group size considerably improves the generation fi-\ndelity.\nGroupDiff-f-1\nGroupDiff-f-2\nGroupDiff-f-4\nGroupDiff-f-8\nSample-1\nSample-2\nSample-6\nSample-8\nSample-3\nSample-4\nSample-5\nSample-7\nFigure 9. Group attention illustration. In each row, samples in\nthe sample group shares the same color block.\nGroupDiff-l* : query method.\nBeyond training from\nscratch, resuming from individual diffusion offers an effi-\ncient solution to adding GroupDiff over existing pipelines.\nThus, we also explore different query methods under this\n12"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 13, "text": "DiT-XL/2\nSiT-XL/2\nSiT-XL/2-REPA\nGroupDiff-4\nGroupDiff-4*\nGroupDiff-4\nGroupDiff-4*\nGroupDiff-4*\nArchitecture\nInput dim.\n32 √ó 32√ó 4\n32 √ó 32 √ó 4\n32 √ó 32 √ó 4\n32 √ó 32 √ó 4\n32 √ó 32 √ó 4\nNum. layers\n28\n28\n28\n28\n28\nHidden dim.\n1,152\n1,152\n1,152\n1,152\n1,152\nNum. heads\n16\n16\n16\n16\n16\nOptimization\nResume\n-\nDiT-XL/2-7M\n-\nSiT-XL/2-7M\nREPA-4M\nTraining Iteration\n4M\n500K\n4M\n500K\n500K\nBatch Size\n256\n256\n256\n256\n256\nOptimzier\nAdamW\nAdamW\nAdamW\nAdamW\nAdamW\nlr\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\nbetas\n(0.9, 0.999)\n(0.9, 0.999)\n(0.9, 0.999)\n(0.9, 0.999)\n(0.9, 0.999)\nweight decay\n0.01\n0.01\n0.01\n0.01\n0.01\nGroupDiff\nMode\nGroupDiff-l\nGroupDiff-l\nGroupDiff-l\nGroupDiff-l\nGroupDiff-l\nQuery Method\nCLIP-L\nCLIP-L\nCLIP-L\nCLIP-L\nCLIP-L\nœÑimg\n0.7\n0.7\n0.7\n0.7\n0.7\nGroup Size\n4\n4\n4\n4\n4\nNoise Var.\n50\n50\n50\n50\n0\nInference\nSteps\n250\n250\n250\n250\n250\nGuidance Scale\n1.70\n1.60\n2.35\n1.85\n2.575\nGuidance Interval\n(0,1)\n(0,1)\n(0.25,1.0)\n(0.15,1.0)\n(0.25,0.75)\nTable 5. Hyperparameter setup.\nMethod\nQuery Method\nFID ‚Üì\nIS ‚Üë\nPre.‚Üë\nRec.‚Üë\nSiT-XL/2\n-\n2.06\n270.3\n0.82\n0.59\n+ GroupDiff-4*\nClass\n1.76\n283.5\n0.81\n0.61\n+ GroupDiff-4*\nCLIP-L\n1.40\n290.7\n0.79\n0.64\nTable 6. Ablation: query method.\n‚àó: continue training from\npre-trained checkpoint for an additional 100 epochs.\nsetting. Table 6 shows CLIP-L yields the optimality perfor-\nmance while the simplest GroupDiff-4‚àóobtains a consider-\nable improvement (14.5%) over the baseline, highlighting\nthe effectiveness of cross-sample attention.\nB.2. Extending to Pixel Diffusion.\nWe further validate GroupDiff on pixel diffusion systems.\nAs shown in Table 7, GroupDiff-4 with JiT-B/16 delivers\na substantial 15.8% improvement with only 100 additional\ntraining steps when resumed from a pre-trained model. This\nagain highlights the effectiveness of cross-sample collabo-\nration in pixel diffusion and its strong potential for broader\napplicability.\nMethod\nparams\nFID\nIS\nADM-G [6]\n559M\n7.72\n172.7\nRIN [19]\n320M\n3.95\n216\nSiD [73], UViT/2\n2B\n2.44\n256.3\nPixelFlow [3], XL/4\n677M\n1.98\n282.1\nPixNerd [60], XL/16\n700M\n2.15\n297\nJiT-H/16 [28]\n953M\n1.86\n303.4\nJiT-B/16 [28]\n131M\n3.66\n275.1\n+ our GroupDiff-4*\n131M\n3.08\n245.6\nTable 7. System-level performance of pixel diffusion models\nevaluated on ImageNet 256√ó256. ‚àó: continue training from pre-\ntrained checkpoint for an additional 100 epochs.\nB.3. Additional Qualitative Results.\nWe provide additional uncurated samples generated by\nGroupDiff-4 in Figures 14‚Äì26.\n13"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 14, "text": "GroupDiff-f-1\nGroupDiff-f-2\nGroupDiff-f-4\nGroupDiff-f-8\nGroupDiff-f-1\nGroupDiff-f-2\nGroupDiff-f-4\nGroupDiff-f-8\nGroupDiff-f-1\nGroupDiff-f-2\nGroupDiff-f-4\nGroupDiff-f-8\nFigure 10. Uncurated generation results of GroupDiff-f without classifier-free guidance. Examples of class-conditional generation on\nImageNet 256√ó256. GroupDiff with a larger group size consistently obtains better generation fidelity.\n14"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 15, "text": "GroupDiff-f-1\nGroupDiff-f-2\nGroupDiff-f-4\nGroupDiff-f-8\nGroupDiff-f-1\nGroupDiff-f-2\nGroupDiff-f-4\nGroupDiff-f-8\nGroupDiff-f-1\nGroupDiff-f-2\nGroupDiff-f-4\nGroupDiff-f-8\nFigure 11. Uncurated generation results of GroupDiff-f without classifier-free guidance. Examples of class-conditional generation on\nImageNet 256√ó256.GroupDiff with a larger group size consistently obtains better generation fidelity.\n15"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 16, "text": "Figure 12. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúloggerhead sea\nturtle‚Äù (33).\nFigure 13. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúmacaw‚Äù (88).\nFigure 14. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúsulphur-crested\ncockatoo, Kakatoe galerita, Cacatua galerita‚Äù (89).\n16"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 17, "text": "Figure 15. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúgolden retriever‚Äù\n(207).\nFigure 16. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚ÄúSiberian husky‚Äù\n(250).\nFigure 17. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúwhite wolf, Arctic\nwolf, Canis lupus tundrarum‚Äù (270).\n17"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 18, "text": "Figure 18. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚ÄúArctic fox, white\nfox, Alopex lagopus‚Äù (279).\nFigure 19. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúacoustic guitar‚Äù\n(402).\nFigure 20. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúballoon‚Äù (417).\n18"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 19, "text": "Figure 21. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúbaseball‚Äù (429).\nFigure 22. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúfire engine, fire\ntruck‚Äù (555).\nFigure 23. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúspace shuttle‚Äù\n(812).\n19"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 20, "text": "Figure 24. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúcheeseburger‚Äù\n(933).\nFigure 25. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúcoral reef‚Äù (973).\nFigure 26. Uncurated generation results of GroupDiff-4. We use classifier-free guidance with w= 3.5. Class label = ‚Äúvolcano‚Äù (980).\n20"}
{"pdf_id": "arxiv_251210954_group_diffusion", "page": 21, "text": "Method\nType\nFID\nAttnGAN [64]\nGAN\n35.49\nDM-GAN [75]\nGAN\n32.64\nVQ-Diffusion [13]\nDiffusion\n19.75\nDF-GAN [52]\nGAN\n19.32\nXMC-GAN [68]\nGAN\n9.33\nFrido [10]\nDiffusion\n8.97\nLAFITE [74]\nGAN\n8.12\nU-Net [1]\nDiffusion\n7.32\nU-ViT-S/2 [1]\nDiffusion\n5.95\nU-ViT/S/2 (Deep) [1]\nDiffusion\n5.45\nMMDiT [9]\nDiffusion\n5.3\nDiT-XL/2 w/ Cross-Attention [40]\nDiffusion\n6.95\n+ our GroupDiff-4\nDiffusion\n6.65\nTable 8. Quantitative comparison on text-to-image generation\n(MS-COCO).\nB.4. Cross-Sample Score Visualization\nAdditionally, we show the relation between FID and cross-\nsample score computed by the group-level mean and max\nof the attention score in Figure 27.\nFID\nCross-Sample Score (%)\nFigure 27.\nFID vs Cross-Sample Score (group-level) Our\nGroupDiff shows a strong correlation (0.94) between cross-\nattention to other samples and generation quality.\nB.5. Text-to-Image Generation\nWe also validate GroupDiff in text-to-image generation. We\nmostly follow the experimental setup used in U-ViT [1] un-\nless otherwise specified: we train the model from scratch on\na train split of the MS-COCO dataset and use a validation\nsplit for evaluation. We use DiT-XL/2 with Cross-Attention\nand train it for 150K iterations with a batch size of 256.\nWe use the frozen CLIP text encoder to extract text prompts\nfrom captions. Table 8 shows that GroupDiff remains effec-\ntive in the T2I generation setting without bells and whistles,\nhighlighting the importance of applying cross-sample atten-\ntion even with text conditions.\n21"}
