{"pdf_id": "arxiv_251210939_gaussianHeadTalk", "page": 1, "text": "GaussianHeadTalk: Wobble-Free 3D Talking Heads with Audio Driven\nGaussian Splatting\nMadhav Agarwal1\nMingtian Zhang2\nLaura Sevilla-Lara1\nSteven McDonagh1\n1University of Edinburgh\n2University College London\nmadhav.agarwal@ed.ac.uk,\nmingtian.zhang.17@ucl.ac.uk, {l.sevilla,s.mcdonagh}@ed.ac.uk\nOurs\nGround Truth\nGaussianTalker\nTalkinĀGaussian\nInput Video \n(Monocular)\n3D Gaussian Head Model\nRendered  Lip-Sync Video\nComparison with \nPrevious Works\nInput \nAudio (Any)\nHello\nÿather\npeace\ncalm\nFigure 1. To address the challenges of temporal instability, slow rendering, and limited photorealism in existing methods, we propose\nGaussianHeadTalk: a real-time system that generates photorealistic, temporally stable 3D talking head avatars directly from monocular\nvideo and arbitrary audio input. Corresponding output frames generated by state-of-the-art methods GaussianTalker [9] and TalkingGaus-\nsian [29] are also provided for visual comparison.\nAbstract\nSpeech-driven talking heads have recently emerged and en-\nable interactive avatars. However, real-world applications\nare limited, as current methods achieve high visual fidelity\nbut slow or fast yet temporally unstable. Diffusion methods\nprovide realistic image generation, yet struggle with one-\nshot settings. Gaussian Splatting approaches are real-time,\nyet inaccuracies in facial tracking, or inconsistent Gaussian\nmappings, lead to unstable outputs and video artifacts that\nare detrimental to realistic use cases. We address this prob-\nlem by mapping Gaussian Splatting using 3D Morphable\nModels to generate person-specific avatars. We introduce\ntransformer-based prediction of model parameters, directly\nfrom audio, to drive temporal consistency. From monocu-\nlar video and independent audio speech inputs, our method\nenables generation of real-time talking head videos where\nwe report competitive quantitative and qualitative perfor-\nmance.\nProject Page: https://madhav1ag.github.io/gaussianheadtalk\n1. Introduction\nGenerating talking head videos, driven directly by audio,\ncan be considered a valuable task with multiple practical\napplications [2, 7]. Whether in the education sector, health\ncare, teleconferencing, or film and entertainment industries,\nhigh-quality personalized talking head avatars serve as an\neffective path for information transfer. For instance, AI-\ndriven virtual assistants for telemedicine can be useful in\nassistive communications and post-stroke rehabilitation [1].\nBy providing a human face to an interactive agent, instead\nof a text-based input-output platform, the user experience is\nmade more immersive [59]. Further uses include dubbing\nmovies into multiple languages, which reduces the produc-\ntion time and cost of VFX studios manyfold [6].\nThe canonical problem involves taking a short input\nvideo of a person, alongside an arbitrary speech audio sig-\nnal, in order to create a person-specific avatar that can gen-\nerate an output video of the subject appearing to speak\nthe audio content (i.e. with visual lip-syncing that accu-\nrately matches the input audio).\nThe task is commonly\nknown as face reenactment and previous solutions involve\n1\narXiv:2512.10939v1  [cs.CV]  11 Dec 2025"}
{"pdf_id": "arxiv_251210939_gaussianHeadTalk", "page": 2, "text": "using GANs [3, 24, 43, 50], Diffusion models [8, 51, 54],\nNeRFs [20, 23] and, more recently, 3D Gaussian Splat-\nting [9, 10, 29, 39]. Diffusion-based methods have robust\ngenerative priors and produce state-of-the-art image qual-\nity yet they are computationally expensive and inference\nspeed is typically slower than GANs and NeRFs. In con-\ntrast, 3D Gaussian Splatting (3DGS) methods are person or\nscene-specific and have recently shown efficacy in render-\ning high-quality images and videos comparable to that of\ndiffusion models, but at real-time speeds.\nAlthough recent advancements in 3DGS have suc-\ncessfully incorporated temporal information for dynamic\nscenes [32, 52], the integration of related techniques into\nthe synthesis of audio-driven 3DGS talking heads remains\nan open challenge. This gap highlights the need for novel\napproaches to combine dynamic, temporally consistent fa-\ncial animation with audio-driven generation. The task is in-\nherently dynamic, requiring precise temporal information\nto ensure realistic and consistent facial movements, par-\nticularly lip synchronization. Current audio-driven 3DGS\nmethods rely on parameter tracking for temporal informa-\ntion, which often falls short for monocular videos. Inac-\ncuracies in such tracking can lead to temporal flickering\n(i.e., ‘wobbling’) in the face region, causing visible arti-\nfacts. Our experiments show that this instability arises due\nto the improper utilization of temporal information from an\ninput video, which manifests itself as either inaccurate 3D\nmesh parameter tracking from RGB videos or independent\nframe-by-frame generation.\nTo address this problem, we leverage a transformer ar-\nchitecture [48] to process the audio signal in a manner that\ncan capture long-range semantic information [36, 44, 46].\nIn tandem, we use the input video to learn a person-specific\nstyle embedding, which can maintain the visual identity of\nthe speaker. We conjecture that directly mapping an audio\nsignal to rasterised pixel space is highly challenging due\nto its high dimensionality, inherent non-linearity, and the\nextensive data required to cover the diverse output distri-\nbution of realistic facial appearances. We therefore alter-\nnatively opt to predict the FLAME [30] parameters for a\ntemplate mesh and use them to render the subject head us-\ning 3DGS [39]. Although previous work has explored pre-\ndicting 3DMM parameters from audio [19, 53], our novel\narchitecture uniquely integrates a person-specific style em-\nbedding to preserve identity information, alongside direct\nFLAME parameter prediction from audio. This direct pre-\ndiction allows temporal information from the audio to inher-\nently influence and constrain consecutive frame predictions,\nsignificantly enhancing temporal consistency and reducing\n‘wobbling’. We transfer the lip movement generated from\nour transformer model and head motion from the original\nvideo through an optimized set of FLAME parameters.\nOne aspect that is widely assessed when judging the\nquality of generated videos is that of stability [21, 41]. Intu-\nitively: “the videos are stable” is a subjective statement. To\nformalize the notion, we propose a stability metric; towards\nquantifying video temporal stability (see Sec. 3.3).\nOur contributions can be summarized as follows:\n• We highlight the utility of transformer-based prediction\nfor person-specific 3D Morphable Model (3DMM) pa-\nrameters, from input audio. Our approach enables a tem-\nporally consistent mesh-based subject rendering.\n• We introduce a metric to quantify the temporal stability\nof synthetic talking head avatars.\n• Our overall pipeline, coined GaussianHeadTalk, achieves\nreal-time video rendering, while maintaining competitive\nperformance across both perceptual quality and video sta-\nbility metrics.\n2. Related Work\n2.1. 2D Talking Head Generation\nImage generation and editing capabilities of modern gener-\native models have inspired many practical applications in-\ncluding talking head synthesis. Early 2D-image based talk-\ning head methods ingest a single input image of a person\nand use GANs to drive video reenactment [3, 18, 22, 24, 43,\n45, 50]. These methods generally make use of an intermedi-\nate representation such as facial keypoints [3, 22, 24, 43, 50]\nor latent vectors [18, 45] to map motions to pixel space.\n3D Morphable Models (3DMM) [25, 47, 61] also provide\nan intermediate representation by mapping a 2D input im-\nage to 3D space and then back to 2D, affording control\nof head rotation. Imperfect mappings, however, can lead\nto a lack of identity preservation in resulting generated\nvideos. Audio-driven methods [37, 60, 63] focus on achiev-\ning accurate lip-sync, while head motion is generally non-\ndeterministically learned from the training dataset.\nGiven the superior generation quality of Diffusion meth-\nods [17], in comparison to GANs, some researchers recently\nemployed them for face reenactment [8, 51, 54].\nThese\nmethods provide better image quality, but inference is slow\nand computationally expensive, making them infeasible for\nreal-time generation. A recent line of works [27, 31] intro-\nduce real-time generation, but the use of single input source\nimages does not provide these models with temporal mo-\ntion information. We conjecture that this leads to problems\nlike unnatural head movement, stiff torso, and output qual-\nity is significantly dependent on identity features such as\nteeth and eye appearance within the single source frame.\n2D based methods also suffer from a lack of detailed 3D\nfacial geometry information. This impedes external control\nover facial motion and consistency during head rotation.\n2"}
{"pdf_id": "arxiv_251210939_gaussianHeadTalk", "page": 3, "text": "2.2. 3D Talking Head Generation\nWith the advent of 3D rendering techniques such as\nNeRFs [34] and Gaussian Splatting [26], researchers have\nstarted to explore these methods to render talking heads.\nNeRF-based approaches [20, 23] learn a radiance field from\nmultiple input images of a single scene. The volumetric\nrendering is performed based on an input controlling sig-\nnal e.g., audio. AD-NerF [23] has an intertwined archi-\ntecture that models the head and torso using two separate\nnetworks, limiting its flexibility. The original NeRF archi-\ntecture results in slow rendering speed (<1 FPS on NVIDIA\nV100 GPUs [55]), for talking head synthesis [33, 42]. Pro-\ntrait4D [15] uses multi-view synthetic data to learn tri-\nplane representations and, subsequently, Protrait4D-v2 [16]\nworks on pseudo-multi-view videos.\nWe note that these\nmethods cannot perform real-time rendering.\nGaussian Splatting has emerged as an effective real-time\nrendering method via Gaussian optimization on input scene\nmeshes. The input meshes are generated from monocular\nor multi-view videos. GaussianAvatar [39] and Gaussian-\nHead [49] use parametric models to control head motion.\nWhile the former binds Gaussians on a FLAME [30] mesh,\nsuch that every mesh triangle has at least one Gaussian, the\nlatter uses a motion deformation field and tri-plane repre-\nsentation. To enable motion, these renders can be condi-\ntioned directly on audio or driving video\n[9, 10, 29] to\ncreate talking heads. GaussianTalker [9] and TalkingGaus-\nsian [29] both utilize a tri-plane representation and fuse an\naudio signal to predict the deformation offsets in an end-to-\nend approach. GaussianSpeech [4] and GaussianTalker [58]\nuse FLAME [30] as an intermediate representation to map\naudio to Gaussians. GaussianSpeech [4] focuses on gener-\nating high-dimensional vertex offsets from audio for multi-\nview videos. GaussianTalker [58] predicts fine-grained off-\nsets for Gaussian position, rotation, and color to synthesize\ndetails like teeth and wrinkles, however layering this detail-\nsynthesis network on top of fully audio-generated motion\nmay risk amplifying any underlying instability from the\nmotion prediction module. These methods are suitable for\nreal-time inference due to high rendering speed; however,\nwe conjecture that independent frame-by-frame generation\nand a lack of optimization, using objectives that account\nfor temporal tracking, have the potential to induce jittering\nartifacts. Another line of work directly predicts 3D Mor-\nphable Model (3DMM) parameters, such as FLAME [30],\nfrom an audio signal [14, 19, 40, 53]. Their focus is on\ncontrolling facial parameters, rather than handling texture\ninformation, and hence, provide semantically meaningful\nmotion controls. In this work, we take advantage of an in-\ntermediate 3DMM representation by mapping audio-to-face\nparameters and then render a video using Gaussian Splats\nwith real-time performance.\n3. Methodology\nOur method is trained using an identity-specific video V =\n{In}, consisting of n image frames. We build our model in\ntwo-stages, where the first stage involves training identity-\nspecific Gaussian Splatting from the input video V , such\nthat each Gaussian is optimized with respect to a 3D Mor-\nphable Model’s triangles by ensuring that every triangle is\nattributed to at least one Gaussian. The first stage of our\npipeline (see Sec. 3.1) builds upon GaussianAvatar [39],\nwhere we replace the original FLAME [30] parameters\nwith parameters optimized by person-specific avatar train-\ning. In the second stage (Sec. 3.2), we learn an audio to\nFLAME [30] mapping, which captures the speech style of\na given identity. We next provide details for each stage.\n3.1. Gaussian Head Model\n3D Gaussian Splatting (3DGS) [26] reconstructs a static\nscene in 3D space using images and intrinsic, extrinsic\ncamera parameters. A scene is represented using a set of\nK anisotropic 3D Gaussians, where each Gaussian is de-\nfined by a center mean µi ∈R3 and a covariance matrix\nΣi ∈R3×3. The density of the i-th Gaussian for a 3D coor-\ndinate x ∈R3 is given by:\n  \\la b el  \n{ eq:gaussia\nn\n} G_i(x) = e^{-\\frac {1}{2}(x - \\mu _i)^\\top \\Sigma _i^{-1} (x - \\mu _i)}. \n(1)\nFurther decomposing the covariance matrix for efficient\nstorage and rendering, we obtain Σ = RSS⊤R⊤where R\nis a rotation matrix and S a scaling matrix. By additionally\nstoring appearance information, a 3D scene can be defined\nby a set of 3D Gaussian primitives:\n  \\la b el { eq: 3Ds cen e} \\ma\nthc al {G} = \\left \\{ G_i = (\\mu _i, s_i, q_i, \\alpha _i, \\text {SH}_i) \\right \\}_{i=1}^K, \n(2)\nwhere µi ∈R3 is the position vector (c.f. Eq. 1), si ∈R3\nis the scaling vector, qi ∈R4 is a quaternion representing\norientation, αi ∈R is an opacity value, and SHi denotes a\nset of spherical harmonics for encoding color as a function\nof view direction.\nAt rendering time, the 2D pixel-wise color C is calcu-\nlated by blending a subset of all 3D Gaussians whose pro-\njection into the image plane overlaps with that pixel loca-\ntion. Let N ⊆{1, . . . , K} denote the set of overlapping\nGaussians:\n  \n\\\ntex\nt {C\n}\n = \n\\\nsum\n _ { i\\\nin \\mathcal {N}} c_i \\alpha _i' \\prod _{j=1}^{i-1} (1 - \\alpha _j'), \\label {eq:1} \n(3)\nwhere ci is the view-dependent color of the i-th Gaussian,\nand α′\ni is the projected 2D opacity of each Gaussian, ob-\ntained by multiplying the projection of the overlapping 3D\nGaussian onto the image plane with the original opacity α.\n3"}
{"pdf_id": "arxiv_251210939_gaussianHeadTalk", "page": 4, "text": "Figure 2. We introduce GaussianHeadTalk, which comprises of Gaussian Head Modeling and audio to facial motion mapping. We first\ngenerate meshes from an input video using VHAP [38] tracking. Given an input audio and a template mesh, The audio to facial motion\nmapping uses a transformer-based architecture with a frozen Wav2Vec 2.0 [5] encoder. It learns long-term audio context and maps it directly\nto the 3D mesh by predicting FLAME [30] parameters. The generated parameters are used to render a person-specific GaussianAvatar [39],\ntrained using the input video.\nGaussianAvatar [39] introduce a method to bind Gaus-\nsians to Morphable Model mesh triangles, in this case, a\nFLAME [30] representation. For a given triangle with ver-\ntices and edges, a Gaussian is initialized using the mean po-\nsition of the vertices, the direction of one edge, and the nor-\nmal vector of the triangle. A process of Gaussian densifica-\ntion helps to adjust to an appropriate number of Gaussians,\nbased on local scene complexity. This involves increasing\nor decreasing the number of Gaussians in a given part of\nthe scene and is achieved by either splitting Gaussians into\ntwo if the view-space positional gradient is large, or cloning\ninto two if it is small. To avoid density explosion, a pruning\nstrategy removes points that have very low opacity, while\nmaintaining at least one splat per triangle.\nThe stability of the rendering process depends heavily\non the accuracy of the binding between Gaussian splats and\nFLAME [30] triangles. In contrast to alternative work, such\nas INSTA [65], where bounding volume hierarchy (BVH)\nbased nearest triangle search [13] leads to flickering arti-\nfacts, GaussianAvatar [39] is agnostic to tracked mesh inac-\ncuracies due to back-propagation of a positional gradient for\neach triangle. This consistent binding between Gaussians\nand the mesh triangles, regardless of pose or expression,\nallows fine-tuning of FLAME parameters. Along with the\noptimization of Gaussian splats parameters for position and\nscaling, FLAME parameters (translation, pose, and expres-\nsion) were therefore also optimized during training. This\nplays a crucial role in stabilizing the rendering output, miti-\ngating misalignment between the triangle meshes and Gaus-\nsians. We leverage this Gaussian-based head modeling and\nFLAME parameter tuning to help generate stable output.\n3.2. Audio to Facial Motion (Audio2Param)\nWe map from an audio signal to facial motion by leveraging\nthe FLAME parametric 3D Morphable Model. FLAME dis-\nentangled parameters control identity, expression, and pose.\nThese parameters can then be used to generate an explicit\n3D head mesh. Distinct from previous work [19, 53], which\noperates directly on full 3D head meshes by predicting tri-\nangle deformations or vertex positions, we take advantage\nof the disentangled FLAME representation. By directly pre-\ndicting FLAME expression parameters, we reduce the com-\nplexity of our learning objective from explicitly predicting\nthe spatial location of thousands of face vertices to the pre-\ndiction of fewer than one hundred parameters that together\ndefine facial expressions and lip motion.\nWe design a transformer-based architecture to capture\nlong-range temporal information from the audio signal con-\ncerning the context of the spoken sentence. To mitigate\nthe lack of diverse 3D audio-video datasets containing ex-\nplicit visual data, 3D meshes and paired audio, we instan-\ntiate our encoder using Wav2Vec 2.0 [5] which has previ-\nously demonstrated strong representation performance for\naudio information [19, 53]. We encode audio signals into\n4"}
{"pdf_id": "arxiv_251210939_gaussianHeadTalk", "page": 5, "text": "Method\nSelf-Reenactment\nCross-Reenactment\nFPS↑\nPSNR↑\nSSIM↑\nLPIPS↓\nSync↑\nStability↓\nSync↑\nStability↓\nIPLap [62]\n29.0412\n0.9462\n0.0340\n3.902\n0.6633\n3.324\n0.6856\n3.4\nEDTalk [45]\n26.9461\n0.8626\n0.0486\n7.144\n0.7802\n6.982\n0.7931\n17.2\nDitto [31]\n21.0595\n0.7412\n0.1284\n7.023\n0.9245\n6.844\n0.9618\n24.2\nMimicTalk [56]\n23.8775\n0.8092\n0.0735\n5.446\n0.8824\n5.286\n0.9227\n12.1\nGaussianTalker [9]\n27.6079\n0.9352\n0.0451\n5.346\n1.7622\n5.042\n1.8745\n59.6\nTalkingGaussian [29]\n27.3053\n0.9335\n0.0342\n6.422\n1.7183\n6.146\n1.8803\n72.2\nGaussianHeadTalk\n29.1233\n0.9477\n0.0338\n6.528\n0.6201\n6.122\n0.6836\n45.4\nTable 1. Self-Reenactment and Cross-Reenactment experimental settings. Our method achieves strong results in terms of stability, realism,\nimage quality and remains competitive for lip-sync.\nfeature vectors by adding a linear projection layer after the\nencoder. Similar to [19], we use a Periodic Positional En-\ncoding (PPE) to provide temporal information to the trans-\nformer decoder and a binary alignment mask to avoid infor-\nmation leakage from future frames.\nFor a single identity m, let the input training set be given\nby L = {A, M 0:T\ngt , Nm}, where M 0:T\ngt\nis a sequence of\nground-truth meshes for T+1 frames, A is an audio sig-\nnal from the ground-truth video that corresponds to those\nframes. The neutral template mesh Nm represents the given\nidentity. Each input training set is generated by processing\nan input video consisting of T+1 frames using the VHAP\ntracker [39] to generate ground truth meshes M 0:T\ngt\nand neu-\ntral template mesh Nm. Our objective is to predict a se-\nquence of meshes M 0:T\npred, given audio and neutral template\nmesh, such that:\n  f_{ \\th e t a }\n(A,N _ m ) =\n M ^{0:T}_{pred} \\approx M^{0:T}_{gt} \\label {eq:objective}. \n(4)\nThe correlation between the audio signal and lip move-\nment is typically high, but the correlation between the audio\nsignal and head movement is not [57]. Since different yet\nplausible head motions and expressions exist for the same\nspeech, there is no one-to-one mapping between speech and\nhead motion. Hence we focus on predicting accurate lip\nmovement from audio, by checking for high correlation be-\ntween audio and lip movement, and transfer head motion\ndirectly from the original tracked video sequence.\nIn addition to the head motion, the (potentially indepen-\ndent) audio speech signal A is processed through the trans-\nformer encoder and linear projection layer. We denote the\noutput from the linear projection layer for T+1 frames as\nC0:T . For a given frame t, the transformer encoder takes au-\ndio for frames [0, . . . , t] and uses the linear projection layer\nto generate Ct. The predicted audio features are passed to\nthe multi-head attention block of the transformer to obtain\nthe latent vertex offsets O0:T\nv\nfor each frame.\nTo utilize latent vertex offsets, an identity-specific tem-\nplate mesh, which is an average of all meshes obtained\nthrough video tracking, is encoded through a style encoder\nnetwork to obtain an identity embedding S. This procedure\nis shown in Fig. 2. Predicted latent vertex offsets Oi\nv for\nframe i are linearly combined with this embedding as:\n  \nO_ { s v }^\ni \n=  S+ O _ v ^ i, \\quad i \\in \\{0, \\ldots , T\\} \\label {eq:mesh_predicted}. \n(5)\nThe style-conditioned latent embeddings O0:T\nsv\nare then\nprocessed by a motion decoder, which comprises a set of\nlinear layers that map them to a low-dimensional FLAME\nparameter space, to obtain a 3D mesh representation. By\nperforming this process for each frame i, we obtain a pre-\ndicted mesh sequence M 0:T\npred.\nToward achieving accurate lip motion and jaw movement\nprediction, we isolate the FLAME parameters responsible\nfor jaw movement. We use these to augment the ground-\ntruth mesh as Mgt′ and calculate a loss as the difference,\nin vertex space, between this and our predicted mesh per\nframe. The remaining FLAME parameter values, used to\ndefine the ground-truth mesh, are instantiated using the tem-\nplate mesh. The model is trained end-to-end using an L2\nloss between the ground-truth and predicted meshes in ver-\ntex space as follows\n  \\ma t\nh\nc\nal \n{ L\n}\n_{m\nesh }\n = \\ s u\nm _{\nn=\n1\n}\n^{N} \\left ( \\sum _{t=1}^{T} \\left \\| M_{gt'}^t - M_{pred}^t \\right \\|_2 \\right ) \\label {eq:vertex_loss}. \n(6)\nDuring inference, the Audio2Param component ingests a\nneutral mesh and audio signal in order to predict a sequence\nof animated 3D facial meshes using the FLAME parameter\nspace. The predicted FLAME parameters are used to drive\nthe motion of a person-specific avatar [39], culminating in\nthe generation of an audio driven talking head.\n3.3. Quantifying temporal consistency\nNoted existing work generate talking head videos by posing\nvideo rendering as a set of, per-frame, independent tasks.\nWe observe that this typically leads to a lack of temporal\nconsistency in the output, which manifests itself as unnat-\nural wobbling, aberrations, and facial oscillations. Toward\nquantifying this problem, we employ a measure of temporal\nsmoothness for a given video.\n5"}
{"pdf_id": "arxiv_251210939_gaussianHeadTalk", "page": 6, "text": "Figure 3. Facial “wobbling” artifacts can be visualized via the\nabsolute difference between generated and ground-truth frames, in\nimage-space. The temporal gap between consecutive columns is\nten frames in each case. Our method exhibits smaller and spatially-\nmore-stable disparities, across time.\nFigure 4. Self-Reenactment Results: We show qualitative results\nbetween the Gaussian based methods by reenacting them using\nthe original audio. Our method, GaussianHeadTalk, shows better\nmouth movement, sharper teeth and fewer artifacts.\nWe first select a video and accompanying audio sample\nfrom the dataset [61] and proceed to render a talking head\nvideo using the original audio signal. This enables a di-\nrect comparison between the generated video and the orig-\ninal video (ground-truth). Towards defining a robust eval-\nuation protocol, we detect and track facial key points [64]\non the nose, as these key points are largely unaffected by\njaw movement and expression changes. The time-domain\nsignal provided, by these key points, can then be compared\nbetween generated and ground truth video frames. Further,\nwe observe that high-frequency wobbling and rapid oscilla-\ntions are challenging to detect using keypoint comparisons\nalone, and thus adopt a hybrid approach by additionally per-\nforming a Fast-Fourier-Transform (FFT) analysis to iden-\ntify frequent and uneven oscillations.\nOur hybrid approach takes an average of mean motion\ndifference Md, variability in motion magnitude Vm, and\nhigh-frequency power Hf. Each term is normalized by their\nrespective maximum values across a given sequence of in-\nput frames. Our compound stability score is then calculated\nby taking the average of these values:\n  \\text { Stabi l it y  s c or\ne\n} = \\frac {M_{d} + V_{m} + H_{f}}{3} \\label {eq:weighted_sum}. \n(7)\n4. Experiments\n4.1. Experimental Settings\nDataset:\nWe perform experiments on two datasets: VO-\nCASET [14] and HDTF [61]. Both datasets provide videos\nand synchronized audio. VOCASET also provides tracked\n3D-scans of the faces. We use VOCASET for pre-training\nthe Audio2Param component, and HDTF for training and\nevaluating person-specific gaussian avatars.\nSince the\nGaussian Splatting and NeRF-based models require sub-\nject specific training, we select ten subjects from HDTF\nthat cover a diverse set of identities and have a minimum\nof four minutes in video length. All videos were converted\nto 25fps to maintain experimental consistency. We synthet-\nically generate 15 audio clips covering five different lan-\nguages, with an average duration of seven seconds, using a\ntext-to-speech model1.\nComparison Baselines:\nWe compare GaussianHeadTalk\nwith current state-of-the-art methods.\nTwo approaches,\nGaussianTalker [9] and TalkingGaussian [29], naturally\nalign with our proposed problem setting as both can be con-\nsidered audio-driven Gaussian methods. We also compare\nwith IP LAP [62] and ED Talk [45] which are GAN based\nmethods, and Ditto [31] which is a Diffusion based method.\nLastly, we use MimicTalk [56] to evaluate performance of a\nrelated NeRF technique.\nImplementation Details:\nOur method is built on Py-\nTorch.\nWe first used VOCASET [14] audio-video and\ntheir tracked FLAME [30] parameters. We train our Au-\ndio2Param component using ADAM [28] optimizer with a\nlearning rate of 1e−4. We pre-train for 50000 steps and\nall experiments were performed on a single NVIDIA Tesla\nA100 GPU (40GB).\n4.2. Quantitative Evaluation\nWe evaluate the performance of our model on two tasks:\nself-reenactment and cross-reenactment.\nFirst, for self-\nreenactment, we extract the first 30 seconds of a video\nas a test set.\nWe train on the remaining part of the\n1https://elevenlabs.io/\n6"}
{"pdf_id": "arxiv_251210939_gaussianHeadTalk", "page": 7, "text": "Figure 5. Cross-Reenactment Results: visual reenactment using various methods with audio from a different speaker. The top row shows\nthe words from the audio, where red text highlights exact phonemes. GaussianHeadTalk can provide improved lip movement for these\naudio samples where other methods struggle with lip motion.\nvideo segment.\nFor cross-reenactment, we use syntheti-\ncally generated audio from a text-to-speech model so that\nthe audio sample contains no information about the trained\nperson identity.\nWe compare our method with state-of-\nthe-art Gaussian Splatting [9, 29], GAN\n[45, 62], Dif-\nfusion [31] and NeRF [56] based methods.\nTo evalu-\nate self-reenactment, we use Peak Signal-to-Noise Ratio\n(PSNR), Structural Similarity Index Measure (SSIM), and\nLearned Perceptual Image Patch Similarity (LPIPS). We\ncalculate the Sync confidence score [11, 12] for both self-\nreenactment and cross-reenactment. We observe that our\nmethod predominantly improves upon the state-of-the-art\n(ref. Table 1). For the perceptual metric GaussianHeadTalk\nperforms better than NeRF and alternative Gaussian based\nmethods. IPLap [62] provides results comparable to ours,\nas it models only the lip region using a GAN-based archi-\ntecture. However, inference is somewhat slower, which may\nimpede its real-time applications (ref. Table 1).\n4.3. Qualitative Evaluation\nWe show visual results in Figure 4 and Figure 5 for quali-\ntative comparison. GAN based methods provide good lip-\nsync in both cases, but their image quality falls short; with\ngenerated videos of resolution up to 256 × 256. Gaussian-\nbased methods (GaussianTalker, TalkingGaussian) generate\nsharper images, but their lip sync scores are low. As ex-\nample, they show lower lip openness while speaking ‘Hey’,\n(see middle column of Fig. 5). They also display wobbling\nartifacts in the generated videos, mainly due to the lack of\nlong-term temporal information and improper tracking of\n3D parameters during training. Our method generates sta-\nble talking head videos, with qualitative results that concur\nwith the relative quantitative metric improvements. We pro-\nvide supplementary videos for further results visualization.\n4.4. User Study\nWe conduct a user study to investigate how generated video\nquality is perceived by humans. We select a group of thirty\n7"}
{"pdf_id": "arxiv_251210939_gaussianHeadTalk", "page": 8, "text": "individuals and present each survey participant with mul-\ntiple video triplets. Our study compares the performance\nof GaussianTalker [9], TalkingGausian [29] and our work.\nParticipants were asked to evaluate videos in terms of “natu-\nralness” (i.e. assess wobbling and artifacts), lip sync quality,\nand image quality (i.e. evaluate identity preservation in gen-\nerated videos). Generated video orderings were random-\nized and models responsible for generation were masked\nfrom participants. Each participant was shown ten sets of\nvideo triplets, each with an average duration of five sec-\nonds. Participants were asked to provide an ordinal ranking\nfor each triplet, for each assessed aspect: ‘Best’, ‘Average’,\nand ‘Worst’, which we numerically map to values 3, 2, and\n1, respectively. For each participant, we sum the ratings a\nmethod received across the 10 triplets, and then divided this\nsum by 3 to normalize the score into a range of 3.3–10. The\nfinal reported scores, for each method, are average normal-\nized scores across all thirty participants (Table 2).\nMethod\nNatural↑\nLipSync↑\nQuality↑\nGaussianTalker [9]\n4.0\n5.0\n4.0\nTalkingGaussian [29]\n6.2\n7.2\n6.5\nGaussianHeadTalk (ours)\n9.8\n7.8\n9.5\nTable 2. User Study assessing human visual perception of gener-\nated video quality. The scores are averaged over different partici-\npants, with ten being the maximum.\nAll participants ranked videos generated by our method\nas the most natural, which supports our improved video sta-\nbility claims. In terms of ‘LipSync’, our method scores\nslightly above TalkingGaussian, which correlates with the\nrelated ‘Sync’ score (Table 1). Performant lip sync quality\nmay be explained by the special focus on the lip region, in\nboth cases. Image quality for our method can also show im-\nproved human rating scores, with respect to the compared\nstate-of-the-art.\n4.5. Ablation Study\nWe perform ablative studies to evaluate our methodological\nchoices (see Table 3). We first explore the effect of using\nnon-person-specific (i.e. non-optimized) FLAME parame-\nters directly for rendering (“w/o Parameter Optimization”).\nResulting generated videos display artifacts around various\nparts of the face which arise due to inaccuracies in parame-\nter alignment, distorting the videos generated.\nWe also tested the effect of more restrictive motion\ntransfer; namely, transferring only the lip motion (FLAME\nmodel jaw parameters) and keeping the head motion static\n(“w/o Full Motion Transfer”). This strategy leads to arti-\nfacts around parts of the generated video, due to the move-\nment interdependence between distinct FLAME parame-\nters. We observe smoother video generation, with fewer\nartifacts, when we transfer the full set of FLAME parame-\nters (remaining pose and expression components) from the\noriginal video.\nMethod\nPSNR↑\nSync↑\nStability↓\nw/o Parameter Optimization\n27.6987\n6.5123\n1.1432\nw/o Full Motion Transfer\n23.5621\n6.4962\n0.9154\nGaussianHeadTalk (ours)\n29.1233\n6.528\n0.6201\nTable 3. Ablation study: removal of key method components re-\nsults in qualitative visual degradations and respective decreases in\nassociated metrics.\n5. Limitations and Discussion\nOur method can generate high-quality talking heads, but\nis restricted to exactly this part of the body and currently\ncannot render partial or full human bodies. This limitation\narises due to our usage of a head-specific parametric model.\nExtension to accommodate full-body reenactment might in-\nvolve designing a Gaussian Splatting model that binds to\nSMPL-X [35], or similar full-body 3D parametric models.\nWe conjecture that a further interesting line of future work\nwill involve exploring any benefits derivable from learning\nfacial expression changes based on the tone and speed of\nthe audio signal, or other external control parameters for\nchanging the emotions of the face.\n6. Ethical Consideration\nThe generation of photo-realistic talking heads is a tech-\nnology that carries potential risks of misuse, particularly in\nthe creation of deepfakes for misinformation, harassment,\nor identity fraud. We advocate for the incorporation of ro-\nbust watermarking and detection mechanisms to help dis-\ntinguish synthetic content from real media and reduce the\npotential for harmful misuse.\n7. Conclusion\nWe introduce a novel method for generating high-quality\n3D talking heads with lip sync in real time. We proposed a\ntemporally stable pipeline that uses transformers to capture\nsematic information and long-range dependencies from au-\ndio signals. We also introduce a stability metric to quantify\nperceptual wobbling in generated videos. Our method of-\nfers strong performance with respect to the existing state-of-\nthe-art in terms of qualitative and quantitative benchmarks\nand we believe that high-quality real-time facial reenact-\nment holds exciting potential for many practical and useful\nreal-time applications.\n8"}
{"pdf_id": "arxiv_251210939_gaussianHeadTalk", "page": 9, "text": "References\n[1] Ayesha Afridi, Sumaiyah Obaid, Neha Raheel, and Fa-\nrooq Azam Rathore.\nIntegrating artificial intelligence in\nstroke rehabilitation: Current trends and future directions;\na mini review. JPMA. The Journal of the Pakistan Medical\nAssociation, 75(2):445–447, 2025. 1\n[2] Madhav Agarwal, Anchit Gupta, Rudrabha Mukhopadhyay,\nVinay P Namboodiri, and CV Jawahar. Compressing video\ncalls using synthetic talking heads. In British Machine Vision\nConference (BMVC), 2022. 1\n[3] Madhav Agarwal, Rudrabha Mukhopadhyay, Vinay P. Nam-\nboodiri, and C. V. Jawahar. Audio-visual face reenactment.\nIn Proceedings of the IEEE/CVF Winter Conference on Ap-\nplications of Computer Vision (WACV), pages 5178–5187,\n2023. 2\n[4] Shivangi Aneja, Artem Sevastopolsky, Tobias Kirschstein,\nJustus Thies, Angela Dai, and Matthias Nießner.\nGaus-\nsianspeech: Audio-driven gaussian avatars. arXiv preprint\narXiv:2411.18675, 2024. 3\n[5] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and\nMichael Auli. wav2vec 2.0: A framework for self-supervised\nlearning of speech representations. Advances in neural infor-\nmation processing systems, 33:12449–12460, 2020. 4\n[6] Dan Bigioi and Peter Corcoran.\nMultilingual video dub-\nbing—a technology review and current challenges. Frontiers\nin signal processing, 3:1230755, 2023. 1\n[7] Bolin Chen, Jie Chen, Shiqi Wang, and Yan Ye. Generative\nface video coding techniques and standardization efforts: A\nreview. In 2024 Data Compression Conference (DCC), pages\n103–112, 2024. 1\n[8] Zhiyuan Chen, Jiajiong Cao, Zhiquan Chen, Yuming Li,\nand Chenguang Ma. Echomimic: Lifelike audio-driven por-\ntrait animations through editable landmark conditions. arXiv\npreprint arXiv:2407.08136, 2024. 2\n[9] Kyusun Cho, Joungbin Lee, Heeji Yoon, Yeobin Hong,\nJaehoon Ko, Sangjun Ahn, and Seungryong Kim.\nGaus-\nsiantalker: Real-time talking head synthesis with 3d gaus-\nsian splatting. In Proceedings of the 32nd ACM International\nConference on Multimedia, page 10985–10994, 2024. 1, 2,\n3, 5, 6, 7, 8\n[10] Xuangeng Chu and Tatsuya Harada. Generalizable and an-\nimatable gaussian head avatar.\nIn The Thirty-eighth An-\nnual Conference on Neural Information Processing Systems,\n2024. 2, 3\n[11] Joon Son Chung and Andrew Zisserman. Out of time: auto-\nmated lip sync in the wild. In Computer Vision–ACCV 2016\nWorkshops: ACCV 2016 International Workshops, Taipei,\nTaiwan, November 20-24, 2016, Revised Selected Papers,\nPart II 13, pages 251–263. Springer, 2017. 7\n[12] Joon Son Chung and Andrew Zisserman. Lip reading in the\nwild.\nIn Computer Vision–ACCV 2016: 13th Asian Con-\nference on Computer Vision, Taipei, Taiwan, November 20-\n24, 2016, Revised Selected Papers, Part II 13, pages 87–103.\nSpringer, 2017. 7\n[13] James H Clark. Hierarchical geometric models for visible\nsurface algorithms. Communications of the ACM, 19(10):\n547–554, 1976. 4\n[14] Daniel Cudeiro, Timo Bolkart, Cassidy Laidlaw, Anurag\nRanjan, and Michael Black. Capture, learning, and synthesis\nof 3D speaking styles. In Proceedings IEEE Conf. on Com-\nputer Vision and Pattern Recognition (CVPR), pages 10101–\n10111, 2019. 3, 6\n[15] Yu Deng, Duomin Wang, Xiaohang Ren, Xingyu Chen, and\nBaoyuan Wang.\nPortrait4d: Learning one-shot 4d head\navatar synthesis using synthetic data.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 7119–7130, 2024. 3\n[16] Yu Deng, Duomin Wang, and Baoyuan Wang. Portrait4d-v2:\nPseudo multi-view data creates better 4d head synthesizer.\narXiv preprint arXiv:2403.13570, 2024. 3\n[17] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780–8794, 2021. 2\n[18] Nikita Drobyshev, Antoni Bigata Casademunt, Konstantinos\nVougioukas, Zoe Landgraf, Stavros Petridis, and Maja Pan-\ntic. Emoportraits: Emotion-enhanced multimodal one-shot\nhead avatars. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 8498–\n8507, 2024. 2\n[19] Yingruo Fan, Zhaojiang Lin, Jun Saito, Wenping Wang, and\nTaku Komura. Faceformer: Speech-driven 3d facial anima-\ntion with transformers.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2022. 2, 3, 4, 5\n[20] Guy Gafni, Justus Thies, Michael Zollh¨ofer, and Matthias\nNießner. Dynamic neural radiance fields for monocular 4d\nfacial avatar reconstruction. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 8649–8658, 2021. 2, 3\n[21] Wilko Guilluy, Laurent Oudre, and Azeddine Beghdadi.\nVideo stabilization:\nOverview, challenges and perspec-\ntives. Signal Processing: Image Communication, 90:116015,\n2021. 2\n[22] Jianzhu Guo, Dingyun Zhang, Xiaoqiang Liu, Zhizhou\nZhong, Yuan Zhang, Pengfei Wan, and Di Zhang. Livepor-\ntrait: Efficient portrait animation with stitching and retarget-\ning control. arXiv preprint arXiv:2407.03168, 2024. 2\n[23] Yudong Guo, Keyu Chen, Sen Liang, Yongjin Liu, Hujun\nBao, and Juyong Zhang. Ad-nerf: Audio driven neural ra-\ndiance fields for talking head synthesis. In IEEE/CVF Inter-\nnational Conference on Computer Vision (ICCV), 2021. 2,\n3\n[24] Fa-Ting Hong, Longhao Zhang, Li Shen, and Dan Xu.\nDepth-aware generative adversarial network for talking head\nvideo generation. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2022. 2\n[25] Xinya Ji,\nHang Zhou,\nKaisiyuan Wang,\nWayne Wu,\nChen Change Loy, Xun Cao, and Feng Xu. Audio-driven\nemotional video portraits. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 14080–14089, 2021. 2\n[26] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk¨uhler,\nand George Drettakis.\n3d gaussian splatting for real-time\nradiance field rendering. ACM Trans. Graph., 42(4):139–1,\n2023. 3\n9"}
{"pdf_id": "arxiv_251210939_gaussianHeadTalk", "page": 10, "text": "[27] Seyeon Kim, Siyoon Jin, Jihye Park, Kihong Kim, Jiyoung\nKim, Jisu Nam, and Seungryong Kim. Moditalker: Motion-\ndisentangled diffusion model for high-fidelity talking head\ngeneration. In Proceedings of the AAAI Conference on Arti-\nficial Intelligence, pages 4302–4310, 2025. 2\n[28] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 6\n[29] Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun\nZhou, and Lin Gu. Talkinggaussian: Structure-persistent 3d\ntalking head synthesis via gaussian splatting. In European\nConference on Computer Vision, pages 127–145. Springer,\n2024. 1, 2, 3, 5, 6, 7, 8\n[30] Tianye Li, Timo Bolkart, Michael. J. Black, Hao Li, and\nJavier Romero. Learning a model of facial shape and ex-\npression from 4D scans. ACM Transactions on Graphics,\n(Proc. SIGGRAPH Asia), 36(6):194:1–194:17, 2017. 2, 3, 4,\n6\n[31] Tianqi Li, Ruobing Zheng, Minghui Yang, Jingdong Chen,\nand Ming Yang.\nDitto: Motion-space diffusion for con-\ntrollable realtime talking head synthesis.\narXiv preprint\narXiv:2411.19509, 2024. 2, 5, 6, 7\n[32] Zhan Li, Zhang Chen, Zhong Li, and Yi Xu. Spacetime gaus-\nsian feature splatting for real-time dynamic view synthesis.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 8508–8520, 2024. 2\n[33] Xian Liu, Yinghao Xu, Qianyi Wu, Hang Zhou, Wayne\nWu, and Bolei Zhou. Semantic-aware implicit neural audio-\ndriven video portrait generation. In European conference on\ncomputer vision, pages 106–125. Springer, 2022. 3\n[34] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99–106, 2021.\n3\n[35] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,\nTimo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and\nMichael J. Black. Expressive body capture: 3D hands, face,\nand body from a single image. In Proceedings IEEE Conf.\non Computer Vision and Pattern Recognition (CVPR), pages\n10975–10985, 2019. 8\n[36] Ziqiao Peng, Yihao Luo, Yue Shi, Hao Xu, Xiangyu Zhu,\nHongyan Liu, Jun He, and Zhaoxin Fan. Selftalk: A self-\nsupervised commutative training diagram to comprehend 3d\ntalking faces. In Proceedings of the 31st ACM International\nConference on Multimedia, pages 5292–5301, 2023. 2\n[37] K R Prajwal, Rudrabha Mukhopadhyay, Vinay P. Nambood-\niri, and C.V. Jawahar.\nA lip sync expert is all you need\nfor speech to lip generation in the wild. In Proceedings of\nthe 28th ACM International Conference on Multimedia, page\n484–492, New York, NY, USA, 2020. Association for Com-\nputing Machinery. 2\n[38] Shenhan Qian. Vhap: Versatile head alignment with adaptive\nappearance priors, 2024. 4\n[39] Shenhan Qian, Tobias Kirschstein, Liam Schoneveld, Davide\nDavoli, Simon Giebenhain, and Matthias Nießner.\nGaus-\nsianavatars: Photorealistic head avatars with rigged 3d gaus-\nsians. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 20299–20309,\n2024. 2, 3, 4, 5\n[40] Alexander Richard, Michael Zollh¨ofer, Yandong Wen, Fer-\nnando de la Torre, and Yaser Sheikh. Meshtalk: 3d face an-\nimation from speech using cross-modality disentanglement.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV), pages 1173–1182, 2021. 3\n[41] Marcos Roberto e Souza, Helena de Almeida Maia, and He-\nlio Pedrini. Survey on digital video stabilization: concepts,\nmethods, and challenges. ACM Computing Surveys (CSUR),\n55(3):1–37, 2022. 2\n[42] Shuai Shen, Wanhua Li, Zheng Zhu, Yueqi Duan, Jie Zhou,\nand Jiwen Lu. Learning dynamic facial radiance fields for\nfew-shot talking head synthesis. In European conference on\ncomputer vision, pages 666–682. Springer, 2022. 3\n[43] Aliaksandr Siarohin, St´ephane Lathuili`ere, Sergey Tulyakov,\nElisa Ricci, and Nicu Sebe. First order motion model for\nimage animation. In Conference on Neural Information Pro-\ncessing Systems (NeurIPS), 2019. 2\n[44] Wenfeng Song, Xuan Wang, Shi Zheng, Shuai Li, Aimin\nHao, and Xia Hou. Talkingstyle: personalized speech-driven\n3d facial animation with style preservation. IEEE Transac-\ntions on Visualization and Computer Graphics, 2024. 2\n[45] Shuai Tan, Bin Ji, Mengxiao Bi, and Ye Pan. Edtalk: Effi-\ncient disentanglement for emotional talking head synthesis.\nIn European Conference on Computer Vision, pages 398–\n416. Springer, 2025. 2, 5, 6, 7\n[46] Balamurugan Thambiraja, Ikhsanul Habibie, Sadegh Aliak-\nbarian, Darren Cosker, Christian Theobalt, and Justus Thies.\nImitator: Personalized speech-driven 3d facial animation. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 20621–20631, 2023. 2\n[47] Justus Thies, Mohamed Elgharib, Ayush Tewari, Christian\nTheobalt, and Matthias Nießner.\nNeural voice puppetry:\nAudio-driven facial reenactment. In Computer Vision–ECCV\n2020: 16th European Conference, Glasgow, UK, August\n23–28, 2020, Proceedings, Part XVI 16, pages 716–731.\nSpringer, 2020. 2\n[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 2\n[49] Jie Wang, Jiu-Cheng Xie, Xianyan Li, Feng Xu, Chi-Man\nPun, and Hao Gao.\nGaussianhead:\nHigh-fidelity head\navatars with learnable gaussian derivation.\narXiv preprint\narXiv:2312.01632, 2023. 3\n[50] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot\nfree-view neural talking-head synthesis for video conferenc-\ning. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 2021. 2\n[51] Huawei Wei, Zejun Yang, and Zhisheng Wang. Aniportrait:\nAudio-driven synthesis of photorealistic portrait animation.\narXiv preprint arXiv:2403.17694, 2024. 2\n[52] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng\nZhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang.\n4d gaussian splatting for real-time dynamic scene rendering.\n10"}
{"pdf_id": "arxiv_251210939_gaussianHeadTalk", "page": 11, "text": "In Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 20310–20320, 2024. 2\n[53] Jinbo Xing, Menghan Xia, Yuechen Zhang, Xiaodong Cun,\nJue Wang, and Tien-Tsin Wong. Codetalker: Speech-driven\n3d facial animation with discrete motion prior. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 12780–12790, 2023. 2, 3, 4\n[54] Mingwang Xu, Hui Li, Qingkun Su, Hanlin Shang, Liwei\nZhang, Ce Liu, Jingdong Wang, Yao Yao, and Siyu Zhu.\nHallo: Hierarchical audio-driven visual synthesis for portrait\nimage animation. arXiv preprint arXiv:2406.08801, 2024. 2\n[55] Xinkai Yan, Jieting Xu, Yuchi Huo, and Hujun Bao. Neural\nrendering and its hardware acceleration: A review. arXiv\npreprint arXiv:2402.00028, 2024. 3\n[56] Zhenhui Ye, Tianyun Zhong, Yi Ren, Ziyue Jiang, Jiawei\nHuang, Rongjie Huang, Jinglin Liu, Jinzheng He, Chen\nZhang, Zehan Wang, et al. Mimictalk: Mimicking a per-\nsonalized and expressive 3d talking face in minutes.\nAd-\nvances in neural information processing systems, 37:1829–\n1853, 2024. 5, 6, 7\n[57] Hani Yehia, Philip Rubin, and Eric Vatikiotis-Bateson.\nQuantitative association of vocal-tract and facial behavior.\nSpeech Communication, 26(1-2):23–43, 1998. 5\n[58] Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen,\nZhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu,\nFei Wu, Chengfei Lv, et al. Gaussiantalker: Speaker-specific\ntalking head synthesis via 3d gaussian splatting. In Proceed-\nings of the 32nd ACM International Conference on Multime-\ndia, pages 3548–3557, 2024. 3\n[59] Lichao Zhang, Jia Yu, Shuai Zhang, Long Li, Yangyang\nZhong, Guanbao Liang, Yuming Yan, Qing Ma, Fangsheng\nWeng, Fayu Pan, Jing Li, Renjun Xu, and Zhenzhong Lan.\nUnveiling the impact of multi-modal interactions on user en-\ngagement: A comprehensive evaluation in ai-driven conver-\nsations, 2024. 1\n[60] Yue Zhang, Minhao Liu, Zhaokang Chen, Bin Wu, Yubin\nZeng, Chao Zhan, Yingjie He, Junxin Huang, and Wenjiang\nZhou. Musetalk: Real-time high quality lip synchorization\nwith latent space inpainting. arxiv, 2024. 2\n[61] Zhimeng Zhang, Lincheng Li, Yu Ding, and Changjie\nFan.\nFlow-guided one-shot talking face generation with\na high-resolution audio-visual dataset.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 3661–3670, 2021. 2, 6\n[62] Weizhi Zhong, Chaowei Fang, Yinqi Cai, Pengxu Wei,\nGangming Zhao, Liang Lin, and Guanbin Li.\nIdentity-\npreserving talking face generation with landmark and ap-\npearance priors. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 9729–9738, 2023. 5, 6, 7\n[63] Yang Zhou, Xintong Han, Eli Shechtman, Jose Echevar-\nria, Evangelos Kalogerakis, and Dingzeyu Li. Makelttalk:\nspeaker-aware talking-head animation. ACM Transactions\nOn Graphics (TOG), 39(6):1–15, 2020. 2\n[64] Zhenglin Zhou, Huaxia Li, Hong Liu, Nanyang Wang, Gang\nYu, and Rongrong Ji.\nStar loss: Reducing semantic am-\nbiguity in facial landmark detection.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 15475–15484, 2023. 6\n[65] Wojciech Zielonka, Timo Bolkart, and Justus Thies. Instant\nvolumetric head avatars. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 4574–4584, 2023. 4\n11"}
{"pdf_id": "arxiv_251210939_gaussianHeadTalk", "page": 12, "text": "GaussianHeadTalk: Wobble-Free 3D Talking Heads with Audio Driven\nGaussian Splatting\nSupplementary Material\nFigure A. For a given audio signal, GaussianHeadTalk generates\na lip-sync 3D mesh and use the generated FLAME parameters to\ntransfer lip motion on a trained GaussianAvatar with optimized\nFLAME parameters.\nA. Qualitative Ablation Study\nFigure B. Ablation Study: Effect of transferring the lip motion\nand keeping other parameters static (w/o Full Motion Transfer).\nThe results shows visible artifacts in the generated avatar, as the\nFLAME parameters are not fully independent.\nFigure C. Ablation Study: Using Non-Optimized FLAME param-\neters (w/o Parameter Optimization). This leads to artifacts around\nthe torso region, and wobbling issues.\nB. Temporal Analysis of Keypoints\nFigure D. Comparison of keypoint movement across time between\nan original video and a video generated using GaussianTalker. The\noverlay graph shows that there is flickering in the rendered video.\nIn ideal case, these two graphs should be perfectly overlapping.\nWe report x-axis, y-axis motion magnitude over time in upper and\nlower plots, respectively.\nC. User Study\n1"}
{"pdf_id": "arxiv_251210939_gaussianHeadTalk", "page": 13, "text": "Category\nMethod\nFinal Score\n‘Best’ (3)\n‘Average’ (2)\n‘Worst’ (1)\nTotal Ratings\nNaturalness\nGaussianHeadTalk (ours)\n9.8\n284\n14\n2\n300\nTalkingGaussian [29]\n6.2\n40\n178\n82\n300\nGaussianTalker [9]\n4.0\n5\n50\n245\n300\nLipSync\nGaussianHeadTalk (ours)\n7.8\n142\n118\n40\n300\nTalkingGaussian [29]\n7.2\n128\n92\n80\n300\nGaussianTalker [9]\n5.0\n25\n100\n175\n300\nQuality\nGaussianHeadTalk (ours)\n9.5\n260\n35\n5\n300\nTalkingGaussian [29]\n6.5\n80\n125\n95\n300\nGaussianTalker [9]\n4.0\n1\n58\n241\n300\nTable A. Detailed breakdown of User Study Ratings. 30 participants evaluate 10 videos of each method, generating 300 ratings in total.\nFor each triplet, a participant assigns the ranks 1, 2, and 3 once each, so the raw sum across the three methods is 1+2+3=6. Over 10 triplets,\nthis gives a total of 10×6=60. After dividing each method’s total by 3 for normalization, the overall sum across all methods is fixed at 60/3\n= 20.\nFigure E. Visualization of frame stability through color channel overlay with ground-truth video over 10 consecutive frames. The signifi-\ncant displacement (wobbling) observed in the GaussianTalker and TalkingGaussian methods contrasts with the high overlap and stability\nachieved by our proposed method. Green and red channels highlight the differences within the blue dashed boxes.\n2"}
{"pdf_id": "arxiv_251210939_gaussianHeadTalk", "page": 14, "text": "Figure F. Cross-Reenactment Results: We show the visual results by reenacting various methods using a different audio, from a different\nspeaker. The top row shows the word from the audio, with red part highlighting the exact phoneme. GaussianHeadTalk provides the best\npossible lip movement for these new audio samples. Other methods struggle to have proper lip motion, generate high-quality videos and\nno artifacts.\n3"}
{"pdf_id": "arxiv_251210939_gaussianHeadTalk", "page": 15, "text": "Figure G. Cross-Reenactment Results: We show the visual results by reenacting various methods using a different audio, from a different\nspeaker. The top row shows the word from the audio, with red part highlighting the exact phoneme. GaussianHeadTalk provides the best\npossible lip movement for these new audio samples. Other methods struggle to have proper lip motion, generate high-quality videos and\nno artifacts.\n4"}
