{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 1, "frag_id": 0, "text": "gaussianheadtalk : wobble - free 3d talking heads with audio driven gaussian splatting madhav agarwal1 mingtian zhang2 laura sevilla - lara1 steven mcdonagh1 1university of edinburgh 2university college london madhav. agarwal @ ed. ac. uk, mingtian. zhang. 17 @ ucl. ac. uk, { l. sevilla, s. mcdonagh } @ ed. ac. uk ours ground truth gaussiantalker talkinagaussian input video ( monocular ) 3d gaussian head model rendered lip - sync video comparison with previous works input audio ( any ) hello yather peace calm figure 1. to address the challenges of temporal instability, slow rendering, and limited photorealism in existing methods, we propose gaussianheadtalk : a real - time system that generates photorealistic, temporally stable 3d talking head avatars directly from monocular video and arbitrary audio input. corresponding output frames generated by state - of - the - art methods gaussiantalker [ 9 ] and talkinggaussian [ 29 ] are also provided for visual comparison. abstract speech - driven talking heads have recently emerged and enable interactive avatars. however, real - world applications are limited, as current methods achieve high visual fidelity but slow or fast yet temporally unstable. diffusion methods provide realistic image generation, yet struggle with oneshot settings. gaussian splatting approaches are real - time, yet inaccuracies in facial tracking, or inconsistent gaussian mappings, lead to unstable outputs and video artifacts that are detrimental to realistic use cases. we address this problem by mapping gaussian splatting using 3d morphable models to generate person - specific avatars. we introduce transformer - based prediction of model parameters, directly from audio, to drive temporal consistency. from monocular video and independent audio speech inputs, our method enables generation of real - time talking head videos where we report competitive quantitative and qualitative performance. project page : https : / / madhav1ag. github. io / gaussianheadtalk 1. introduction generating talking head videos, driven directly by audio, can be considered a valuable task with multiple practical applications [ 2, 7 ]. whether in the education sector, health care, teleconferencing, or film and entertainment industries, high - quality personalized talking head avatars", "token_count": 500}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 1, "frag_id": 1, "text": "using 3d morphable models to generate person - specific avatars. we introduce transformer - based prediction of model parameters, directly from audio, to drive temporal consistency. from monocular video and independent audio speech inputs, our method enables generation of real - time talking head videos where we report competitive quantitative and qualitative performance. project page : https : / / madhav1ag. github. io / gaussianheadtalk 1. introduction generating talking head videos, driven directly by audio, can be considered a valuable task with multiple practical applications [ 2, 7 ]. whether in the education sector, health care, teleconferencing, or film and entertainment industries, high - quality personalized talking head avatars serve as an effective path for information transfer. for instance, aidriven virtual assistants for telemedicine can be useful in assistive communications and post - stroke rehabilitation [ 1 ]. by providing a human face to an interactive agent, instead of a text - based input - output platform, the user experience is made more immersive [ 59 ]. further uses include dubbing movies into multiple languages, which reduces the production time and cost of vfx studios manyfold [ 6 ]. the canonical problem involves taking a short input video of a person, alongside an arbitrary speech audio signal, in order to create a person - specific avatar that can generate an output video of the subject appearing to speak the audio content ( i. e. with visual lip - syncing that accurately matches the input audio ). the task is commonly known as face reenactment and previous solutions involve 1 arxiv : 2512. 10939v1 [ cs. cv ] 11 dec 2025", "token_count": 349}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 2, "frag_id": 0, "text": "using gans [ 3, 24, 43, 50 ], diffusion models [ 8, 51, 54 ], nerfs [ 20, 23 ] and, more recently, 3d gaussian splatting [ 9, 10, 29, 39 ]. diffusion - based methods have robust generative priors and produce state - of - the - art image quality yet they are computationally expensive and inference speed is typically slower than gans and nerfs. in contrast, 3d gaussian splatting ( 3dgs ) methods are person or scene - specific and have recently shown efficacy in rendering high - quality images and videos comparable to that of diffusion models, but at real - time speeds. although recent advancements in 3dgs have successfully incorporated temporal information for dynamic scenes [ 32, 52 ], the integration of related techniques into the synthesis of audio - driven 3dgs talking heads remains an open challenge. this gap highlights the need for novel approaches to combine dynamic, temporally consistent facial animation with audio - driven generation. the task is inherently dynamic, requiring precise temporal information to ensure realistic and consistent facial movements, particularly lip synchronization. current audio - driven 3dgs methods rely on parameter tracking for temporal information, which often falls short for monocular videos. inaccuracies in such tracking can lead to temporal flickering ( i. e., ‘ wobbling ’ ) in the face region, causing visible artifacts. our experiments show that this instability arises due to the improper utilization of temporal information from an input video, which manifests itself as either inaccurate 3d mesh parameter tracking from rgb videos or independent frame - by - frame generation. to address this problem, we leverage a transformer architecture [ 48 ] to process the audio signal in a manner that can capture long - range semantic information [ 36, 44, 46 ]. in tandem, we use the input video to learn a person - specific style embedding, which can maintain the visual identity of the speaker. we conjecture that directly mapping an audio signal to rasterised pixel space is highly challenging due to its high dimensionality, inherent non - linearity, and the extensive data required to cover the diverse output distribution of realistic facial appearances. we therefore alternatively opt to predict the flame [ 30 ] parameters for a template mesh and use them to render the subject head using 3dgs [ 39 ]. although previous work has explored predicting 3dmm parameters from audio [ 19, 53 ], our novel architecture uniquely integrates a", "token_count": 500}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 2, "frag_id": 1, "text": "signal in a manner that can capture long - range semantic information [ 36, 44, 46 ]. in tandem, we use the input video to learn a person - specific style embedding, which can maintain the visual identity of the speaker. we conjecture that directly mapping an audio signal to rasterised pixel space is highly challenging due to its high dimensionality, inherent non - linearity, and the extensive data required to cover the diverse output distribution of realistic facial appearances. we therefore alternatively opt to predict the flame [ 30 ] parameters for a template mesh and use them to render the subject head using 3dgs [ 39 ]. although previous work has explored predicting 3dmm parameters from audio [ 19, 53 ], our novel architecture uniquely integrates a person - specific style embedding to preserve identity information, alongside direct flame parameter prediction from audio. this direct prediction allows temporal information from the audio to inherently influence and constrain consecutive frame predictions, significantly enhancing temporal consistency and reducing ‘ wobbling ’. we transfer the lip movement generated from our transformer model and head motion from the original video through an optimized set of flame parameters. one aspect that is widely assessed when judging the quality of generated videos is that of stability [ 21, 41 ]. intuitively : “ the videos are stable ” is a subjective statement. to formalize the notion, we propose a stability metric ; towards quantifying video temporal stability ( see sec. 3. 3 ). our contributions can be summarized as follows : • we highlight the utility of transformer - based prediction for person - specific 3d morphable model ( 3dmm ) parameters, from input audio. our approach enables a temporally consistent mesh - based subject rendering. • we introduce a metric to quantify the temporal stability of synthetic talking head avatars. • our overall pipeline, coined gaussianheadtalk, achieves real - time video rendering, while maintaining competitive performance across both perceptual quality and video stability metrics. 2. related work 2. 1. 2d talking head generation image generation and editing capabilities of modern generative models have inspired many practical applications including talking head synthesis. early 2d - image based talking head methods ingest a single input image of a person and use gans to drive video reenactment [ 3, 18, 22, 24, 43, 45, 50 ]. these methods generally make use of an intermediate representation such as facial keypoints [ 3, 22, 24, 43, 50", "token_count": 500}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 2, "frag_id": 2, "text": "to quantify the temporal stability of synthetic talking head avatars. • our overall pipeline, coined gaussianheadtalk, achieves real - time video rendering, while maintaining competitive performance across both perceptual quality and video stability metrics. 2. related work 2. 1. 2d talking head generation image generation and editing capabilities of modern generative models have inspired many practical applications including talking head synthesis. early 2d - image based talking head methods ingest a single input image of a person and use gans to drive video reenactment [ 3, 18, 22, 24, 43, 45, 50 ]. these methods generally make use of an intermediate representation such as facial keypoints [ 3, 22, 24, 43, 50 ] or latent vectors [ 18, 45 ] to map motions to pixel space. 3d morphable models ( 3dmm ) [ 25, 47, 61 ] also provide an intermediate representation by mapping a 2d input image to 3d space and then back to 2d, affording control of head rotation. imperfect mappings, however, can lead to a lack of identity preservation in resulting generated videos. audio - driven methods [ 37, 60, 63 ] focus on achieving accurate lip - sync, while head motion is generally nondeterministically learned from the training dataset. given the superior generation quality of diffusion methods [ 17 ], in comparison to gans, some researchers recently employed them for face reenactment [ 8, 51, 54 ]. these methods provide better image quality, but inference is slow and computationally expensive, making them infeasible for real - time generation. a recent line of works [ 27, 31 ] introduce real - time generation, but the use of single input source images does not provide these models with temporal motion information. we conjecture that this leads to problems like unnatural head movement, stiff torso, and output quality is significantly dependent on identity features such as teeth and eye appearance within the single source frame. 2d based methods also suffer from a lack of detailed 3d facial geometry information. this impedes external control over facial motion and consistency during head rotation. 2", "token_count": 431}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 3, "frag_id": 0, "text": "2. 2. 3d talking head generation with the advent of 3d rendering techniques such as nerfs [ 34 ] and gaussian splatting [ 26 ], researchers have started to explore these methods to render talking heads. nerf - based approaches [ 20, 23 ] learn a radiance field from multiple input images of a single scene. the volumetric rendering is performed based on an input controlling signal e. g., audio. ad - nerf [ 23 ] has an intertwined architecture that models the head and torso using two separate networks, limiting its flexibility. the original nerf architecture results in slow rendering speed ( < 1 fps on nvidia v100 gpus [ 55 ] ), for talking head synthesis [ 33, 42 ]. protrait4d [ 15 ] uses multi - view synthetic data to learn triplane representations and, subsequently, protrait4d - v2 [ 16 ] works on pseudo - multi - view videos. we note that these methods cannot perform real - time rendering. gaussian splatting has emerged as an effective real - time rendering method via gaussian optimization on input scene meshes. the input meshes are generated from monocular or multi - view videos. gaussianavatar [ 39 ] and gaussianhead [ 49 ] use parametric models to control head motion. while the former binds gaussians on a flame [ 30 ] mesh, such that every mesh triangle has at least one gaussian, the latter uses a motion deformation field and tri - plane representation. to enable motion, these renders can be conditioned directly on audio or driving video [ 9, 10, 29 ] to create talking heads. gaussiantalker [ 9 ] and talkinggaussian [ 29 ] both utilize a tri - plane representation and fuse an audio signal to predict the deformation offsets in an end - toend approach. gaussianspeech [ 4 ] and gaussiantalker [ 58 ] use flame [ 30 ] as an intermediate representation to map audio to gaussians. gaussianspeech [ 4 ] focuses on generating high - dimensional vertex offsets from audio for multiview videos. gaussiantalker [ 58 ] predicts fine - grained offsets for gaussian position, rotation, and color to synthesize details like teeth and wrinkles, however layering this detailsynthesis network on top of fully audio - generated motion may", "token_count": 500}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 3, "frag_id": 1, "text": "##er [ 9 ] and talkinggaussian [ 29 ] both utilize a tri - plane representation and fuse an audio signal to predict the deformation offsets in an end - toend approach. gaussianspeech [ 4 ] and gaussiantalker [ 58 ] use flame [ 30 ] as an intermediate representation to map audio to gaussians. gaussianspeech [ 4 ] focuses on generating high - dimensional vertex offsets from audio for multiview videos. gaussiantalker [ 58 ] predicts fine - grained offsets for gaussian position, rotation, and color to synthesize details like teeth and wrinkles, however layering this detailsynthesis network on top of fully audio - generated motion may risk amplifying any underlying instability from the motion prediction module. these methods are suitable for real - time inference due to high rendering speed ; however, we conjecture that independent frame - by - frame generation and a lack of optimization, using objectives that account for temporal tracking, have the potential to induce jittering artifacts. another line of work directly predicts 3d morphable model ( 3dmm ) parameters, such as flame [ 30 ], from an audio signal [ 14, 19, 40, 53 ]. their focus is on controlling facial parameters, rather than handling texture information, and hence, provide semantically meaningful motion controls. in this work, we take advantage of an intermediate 3dmm representation by mapping audio - to - face parameters and then render a video using gaussian splats with real - time performance. 3. methodology our method is trained using an identity - specific video v = { in }, consisting of n image frames. we build our model in two - stages, where the first stage involves training identityspecific gaussian splatting from the input video v, such that each gaussian is optimized with respect to a 3d morphable model ’ s triangles by ensuring that every triangle is attributed to at least one gaussian. the first stage of our pipeline ( see sec. 3. 1 ) builds upon gaussianavatar [ 39 ], where we replace the original flame [ 30 ] parameters with parameters optimized by person - specific avatar training. in the second stage ( sec. 3. 2 ), we learn an audio to flame [ 30 ] mapping, which captures the speech style of a given identity. we next provide details for each stage. 3. 1.", "token_count": 500}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 3, "frag_id": 2, "text": "the first stage involves training identityspecific gaussian splatting from the input video v, such that each gaussian is optimized with respect to a 3d morphable model ’ s triangles by ensuring that every triangle is attributed to at least one gaussian. the first stage of our pipeline ( see sec. 3. 1 ) builds upon gaussianavatar [ 39 ], where we replace the original flame [ 30 ] parameters with parameters optimized by person - specific avatar training. in the second stage ( sec. 3. 2 ), we learn an audio to flame [ 30 ] mapping, which captures the speech style of a given identity. we next provide details for each stage. 3. 1. gaussian head model 3d gaussian splatting ( 3dgs ) [ 26 ] reconstructs a static scene in 3d space using images and intrinsic, extrinsic camera parameters. a scene is represented using a set of k anisotropic 3d gaussians, where each gaussian is defined by a center mean µi ∈r3 and a covariance matrix σi ∈r3×3. the density of the i - th gaussian for a 3d coordinate x ∈r3 is given by : \\ la b el { eq : gaussia n } g _ i ( x ) = e ^ { - \\ frac { 1 } { 2 } ( x - \\ mu _ i ) ^ \\ top \\ sigma _ i ^ { - 1 } ( x - \\ mu _ i ) }. ( 1 ) further decomposing the covariance matrix for efficient storage and rendering, we obtain σ = [UNK] r is a rotation matrix and s a scaling matrix. by additionally storing appearance information, a 3d scene can be defined by a set of 3d gaussian primitives : \\ la b el { eq : 3ds cen e } \\ ma thc al { g } = \\ left \\ { g _ i = ( \\ mu _ i, s _ i, q _ i, \\ alpha _ i, \\ text { sh } _ i ) \\ right \\ } _ { i = 1 } ^ k, ( 2 ) where µi ∈r3 is the position vector ( c. f. eq. 1 ), si ∈r3 is the scaling vector, qi ∈r4 is a quaternion representing orientation, αi", "token_count": 500}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 3, "frag_id": 3, "text": "rotation matrix and s a scaling matrix. by additionally storing appearance information, a 3d scene can be defined by a set of 3d gaussian primitives : \\ la b el { eq : 3ds cen e } \\ ma thc al { g } = \\ left \\ { g _ i = ( \\ mu _ i, s _ i, q _ i, \\ alpha _ i, \\ text { sh } _ i ) \\ right \\ } _ { i = 1 } ^ k, ( 2 ) where µi ∈r3 is the position vector ( c. f. eq. 1 ), si ∈r3 is the scaling vector, qi ∈r4 is a quaternion representing orientation, αi ∈r is an opacity value, and shi denotes a set of spherical harmonics for encoding color as a function of view direction. at rendering time, the 2d pixel - wise color c is calculated by blending a subset of all 3d gaussians whose projection into the image plane overlaps with that pixel location. let n ⊆ { 1,..., k } denote the set of overlapping gaussians : \\ tex t { c } = \\ sum _ { i \\ in \\ mathcal { n } } c _ i \\ alpha _ i ' \\ prod _ { j = 1 } ^ { i - 1 } ( 1 - \\ alpha _ j ' ), \\ label { eq : 1 } ( 3 ) where ci is the view - dependent color of the i - th gaussian, and α ′ i is the projected 2d opacity of each gaussian, obtained by multiplying the projection of the overlapping 3d gaussian onto the image plane with the original opacity α. 3", "token_count": 361}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 4, "frag_id": 0, "text": "figure 2. we introduce gaussianheadtalk, which comprises of gaussian head modeling and audio to facial motion mapping. we first generate meshes from an input video using vhap [ 38 ] tracking. given an input audio and a template mesh, the audio to facial motion mapping uses a transformer - based architecture with a frozen wav2vec 2. 0 [ 5 ] encoder. it learns long - term audio context and maps it directly to the 3d mesh by predicting flame [ 30 ] parameters. the generated parameters are used to render a person - specific gaussianavatar [ 39 ], trained using the input video. gaussianavatar [ 39 ] introduce a method to bind gaussians to morphable model mesh triangles, in this case, a flame [ 30 ] representation. for a given triangle with vertices and edges, a gaussian is initialized using the mean position of the vertices, the direction of one edge, and the normal vector of the triangle. a process of gaussian densification helps to adjust to an appropriate number of gaussians, based on local scene complexity. this involves increasing or decreasing the number of gaussians in a given part of the scene and is achieved by either splitting gaussians into two if the view - space positional gradient is large, or cloning into two if it is small. to avoid density explosion, a pruning strategy removes points that have very low opacity, while maintaining at least one splat per triangle. the stability of the rendering process depends heavily on the accuracy of the binding between gaussian splats and flame [ 30 ] triangles. in contrast to alternative work, such as insta [ 65 ], where bounding volume hierarchy ( bvh ) based nearest triangle search [ 13 ] leads to flickering artifacts, gaussianavatar [ 39 ] is agnostic to tracked mesh inaccuracies due to back - propagation of a positional gradient for each triangle. this consistent binding between gaussians and the mesh triangles, regardless of pose or expression, allows fine - tuning of flame parameters. along with the optimization of gaussian splats parameters for position and scaling, flame parameters ( translation, pose, and expression ) were therefore also optimized during training. this plays a crucial role in stabilizing the rendering output, mitigating misalignment between the triangle meshes and gaussians", "token_count": 500}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 4, "frag_id": 1, "text": "##ta [ 65 ], where bounding volume hierarchy ( bvh ) based nearest triangle search [ 13 ] leads to flickering artifacts, gaussianavatar [ 39 ] is agnostic to tracked mesh inaccuracies due to back - propagation of a positional gradient for each triangle. this consistent binding between gaussians and the mesh triangles, regardless of pose or expression, allows fine - tuning of flame parameters. along with the optimization of gaussian splats parameters for position and scaling, flame parameters ( translation, pose, and expression ) were therefore also optimized during training. this plays a crucial role in stabilizing the rendering output, mitigating misalignment between the triangle meshes and gaussians. we leverage this gaussian - based head modeling and flame parameter tuning to help generate stable output. 3. 2. audio to facial motion ( audio2param ) we map from an audio signal to facial motion by leveraging the flame parametric 3d morphable model. flame disentangled parameters control identity, expression, and pose. these parameters can then be used to generate an explicit 3d head mesh. distinct from previous work [ 19, 53 ], which operates directly on full 3d head meshes by predicting triangle deformations or vertex positions, we take advantage of the disentangled flame representation. by directly predicting flame expression parameters, we reduce the complexity of our learning objective from explicitly predicting the spatial location of thousands of face vertices to the prediction of fewer than one hundred parameters that together define facial expressions and lip motion. we design a transformer - based architecture to capture long - range temporal information from the audio signal concerning the context of the spoken sentence. to mitigate the lack of diverse 3d audio - video datasets containing explicit visual data, 3d meshes and paired audio, we instantiate our encoder using wav2vec 2. 0 [ 5 ] which has previously demonstrated strong representation performance for audio information [ 19, 53 ]. we encode audio signals into 4", "token_count": 412}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 5, "frag_id": 0, "text": "method self - reenactment cross - reenactment fps↑ psnr↑ ssim↑ lpips↓ sync↑ stability↓ sync↑ stability↓ iplap [ 62 ] 29. 0412 0. 9462 0. 0340 3. 902 0. 6633 3. 324 0. 6856 3. 4 edtalk [ 45 ] 26. 9461 0. 8626 0. 0486 7. 144 0. 7802 6. 982 0. 7931 17. 2 ditto [ 31 ] 21. 0595 0. 7412 0. 1284 7. 023 0. 9245 6. 844 0. 9618 24. 2 mimictalk [ 56 ] 23. 8775 0. 8092 0. 0735 5. 446 0. 8824 5. 286 0. 9227 12. 1 gaussiantalker [ 9 ] 27. 6079 0. 9352 0. 0451 5. 346 1. 7622 5. 042 1. 8745 59. 6 talkinggaussian [ 29 ] 27. 3053 0. 9335 0. 0342 6. 422 1. 7183 6. 146 1. 8803 72. 2 gaussianheadtalk 29. 1233 0. 9477 0. 0338 6. 528 0. 6201 6. 122 0. 6836 45. 4 table 1. self - reenactment and cross - reenactment experimental settings. our method achieves strong results in terms of stability, realism, image quality and remains competitive for lip - sync. feature vectors by adding a linear projection layer after the encoder. similar to [ 19 ], we use a periodic positional encoding ( ppe ) to provide temporal information to the transformer decoder and a binary alignment mask to avoid information leakage from future frames. for a single identity m, let the input training set be given by l = { a, m 0 : t gt, nm }, where m 0 : t gt is a sequence of ground - truth meshes for t + 1 frames, a is an audio signal from the ground - truth video that corresponds to those frames. the neutral template mesh nm represents the given identity. each input training set is generated by processing an input video consisting of t + 1 frames using the vhap tracker [ 39 ] to generate ground truth meshes m 0", "token_count": 500}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 5, "frag_id": 1, "text": "to [ 19 ], we use a periodic positional encoding ( ppe ) to provide temporal information to the transformer decoder and a binary alignment mask to avoid information leakage from future frames. for a single identity m, let the input training set be given by l = { a, m 0 : t gt, nm }, where m 0 : t gt is a sequence of ground - truth meshes for t + 1 frames, a is an audio signal from the ground - truth video that corresponds to those frames. the neutral template mesh nm represents the given identity. each input training set is generated by processing an input video consisting of t + 1 frames using the vhap tracker [ 39 ] to generate ground truth meshes m 0 : t gt and neutral template mesh nm. our objective is to predict a sequence of meshes m 0 : t pred, given audio and neutral template mesh, such that : f _ { \\ th e t a } ( a, n _ m ) = m ^ { 0 : t } _ { pred } \\ approx m ^ { 0 : t } _ { gt } \\ label { eq : objective }. ( 4 ) the correlation between the audio signal and lip movement is typically high, but the correlation between the audio signal and head movement is not [ 57 ]. since different yet plausible head motions and expressions exist for the same speech, there is no one - to - one mapping between speech and head motion. hence we focus on predicting accurate lip movement from audio, by checking for high correlation between audio and lip movement, and transfer head motion directly from the original tracked video sequence. in addition to the head motion, the ( potentially independent ) audio speech signal a is processed through the transformer encoder and linear projection layer. we denote the output from the linear projection layer for t + 1 frames as c0 : t. for a given frame t, the transformer encoder takes audio for frames [ 0,..., t ] and uses the linear projection layer to generate ct. the predicted audio features are passed to the multi - head attention block of the transformer to obtain the latent vertex offsets o0 : t v for each frame. to utilize latent vertex offsets, an identity - specific template mesh, which is an average of all meshes obtained through video tracking, is encoded through a style encoder network to obtain an identity embedding s. this procedure is", "token_count": 500}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 5, "frag_id": 2, "text": "through the transformer encoder and linear projection layer. we denote the output from the linear projection layer for t + 1 frames as c0 : t. for a given frame t, the transformer encoder takes audio for frames [ 0,..., t ] and uses the linear projection layer to generate ct. the predicted audio features are passed to the multi - head attention block of the transformer to obtain the latent vertex offsets o0 : t v for each frame. to utilize latent vertex offsets, an identity - specific template mesh, which is an average of all meshes obtained through video tracking, is encoded through a style encoder network to obtain an identity embedding s. this procedure is shown in fig. 2. predicted latent vertex offsets oi v for frame i are linearly combined with this embedding as : o _ { s v } ^ i = s + o _ v ^ i, \\ quad i \\ in \\ { 0, \\ ldots, t \\ } \\ label { eq : mesh _ predicted }. ( 5 ) the style - conditioned latent embeddings o0 : t sv are then processed by a motion decoder, which comprises a set of linear layers that map them to a low - dimensional flame parameter space, to obtain a 3d mesh representation. by performing this process for each frame i, we obtain a predicted mesh sequence m 0 : t pred. toward achieving accurate lip motion and jaw movement prediction, we isolate the flame parameters responsible for jaw movement. we use these to augment the groundtruth mesh as mgt ′ and calculate a loss as the difference, in vertex space, between this and our predicted mesh per frame. the remaining flame parameter values, used to define the ground - truth mesh, are instantiated using the template mesh. the model is trained end - to - end using an l2 loss between the ground - truth and predicted meshes in vertex space as follows \\ ma t h c al { l } _ { m esh } = \\ s u m _ { n = 1 } ^ { n } \\ left ( \\ sum _ { t = 1 } ^ { t } \\ left \\ | m _ { gt ' } ^ t - m _ { pred } ^ t \\ right \\ | _ 2 \\ right ) \\ label { eq : vertex _ loss }. ( 6 ) during inference, the audio2", "token_count": 500}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 5, "frag_id": 3, "text": "mesh per frame. the remaining flame parameter values, used to define the ground - truth mesh, are instantiated using the template mesh. the model is trained end - to - end using an l2 loss between the ground - truth and predicted meshes in vertex space as follows \\ ma t h c al { l } _ { m esh } = \\ s u m _ { n = 1 } ^ { n } \\ left ( \\ sum _ { t = 1 } ^ { t } \\ left \\ | m _ { gt ' } ^ t - m _ { pred } ^ t \\ right \\ | _ 2 \\ right ) \\ label { eq : vertex _ loss }. ( 6 ) during inference, the audio2param component ingests a neutral mesh and audio signal in order to predict a sequence of animated 3d facial meshes using the flame parameter space. the predicted flame parameters are used to drive the motion of a person - specific avatar [ 39 ], culminating in the generation of an audio driven talking head. 3. 3. quantifying temporal consistency noted existing work generate talking head videos by posing video rendering as a set of, per - frame, independent tasks. we observe that this typically leads to a lack of temporal consistency in the output, which manifests itself as unnatural wobbling, aberrations, and facial oscillations. toward quantifying this problem, we employ a measure of temporal smoothness for a given video. 5", "token_count": 300}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 6, "frag_id": 0, "text": "figure 3. facial “ wobbling ” artifacts can be visualized via the absolute difference between generated and ground - truth frames, in image - space. the temporal gap between consecutive columns is ten frames in each case. our method exhibits smaller and spatiallymore - stable disparities, across time. figure 4. self - reenactment results : we show qualitative results between the gaussian based methods by reenacting them using the original audio. our method, gaussianheadtalk, shows better mouth movement, sharper teeth and fewer artifacts. we first select a video and accompanying audio sample from the dataset [ 61 ] and proceed to render a talking head video using the original audio signal. this enables a direct comparison between the generated video and the original video ( ground - truth ). towards defining a robust evaluation protocol, we detect and track facial key points [ 64 ] on the nose, as these key points are largely unaffected by jaw movement and expression changes. the time - domain signal provided, by these key points, can then be compared between generated and ground truth video frames. further, we observe that high - frequency wobbling and rapid oscillations are challenging to detect using keypoint comparisons alone, and thus adopt a hybrid approach by additionally performing a fast - fourier - transform ( fft ) analysis to identify frequent and uneven oscillations. our hybrid approach takes an average of mean motion difference md, variability in motion magnitude vm, and high - frequency power hf. each term is normalized by their respective maximum values across a given sequence of input frames. our compound stability score is then calculated by taking the average of these values : \\ text { stabi l it y s c or e } = \\ frac { m _ { d } + v _ { m } + h _ { f } } { 3 } \\ label { eq : weighted _ sum }. ( 7 ) 4. experiments 4. 1. experimental settings dataset : we perform experiments on two datasets : vocaset [ 14 ] and hdtf [ 61 ]. both datasets provide videos and synchronized audio. vocaset also provides tracked 3d - scans of the faces. we use vocaset for pre - training the audio2param component, and hdtf for training and evaluating person - specific gaussian avatars. since the gaussian splatting and nerf - based models require subject specific training,", "token_count": 500}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 6, "frag_id": 1, "text": "e } = \\ frac { m _ { d } + v _ { m } + h _ { f } } { 3 } \\ label { eq : weighted _ sum }. ( 7 ) 4. experiments 4. 1. experimental settings dataset : we perform experiments on two datasets : vocaset [ 14 ] and hdtf [ 61 ]. both datasets provide videos and synchronized audio. vocaset also provides tracked 3d - scans of the faces. we use vocaset for pre - training the audio2param component, and hdtf for training and evaluating person - specific gaussian avatars. since the gaussian splatting and nerf - based models require subject specific training, we select ten subjects from hdtf that cover a diverse set of identities and have a minimum of four minutes in video length. all videos were converted to 25fps to maintain experimental consistency. we synthetically generate 15 audio clips covering five different languages, with an average duration of seven seconds, using a text - to - speech model1. comparison baselines : we compare gaussianheadtalk with current state - of - the - art methods. two approaches, gaussiantalker [ 9 ] and talkinggaussian [ 29 ], naturally align with our proposed problem setting as both can be considered audio - driven gaussian methods. we also compare with ip lap [ 62 ] and ed talk [ 45 ] which are gan based methods, and ditto [ 31 ] which is a diffusion based method. lastly, we use mimictalk [ 56 ] to evaluate performance of a related nerf technique. implementation details : our method is built on pytorch. we first used vocaset [ 14 ] audio - video and their tracked flame [ 30 ] parameters. we train our audio2param component using adam [ 28 ] optimizer with a learning rate of 1e−4. we pre - train for 50000 steps and all experiments were performed on a single nvidia tesla a100 gpu ( 40gb ). 4. 2. quantitative evaluation we evaluate the performance of our model on two tasks : self - reenactment and cross - reenactment. first, for selfreenactment, we extract the first 30 seconds of a video as a test set. we train on the remaining part of the 1https : / / elevenlabs. io / 6", "token_count": 497}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 7, "frag_id": 0, "text": "figure 5. cross - reenactment results : visual reenactment using various methods with audio from a different speaker. the top row shows the words from the audio, where red text highlights exact phonemes. gaussianheadtalk can provide improved lip movement for these audio samples where other methods struggle with lip motion. video segment. for cross - reenactment, we use synthetically generated audio from a text - to - speech model so that the audio sample contains no information about the trained person identity. we compare our method with state - ofthe - art gaussian splatting [ 9, 29 ], gan [ 45, 62 ], diffusion [ 31 ] and nerf [ 56 ] based methods. to evaluate self - reenactment, we use peak signal - to - noise ratio ( psnr ), structural similarity index measure ( ssim ), and learned perceptual image patch similarity ( lpips ). we calculate the sync confidence score [ 11, 12 ] for both selfreenactment and cross - reenactment. we observe that our method predominantly improves upon the state - of - the - art ( ref. table 1 ). for the perceptual metric gaussianheadtalk performs better than nerf and alternative gaussian based methods. iplap [ 62 ] provides results comparable to ours, as it models only the lip region using a gan - based architecture. however, inference is somewhat slower, which may impede its real - time applications ( ref. table 1 ). 4. 3. qualitative evaluation we show visual results in figure 4 and figure 5 for qualitative comparison. gan based methods provide good lipsync in both cases, but their image quality falls short ; with generated videos of resolution up to 256 × 256. gaussianbased methods ( gaussiantalker, talkinggaussian ) generate sharper images, but their lip sync scores are low. as example, they show lower lip openness while speaking ‘ hey ’, ( see middle column of fig. 5 ). they also display wobbling artifacts in the generated videos, mainly due to the lack of long - term temporal information and improper tracking of 3d parameters during training. our method generates stable talking head videos, with qualitative results that concur with the relative quantitative metric improvements. we provide supplementary videos for further results visualization. 4. 4. user study we conduct", "token_count": 500}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 7, "frag_id": 1, "text": "both cases, but their image quality falls short ; with generated videos of resolution up to 256 × 256. gaussianbased methods ( gaussiantalker, talkinggaussian ) generate sharper images, but their lip sync scores are low. as example, they show lower lip openness while speaking ‘ hey ’, ( see middle column of fig. 5 ). they also display wobbling artifacts in the generated videos, mainly due to the lack of long - term temporal information and improper tracking of 3d parameters during training. our method generates stable talking head videos, with qualitative results that concur with the relative quantitative metric improvements. we provide supplementary videos for further results visualization. 4. 4. user study we conduct a user study to investigate how generated video quality is perceived by humans. we select a group of thirty 7", "token_count": 171}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 8, "frag_id": 0, "text": "individuals and present each survey participant with multiple video triplets. our study compares the performance of gaussiantalker [ 9 ], talkinggausian [ 29 ] and our work. participants were asked to evaluate videos in terms of “ naturalness ” ( i. e. assess wobbling and artifacts ), lip sync quality, and image quality ( i. e. evaluate identity preservation in generated videos ). generated video orderings were randomized and models responsible for generation were masked from participants. each participant was shown ten sets of video triplets, each with an average duration of five seconds. participants were asked to provide an ordinal ranking for each triplet, for each assessed aspect : ‘ best ’, ‘ average ’, and ‘ worst ’, which we numerically map to values 3, 2, and 1, respectively. for each participant, we sum the ratings a method received across the 10 triplets, and then divided this sum by 3 to normalize the score into a range of 3. 3 – 10. the final reported scores, for each method, are average normalized scores across all thirty participants ( table 2 ). method natural↑ lipsync↑ quality↑ gaussiantalker [ 9 ] 4. 0 5. 0 4. 0 talkinggaussian [ 29 ] 6. 2 7. 2 6. 5 gaussianheadtalk ( ours ) 9. 8 7. 8 9. 5 table 2. user study assessing human visual perception of generated video quality. the scores are averaged over different participants, with ten being the maximum. all participants ranked videos generated by our method as the most natural, which supports our improved video stability claims. in terms of ‘ lipsync ’, our method scores slightly above talkinggaussian, which correlates with the related ‘ sync ’ score ( table 1 ). performant lip sync quality may be explained by the special focus on the lip region, in both cases. image quality for our method can also show improved human rating scores, with respect to the compared state - of - the - art. 4. 5. ablation study we perform ablative studies to evaluate our methodological choices ( see table 3 ). we first explore the effect of using non - person - specific ( i. e. non - optimized ) flame parameters directly for rendering ( “ w / o parameter optimization ” ). resulting generated videos display artifacts around various parts of the face which arise due to inaccura", "token_count": 500}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 8, "frag_id": 1, "text": "slightly above talkinggaussian, which correlates with the related ‘ sync ’ score ( table 1 ). performant lip sync quality may be explained by the special focus on the lip region, in both cases. image quality for our method can also show improved human rating scores, with respect to the compared state - of - the - art. 4. 5. ablation study we perform ablative studies to evaluate our methodological choices ( see table 3 ). we first explore the effect of using non - person - specific ( i. e. non - optimized ) flame parameters directly for rendering ( “ w / o parameter optimization ” ). resulting generated videos display artifacts around various parts of the face which arise due to inaccuracies in parameter alignment, distorting the videos generated. we also tested the effect of more restrictive motion transfer ; namely, transferring only the lip motion ( flame model jaw parameters ) and keeping the head motion static ( “ w / o full motion transfer ” ). this strategy leads to artifacts around parts of the generated video, due to the movement interdependence between distinct flame parameters. we observe smoother video generation, with fewer artifacts, when we transfer the full set of flame parameters ( remaining pose and expression components ) from the original video. method psnr↑ sync↑ stability↓ w / o parameter optimization 27. 6987 6. 5123 1. 1432 w / o full motion transfer 23. 5621 6. 4962 0. 9154 gaussianheadtalk ( ours ) 29. 1233 6. 528 0. 6201 table 3. ablation study : removal of key method components results in qualitative visual degradations and respective decreases in associated metrics. 5. limitations and discussion our method can generate high - quality talking heads, but is restricted to exactly this part of the body and currently cannot render partial or full human bodies. this limitation arises due to our usage of a head - specific parametric model. extension to accommodate full - body reenactment might involve designing a gaussian splatting model that binds to smpl - x [ 35 ], or similar full - body 3d parametric models. we conjecture that a further interesting line of future work will involve exploring any benefits derivable from learning facial expression changes based on the tone and speed of the audio signal, or other external control parameters for changing the emotions of the face. 6. ethical consideration the generation", "token_count": 500}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 8, "frag_id": 2, "text": "associated metrics. 5. limitations and discussion our method can generate high - quality talking heads, but is restricted to exactly this part of the body and currently cannot render partial or full human bodies. this limitation arises due to our usage of a head - specific parametric model. extension to accommodate full - body reenactment might involve designing a gaussian splatting model that binds to smpl - x [ 35 ], or similar full - body 3d parametric models. we conjecture that a further interesting line of future work will involve exploring any benefits derivable from learning facial expression changes based on the tone and speed of the audio signal, or other external control parameters for changing the emotions of the face. 6. ethical consideration the generation of photo - realistic talking heads is a technology that carries potential risks of misuse, particularly in the creation of deepfakes for misinformation, harassment, or identity fraud. we advocate for the incorporation of robust watermarking and detection mechanisms to help distinguish synthetic content from real media and reduce the potential for harmful misuse. 7. conclusion we introduce a novel method for generating high - quality 3d talking heads with lip sync in real time. we proposed a temporally stable pipeline that uses transformers to capture sematic information and long - range dependencies from audio signals. we also introduce a stability metric to quantify perceptual wobbling in generated videos. our method offers strong performance with respect to the existing state - ofthe - art in terms of qualitative and quantitative benchmarks and we believe that high - quality real - time facial reenactment holds exciting potential for many practical and useful real - time applications. 8", "token_count": 339}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 9, "frag_id": 0, "text": "references [ 1 ] ayesha afridi, sumaiyah obaid, neha raheel, and farooq azam rathore. integrating artificial intelligence in stroke rehabilitation : current trends and future directions ; a mini review. jpma. the journal of the pakistan medical association, 75 ( 2 ) : 445 – 447, 2025. 1 [ 2 ] madhav agarwal, anchit gupta, rudrabha mukhopadhyay, vinay p namboodiri, and cv jawahar. compressing video calls using synthetic talking heads. in british machine vision conference ( bmvc ), 2022. 1 [ 3 ] madhav agarwal, rudrabha mukhopadhyay, vinay p. namboodiri, and c. v. jawahar. audio - visual face reenactment. in proceedings of the ieee / cvf winter conference on applications of computer vision ( wacv ), pages 5178 – 5187, 2023. 2 [ 4 ] shivangi aneja, artem sevastopolsky, tobias kirschstein, justus thies, angela dai, and matthias nießner. gaussianspeech : audio - driven gaussian avatars. arxiv preprint arxiv : 2411. 18675, 2024. 3 [ 5 ] alexei baevski, yuhao zhou, abdelrahman mohamed, and michael auli. wav2vec 2. 0 : a framework for self - supervised learning of speech representations. advances in neural information processing systems, 33 : 12449 – 12460, 2020. 4 [ 6 ] dan bigioi and peter corcoran. multilingual video dubbing — a technology review and current challenges. frontiers in signal processing, 3 : 1230755, 2023. 1 [ 7 ] bolin chen, jie chen, shiqi wang, and yan ye. generative face video coding techniques and standardization efforts : a review. in 2024 data compression conference ( dcc ), pages 103 – 112, 2024. 1 [ 8 ] zhiyuan chen, jiajiong cao, zhiquan chen, yuming li, and chenguang ma. echomimic : lifelike audio - driven portrait animations through editable landmark conditions. arxiv prep", "token_count": 500}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 9, "frag_id": 1, "text": ", 2020. 4 [ 6 ] dan bigioi and peter corcoran. multilingual video dubbing — a technology review and current challenges. frontiers in signal processing, 3 : 1230755, 2023. 1 [ 7 ] bolin chen, jie chen, shiqi wang, and yan ye. generative face video coding techniques and standardization efforts : a review. in 2024 data compression conference ( dcc ), pages 103 – 112, 2024. 1 [ 8 ] zhiyuan chen, jiajiong cao, zhiquan chen, yuming li, and chenguang ma. echomimic : lifelike audio - driven portrait animations through editable landmark conditions. arxiv preprint arxiv : 2407. 08136, 2024. 2 [ 9 ] kyusun cho, joungbin lee, heeji yoon, yeobin hong, jaehoon ko, sangjun ahn, and seungryong kim. gaussiantalker : real - time talking head synthesis with 3d gaussian splatting. in proceedings of the 32nd acm international conference on multimedia, page 10985 – 10994, 2024. 1, 2, 3, 5, 6, 7, 8 [ 10 ] xuangeng chu and tatsuya harada. generalizable and animatable gaussian head avatar. in the thirty - eighth annual conference on neural information processing systems, 2024. 2, 3 [ 11 ] joon son chung and andrew zisserman. out of time : automated lip sync in the wild. in computer vision – accv 2016 workshops : accv 2016 international workshops, taipei, taiwan, november 20 - 24, 2016, revised selected papers, part ii 13, pages 251 – 263. springer, 2017. 7 [ 12 ] joon son chung and andrew zisserman. lip reading in the wild. in computer vision – accv 2016 : 13th asian conference on computer vision, taipei, taiwan, november 2024, 2016, revised selected papers, part ii 13, pages 87 – 103. springer, 2017. 7 [ 13 ] james h clark. hierarchical geometric models for visible surface algorithms. communications of the acm, 19 ( 10 ) : 547 – 554, 1976. 4 [ 14 ] daniel cudeiro, timo bolkart, cassidy laidlaw, anurag ranjan, and michael", "token_count": 500}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 9, "frag_id": 2, "text": "taiwan, november 20 - 24, 2016, revised selected papers, part ii 13, pages 251 – 263. springer, 2017. 7 [ 12 ] joon son chung and andrew zisserman. lip reading in the wild. in computer vision – accv 2016 : 13th asian conference on computer vision, taipei, taiwan, november 2024, 2016, revised selected papers, part ii 13, pages 87 – 103. springer, 2017. 7 [ 13 ] james h clark. hierarchical geometric models for visible surface algorithms. communications of the acm, 19 ( 10 ) : 547 – 554, 1976. 4 [ 14 ] daniel cudeiro, timo bolkart, cassidy laidlaw, anurag ranjan, and michael black. capture, learning, and synthesis of 3d speaking styles. in proceedings ieee conf. on computer vision and pattern recognition ( cvpr ), pages 10101 – 10111, 2019. 3, 6 [ 15 ] yu deng, duomin wang, xiaohang ren, xingyu chen, and baoyuan wang. portrait4d : learning one - shot 4d head avatar synthesis using synthetic data. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 7119 – 7130, 2024. 3 [ 16 ] yu deng, duomin wang, and baoyuan wang. portrait4d - v2 : pseudo multi - view data creates better 4d head synthesizer. arxiv preprint arxiv : 2403. 13570, 2024. 3 [ 17 ] prafulla dhariwal and alexander nichol. diffusion models beat gans on image synthesis. advances in neural information processing systems, 34 : 8780 – 8794, 2021. 2 [ 18 ] nikita drobyshev, antoni bigata casademunt, konstantinos vougioukas, zoe landgraf, stavros petridis, and maja pantic. emoportraits : emotion - enhanced multimodal one - shot head avatars. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 8498 – 8507, 2024. 2 [ 19 ] yingruo fan, zhaojiang lin, jun saito, wenping wang, and taku komura. faceformer : speech - driven 3d facial animation with transformers. in proceedings of the ieee / cvf conference on computer vision and", "token_count": 500}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 9, "frag_id": 3, "text": "##94, 2021. 2 [ 18 ] nikita drobyshev, antoni bigata casademunt, konstantinos vougioukas, zoe landgraf, stavros petridis, and maja pantic. emoportraits : emotion - enhanced multimodal one - shot head avatars. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 8498 – 8507, 2024. 2 [ 19 ] yingruo fan, zhaojiang lin, jun saito, wenping wang, and taku komura. faceformer : speech - driven 3d facial animation with transformers. in proceedings of the ieee / cvf conference on computer vision and pattern recognition ( cvpr ), 2022. 2, 3, 4, 5 [ 20 ] guy gafni, justus thies, michael zollh¨ofer, and matthias nießner. dynamic neural radiance fields for monocular 4d facial avatar reconstruction. in proceedings of the ieee / cvf conference on computer vision and pattern recognition ( cvpr ), pages 8649 – 8658, 2021. 2, 3 [ 21 ] wilko guilluy, laurent oudre, and azeddine beghdadi. video stabilization : overview, challenges and perspectives. signal processing : image communication, 90 : 116015, 2021. 2 [ 22 ] jianzhu guo, dingyun zhang, xiaoqiang liu, zhizhou zhong, yuan zhang, pengfei wan, and di zhang. liveportrait : efficient portrait animation with stitching and retargeting control. arxiv preprint arxiv : 2407. 03168, 2024. 2 [ 23 ] yudong guo, keyu chen, sen liang, yongjin liu, hujun bao, and juyong zhang. ad - nerf : audio driven neural radiance fields for talking head synthesis. in ieee / cvf international conference on computer vision ( iccv ), 2021. 2, 3 [ 24 ] fa - ting hong, longhao zhang, li shen, and dan xu. depth - aware generative adversarial network for talking head video generation. in ieee / cvf conference on computer vision and pattern recognition ( cvpr ), 2022. 2 [ 25 ] xinya ji, hang zhou, kaisiyuan wang, wayne", "token_count": 500}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 9, "frag_id": 4, "text": ": 2407. 03168, 2024. 2 [ 23 ] yudong guo, keyu chen, sen liang, yongjin liu, hujun bao, and juyong zhang. ad - nerf : audio driven neural radiance fields for talking head synthesis. in ieee / cvf international conference on computer vision ( iccv ), 2021. 2, 3 [ 24 ] fa - ting hong, longhao zhang, li shen, and dan xu. depth - aware generative adversarial network for talking head video generation. in ieee / cvf conference on computer vision and pattern recognition ( cvpr ), 2022. 2 [ 25 ] xinya ji, hang zhou, kaisiyuan wang, wayne wu, chen change loy, xun cao, and feng xu. audio - driven emotional video portraits. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 14080 – 14089, 2021. 2 [ 26 ] bernhard kerbl, georgios kopanas, thomas leimk¨uhler, and george drettakis. 3d gaussian splatting for real - time radiance field rendering. acm trans. graph., 42 ( 4 ) : 139 – 1, 2023. 3 9", "token_count": 265}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 10, "frag_id": 0, "text": "[ 27 ] seyeon kim, siyoon jin, jihye park, kihong kim, jiyoung kim, jisu nam, and seungryong kim. moditalker : motiondisentangled diffusion model for high - fidelity talking head generation. in proceedings of the aaai conference on artificial intelligence, pages 4302 – 4310, 2025. 2 [ 28 ] diederik p kingma and jimmy ba. adam : a method for stochastic optimization. arxiv preprint arxiv : 1412. 6980, 2014. 6 [ 29 ] jiahe li, jiawei zhang, xiao bai, jin zheng, xin ning, jun zhou, and lin gu. talkinggaussian : structure - persistent 3d talking head synthesis via gaussian splatting. in european conference on computer vision, pages 127 – 145. springer, 2024. 1, 2, 3, 5, 6, 7, 8 [ 30 ] tianye li, timo bolkart, michael. j. black, hao li, and javier romero. learning a model of facial shape and expression from 4d scans. acm transactions on graphics, ( proc. siggraph asia ), 36 ( 6 ) : 194 : 1 – 194 : 17, 2017. 2, 3, 4, 6 [ 31 ] tianqi li, ruobing zheng, minghui yang, jingdong chen, and ming yang. ditto : motion - space diffusion for controllable realtime talking head synthesis. arxiv preprint arxiv : 2411. 19509, 2024. 2, 5, 6, 7 [ 32 ] zhan li, zhang chen, zhong li, and yi xu. spacetime gaussian feature splatting for real - time dynamic view synthesis. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 8508 – 8520, 2024. 2 [ 33 ] xian liu, yinghao xu, qianyi wu, hang zhou, wayne wu, and bolei zhou. semantic - aware implicit neural audiodriven video portrait generation. in european conference on computer vision, pages 106 – 125. springer, 2022. 3 [ 34 ] ben mildenhall, pratul p srinivasan, matthew tancik, jonathan t barron, ravi ramamoorthi, and ren ng. ne", "token_count": 500}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 10, "frag_id": 1, "text": ", zhang chen, zhong li, and yi xu. spacetime gaussian feature splatting for real - time dynamic view synthesis. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 8508 – 8520, 2024. 2 [ 33 ] xian liu, yinghao xu, qianyi wu, hang zhou, wayne wu, and bolei zhou. semantic - aware implicit neural audiodriven video portrait generation. in european conference on computer vision, pages 106 – 125. springer, 2022. 3 [ 34 ] ben mildenhall, pratul p srinivasan, matthew tancik, jonathan t barron, ravi ramamoorthi, and ren ng. nerf : representing scenes as neural radiance fields for view synthesis. communications of the acm, 65 ( 1 ) : 99 – 106, 2021. 3 [ 35 ] georgios pavlakos, vasileios choutas, nima ghorbani, timo bolkart, ahmed a. a. osman, dimitrios tzionas, and michael j. black. expressive body capture : 3d hands, face, and body from a single image. in proceedings ieee conf. on computer vision and pattern recognition ( cvpr ), pages 10975 – 10985, 2019. 8 [ 36 ] ziqiao peng, yihao luo, yue shi, hao xu, xiangyu zhu, hongyan liu, jun he, and zhaoxin fan. selftalk : a selfsupervised commutative training diagram to comprehend 3d talking faces. in proceedings of the 31st acm international conference on multimedia, pages 5292 – 5301, 2023. 2 [ 37 ] k r prajwal, rudrabha mukhopadhyay, vinay p. namboodiri, and c. v. jawahar. a lip sync expert is all you need for speech to lip generation in the wild. in proceedings of the 28th acm international conference on multimedia, page 484 – 492, new york, ny, usa, 2020. association for computing machinery. 2 [ 38 ] shenhan qian. vhap : versatile head alignment with adaptive appearance priors, 2024. 4 [ 39 ] shenhan qian, tobias kirschstein, liam schoneveld, davide davoli, simon gieben", "token_count": 500}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 10, "frag_id": 2, "text": "5301, 2023. 2 [ 37 ] k r prajwal, rudrabha mukhopadhyay, vinay p. namboodiri, and c. v. jawahar. a lip sync expert is all you need for speech to lip generation in the wild. in proceedings of the 28th acm international conference on multimedia, page 484 – 492, new york, ny, usa, 2020. association for computing machinery. 2 [ 38 ] shenhan qian. vhap : versatile head alignment with adaptive appearance priors, 2024. 4 [ 39 ] shenhan qian, tobias kirschstein, liam schoneveld, davide davoli, simon giebenhain, and matthias nießner. gaussianavatars : photorealistic head avatars with rigged 3d gaussians. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 20299 – 20309, 2024. 2, 3, 4, 5 [ 40 ] alexander richard, michael zollh¨ofer, yandong wen, fernando de la torre, and yaser sheikh. meshtalk : 3d face animation from speech using cross - modality disentanglement. in proceedings of the ieee / cvf international conference on computer vision ( iccv ), pages 1173 – 1182, 2021. 3 [ 41 ] marcos roberto e souza, helena de almeida maia, and helio pedrini. survey on digital video stabilization : concepts, methods, and challenges. acm computing surveys ( csur ), 55 ( 3 ) : 1 – 37, 2022. 2 [ 42 ] shuai shen, wanhua li, zheng zhu, yueqi duan, jie zhou, and jiwen lu. learning dynamic facial radiance fields for few - shot talking head synthesis. in european conference on computer vision, pages 666 – 682. springer, 2022. 3 [ 43 ] aliaksandr siarohin, st´ephane lathuili ` ere, sergey tulyakov, elisa ricci, and nicu sebe. first order motion model for image animation. in conference on neural information processing systems ( neurips ), 2019. 2 [ 44 ] wenfeng song, xuan wang, shi zheng, shuai li, aimin hao, and xia hou. talkingsty", "token_count": 500}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 10, "frag_id": 3, "text": "wanhua li, zheng zhu, yueqi duan, jie zhou, and jiwen lu. learning dynamic facial radiance fields for few - shot talking head synthesis. in european conference on computer vision, pages 666 – 682. springer, 2022. 3 [ 43 ] aliaksandr siarohin, st´ephane lathuili ` ere, sergey tulyakov, elisa ricci, and nicu sebe. first order motion model for image animation. in conference on neural information processing systems ( neurips ), 2019. 2 [ 44 ] wenfeng song, xuan wang, shi zheng, shuai li, aimin hao, and xia hou. talkingstyle : personalized speech - driven 3d facial animation with style preservation. ieee transactions on visualization and computer graphics, 2024. 2 [ 45 ] shuai tan, bin ji, mengxiao bi, and ye pan. edtalk : efficient disentanglement for emotional talking head synthesis. in european conference on computer vision, pages 398 – 416. springer, 2025. 2, 5, 6, 7 [ 46 ] balamurugan thambiraja, ikhsanul habibie, sadegh aliakbarian, darren cosker, christian theobalt, and justus thies. imitator : personalized speech - driven 3d facial animation. in proceedings of the ieee / cvf international conference on computer vision, pages 20621 – 20631, 2023. 2 [ 47 ] justus thies, mohamed elgharib, ayush tewari, christian theobalt, and matthias nießner. neural voice puppetry : audio - driven facial reenactment. in computer vision – eccv 2020 : 16th european conference, glasgow, uk, august 23 – 28, 2020, proceedings, part xvi 16, pages 716 – 731. springer, 2020. 2 [ 48 ] ashish vaswani, noam shazeer, niki parmar, jakob uszkoreit, llion jones, aidan n gomez, łukasz kaiser, and illia polosukhin. attention is all you need. advances in neural information processing systems, 30, 2017. 2 [ 49 ] jie wang, jiu - cheng xie, xianyan li, feng xu, chi - man pun, and ha", "token_count": 500}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 10, "frag_id": 4, "text": "neural voice puppetry : audio - driven facial reenactment. in computer vision – eccv 2020 : 16th european conference, glasgow, uk, august 23 – 28, 2020, proceedings, part xvi 16, pages 716 – 731. springer, 2020. 2 [ 48 ] ashish vaswani, noam shazeer, niki parmar, jakob uszkoreit, llion jones, aidan n gomez, łukasz kaiser, and illia polosukhin. attention is all you need. advances in neural information processing systems, 30, 2017. 2 [ 49 ] jie wang, jiu - cheng xie, xianyan li, feng xu, chi - man pun, and hao gao. gaussianhead : high - fidelity head avatars with learnable gaussian derivation. arxiv preprint arxiv : 2312. 01632, 2023. 3 [ 50 ] ting - chun wang, arun mallya, and ming - yu liu. one - shot free - view neural talking - head synthesis for video conferencing. in proceedings of the ieee conference on computer vision and pattern recognition, 2021. 2 [ 51 ] huawei wei, zejun yang, and zhisheng wang. aniportrait : audio - driven synthesis of photorealistic portrait animation. arxiv preprint arxiv : 2403. 17694, 2024. 2 [ 52 ] guanjun wu, taoran yi, jiemin fang, lingxi xie, xiaopeng zhang, wei wei, wenyu liu, qi tian, and xinggang wang. 4d gaussian splatting for real - time dynamic scene rendering. 10", "token_count": 360}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 11, "frag_id": 0, "text": "in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 20310 – 20320, 2024. 2 [ 53 ] jinbo xing, menghan xia, yuechen zhang, xiaodong cun, jue wang, and tien - tsin wong. codetalker : speech - driven 3d facial animation with discrete motion prior. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 12780 – 12790, 2023. 2, 3, 4 [ 54 ] mingwang xu, hui li, qingkun su, hanlin shang, liwei zhang, ce liu, jingdong wang, yao yao, and siyu zhu. hallo : hierarchical audio - driven visual synthesis for portrait image animation. arxiv preprint arxiv : 2406. 08801, 2024. 2 [ 55 ] xinkai yan, jieting xu, yuchi huo, and hujun bao. neural rendering and its hardware acceleration : a review. arxiv preprint arxiv : 2402. 00028, 2024. 3 [ 56 ] zhenhui ye, tianyun zhong, yi ren, ziyue jiang, jiawei huang, rongjie huang, jinglin liu, jinzheng he, chen zhang, zehan wang, et al. mimictalk : mimicking a personalized and expressive 3d talking face in minutes. advances in neural information processing systems, 37 : 1829 – 1853, 2024. 5, 6, 7 [ 57 ] hani yehia, philip rubin, and eric vatikiotis - bateson. quantitative association of vocal - tract and facial behavior. speech communication, 26 ( 1 - 2 ) : 23 – 43, 1998. 5 [ 58 ] hongyun yu, zhan qu, qihang yu, jianchuan chen, zhonghua jiang, zhiwen chen, shengyu zhang, jimin xu, fei wu, chengfei lv, et al. gaussiantalker : speaker - specific talking head synthesis via 3d gaussian splatting. in proceedings of the 32nd acm international conference on multimedia, pages 3548 – 3557, 2024. 3 [ 59 ] lichao zhang, jia yu, shuai zhang, long li, yangyang zhong, guanbao liang, yuming yan, qing ma", "token_count": 500}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 11, "frag_id": 1, "text": "facial behavior. speech communication, 26 ( 1 - 2 ) : 23 – 43, 1998. 5 [ 58 ] hongyun yu, zhan qu, qihang yu, jianchuan chen, zhonghua jiang, zhiwen chen, shengyu zhang, jimin xu, fei wu, chengfei lv, et al. gaussiantalker : speaker - specific talking head synthesis via 3d gaussian splatting. in proceedings of the 32nd acm international conference on multimedia, pages 3548 – 3557, 2024. 3 [ 59 ] lichao zhang, jia yu, shuai zhang, long li, yangyang zhong, guanbao liang, yuming yan, qing ma, fangsheng weng, fayu pan, jing li, renjun xu, and zhenzhong lan. unveiling the impact of multi - modal interactions on user engagement : a comprehensive evaluation in ai - driven conversations, 2024. 1 [ 60 ] yue zhang, minhao liu, zhaokang chen, bin wu, yubin zeng, chao zhan, yingjie he, junxin huang, and wenjiang zhou. musetalk : real - time high quality lip synchorization with latent space inpainting. arxiv, 2024. 2 [ 61 ] zhimeng zhang, lincheng li, yu ding, and changjie fan. flow - guided one - shot talking face generation with a high - resolution audio - visual dataset. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 3661 – 3670, 2021. 2, 6 [ 62 ] weizhi zhong, chaowei fang, yinqi cai, pengxu wei, gangming zhao, liang lin, and guanbin li. identitypreserving talking face generation with landmark and appearance priors. in proceedings of the ieee / cvf conference on computer vision and pattern recognition ( cvpr ), pages 9729 – 9738, 2023. 5, 6, 7 [ 63 ] yang zhou, xintong han, eli shechtman, jose echevarria, evangelos kalogerakis, and dingzeyu li. makelttalk : speaker - aware talking - head animation. acm transactions on graphics ( tog ), 39 ( 6 ) : 1 – 15, 2020. 2 [ 64 ] zhenglin", "token_count": 500}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 11, "frag_id": 2, "text": ", chaowei fang, yinqi cai, pengxu wei, gangming zhao, liang lin, and guanbin li. identitypreserving talking face generation with landmark and appearance priors. in proceedings of the ieee / cvf conference on computer vision and pattern recognition ( cvpr ), pages 9729 – 9738, 2023. 5, 6, 7 [ 63 ] yang zhou, xintong han, eli shechtman, jose echevarria, evangelos kalogerakis, and dingzeyu li. makelttalk : speaker - aware talking - head animation. acm transactions on graphics ( tog ), 39 ( 6 ) : 1 – 15, 2020. 2 [ 64 ] zhenglin zhou, huaxia li, hong liu, nanyang wang, gang yu, and rongrong ji. star loss : reducing semantic ambiguity in facial landmark detection. in proceedings of the ieee / cvf conference on computer vision and pattern recognition ( cvpr ), pages 15475 – 15484, 2023. 6 [ 65 ] wojciech zielonka, timo bolkart, and justus thies. instant volumetric head avatars. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 4574 – 4584, 2023. 4 11", "token_count": 276}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 12, "frag_id": 0, "text": "gaussianheadtalk : wobble - free 3d talking heads with audio driven gaussian splatting supplementary material figure a. for a given audio signal, gaussianheadtalk generates a lip - sync 3d mesh and use the generated flame parameters to transfer lip motion on a trained gaussianavatar with optimized flame parameters. a. qualitative ablation study figure b. ablation study : effect of transferring the lip motion and keeping other parameters static ( w / o full motion transfer ). the results shows visible artifacts in the generated avatar, as the flame parameters are not fully independent. figure c. ablation study : using non - optimized flame parameters ( w / o parameter optimization ). this leads to artifacts around the torso region, and wobbling issues. b. temporal analysis of keypoints figure d. comparison of keypoint movement across time between an original video and a video generated using gaussiantalker. the overlay graph shows that there is flickering in the rendered video. in ideal case, these two graphs should be perfectly overlapping. we report x - axis, y - axis motion magnitude over time in upper and lower plots, respectively. c. user study 1", "token_count": 245}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 13, "frag_id": 0, "text": "category method final score ‘ best ’ ( 3 ) ‘ average ’ ( 2 ) ‘ worst ’ ( 1 ) total ratings naturalness gaussianheadtalk ( ours ) 9. 8 284 14 2 300 talkinggaussian [ 29 ] 6. 2 40 178 82 300 gaussiantalker [ 9 ] 4. 0 5 50 245 300 lipsync gaussianheadtalk ( ours ) 7. 8 142 118 40 300 talkinggaussian [ 29 ] 7. 2 128 92 80 300 gaussiantalker [ 9 ] 5. 0 25 100 175 300 quality gaussianheadtalk ( ours ) 9. 5 260 35 5 300 talkinggaussian [ 29 ] 6. 5 80 125 95 300 gaussiantalker [ 9 ] 4. 0 1 58 241 300 table a. detailed breakdown of user study ratings. 30 participants evaluate 10 videos of each method, generating 300 ratings in total. for each triplet, a participant assigns the ranks 1, 2, and 3 once each, so the raw sum across the three methods is 1 + 2 + 3 = 6. over 10 triplets, this gives a total of 10×6 = 60. after dividing each method ’ s total by 3 for normalization, the overall sum across all methods is fixed at 60 / 3 = 20. figure e. visualization of frame stability through color channel overlay with ground - truth video over 10 consecutive frames. the significant displacement ( wobbling ) observed in the gaussiantalker and talkinggaussian methods contrasts with the high overlap and stability achieved by our proposed method. green and red channels highlight the differences within the blue dashed boxes. 2", "token_count": 338}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 14, "frag_id": 0, "text": "figure f. cross - reenactment results : we show the visual results by reenacting various methods using a different audio, from a different speaker. the top row shows the word from the audio, with red part highlighting the exact phoneme. gaussianheadtalk provides the best possible lip movement for these new audio samples. other methods struggle to have proper lip motion, generate high - quality videos and no artifacts. 3", "token_count": 87}
{"doc_id": "arxiv_251210939_gaussianHeadTalk", "page": 15, "frag_id": 0, "text": "figure g. cross - reenactment results : we show the visual results by reenacting various methods using a different audio, from a different speaker. the top row shows the word from the audio, with red part highlighting the exact phoneme. gaussianheadtalk provides the best possible lip movement for these new audio samples. other methods struggle to have proper lip motion, generate high - quality videos and no artifacts. 4", "token_count": 87}
