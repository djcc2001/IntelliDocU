{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 1, "frag_id": 0, "text": "bidirectional normalizing flow : from data to noise and back yiyang lu1, 2, ∗, †, ‡ qiao sun1, ∗, † xianbang wang1, ∗zhicheng jiang1 hanhong zhao1 kaiming he1 ∗equal technical contribution † project lead 1mit 2tsinghua university abstract normalizing flows ( nfs ) have been established as a principled framework for generative modeling. standard nfs consist of a forward process and a reverse process : the forward process maps data to noise, while the reverse process generates samples by inverting it. typical nf forward transformations are constrained by explicit invertibility, ensuring that the reverse process can serve as their exact analytic inverse. recent developments in tarflow and its variants have revitalized nf methods by combining transformers and autoregressive flows, but have also exposed causal decoding as a major bottleneck. in this work, we introduce bidirectional normalizing flow ( biflow ), a framework that removes the need for an exact analytic inverse. biflow learns a reverse model that approximates the underlying noise - todata inverse mapping, enabling more flexible loss functions and architectures. experiments on imagenet demonstrate that biflow, compared to its causal decoding counterpart, improves generation quality while accelerating sampling by up to two orders of magnitude. biflow yields state - ofthe - art results among nf - based methods and competitive performance among single - evaluation ( “ 1 - nfe ” ) methods. following recent encouraging progress on nfs, we hope our work will draw further attention to this classical paradigm. 1. introduction normalizing flows ( nfs ) are a long - standing family of generative models [ 45, 10, 30 ]. they contain two processes : a forward process that learns to transform data into noise, and a reverse process that generates samples by inverting this transformation. a notable property of nfs is that the underlying flow trajectories from data to noise are learned rather than imposed. this differs from their modern continuous - time counterparts [ 7 ], such as flow matching ( fm ) [ 36, 37, 1 ], whose ground - truth trajectories are predetermined via time - scheduling. however, this advantage of nfs comes at the cost of increased learning difficulty, typically leading to more demanding constraints on forward architectures and objective formulations. ‡ work done as an intern at mit. ( a", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 1, "frag_id": 1, "text": "45, 10, 30 ]. they contain two processes : a forward process that learns to transform data into noise, and a reverse process that generates samples by inverting this transformation. a notable property of nfs is that the underlying flow trajectories from data to noise are learned rather than imposed. this differs from their modern continuous - time counterparts [ 7 ], such as flow matching ( fm ) [ 36, 37, 1 ], whose ground - truth trajectories are predetermined via time - scheduling. however, this advantage of nfs comes at the cost of increased learning difficulty, typically leading to more demanding constraints on forward architectures and objective formulations. ‡ work done as an intern at mit. ( a ) standard normalizing flow : explicit inverse. ( b ) bidirectional normalizing flow : learned inverse. figure 1. conceptual comparison between standard normalizing flows and our proposed bidirectional normalizing flow ( biflow ). instead of constraining the forward model f to be explicitly invertible and using its exact analytic inverse for generation, biflow introduces a learnable reverse model g that approximates this inverse through our hidden alignment objective. this design frees biflow from architectural constraints and enables flexible loss design, allowing for efficient generation with improved quality in a single forward pass. the standard nf paradigm [ 45, 10 ] requires the reverse process to be the exact analytic inverse of the forward process ( fig. 1a ). this requirement restricts the range of forward model architectures that can be employed, as the model must be explicitly invertible and its jacobian determinant must be computable, tractable, and differentiable. existing work on nfs [ 45, 10, 57, 30, 41, 29 ] have largely focused on designing compound forward functions that satisfy these requirements. despite these diverse attempts, nf - based methods remain limited in their ability to use powerful, general - purpose architectures ( e. g., u - nets [ 47 ] or vision transformers [ 12 ] ), in contrast to many modern generative model families. recently, the gap between nfs and other generative models has been largely closed by tarflow [ 65 ] and its extensions [ 21 ]. tarflow has effectively integrated transformers [ 58 ] with autoregressive flows [ 30, 41 ] into the nf paradigm. this design allows nf methods to benefit from the powerful transformers, substantially", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 1, "frag_id": 2, "text": "[ 45, 10, 57, 30, 41, 29 ] have largely focused on designing compound forward functions that satisfy these requirements. despite these diverse attempts, nf - based methods remain limited in their ability to use powerful, general - purpose architectures ( e. g., u - nets [ 47 ] or vision transformers [ 12 ] ), in contrast to many modern generative model families. recently, the gap between nfs and other generative models has been largely closed by tarflow [ 65 ] and its extensions [ 21 ]. tarflow has effectively integrated transformers [ 58 ] with autoregressive flows [ 30, 41 ] into the nf paradigm. this design allows nf methods to benefit from the powerful transformers, substantially mitigating a major limitation of traditional nfs. however, to maintain computable and tractable jacobian determinants, tarflow decomposes the forward process into a long chain ( e. g., thousands of steps ) 1 arxiv : 2512. 10953v1 [ cs. lg ] 11 dec 2025", "token_count": 221}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 2, "frag_id": 0, "text": "0. 001 0. 01 0. 1 0. 5 reverse process time ( s, log - scale ) 0 2 4 6 8 fid - 50k ( w / cfg ) 2. 39 4. 54 6. 83 2. 1 points better 697× faster 4. 4 points better 224× faster improved tarflow biflow figure 2. biflow surpasses our improved tarflow baseline by a wide margin in generation quality, despite using a base - size model versus an extra - large model, and it achieves markedly faster sampling as well. the x - axis denotes the wall - clock time ( log scale ) for generating one image on 8 v4 tpu cores. vae decoding is omitted from this figure ; comprehensive inference cost comparison appears in tab. 3. of autoregressive operations. the resulting explicit inverse therefore requires a large number of causal steps at inference time, which is difficult to parallelize. this design not only slows down sampling, but also retains the undesirable architectural constraints during inference, e. g., the reverse model cannot perform feedforward, non - causal attention. in this work, we introduce bidirectional normalizing flow ( biflow ), a framework in which both the forward and reverse processes are learned. in our framework, the designs of the forward and reverse processes are decoupled : the forward process can be any nf model fθ that is computable, tractable, and easy to learn ( e. g., an improved tarflow ), while the reverse process learns a separate model [UNK] to approximate its inverse ( fig. 1b ). in contrast to the explicit inverse, our reverse model is highly flexible : it can be a feedforward, non - causal transformer that is both expressive and efficient to run, naturally enabling high - quality, single function evaluation ( 1 - nfe ) generation. learning the reverse model [UNK] is not merely a form of distillation, even though we use a pre - trained forward model fθ : in fact, our learned reverse model [UNK] can outperform the explicit inverse of fθ. compared to distilling the noise - to - data trajectories, we find that aligning the intermediate hidden states yields results even better than the explicit inverse. in addition, our learnable reverse model can naturally eliminate the extra step of score - based denoising in tarflow, simplifying and accelerating inference", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 2, "frag_id": 1, "text": "can be a feedforward, non - causal transformer that is both expressive and efficient to run, naturally enabling high - quality, single function evaluation ( 1 - nfe ) generation. learning the reverse model [UNK] is not merely a form of distillation, even though we use a pre - trained forward model fθ : in fact, our learned reverse model [UNK] can outperform the explicit inverse of fθ. compared to distilling the noise - to - data trajectories, we find that aligning the intermediate hidden states yields results even better than the explicit inverse. in addition, our learnable reverse model can naturally eliminate the extra step of score - based denoising in tarflow, simplifying and accelerating inference while improving quality. such a “ what - you - see - is - what - you - get ” property further enables the use of perceptual loss [ 68 ], which is impossible or difficult to leverage with an explicit inverse. putting these factors together, our learned reverse model can substantially outperform its explicit - inverse counterpart. we report competitive results on the imagenet 256×256 generation. comparing with an improved tarflow ( which is also the forward model for biflow ), biflow achieves an fid of 2. 39 using a dit - b size [ 42 ] model, while being two orders of magnitude faster ( see fig. 2 ; detailed in tab. 3 ). this not only sets a new state - of - the - art result among nfbased methods, but also represents a strong 1 - nfe result in comparison with other generative model families. following the progress established by tarflow and extensions, our work on biflow further unleashes the potential of nfs as a strong competitor among modern generative model families. our findings indicate that the nf principle of learning the forward trajectories, rather than pre - scheduling them, can be advantageous and need not introduce inferencetime limitations. considering that modern flow matching methods are continuous - time nfs with pre - scheduled trajectories, we hope our study will shed light on the potential synergy among these related methods. 2. related work normalizing flows. normalizing flows ( nfs ) have long served as a principled framework for probabilistic generative modeling. over the past decade, extensive research has focused on enhancing the expressivity and scalability of nfs under the constraint of in", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 2, "frag_id": 2, "text": "##leashes the potential of nfs as a strong competitor among modern generative model families. our findings indicate that the nf principle of learning the forward trajectories, rather than pre - scheduling them, can be advantageous and need not introduce inferencetime limitations. considering that modern flow matching methods are continuous - time nfs with pre - scheduled trajectories, we hope our study will shed light on the potential synergy among these related methods. 2. related work normalizing flows. normalizing flows ( nfs ) have long served as a principled framework for probabilistic generative modeling. over the past decade, extensive research has focused on enhancing the expressivity and scalability of nfs under the constraint of invertible transformations. planar flows [ 45 ] and nice [ 10 ] pioneered the use of simple reversible mappings to construct deep generative models. real nvp [ 11 ] and glow [ 29 ] extended this framework with nonvolume - preserving transformations and convolutional architectures. iaf [ 30 ] and maf [ 41 ] introduced autoregressive flows to improve expressivity while maintaining tractable likelihoods. tarflow [ 65 ] and starflow [ 21 ] further revitalized the nf family by incorporating transformer into autoregressive flows. they demonstrated significant gains in generation quality and scalability, reaffirming nfs as a competitive paradigm in modern generative modeling. despite these advances, standard nfs still inherit limitations from their invertibility requirement. in particular, autoregressive flow formulations impose strict causal ordering and sequential dependencies, which constrain architectural design and lead to slow inference. continuous normalizing flows. continuous normalizing flows ( cnfs ) [ 20, 14, 19 ] generalize discrete flows by modeling transformations as continuous - time dynamics governed by ordinary differential equations ( odes ) [ 7 ]. cnfs enable more flexible architectures and tractable likelihood computation via numerical ode simulations. fm [ 36, 37, 13 ] reformulates the explicit maximum - likelihood training objective into an equivalent implicit objective. diffusion models [ 25, 51, 9 ] can be interpreted as a special case of flow matching with stochastic dynamics, achieving impressive fidelity and scalability. despite their empirical success, the 2", "token_count": 474}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 3, "frag_id": 0, "text": "tarflow block tarflow block figure 3. illustration on the autoregressive inference process of tarflow. in each block, each token is transformed one by one, depending on previous tokens. this is repeated for a sequence with length 256 for a 32×32 input with patch size 2, and is further repeated for all blocks ( e. g., 8 blocks ). altogether, tarflow inference requires 8×256 sequential function evaluations. implicit formulation of fm and diffusion models sacrifices the learnable bidirectional mapping that characterizes nfs. 3. background : normalizing flows normalizing flows ( nfs ) are a class of generative models that establish a bijective transformation between a gaussian prior distribution p0 and a complex data distribution pdata. an nf consists of a forward process and a reverse process. given a data sample x ∈rd [UNK], the forward process f maps it into the gaussian prior space z = f ( x ). the model assigns the data likelihood p ( x ) through the changeof - variables formula. training is performed by optimizing f to maximize the log - likelihood log p ( x ) over data samples. classical nf requires the forward process f to be explicitly invertible for exact likelihood computation and efficient sampling. once trained, its exact inverse, f−1, can be used for generation by transforming gaussian noise back to the data space, i. e., x = f−1 ( z ) where z [UNK]. in practice, to enhance expressiveness, the forward process is commonly constructed as a composition of multiple simpler bijective transformations f : = fb−1 [UNK] · · · [UNK] [UNK] ( [UNK] function composition ). under this formulation, the log - likelihood objective becomes log p ( x ) = log p0 ( z ) + x i log det ∂fi ( xi ) ∂xi, ( 1 ) with x0 = x and xi + 1 = fi ( xi ). here, det ( · ) denotes the determinant operator. designing transformations that yield computable and differentiable determinant has been a key consideration in prior nf formulations. this requirement motivates specialized designs such as affine coupling [ 10, 11 ] and autoregressive flows [ 30, 41 ], which preserve tractable jacobians. importantly, while the log - determinant term in eq. ( 1 ) requires the forward process f", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 3, "frag_id": 1, "text": "under this formulation, the log - likelihood objective becomes log p ( x ) = log p0 ( z ) + x i log det ∂fi ( xi ) ∂xi, ( 1 ) with x0 = x and xi + 1 = fi ( xi ). here, det ( · ) denotes the determinant operator. designing transformations that yield computable and differentiable determinant has been a key consideration in prior nf formulations. this requirement motivates specialized designs such as affine coupling [ 10, 11 ] and autoregressive flows [ 30, 41 ], which preserve tractable jacobians. importantly, while the log - determinant term in eq. ( 1 ) requires the forward process f to be invertible, it does not necessitate an explicitly invertible formulation. the explicit inverse is only required at inference time, where we need to map samples from prior back to the data space. tarflow. tarflow [ 65 ] integrates transformer architectures into autoregressive flows ( af ), substantially improving their expressiveness and scalability. the core idea in af is to further decompose each sub - transformation fi, parameterized by a block, into t steps, where t denotes the sequence length of the input tokens. each step transforms the i - th token only conditioned on its predecessors, which can naturally be realized through transformer layers with causal masks. to capture bidirectional context, af flips the sequence order in alternating blocks. by combining expressive transformer architectures with autoregressive flows, tarflow successfully revives nf to remain competitive with today ’ s state - of - the - art generative models. however, af parameterization introduces asymmetry between training and sampling. similar to next - tokenprediction language models, although likelihood evaluation and training can be parallelized efficiently, sampling must proceed sequentially due to the autoregressive nature, as illustrated in fig. 3. in practice, this requires performing, e. g., thousands of ( 8×256 ) inverse transformations one after another, resulting in substantial inference latency. 4. bidirectional normalizing flow we propose a bidirectional normalizing flow ( biflow ) framework, which has : ( i ) a forward model fθ that transforms data samples into pure noise, and ( ii ) a learnable, separate reverse model [UNK] that approximate", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 3, "frag_id": 2, "text": ". however, af parameterization introduces asymmetry between training and sampling. similar to next - tokenprediction language models, although likelihood evaluation and training can be parallelized efficiently, sampling must proceed sequentially due to the autoregressive nature, as illustrated in fig. 3. in practice, this requires performing, e. g., thousands of ( 8×256 ) inverse transformations one after another, resulting in substantial inference latency. 4. bidirectional normalizing flow we propose a bidirectional normalizing flow ( biflow ) framework, which has : ( i ) a forward model fθ that transforms data samples into pure noise, and ( ii ) a learnable, separate reverse model [UNK] that approximates its inverse, mapping noise back to the data space. training is performed in two stages : first, similar to classical nf, we train the forward model using maximum likelihood estimation ; then, keeping the forward model fixed, we train the reverse model to approximate its inverse mapping. notably, our reverse model [UNK] is not constrained by explicit invertibility. as a result, this allows us to design the reverse model with arbitrary architectures ( e. g., bidirectional attention - based transformers ) and training objectives. next, we discuss the formulation, objectives, and learning dynamics of the reverse process. 4. 1. learning to approximate the inverse given a pre - trained forward model fθ, our goal is to optimize a reverse model [UNK] that approximates its inverse. we consider three strategies : ( i ) naive distillation ; ( ii ) hidden distillation ; ( iii ) hidden alignment, as approaches to learning the reverse model. fig. 4 illustrates the differences among these methods, as we describe next. naive distillation. a straightforward strategy is to impose a direct distillation loss : lnaive ( x ) = d x, x ′, 3", "token_count": 391}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 4, "frag_id": 0, "text": "( a ) naive distillation. ( b ) hidden distillation. ( c ) hidden alignment. figure 4. comparison of three approaches for learning the reverse process. each [UNK] a position where the model returns to the same dimension as input x. blue arrows with d refer to distance loss terms. our hidden alignment strategy ( fig. 4c ) combines the strengths of fig. 4a and fig. 4b, leveraging the entire trajectory for supervision without repeatedly returning to input space. where x is a data sample, x ′ = [UNK] ( fθ ( x ) ) is the reconstructed data, and d denotes a distance metric ( e. g., l2 distance ). the reverse model is trained to minimize the reconstruction error on data samples ( see fig. 4a ). this simple approach provides supervision only at the final output, which may be insufficient for effectively training the reverse model. directly mapping pure noise to data in one step is highly under - constrained, making it difficult for the reverse network to learn a reliable inverse from a single reconstruction loss. hidden distillation. a typical nf is composed of a sequence of simple sub - transformations, i. e., fθ = fb−1 [UNK] · · · [UNK] [UNK], where each fi is a transformation block and b denotes the total number of blocks. we can strengthen the training signal by leveraging the full sequence of intermediate states generated along the forward trajectory. as illustrated in fig. 4b, starting from x [UNK], the forward model produces a trajectory of intermediate hidden states { xi } with z = fθ ( x ) as the final output prior. analogously, we also design the reverse model to be composed of b blocks, generating a reverse trajectory { hi } from z. we distill the reverse model by enforcing the two trajectories to be close. formally, the loss is defined as : lhidden ( x ) = x i d xi, hi, where h0 corresponds to the reconstructed output x ′. optionally, each term can be assigned a distinct weighting factor. this formulation encourages the reverse model to invert each sub - transformation individually, which could help guide the reverse model to invert the mapping fθ step by step. the intermediate hidden states { xi } serve as auxiliary supervision for learning the correspondence between x and z. although this hidden distillation strategy provides more supervision than naive distillation, it introduces structural constraints on model design. since each intermediate", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 4, "frag_id": 1, "text": "} from z. we distill the reverse model by enforcing the two trajectories to be close. formally, the loss is defined as : lhidden ( x ) = x i d xi, hi, where h0 corresponds to the reconstructed output x ′. optionally, each term can be assigned a distinct weighting factor. this formulation encourages the reverse model to invert each sub - transformation individually, which could help guide the reverse model to invert the mapping fθ step by step. the intermediate hidden states { xi } serve as auxiliary supervision for learning the correspondence between x and z. although this hidden distillation strategy provides more supervision than naive distillation, it introduces structural constraints on model design. since each intermediate state xi has the same dimensionality as the input, the reverse model is forced to repeatedly project features down to the input space and then back up into the hidden space. this rigid requirement restricts architectural flexibility, ultimately limiting the model ’ s effectiveness. hidden alignment. we propose a more flexible strategy, termed hidden alignment. crucially, it leverages the full forward trajectory for supervision while relaxing the restrictive requirement in hidden distillation that intermediate hidden states must lie in the input space. as shown in fig. 4c, we extract intermediate hidden states { hi } from the reverse model [UNK]. unlike hidden distillation, which enforces each hi to directly match its input - space counterpart xi, we introduce a set of learnable projection heads { φi } to align the projected representations φi ( hi ) with the corresponding forward states xi. the training objective then becomes : lalign ( x ) = x i d xi, φi ( hi ), ( 2 ) where h0 = x ′ and φ0 is the identity mapping. this simple modification allows the reverse model to benefit from full trajectory supervision while maintaining architectural and representational flexibility. by decoupling the representation space from the input token space, hidden alignment avoids the potential semantic distortion caused by repeated projections. 4. 2. eliminating score - based denoising existing state - of - the - art nfs such as tarflow [ 65 ] deviate from standard flow - based modeling in that they learn a noise - perturbed distribution and then denoise the output. specifically, during training, tarflow takes a noiseperturbed input [UNK] = x + [UNK], where [UNK] [UNK] ( 0, i ), and during inference, tarflow first generates [UNK] = f", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 4, "frag_id": 2, "text": "x ′ and φ0 is the identity mapping. this simple modification allows the reverse model to benefit from full trajectory supervision while maintaining architectural and representational flexibility. by decoupling the representation space from the input token space, hidden alignment avoids the potential semantic distortion caused by repeated projections. 4. 2. eliminating score - based denoising existing state - of - the - art nfs such as tarflow [ 65 ] deviate from standard flow - based modeling in that they learn a noise - perturbed distribution and then denoise the output. specifically, during training, tarflow takes a noiseperturbed input [UNK] = x + [UNK], where [UNK] [UNK] ( 0, i ), and during inference, tarflow first generates [UNK] = f−1 θ ( z ), then performs an additional score - based denoising step : x [UNK] + σ2 [UNK] log p ( [UNK] ), ( 3 ) as illustrated in fig. 5a, where the score term is computed via a forward - backward pass. this post - processing almost doubles the inference cost, becoming a clear computational bottleneck for efficient generation. learned denoising. we eliminate the explicit score - based denoising step by integrating denoising directly into the reverse model. as illustrated in fig. 5b, we extend the forward trajectory from [UNK] to z by appending the clean data x at its start, and extend the reverse model with one additional block h0 →x ′ that learns denoising jointly with the inverse. the 4", "token_count": 305}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 5, "frag_id": 0, "text": "( a ) tarflow : explicit denoising. ( b ) biflow : learned denoising. figure 5. incorporating the denoising step into our hidden alignment framework. the reverse model is extended with an additional block dedicated to denoising. our learned denoising eliminates the need for calculating the score function through a whole forwardbackward pass, incurring only a single additional block forward. resulting reverse network, with one extra block for denoising, maps z to a clean sample x ′ in a single pass. as such, our reverse model directly learns the correspondence between z and the clean data x directly, rather than the noisy data [UNK]. the training process follows the same objective as eq. ( 2 ), with a reconstruction loss on ( x, x ′ ) and hidden alignment losses on intermediate states. by integrating denoising into the reverse process itself, biflow achieves a unified learned formulation for generation, where inverse and denoising are seamlessly coupled within a single direct generative model, eliminating the need for any extra refinement step. 4. 3. distance metric biflow provides a flexible supervised - learning framework for tackling the generation problem. this flexibility stems from two key properties of biflow : ( i ) 1 - nfe generation — the learned reverse model produces a sample x ′ in a single forward pass, so generated samples are directly accessible during training ; and ( ii ) explicit pairing — the forward process establishes a direct correspondence between data x and noise z, serving as training pairs for the reverse model. together, these properties realize a what - you - see - is - whatyou - get training regime : generated samples are available for immediate loss evaluation and backpropagation, enabling rich semantic supervision signals. our framework is highly flexible in the choice of loss functions : almost any distance metric can be used, and multiple metrics can be combined. our default choice for the distance metric d in eq. ( 2 ) is simply mean squared error ( mse ). to enhance realism, we further apply perceptual loss at the final vae - decoded image, while intermediate hidden states remain aligned by mse. in this work, we adopt both vgg [ 50 ] and convnext v2 [ 61 ] feature spaces for perceptual loss ( our implementation for vgg features follows lpips [ 68 ] ). as in prior work [ 16, 52, 17 ]", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 5, "frag_id": 1, "text": "##gation, enabling rich semantic supervision signals. our framework is highly flexible in the choice of loss functions : almost any distance metric can be used, and multiple metrics can be combined. our default choice for the distance metric d in eq. ( 2 ) is simply mean squared error ( mse ). to enhance realism, we further apply perceptual loss at the final vae - decoded image, while intermediate hidden states remain aligned by mse. in this work, we adopt both vgg [ 50 ] and convnext v2 [ 61 ] feature spaces for perceptual loss ( our implementation for vgg features follows lpips [ 68 ] ). as in prior work [ 16, 52, 17 ], all loss terms can be adaptively re - weighted during training. details are provided in appendix b. 3. 4. 4. norm control the intermediate states produced by the forward model are unconstrained under the nf formulation, often exhibiting large norm fluctuations across blocks ( see fig. 8a ). these variations can lead to imbalanced supervision when using magnitude - sensitive losses such as mse for reverse - model training. to mitigate this issue, we introduce two complementary norm - control strategies applied to the forward and reverse models to ensure stable and consistent supervision strength ( details in appendix b. 3 ). on the forward model, we clip the output parameters of each transformation fi within a fixed range [ −c, c ], limiting excessive scaling and stabilizing intermediate state norms without compromising expressiveness. on the reverse model, we normalize each intermediate state before performing hidden alignment, which equalizes the contribution across trajectory depth and promotes scale - invariant learning. 4. 5. biflow with guidance classifier - free guidance ( cfg ) [ 24 ] was originally proposed for diffusion models to control the trade - off between sample diversity and fidelity. due to its effectiveness, it has been widely adopted in diffusion - based generative models. following this success, recent normalizing flows [ 65, 21 ] and autoregressive models [ 56, 35 ] also incorporate cfg to further improve generation quality. cfg can be seamlessly integrated into biflow ’ s inference process by extrapolating conditional and unconditional predictions of [UNK] at each hidden state hi, i. e., hi + 1 = ( 1 + wi ) gi [UNK] ( hi | c ) −wi gi [UNK] ( hi ), (", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 5, "frag_id": 2, "text": ". biflow with guidance classifier - free guidance ( cfg ) [ 24 ] was originally proposed for diffusion models to control the trade - off between sample diversity and fidelity. due to its effectiveness, it has been widely adopted in diffusion - based generative models. following this success, recent normalizing flows [ 65, 21 ] and autoregressive models [ 56, 35 ] also incorporate cfg to further improve generation quality. cfg can be seamlessly integrated into biflow ’ s inference process by extrapolating conditional and unconditional predictions of [UNK] at each hidden state hi, i. e., hi + 1 = ( 1 + wi ) gi [UNK] ( hi | c ) −wi gi [UNK] ( hi ), ( 4 ) where c is the class condition and wi is the guidance scale ( our w definition follows the original cfg formulation [ 24 ], i. e., w = 0 is w / o cfg ). the subscript i indicates that wi can differ among blocks, supporting cfg interval [ 31 ]. more results are provided in appendix c. 2. directly applying cfg doubles the computational cost during inference, since each guided block requires two forward passes. to alleviate this, following [ 5, 55 ], we incorporate cfg into the training stage, enabling inference with only one function evaluation ( 1 - nfe ) while preserving the benefits of guidance. additionally, to retain the flexibility of adjusting guidance scales at inference time, we allow the reverse model to leverage cfg scale as condition [ 39, 18 ]. by training the model with a range of guidance scales, biflow can generate outputs corresponding to various guidance strengths within a single forward pass. further details are provided in appendix b. 2. 5", "token_count": 354}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 6, "frag_id": 0, "text": "5. experiments experiment settings. our experiments are conducted on class - conditional imagenet [ 8 ] generation at 256×256 resolution. we evaluate fr´echet inception distance ( fid ) [ 23 ] and inception score ( is ) [ 48 ] on 50000 generated images. following [ 46, 13, 21 ], we implement our models on the latent space of a pre - trained vae tokenizer. for imagenet 256×256, the tokenizer maps images to a 32×32×4 latent representation, serving as the input and output domain of our models. improved tarflow as baseline. our biflow framework builds upon tarflow [ 65 ] as our forward model. we introduce several modifications to the original tarflow to enhance stability and performance. specifically, we replace additive conditioning with in - context conditioning [ 42 ] and apply the norm control strategy in sec. 4. 4, while omitting starflow - specific components such as deep - shallow design, decoder finetuning, and customized cfg. we denote this enhanced version as improved tarflow ( itarflow ). as shown in the table below, it achieves substantial gains over the original tarflow, both with or without cfg, establishing a strong baseline for biflow. method fid ( ↓ ) # params w / o cfg w / cfg latent tarflow - b / 2 * 59. 43 10. 89 118m + in - context conditioning 53. 87 8. 25 120m + 160 epochs →960 epochs 45. 48 7. 05 120m + norm control ( itarflow ) 44. 46 6. 83 120m configurations. our reverse model adopts a vit backbone with modern transformer components [ 53, 66 ] and multitoken in - context conditioning [ 18 ]. we name our model as biflow - b / 2, where b / 2 indicates a base - sized model with patch size 2, resulting in a sequence length of 256. in our ablation studies, we choose an itarflow as our forward model and train the reverse model with the forward model fixed. unless otherwise specified, our ablations employ the adaptive - weighted mse, while final comparisons in tab. 4 incorporate perceptual distance mentioned in sec. 4. 3 for optimal performance. details are provided in appendix a. 5. 1. ablation : learning to approximate the inverse we evaluate three strategies for learning", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 6, "frag_id": 1, "text": "backbone with modern transformer components [ 53, 66 ] and multitoken in - context conditioning [ 18 ]. we name our model as biflow - b / 2, where b / 2 indicates a base - sized model with patch size 2, resulting in a sequence length of 256. in our ablation studies, we choose an itarflow as our forward model and train the reverse model with the forward model fixed. unless otherwise specified, our ablations employ the adaptive - weighted mse, while final comparisons in tab. 4 incorporate perceptual distance mentioned in sec. 4. 3 for optimal performance. details are provided in appendix a. 5. 1. ablation : learning to approximate the inverse we evaluate three strategies for learning the reverse model, as described in sec. 4. 1, and report generation quality ( fid in tab. 1 ) as well as reconstruction error ( see appendix c. 1 ). the naive distillation approach, trained with a simple mse objective, already outperforms the exact inverse baseline, indicating that a learned reverse model is a practical and competitive alternative to the analytic inverse. hidden distillation supervises the reverse model using the entire forward trajectory. however, repeated projections * the latent tarflow - b / 2 is our tarflow reproduction in vae latent. fid ( ↓ ) attention exact inverse 44. 46 causal naive distillation 43. 41 −1. 05 bidirect hidden distillation 55. 00 + 10. 54 bidirect hidden alignment 36. 93 −7. 53 bidirect table 1. reverse learning method. naive distillation can exceed the exact inverse with a simple mse objective. our hidden alignment yields the best result among the three strategies. ( settings : biflow - b / 2, 160 epochs, adaptive weighted mse loss, w / o cfg ) between representation and input spaces cause information loss and limit architectural expressiveness. this results in degraded performance compared to the naive distillation. our proposed hidden alignment method removes the repeated projections inherent in hidden distillation while retaining full trajectory - level supervision, thereby preserving both architectural flexibility and representational richness. it achieves the best performance among the three strategies and surpasses the exact inverse by a clear margin in generation quality. these results collectively demonstrate that hidden alignment is an effective and robust strategy for learning an approximate inverse in biflow. 5. 2. other ablations we ab", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 6, "frag_id": 2, "text": "result among the three strategies. ( settings : biflow - b / 2, 160 epochs, adaptive weighted mse loss, w / o cfg ) between representation and input spaces cause information loss and limit architectural expressiveness. this results in degraded performance compared to the naive distillation. our proposed hidden alignment method removes the repeated projections inherent in hidden distillation while retaining full trajectory - level supervision, thereby preserving both architectural flexibility and representational richness. it achieves the best performance among the three strategies and surpasses the exact inverse by a clear margin in generation quality. these results collectively demonstrate that hidden alignment is an effective and robust strategy for learning an approximate inverse in biflow. 5. 2. other ablations we ablate several key design choices in biflow and analyze their impact on performance in tab. 2. biflow with guidance. biflow is conditioned on the cfg scale and learns across a range of cfg scales during training. this enables 1 - nfe inference while preserving the benefits and flexibility of guidance. as shown in tab. 2a, compared to standard cfg approach, our training - time cfg mechanism reduces inference cost by half while achieving better fid. learned denoising. tab. 2b demonstrates the effectiveness of our learned denoising strategy. by jointly training denoising with the inverse, our learned one - block denoiser improves generation quality over the score - based denoising used in tarflow. moreover, our approach introduces only a single additional block, whereas tarflow ’ s score - based denoising requires an extra forward - backward pass ( incurring 15. 8× flops ). this substantially reduces inference overhead. norm control. we introduce two norm control strategies in sec. 4. 4 and evaluate their effectiveness in tab. 2c. applying either strategy alleviates imbalance in mse loss across blocks, thereby enhancing performance. we provide visualizations of the norm statistics in appendix c. 4. distance metric. our framework supports various distance metric designs. as shown in tab. 2d, incorporating perceptual distance [ 68, 61 ] at the image end can largely improve generation quality. notably, when both vgg and convnext features are used for the perceptual loss, the optimal guidance scale in eq. ( 4 ) for this model is close to 0. 0, resulting in performance similar to no - cfg setting. this suggests these features already provide", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 6, "frag_id": 3, "text": "norm control strategies in sec. 4. 4 and evaluate their effectiveness in tab. 2c. applying either strategy alleviates imbalance in mse loss across blocks, thereby enhancing performance. we provide visualizations of the norm statistics in appendix c. 4. distance metric. our framework supports various distance metric designs. as shown in tab. 2d, incorporating perceptual distance [ 68, 61 ] at the image end can largely improve generation quality. notably, when both vgg and convnext features are used for the perceptual loss, the optimal guidance scale in eq. ( 4 ) for this model is close to 0. 0, resulting in performance similar to no - cfg setting. this suggests these features already provide strong class - discriminative information. more results are provided in appendix c. 3. 6", "token_count": 170}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 7, "frag_id": 0, "text": "fid, w / o cfg fid, w / cfg inference - time cfg 36. 93 ( nfe = 1 ) 6. 90 ( nfe = 2 ) →training - time cfg 31. 88 ( nfe = 1 ) 6. 79 ( nfe = 1 ) ( a ) biflow with guidance. conditioning on the cfg scale during training improves fid both w / and w / o cfg while preserving flexible 1 - nfe inference. the baseline w / o cfg is final results in tab. 1. fid, w / o cfg fid, w / cfg learned denoise 31. 88 6. 79 →no denoise 100. 51 26. 20 →score - based denoise 42. 62 10. 98 ( b ) learned denoising. our learned denoising scheme is effective. compared to score - based denoising in tarflow, it eliminates an extra forwardbackward calculation, and unifies the denoising step into our framework. fid, w / o cfg fid, w / cfg norm control : clip 31. 88 6. 79 norm control : none 45. 54 12. 33 norm control : traj. 34. 88 8. 03 ( c ) norm control. either clipping the forward model ’ s output or normalizing the forward trajectory improves generation quality by ensuring balanced supervision strength across blocks. fid, w / o cfg fid, w / cfg mse 31. 88 6. 79 + lpips 14. 15 4. 91 + lpips + convnext 2. 46 2. 46 ( d ) distance metric. our framework enables a flexible design of distance metrics. incorporating perceptual distance improves generation quality. table 2. ablation study on imagenet 256×256 generation. fid50k with 1 - nfe is reported by default. ( settings : biflow - b / 2, 160 epochs. by default : adaptive weighted mse loss without perceptual loss, training - time cfg. ) scaling behavior. we investigate the scaling behavior of biflow under different distance metrics, using itarflow of corresponding size as forward models. we summarize preliminary results in the table below. fid, w / cfg b xl mse 6. 79 4. 61 + lpips 4. 91 3. 36 + lp", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 7, "frag_id": 1, "text": "flexible design of distance metrics. incorporating perceptual distance improves generation quality. table 2. ablation study on imagenet 256×256 generation. fid50k with 1 - nfe is reported by default. ( settings : biflow - b / 2, 160 epochs. by default : adaptive weighted mse loss without perceptual loss, training - time cfg. ) scaling behavior. we investigate the scaling behavior of biflow under different distance metrics, using itarflow of corresponding size as forward models. we summarize preliminary results in the table below. fid, w / cfg b xl mse 6. 79 4. 61 + lpips 4. 91 3. 36 + lpips + convnext 2. 46 2. 57 overall, biflow exhibits clear gains from increased model capacity when trained without the convnext - based perceptual loss. however, after incorporating convnext features, further scaling yields diminishing returns, with fid improvements gradually saturating. we hypothesize this behavior may be related to overfitting, as evidenced by an increase in fid during training. a comprehensive investigation of biflow ’ s scaling behavior is left for future work. 5. 3. biflow vs. improved tarflow we compare our learned reverse model ( biflow ) with the exact analytic inverse baseline ( improved tarflow ) of the forward process. in tab. 3, we benchmark in terms of generation quality ( fid score ) and inference efficiency ( flops and wall - clock time for generating a single image ). details of our benchmarking setup are provided in appendix a. biflow improved tarflow b / 2 b / 2 m / 2 l / 2 xl / 2 fid 2. 39 6. 83 5. 22 4. 82 4. 54 # params 133m 120m 296m 448m 690m gflops 38 152 363 552 836 wall - clock time ( ms ) tpu 0. 29 + 1. 3 65 + 1. 3 85 + 1. 3 165 + 1. 3 202 + 1. 3 gpu 2. 15 + 2. 7 129 + 2. 7 208 + 2. 7 349 + 2. 7 400 + 2. 7 cpu 80 + 240 9040 + 240 16200 + 240 20400 + 240 26300 + 240 wall - clock speedup,", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 7, "frag_id": 2, "text": "##flow b / 2 b / 2 m / 2 l / 2 xl / 2 fid 2. 39 6. 83 5. 22 4. 82 4. 54 # params 133m 120m 296m 448m 690m gflops 38 152 363 552 836 wall - clock time ( ms ) tpu 0. 29 + 1. 3 65 + 1. 3 85 + 1. 3 165 + 1. 3 202 + 1. 3 gpu 2. 15 + 2. 7 129 + 2. 7 208 + 2. 7 349 + 2. 7 400 + 2. 7 cpu 80 + 240 9040 + 240 16200 + 240 20400 + 240 26300 + 240 wall - clock speedup, biflow - b / 2 vs. itarflow : ( vae excluded, see also fig. 2 ) tpu 224× 293× 569× 697× gpu 60× 97× 162× 186× cpu 113× 203× 255× 329× wall - clock speedup, biflow - b / 2 vs. itarflow : ( vae included ) tpu 42× 54× 105× 128× gpu 27× 43× 73× 83× cpu 29× 51× 65× 83× table 3. comparison between biflow and itarflow baseline. we report both generation quality ( fid - 50k ) and inference cost per image. all wall - clock time measurements are reported as “ generator + vae decoding ”. compared to itarflow, biflow achieves one to two orders of magnitude faster sampling on tpu, gpu, and cpu, while attaining superior generation quality. ( the vae decoder contains 49m parameters and requires 308 gflops. ) experiments show that our biflow - b / 2 surpasses the exact inverse of the improved tarflow - xl / 2 baseline in generation quality. remarkably, biflow requires only a single function evaluation ( 1 - nfe ), compared to 256×2 sequential decoding steps for the autoregressive inference of the exact analytic inverse — resulting in up to a 42× speedup for models of similar size on tpu. why can a learned inverse outperform the exact inverse? our reverse model [UNK] is trained to reconstruct real images directly, rather than to replicate synthetic samples produced by the exact inverse as in conventional distillation. this encourages its predictions to align more closely with the true", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 7, "frag_id": 3, "text": "requires 308 gflops. ) experiments show that our biflow - b / 2 surpasses the exact inverse of the improved tarflow - xl / 2 baseline in generation quality. remarkably, biflow requires only a single function evaluation ( 1 - nfe ), compared to 256×2 sequential decoding steps for the autoregressive inference of the exact analytic inverse — resulting in up to a 42× speedup for models of similar size on tpu. why can a learned inverse outperform the exact inverse? our reverse model [UNK] is trained to reconstruct real images directly, rather than to replicate synthetic samples produced by the exact inverse as in conventional distillation. this encourages its predictions to align more closely with the true data distribution. in addition, [UNK] is optimized end - to - end with the forward map fixed, learning to directly transform noise into clean data. this joint optimization can help the model to learn a stable and globally consistent mapping. why is a learned inverse significantly faster than the exact inverse? from an algorithmic perspective, two key improvements reduce the computational cost of biflow. first, biflow eliminates the score - based denoising step required by the exact inverse of tarflow, removing a major computational bottleneck. second, we integrate cfg into the training stage, effectively halving the inference cost compared to applying cfg during sampling. together, these two improvements reduce the flops by roughly 4×. from an architectural perspective, the autoregressive design of tarflow imposes inherent limitations on parallelism during inference. our bidirectional attention transformer design allows for fully parallelized computation across the sequence dimension, which leads to significant speedups on modern accelerators. notably, due to the efficiency of biflow, the vae decoder has become a dominant computational overhead, which is outside the scope of this work. 7", "token_count": 383}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 8, "frag_id": 0, "text": "method # params nfe fid ( ↓ ) is ( ↑ ) autoregressive normalizing flow tarflow - xl / 8 @ pix [ 65 ] 1. 3b [UNK] 5. 56 starflow - xl / 1 [ 21 ] 1. 4b [UNK] 2. 40 autoregressive normalizing flow ( our impl. ) itarflow - b / 2 120m [UNK] 6. 83 226. 2 itarflow - m / 2 296m [UNK] 5. 22 255. 5 itarflow - l / 2 448m [UNK] 4. 82 254. 8 itarflow - xl / 2 690m [UNK] 4. 54 259. 3 1 - nfe normalizing flow biflow - b / 2 ( ours ) 133m 1 2. 39 303. 0 method # params nfe fid ( ↓ ) is ( ↑ ) gans biggan - deep [ 3 ] 112m 1 6. 95 202. 6 gigagan [ 26 ] 569m 1 3. 45 225. 5 stylegan - xl [ 49 ] 166m 1 2. 30 265. 1 1 - nfe diffusion / flow matching from scratch ict - xl / 2 [ 52 ] 675m 1 34. 24 shortcut - xl / 2 [ 15 ] 675m 1 10. 60 meanflow - xl / 2 [ 17 ] 676m 1 3. 43 247. 5 tim - xl / 2 [ 60 ] 664m 1 3. 26 210. 3 α - flow - xl / 2 + [ 67 ] 676m 1 2. 58 imf - xl / 2 [ 18 ] 610m 1 1. 72 282. 0 1 - nfe diffusion / flow matching ( distillation ) π - flow - xl / 2 [ 6 ] 675m 1 2. 85 dmf - xl / 2 + [ 32 ] 675m 1 2. 16 facm - xl / 2 [ 43 ] 675m 1 1. 76 290. 0 method # params nfe fid ( ↓ ) is ( ↑ ) autoregressive / masking maskgit [ 4 ] 227m [UNK] 6. 18 182. 1 rcg, conditional [ 34 ] 512m [UNK] 2. 12 267. 7 var - d30 [ 56 ] 2. 0b [UNK] 1. 92 323. 1 mar - h [ 35 ] 943m [UNK] 1. 55 303. 7 rar -", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 8, "frag_id": 1, "text": "matching ( distillation ) π - flow - xl / 2 [ 6 ] 675m 1 2. 85 dmf - xl / 2 + [ 32 ] 675m 1 2. 16 facm - xl / 2 [ 43 ] 675m 1 1. 76 290. 0 method # params nfe fid ( ↓ ) is ( ↑ ) autoregressive / masking maskgit [ 4 ] 227m [UNK] 6. 18 182. 1 rcg, conditional [ 34 ] 512m [UNK] 2. 12 267. 7 var - d30 [ 56 ] 2. 0b [UNK] 1. 92 323. 1 mar - h [ 35 ] 943m [UNK] 1. 55 303. 7 rar - xxl [ 63 ] 1. 5b [UNK] 1. 48 326. 0 xar - h [ 44 ] 1. 1b [UNK] 1. 24 301. 6 multi - nfe diffusion / flow matching adm - g [ 9 ] 554m 250×2 4. 59 ldm - 4 - g [ 46 ] 400m 250×2 3. 60 247. 7 dit - xl / 2 [ 42 ] 675m 250×2 2. 27 278. 2 sit - xl / 2 [ 38 ] 675m 250×2 2. 06 252. 2 jit - g / 16 [ 33 ] 2b 100×2 1. 82 292. 6 sit - xl / 2 + repa [ 64 ] 675m 250×2 1. 42 305. 7 lightningdit - xl / 1 [ 62 ] 675m 250×2 1. 35 295. 3 ddt - xl / 2 [ 59 ] 675m 250×2 1. 26 310. 6 ditdh - xl + rae [ 69 ] 839m 50×2 1. 13 262. 6 table 4. system - level comparison on imagenet 256×256 class - conditional generation. all results are reported with cfg if applicable. left : comparison with normalizing flow models. middle : other 1 - nfe generative models, including gans and diffusion / flow matchingbased models. right : other families of generative models. our biflow model is trained for 350 epochs with perceptual distance. in all tables, ×2 indicates the use of cfg incurs double nfes. [UNK] : all ar - based methods, including ar normalizing flow ( left ) and other ar", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 8, "frag_id": 2, "text": "310. 6 ditdh - xl + rae [ 69 ] 839m 50×2 1. 13 262. 6 table 4. system - level comparison on imagenet 256×256 class - conditional generation. all results are reported with cfg if applicable. left : comparison with normalizing flow models. middle : other 1 - nfe generative models, including gans and diffusion / flow matchingbased models. right : other families of generative models. our biflow model is trained for 350 epochs with perceptual distance. in all tables, ×2 indicates the use of cfg incurs double nfes. [UNK] : all ar - based methods, including ar normalizing flow ( left ) and other ar models ( right ), involve a large number of forward evaluations, yet each evaluation is on one or a very few tokens. for example, for standard left - to - right order ar, the average nfe of the entire ar process is roughly 1 ( or 2× w / cfg ), that is, k evaluations with a 1 k fraction of tokens each. in addition, tarflow / itarflow has an extra nfe of 2 due to the score - based denoising post - processing. results of [ 52 ] is collected from [ 70 ] ; results of [ 65 ] is collected from [ 21 ] ; tarflow - xl / 8 @ pix denotes tarflow on pixel - space with patch size 8. 5. 4. comparison with prior works in tab. 4, we provide system - level comparisons with previous methods on class - conditional imagenet 256×256 generation. we categorize prior works into three groups : normalizing flows ( tab. 4, left ), 1 - nfe generative models ( tab. 4, middle ), and other families of generative models ( tab. 4, right ). all our models are trained to convergence. comparison with normalizing flows. tab. 4 ( left ) compares biflow with previous state - of - the - art normalizing flows models. our biflow - b / 2, with only 133 million parameters, achieves an fid of 2. 39 in a single function evaluation ( 1 - nfe ), establishing a new state - of - the - art among normalizing flows. in contrast, starflow uses thousands of sequential decoding steps due to their autoregressive sampling process. it yields a", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 8, "frag_id": 3, "text": "tab. 4, left ), 1 - nfe generative models ( tab. 4, middle ), and other families of generative models ( tab. 4, right ). all our models are trained to convergence. comparison with normalizing flows. tab. 4 ( left ) compares biflow with previous state - of - the - art normalizing flows models. our biflow - b / 2, with only 133 million parameters, achieves an fid of 2. 39 in a single function evaluation ( 1 - nfe ), establishing a new state - of - the - art among normalizing flows. in contrast, starflow uses thousands of sequential decoding steps due to their autoregressive sampling process. it yields a similar fid score with about 10× parameters and more than 400× inference wall - clock time ( see tab. 6 for details ). more broadly, biflow represents a significant advancement in normalizing flows, demonstrating that direct and efficient generation can coexist with high fidelity. comparison with other generative models. we compare biflow with other generative model families, especially 1 - nfe methods. as shown in tab. 4, biflow offers an excellent balance between generation quality and sampling efficiency. these results demonstrate that biflow achieves performance on par with leading 1 - nfe generative models. 6. conclusion this work revisits one of the oldest, yet most principled, foundations of generative modeling — normalizing flows — and redefines its boundaries. we challenge the conventional figure 6. 1 - nfe generation results. we show selected samples generated by our biflow - b / 2 model with guidance scale 2. 0 on imagenet 256×256. biflow achieves high - fidelity generation with only a single function evaluation ( 1 - nfe ) from noise. wisdom that the reverse process must be the exact analytic inverse of the forward process, and demonstrate that the long - held constraint is unnecessary. by introducing a learnable reverse model, biflow pushes normalizing flows from analytically invertible mappings to trainable bidirectional systems, from autoregressive sampling to fully parallelized, efficient 1 - nfe generation, and from an implicit generative model towards a direct generative model. experiments demonstrate that biflow achieves competitive generation quality among normalizing flows, while delivering up to two orders of magnitude faster inference than its explicit inverse counterpart. we hope this work can serve", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 8, "frag_id": 4, "text": "##flow achieves high - fidelity generation with only a single function evaluation ( 1 - nfe ) from noise. wisdom that the reverse process must be the exact analytic inverse of the forward process, and demonstrate that the long - held constraint is unnecessary. by introducing a learnable reverse model, biflow pushes normalizing flows from analytically invertible mappings to trainable bidirectional systems, from autoregressive sampling to fully parallelized, efficient 1 - nfe generation, and from an implicit generative model towards a direct generative model. experiments demonstrate that biflow achieves competitive generation quality among normalizing flows, while delivering up to two orders of magnitude faster inference than its explicit inverse counterpart. we hope this work can serve as a step toward rethinking and expanding the scope of normalizing flows, inspiring future research on direct, flexible, and efficient nf - based generation. 8", "token_count": 184}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 9, "frag_id": 0, "text": "config biflow improved tarflow b / 2 b / 2 m / 2 l / 2 xl / 2 # params ( m ) 133 120 296 448 690 block 9 8 10 12 15 layer 8 8 9 9 9 hidden dim 384 384 512 576 640 attn heads 6 6 8 9 10 patch size 2 2 class tokens 8 — guidance tokens 4 † / 1 — epochs 160 † / 350 960 640 640 480 batch size 256 256 learning rate 4e - 4 4e - 4 4e - 4 2e - 4 1e - 4 lr schedule constant constant lr warmup 10 epochs 10 epochs optimizer adam adam [ 28 ] adam ( β1, β2 ) ( 0. 9, 0. 95 ) ( 0. 9, 0. 95 ) weight decay 0. 0 0. 0 dropout 0. 0 0. 0 ema decay 0. 9999 0. 9999 label drop 0. 1 0. 1 adaptive weight p 1 — wvgg 1. 0 † / 0. 8 — wconvnext 0. 4 † / 0. 6 — clip range c — 1. 0 1. 0 3. 0 3. 0 noise level σ — 0. 3 table 5. configurations and training hyperparameters on imagenet 256×256. † indicates the setting in the ablation study. a. implementation details we implement all experiments using the jax framework [ 2 ] on google tpu hardware. all reported results are obtained on tpu v4, v5p, and v6e cores. the configurations and training hyperparameters for improved tarflow and biflow are provided in tab. 5. for the mse - only ablation in sec. 5, we employ adaptive weighting with exponent p = 2 ; for all other experiments we use p = 1 ( see appendix c. 3 for detailed ablations ). fid evaluation. for generative evaluation, we compute the fr´echet inception distance ( fid ) [ 23 ] between 50, 000 generated images and training images, without applying any data augmentation. we use the inception - v3 model [ 54 ] provided by stylegan3 [ 27 ], converted into a jax - compatible implementation. we sample 50 images per class for all 1000 imagenet classes, following the protocol in [ 69 ]. inference cost evaluation. in fig. 2 and tab. 3,", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 9, "frag_id": 1, "text": "mse - only ablation in sec. 5, we employ adaptive weighting with exponent p = 2 ; for all other experiments we use p = 1 ( see appendix c. 3 for detailed ablations ). fid evaluation. for generative evaluation, we compute the fr´echet inception distance ( fid ) [ 23 ] between 50, 000 generated images and training images, without applying any data augmentation. we use the inception - v3 model [ 54 ] provided by stylegan3 [ 27 ], converted into a jax - compatible implementation. we sample 50 images per class for all 1000 imagenet classes, following the protocol in [ 69 ]. inference cost evaluation. in fig. 2 and tab. 3, we report inference cost across three hardware configurations : gpu, tpu, and cpu. for all metrics, we report the average perimage runtime in seconds, averaged over multiple runs to ensure stability. all measurements include the overhead of cfg and vae decoding time when applicable. we also provide a comparison with prior normalizing flow models [ 65, 21 ] in tab. 6. all autoregressive models utilize kv - cache to accelerate inference, and gflops in tab. 3 is estimated using jax ’ s cost analysis function. method # params time ( ms ) speed vae? tarflow - xl / 8 @ pix [ 65 ] 1. 3b 1192 1× [UNK] starflow - xl / 1 [ 21 ] 1. 4b 677 + 1. 3 1. 76× [UNK] itarflow - b / 2 120m 65 + 1. 3 18. 0× [UNK] itarflow - m / 2 296m 85 + 1. 3 13. 8× [UNK] itarflow - l / 2 446m 165 + 1. 3 7. 17× [UNK] itarflow - xl / 2 675m 202 + 1. 3 5. 86× [UNK] biflow - b / 2 ( ours ) 133m 0. 29 + 1. 3 750× [UNK] ( a ) tpu inference time comparison, benchmarked on 8 tpu v4 cores with a pre - compiled jax sampling function. method # params time ( ms ) speed vae? tarflow - xl / 8 @ pix [ 65 ] 1. 3b 3452 1× [UNK] starflow - xl / 1 [ 21 ] 1. 4b 2193 + 2. 7", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 9, "frag_id": 2, "text": "+ 1. 3 13. 8× [UNK] itarflow - l / 2 446m 165 + 1. 3 7. 17× [UNK] itarflow - xl / 2 675m 202 + 1. 3 5. 86× [UNK] biflow - b / 2 ( ours ) 133m 0. 29 + 1. 3 750× [UNK] ( a ) tpu inference time comparison, benchmarked on 8 tpu v4 cores with a pre - compiled jax sampling function. method # params time ( ms ) speed vae? tarflow - xl / 8 @ pix [ 65 ] 1. 3b 3452 1× [UNK] starflow - xl / 1 [ 21 ] 1. 4b 2193 + 2. 7 1. 57× [UNK] itarflow - b / 2 120m 129 + 2. 7 26. 2× [UNK] itarflow - m / 2 296m 208 + 2. 7 16. 4× [UNK] itarflow - l / 2 446m 349 + 2. 7 9. 82× [UNK] itarflow - xl / 2 675m 400 + 2. 7 8. 57× [UNK] biflow - b / 2 ( ours ) 133m 2. 15 + 2. 7 712× [UNK] ( b ) gpu inference time comparison, benchmarked on 1 nvidia h200 core with pytorch and torch. compile optimization if beneficial. method # params time ( ms ) speed vae? tarflow - xl / 8 @ pix [ 65 ] 1. 3b 512000 1× [UNK] starflow - xl / 1 [ 21 ] 1. 4b 276700 + 240 1. 85× [UNK] itarflow - b / 2 120m 9040 + 240 55. 2× [UNK] itarflow - m / 2 296m 16200 + 240 31. 1× [UNK] itarflow - l / 2 446m 20400 + 240 24. 8× [UNK] itarflow - xl / 2 675m 26300 + 240 19. 3× [UNK] biflow - b / 2 ( ours ) 133m 80 + 240 1600× [UNK] ( c ) cpu inference time comparison, benchmarked on 1 amd epyc 7b12 node with 120 physical cpu cores and 400gb ram. we reuse the pytorch implementations with torch. compile optimization if beneficial. table 6. comparison of nf models ’ inference wall -", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 9, "frag_id": 3, "text": "- b / 2 120m 9040 + 240 55. 2× [UNK] itarflow - m / 2 296m 16200 + 240 31. 1× [UNK] itarflow - l / 2 446m 20400 + 240 24. 8× [UNK] itarflow - xl / 2 675m 26300 + 240 19. 3× [UNK] biflow - b / 2 ( ours ) 133m 80 + 240 1600× [UNK] ( c ) cpu inference time comparison, benchmarked on 1 amd epyc 7b12 node with 120 physical cpu cores and 400gb ram. we reuse the pytorch implementations with torch. compile optimization if beneficial. table 6. comparison of nf models ’ inference wall - clock time on tpu, gpu, and cpu. the wall - clock time is evaluated per image on average in milliseconds. all models include the overhead of cfg at inference time, as well as the vae decoding time when applicable. all autoregressive models utilize kv - cache to accelerate inference. see appendix a for further details. for tpu wall - clock time, all models are evaluated using a pre - compiled jax sampling function on 8 tpu v4 cores. reported times exclude compilation overhead. we use a local device batch size of 10 for model inference, and 200 for vae decoding. for gpu wall - clock time, all models are re - implemented in pytorch and evaluated on a single nvidia h200 gpu with a batch size of 128. the vae decoding time is obtained with torch. compile optimization. for cpu wall - clock time, we reuse the pytorch implementation on a single amd epyc 7b12 node ( 120 physical cpu cores and 400 gb ram ). we use a smaller batch size of 64 for most models ; however, tarflow and starflow are restricted to a batch size of 4 due to efficiency concerns. we observe that batch size has a negligible impact on perimage cpu inference time. all other experimental settings remain consistent with the gpu evaluation. 9", "token_count": 436}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 10, "frag_id": 0, "text": "algorithm 1 biflow : training. # x : training batch, ( n, h, w, c ) # f : forward model ( b blocks ), frozen # g : reverse model ( b + 1 blocks ) # phi : projection heads # noise injection e = randn like ( x ) x _ tilde = x + noise _ level * e # get forward trajectory xs and prior z xs, z = f ( x _ tilde ) # get reverse trajectory hs and reconstructed x ’ hs, x _ prime = g ( z ) # project hidden into input space for i in range ( b ) : hs [ i ] = phi [ i ] ( hs [ i ] ) # compute loss loss _ align = mse ( xs, hs ) loss _ recon = metric ( x, x _ prime ) loss = loss _ align + loss _ recon algorithm 2 biflow : 1 - nfe sampling. e = randn ( x _ shape ) _, x = g ( e ) b. method details b. 1. pseudocode we provide the pseudocode for training our biflow model with hidden alignment in alg. 1, as well as the 1 - nfe sampling procedure in alg. 2. in the algorithm, the forward model f produces the entire forward trajectory xs, i. e., [UNK], x1, x2,..., xb−1, along with the prior z = xb. similarly, the reverse model g outputs the sequence of intermediate hiddens states hs as reverse trajectory : hb−1, hb−2,..., h0, along with the reconstructed clean input x prime. the final loss function consists of alignment loss between forward and reverse hidden states in eq. ( 2 ), and reconstruction loss between the clean input x and reconstructed output x prime. b. 2. biflow with guidance training - time cfg. as discussed in sec. 4. 5, to enable guided sampling within a single forward pass ( 1 - nfe ), we directly train a guided reverse model gcfg [UNK] defined as gi, cfg [UNK] ( hi | c ) = ( 1 + wi ) gi [UNK] ( hi | c ) −wigi [UNK] ( hi ). 0. 0 0. 5 1. 0 1. 5 cfg scale 0 10 20 30 40 fid - 50k inference - time cfg ( 2 - nfe ) training", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 10, "frag_id": 1, "text": "loss between forward and reverse hidden states in eq. ( 2 ), and reconstruction loss between the clean input x and reconstructed output x prime. b. 2. biflow with guidance training - time cfg. as discussed in sec. 4. 5, to enable guided sampling within a single forward pass ( 1 - nfe ), we directly train a guided reverse model gcfg [UNK] defined as gi, cfg [UNK] ( hi | c ) = ( 1 + wi ) gi [UNK] ( hi | c ) −wigi [UNK] ( hi ). 0. 0 0. 5 1. 0 1. 5 cfg scale 0 10 20 30 40 fid - 50k inference - time cfg ( 2 - nfe ) training - time cfg ( 1 - nfe ) figure 7. comparison between training - time and inference - time cfg of biflow. training - time cfg achieves similar or better performance compared to inference - time cfg while requiring only half inference compute and retaining full post - hoc tuning flexibility. ( settings : biflow - b / 2, 160 epochs, mse - only baseline. ) where wi is the guidance scale at block i. the unconditional output of gi, cfg [UNK] matches that of the original gi [UNK]. therefore, the unguided block output can be expressed as hi + 1 = gi, cfg [UNK] ( hi | c ) + wigi, cfg [UNK] ( hi ) 1 + wi. ( 5 ) during training, we compute our hidden - alignment loss directly on hi + 1 from eq. ( 5 ). at inference time, this formulation allows us to use gi, cfg [UNK] directly, producing guided samples with only a 1 - nfe forward pass. we add stop gradient to the unconditional output to stabilize training. guidance conditioning. to retain the ability to adjust the cfg scale at inference time, we explicitly condition the reverse model on the guidance scale [ 39, 18 ], i. e., gi, cfg [UNK] ( hi | c, wi ). during training, we sample wi from a uniform distribution u ( 0, wmax ) and apply eq. ( 5 ) to compute the unguided output hi + 1 for hidden alignment loss. we compare this training - time cfg scheme with the more conventional inference - time cfg in fig. 7. trainingtime cfg achieves similar ( even better ) performance while preserving the", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 10, "frag_id": 2, "text": "1 - nfe forward pass. we add stop gradient to the unconditional output to stabilize training. guidance conditioning. to retain the ability to adjust the cfg scale at inference time, we explicitly condition the reverse model on the guidance scale [ 39, 18 ], i. e., gi, cfg [UNK] ( hi | c, wi ). during training, we sample wi from a uniform distribution u ( 0, wmax ) and apply eq. ( 5 ) to compute the unguided output hi + 1 for hidden alignment loss. we compare this training - time cfg scheme with the more conventional inference - time cfg in fig. 7. trainingtime cfg achieves similar ( even better ) performance while preserving the 1 - nfe efficiency and the flexibility to sweep cfg scales at inference time. b. 3. distance metric adaptive weighting. we adopt the adaptive loss reweighting strategy from [ 16, 52, 17 ]. given a prediction x and target y, the adaptive - weighted distance is defined as : dp = sg ( wp ) · d ( x, y ), wp = ( d ( x, y ) + c ) −p, where c is a small constant and p ≥0 is adaptive weight. sg ( · ) denotes the stop - gradient operator. we apply adaptive weighting to all loss terms in our training objective. vgg feature. for the perceptual loss based on vgg features, we follow the lpips formulation [ 68 ]. since our model operates in the latent space of a pre - trained vae, we decode the predicted latent x ′ back into image space and compute the lpips loss against the ground - truth image. 10", "token_count": 356}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 11, "frag_id": 0, "text": "mse lpips naive distillation 0. 115 0. 331 hidden distillation 0. 156 0. 392 hidden alignment 0. 111 0. 321 table 7. reverse learning methods : reconstruction fidelity. we report mse and lpips between the original sample x and the reconstructed sample x ′ produced by the learned reverse model. among the three strategies for approximating the inverse transformation, the hidden alignment method achieves the most accurate reconstruction. the corresponding fids are shown in tab. 1 ( settings : biflow - b / 2, 160 epochs, no cfg, adaptive - weighted mse loss only. all three rows share the same forward model. ) convnext feature. in addition to vgg features, we incorporate convnext v2 [ 61 ] ( imagenet - 22k pre - trained, base - size ) as a complementary perceptual feature extractor. similar to the lpips, both the reconstructed image x ′ and the ground - truth image x are passed through the convnext network after vae decoding. the perceptual distance is computed using the extracted features, excluding the final classification head. usage of loss terms. in the ablation studies in sec. 5, we use only the adaptively weighted mse loss unless otherwise noted. in tab. 4, we combine all three loss terms : l ( x ) = x i lmse ( xi, φi ( hi ) ) + wvgglvgg ( x, x ′ ) + wconvnextlconvnext ( x, x ′ ), where wvgg and wconvnext are tunable hyperparameters. we observe that the final performance is particularly sensitive to wconvnext. concrete weights are specified in appendix a. normalized trajectory. as described in sec. 4. 4, the reverse model is trained to align with a normalized forward trajectory. specifically, we pre - compute the squared norm [UNK] of each trajectory point and average it over the entire dataset. during reverse model training, the intermediate trajectory points are divided by p e [ [UNK] ], ensuring scale consistency across different blocks. we do not use normalized trajectories in any experiments except the one in tab. 2c, as we observe no significant difference when combined with itarflow. nonetheless, normalized trajectories are worth noting for scenarios where one wishes to", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 11, "frag_id": 1, "text": "observe that the final performance is particularly sensitive to wconvnext. concrete weights are specified in appendix a. normalized trajectory. as described in sec. 4. 4, the reverse model is trained to align with a normalized forward trajectory. specifically, we pre - compute the squared norm [UNK] of each trajectory point and average it over the entire dataset. during reverse model training, the intermediate trajectory points are divided by p e [ [UNK] ], ensuring scale consistency across different blocks. we do not use normalized trajectories in any experiments except the one in tab. 2c, as we observe no significant difference when combined with itarflow. nonetheless, normalized trajectories are worth noting for scenarios where one wishes to use a pre - trained nf model without clipping. c. additional experiments c. 1. learning to approximate the inverse in tab. 1, we compare the empirical performance of the three reverse learning approaches introduced in sec. 4. here, we further provide quantitative results on their reconstruction fidelity in tab. 7. specifically, we evaluate the reconstruction distance d ( x, x ′ ) using mse and lpips ( vgg - based ) as metrics. wd \\ w 0. 5 0. 6 0. 7 0. 5 10. 51 9. 45 8. 77 0. 6 10. 21 9. 21 8. 59 0. 7 9. 93 8. 99 8. 41 3. 5 7. 05 6. 87 6. 89 4. 0 6. 94 6. 81 6. 87 4. 5 6. 88 6. 79 6. 87 5. 0 6. 86 6. 80 6. 89 table 8. separate guidance scale for the denoising block. biflow eliminates the score - based denoising step in tarflow by learning a dedicated denoising block, jointly trained with other blocks. this denoising block serves a different purpose from the rest nf reverse process. using a separate, larger guidance scale for the denoising block improves sample quality. ( settings : biflow - b / 2, 160 epochs, adaptively - weighted mse only, training - time cfg. fid w / o cfg : 31. 88. ) we observe that the proposed hidden - alignment strategy achieves the lowest regression loss across both metrics. this indicates that hidden alignment provides a more accurate mapping between x and x ′, leading to a better -", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 11, "frag_id": 2, "text": "##oising block. biflow eliminates the score - based denoising step in tarflow by learning a dedicated denoising block, jointly trained with other blocks. this denoising block serves a different purpose from the rest nf reverse process. using a separate, larger guidance scale for the denoising block improves sample quality. ( settings : biflow - b / 2, 160 epochs, adaptively - weighted mse only, training - time cfg. fid w / o cfg : 31. 88. ) we observe that the proposed hidden - alignment strategy achieves the lowest regression loss across both metrics. this indicates that hidden alignment provides a more accurate mapping between x and x ′, leading to a better - behaved reverse learning process. c. 2. biflow with guidance as discussed in sec. 4. 2, the additional denoising block in our reverse model functions as a dedicated denoiser, while the preceding b blocks focus on inverting the forward subtransformations. this structure naturally motivates applying cfg differently across these two components. we empirically validate this design choice in tab. 8. for training - time cfg, we use a shared guidance scale across all blocks, sampling w from a simple uniform prior u [ 0, 0. 5 ]. in ablation studies that use mse loss only ( sec. 5 ), we decouple the guidance scales for the inverse blocks and the denoising block, since the optimal pair ( w, wd ) typically satisfies wd [UNK]. in this case, we sample w [UNK] [ 0, 1 ] and wd [UNK] [ 0, 8 ]. c. 3. distance metric in sec. 4. 3, we discuss different choices of distance metrics for training biflow. we ablate the choice of perceptual distance terms in tab. 9. first, we compare different feature extractors, including a resnet - 101 [ 22 ] pre - trained for classification and a dinov2 - b model [ 40 ], as reported in tab. 9a. for resnet - 101, we extract features by removing the final mlp head, following the same procedure as convnext. among all tested feature extractors, convnext achieves the best empirical performance. we further evaluate the combination of vgg and convnext features in tab. 9b. the results indicate that using", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 11, "frag_id": 3, "text": "in sec. 4. 3, we discuss different choices of distance metrics for training biflow. we ablate the choice of perceptual distance terms in tab. 9. first, we compare different feature extractors, including a resnet - 101 [ 22 ] pre - trained for classification and a dinov2 - b model [ 40 ], as reported in tab. 9a. for resnet - 101, we extract features by removing the final mlp head, following the same procedure as convnext. among all tested feature extractors, convnext achieves the best empirical performance. we further evaluate the combination of vgg and convnext features in tab. 9b. the results indicate that using both features together yields better fid scores than using either one individually. furthermore, we study the effect of adaptive weighting in the mse loss in tab. 10. mse with adaptive weighting consistently outperforms the naive mse loss. 11", "token_count": 201}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 12, "frag_id": 0, "text": "feature model fid, w / o cfg fid, w / cfg none 31. 88 6. 79 vgg + resnet 9. 69 4. 34 vgg + dino 9. 33 4. 36 convnext + dino 3. 19 3. 19 vgg + convnext 2. 46 2. 46 ( a ) feature model ablation. wvgg wconvnext fid, w / o cfg fid, w / cfg 0. 0 0. 0 31. 88 6. 79 1. 0 0. 0 16. 97 5. 31 0. 0 0. 4 2. 62 2. 62 1. 0 0. 4 2. 46 2. 46 ( b ) vgg and convnext weight ablation. table 9. ablation on perceptual loss for biflow - b / 2. fid - 50k with / without cfg are reported. ( settings : biflow - b / 2, 160 epochs, training - time cfg, weight for two perceptual losses are 1. 0 and 0. 4 by default. ) adaptive weight p fid, w / o cfg fid, w / cfg 0. 0 38. 23 7. 49 0. 5 34. 74 7. 08 1. 0 34. 43 6. 70 2. 0 31. 88 6. 79 table 10. ablation on adaptive weighting. adaptive weighted mse loss works better than naive mse ( p = 0. 0 ). ( settings : biflowb / 2, 160 epochs, adaptive weighted mse only, training - time cfg. ) c. 4. improved tarflow norm control to mitigate the imbalance in loss magnitudes across different blocks of biflow, we introduce a simple yet effective modification to the original tarflow [ 65 ] in sec. 4. 4 : clipping the parameters µ and α within a fixed range. this adjustment stabilizes training and improves final fid performance. in fig. 8, we visualize the norms of intermediate trajectory states during training of the improved tarflow. without clipping, the norms across blocks diverge sharply and continue to grow as training progresses. with clipping, the norms remain stable and well - controlled within a reasonable range. such normalization substantially benefits the training of the reverse model in the downstream. c. 5. improved tarflow cfg for complete", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 12, "frag_id": 1, "text": "##flow norm control to mitigate the imbalance in loss magnitudes across different blocks of biflow, we introduce a simple yet effective modification to the original tarflow [ 65 ] in sec. 4. 4 : clipping the parameters µ and α within a fixed range. this adjustment stabilizes training and improves final fid performance. in fig. 8, we visualize the norms of intermediate trajectory states during training of the improved tarflow. without clipping, the norms across blocks diverge sharply and continue to grow as training progresses. with clipping, the norms remain stable and well - controlled within a reasonable range. such normalization substantially benefits the training of the reverse model in the downstream. c. 5. improved tarflow cfg for completeness, we also examine classifier - free guidance ( cfg ) designs for tarflow, although this component is orthogonal to our main contributions. in the original tarflow [ 65 ], the reverse update rule at block i is zi t, cfg = zi + 1 t [UNK] ( αi t, cfg ) + µi t, cfg, where guidance is applied to the predicted parameters by αi t, cfg = ( 1 + wt ) αi t ( · | c ) −wt αi t ( · ), µi t, cfg = ( 1 + wt ) µi t ( · | c ) −wt µi t ( · ), 0 1 2 3 4 5 training steps ( ×106 ) 0 10 20 30 40 50 60 squared l2 norm | | x1 | | 2 | | x2 | | 2 | | x3 | | 2 | | x4 | | 2 | | x5 | | 2 | | x6 | | 2 | | x7 | | 2 ( a ) improved tarflow w / o clipping. 0 1 2 3 4 5 training steps ( ×106 ) 0. 2 0. 4 0. 6 0. 8 squared l2 norm | | x1 | | 2 | | x2 | | 2 | | x3 | | 2 | | x4 | | 2 | | x5 | | 2 | | x6 | | 2 | | x7 | | 2 ( b ) improved tarflow w / clipping. figure 8. comparison of intermediate states ’ norm during tarflow training between training with and without clipping. left : without clipping, the norms at different blocks", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 12, "frag_id": 2, "text": "| | 2 | | x6 | | 2 | | x7 | | 2 ( a ) improved tarflow w / o clipping. 0 1 2 3 4 5 training steps ( ×106 ) 0. 2 0. 4 0. 6 0. 8 squared l2 norm | | x1 | | 2 | | x2 | | 2 | | x3 | | 2 | | x4 | | 2 | | x5 | | 2 | | x6 | | 2 | | x7 | | 2 ( b ) improved tarflow w / clipping. figure 8. comparison of intermediate states ’ norm during tarflow training between training with and without clipping. left : without clipping, the norms at different blocks diverge significantly, and continue to increase as training proceeds. right : with clipping, the norms are well controlled within a reasonable range, stabilizing training and improving final fid scores. linear const µ, α z µ, α z online 6. 83 ( 2. 8 ) 6. 82 ( 2. 8 ) 7. 26 ( 1. 3 ) 7. 24 ( 1. 3 ) offline 22. 14 ( 1. 2 ) 22. 03 ( 1. 2 ) 18. 23 ( 1. 0 ) 18. 11 ( 1. 0 ) table 11. improved tarflow cfg ablation. online guidance substantially outperforms the offline variants, whereas the choice of guidance schedule ( linear vs. constant ) and the level at which guidance is applied ( µ, α vs. z ) has only minor impact. numbers in gray parentheses denote the corresponding optimal cfg scale. ( settings : improved tarflow - b / 2, fid w / o cfg : 44. 46. ) with a linearly increasing guidance schedule wt = t t −1w along the token dimension. here, subscript t denotes the token dimension, superscript i denotes the block dimension, and ( · | c ) and ( · ) represent the conditional and unconditional counterparts, respectively. following prior cfg studies in diffusion models, we decompose the design space into three orthogonal choices : schedule. we can replace the original linearly increasing wt with a constant guidance scale : wt = w. space for applying guidance. parameter - space cfg ( µ, α ) vs. pixel - space cfg applied directly to z : zi t, cfg = ( 1 + wt", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 12, "frag_id": 3, "text": "cfg : 44. 46. ) with a linearly increasing guidance schedule wt = t t −1w along the token dimension. here, subscript t denotes the token dimension, superscript i denotes the block dimension, and ( · | c ) and ( · ) represent the conditional and unconditional counterparts, respectively. following prior cfg studies in diffusion models, we decompose the design space into three orthogonal choices : schedule. we can replace the original linearly increasing wt with a constant guidance scale : wt = w. space for applying guidance. parameter - space cfg ( µ, α ) vs. pixel - space cfg applied directly to z : zi t, cfg = ( 1 + wt ) zi t ( · | c ) −wtzi t ( · ). we denote these two settings by “ µ, α ” and “ z ”, respectively. 12", "token_count": 186}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 13, "frag_id": 0, "text": "figure 9. inpainting. biflow enables efficient image inpainting by leveraging its bidirectional mapping between images and noise. by resampling the masked part of the noise, biflow can perform training - free inpainting on various image masks. each triplet contains ground - truth image ( left ), masked image ( middle ), and reconstructed image ( right ). online vs. offline. we distinguish between online and offline cfg strategies. the online approach ( tarflow ’ s practice ) applies guidance at each generation step ; the offline approach generates the entire conditional and unconditional sequences independently and performs extrapolation only once on the final outputs. the difference lies only in how guidance interacts with intermediate states. while both approaches have similar runtimes, tab. 11 shows that online cfg significantly outperforms the offline variant. regarding other hyperparameters, a linear schedule offers a slight advantage over a constant one, while applying guidance in parameter space versus pixel space yields similar performance. overall, tarflow ’ s original cfg formulation is close to optimal. based on these results, we use the original tarflow cfg formulation ( online, linear, parameter - space ) as our baseline. it is important to note that this cfg setting only affects the inference quality of the forward model ; the training of our biflow reverse model always relies on the unguided forward trajectory. d. training - free image editing biflow naturally supports several training - free image editing applications by explicitly modeling a bidirectional mapping between the data and noise domains. we showcase two representative applications : inpainting and class editing. for brevity, we omit the vae encoder / decoder in the following descriptions, as they always serve as pre - / post - processing steps in our experiments. inpainting. the forward model fθ encodes an image x into noise z = fθ ( x ). we empirically observe that localized perturbations in z predominantly affect corresponding spatial regions in the reconstructed image. egyptian cat −→ kit fox hen −→ flamingo tennis ball −→ golden retriever daisy −→ custard apple figure 10. class editing. biflow constructs an explicit bidirectional mapping between images and noise. with this property, biflow is able to conduct training - free class editing by modifying only the label condition in the forward and reverse", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 13, "frag_id": 1, "text": "the vae encoder / decoder in the following descriptions, as they always serve as pre - / post - processing steps in our experiments. inpainting. the forward model fθ encodes an image x into noise z = fθ ( x ). we empirically observe that localized perturbations in z predominantly affect corresponding spatial regions in the reconstructed image. egyptian cat −→ kit fox hen −→ flamingo tennis ball −→ golden retriever daisy −→ custard apple figure 10. class editing. biflow constructs an explicit bidirectional mapping between images and noise. with this property, biflow is able to conduct training - free class editing by modifying only the label condition in the forward and reverse process. based on this property, biflow enables inpainting with an arbitrary binary mask m ∈ { 0, 1 } h×w. given a masked image xmask = m [UNK], we first map it to the noise domain using the forward model : zmask = fθ ( xmask ). we then resample the masked portion of the prior as z ′ = m [UNK] + ( 1 −m ) [UNK], [UNK] [UNK] ( 0, i ). finally, the modified noise z ′ is mapped back to the image domain by the reverse model [UNK]. this procedure fills the masked region with content coherent with the context. representative examples are shown in fig. 9. class editing. the reverse model [UNK] allows us to generate images from noise z under different class conditions. for a fixed z, changing the class label c primarily modifies the class - dependent appearance while largely preserving the global spatial structure. concretely, given an image x with label c, we obtain its prior variable z = fθ ( x | c ), and reconstruct it using a different label c ′, writing x ′ = [UNK] ( z | c ′ ). as illustrated in fig. 10, biflow effectively alters class - specific attributes while maintaining the overall structure, enabling intuitive class editing without retraining. efficiency. both inpainting and class editing require only a single forward pass from data to noise and a single reverse pass from noise to data, making biflow a lightweight and efficient tool for training - free image manipulation. e. visualizations we provide uncurated 1 - nfe generation results of biflow - b / 2 in fig. 11 to fig. 13. acknowledgements. we greatly thank", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 13, "frag_id": 2, "text": "its prior variable z = fθ ( x | c ), and reconstruct it using a different label c ′, writing x ′ = [UNK] ( z | c ′ ). as illustrated in fig. 10, biflow effectively alters class - specific attributes while maintaining the overall structure, enabling intuitive class editing without retraining. efficiency. both inpainting and class editing require only a single forward pass from data to noise and a single reverse pass from noise to data, making biflow a lightweight and efficient tool for training - free image manipulation. e. visualizations we provide uncurated 1 - nfe generation results of biflow - b / 2 in fig. 11 to fig. 13. acknowledgements. we greatly thank google tpu research cloud ( trc ) for granting us access to tpus. q. sun, x. wang, z. jiang, and h. zhao are supported by the mit undergraduate research opportunities program ( urop ). we thank zhengyang geng, tianhong li, and our other group members for their helpful discussions and feedback on the draft. 13", "token_count": 224}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 14, "frag_id": 0, "text": "references [ 1 ] michael s. albergo and eric vanden - eijnden. building normalizing flows with stochastic interpolants. in iclr, 2023. [ 2 ] james bradbury, roy frostig, peter hawkins, matthew james johnson, chris leary, dougal maclaurin, george necula, adam paszke, jake vanderplas, skye wanderman - milne, and qiao zhang. jax : composable transformations of python + numpy programs, 2018. [ 3 ] andrew brock, jeff donahue, and karen simonyan. large scale gan training for high fidelity natural image synthesis. in iclr, 2018. [ 4 ] huiwen chang, han zhang, lu jiang, ce liu, and william t. freeman. maskgit : masked generative image transformer. in cvpr, 2022. [ 5 ] huayu chen, kai jiang, kaiwen zheng, jianfei chen, hang su, and jun zhu. visual generation without guidance. in icml, 2025. [ 6 ] hansheng chen, kai zhang, hao tan, leonidas guibas, gordon wetzstein, and sai bi. pi - flow : policy - based fewstep generation via imitation distillation. arxiv preprint arxiv : 2510. 14974, 2025. [ 7 ] ricky t. q. chen, yulia rubanova, jesse bettencourt, and david duvenaud. neural ordinary differential equations. in neurips, 2018. [ 8 ] jia deng, wei dong, richard socher, li - jia li, kai li, and li fei - fei. imagenet : a large - scale hierarchical image database. in cvpr, 2009. [ 9 ] prafulla dhariwal and alex nichol. diffusion models beat gans on image synthesis. in neurips, 2021. [ 10 ] laurent dinh, david krueger, and yoshua bengio. nice : non - linear independent components estimation. in iclr workshop, 2015. [ 11 ] laurent dinh, jascha sohl - dickstein, and samy bengio. density estimation using real nvp. in iclr, 2017. [ 12 ] alexey dosovitskiy, lucas beyer, alexander kolesnikov, dirk weissenborn, xiaohua zhai, thomas unterthi", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 14, "frag_id": 1, "text": ": a large - scale hierarchical image database. in cvpr, 2009. [ 9 ] prafulla dhariwal and alex nichol. diffusion models beat gans on image synthesis. in neurips, 2021. [ 10 ] laurent dinh, david krueger, and yoshua bengio. nice : non - linear independent components estimation. in iclr workshop, 2015. [ 11 ] laurent dinh, jascha sohl - dickstein, and samy bengio. density estimation using real nvp. in iclr, 2017. [ 12 ] alexey dosovitskiy, lucas beyer, alexander kolesnikov, dirk weissenborn, xiaohua zhai, thomas unterthiner, mostafa dehghani, matthias minderer, georg heigold, sylvain gelly, jakob uszkoreit, and neil houlsby. an image is worth 16x16 words : transformers for image recognition at scale. in iclr, 2021. [ 13 ] patrick esser, sumith kulal, andreas blattmann, rahim entezari, jonas m¨uller, harry saini, yam levi, dominik lorenz, axel sauer, frederic boesel, dustin podell, tim dockhorn, zion english, kyle lacey, alex goodwin, yannik marek, and robin rombach. scaling rectified flow transformers for high - resolution image synthesis. in icml, 2024. [ 14 ] chris finlay, j¨orn - henrik jacobsen, levon nurbekyan, and adam m oberman. how to train your neural ode : the world of jacobian and kinetic regularization. in icml, 2020. [ 15 ] kevin frans, danijar hafner, sergey levine, and pieter abbeel. one step diffusion via shortcut models. in iclr, 2024. [ 16 ] zhengyang geng, ashwini pokle, weijian luo, justin lin, and j. zico kolter. consistency models made easy. in iclr, 2024. [ 17 ] zhengyang geng, mingyang deng, xingjian bai, j. zico kolter, and kaiming he. mean flows for one - step generative modeling. in neurips, 2025. [ 18 ] zhengyang geng, yiyang lu, zongze", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 14, "frag_id": 2, "text": "regularization. in icml, 2020. [ 15 ] kevin frans, danijar hafner, sergey levine, and pieter abbeel. one step diffusion via shortcut models. in iclr, 2024. [ 16 ] zhengyang geng, ashwini pokle, weijian luo, justin lin, and j. zico kolter. consistency models made easy. in iclr, 2024. [ 17 ] zhengyang geng, mingyang deng, xingjian bai, j. zico kolter, and kaiming he. mean flows for one - step generative modeling. in neurips, 2025. [ 18 ] zhengyang geng, yiyang lu, zongze wu, eli shechtman, j. zico kolter, and kaiming he. improved mean flows : on the challenges of fastforward generative models. arxiv preprint arxiv : 2512. 02012, 2025. [ 19 ] arnab ghosh, harkirat singh behl, emilien dupont, philip h. s. torr, and vinay namboodiri. steer : simple temporal regularization for neural ode. in neurips, 2020. [ 20 ] will grathwohl, ricky t. q. chen, jesse bettencourt, ilya sutskever, and david duvenaud. ffjord : free - form continuous dynamics for scalable reversible generative models. in iclr, 2018. [ 21 ] jiatao gu, tianrong chen, david berthelot, huangjie zheng, yuyang wang, ruixiang zhang, laurent dinh, miguel angel bautista, josh susskind, and shuangfei zhai. starflow : scaling latent normalizing flows for high - resolution image synthesis. in neurips, 2025. [ 22 ] kaiming he, xiangyu zhang, shaoqing ren, and jian sun. deep residual learning for image recognition. in cvpr, 2016. [ 23 ] martin heusel, hubert ramsauer, thomas unterthiner, bernhard nessler, and sepp hochreiter. gans trained by a two time - scale update rule converge to a local nash equilibrium. in neurips, 2017. [ 24 ] jonathan ho and tim salimans. classifier - free diffusion guidance. in neurips workshop", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 14, "frag_id": 3, "text": "miguel angel bautista, josh susskind, and shuangfei zhai. starflow : scaling latent normalizing flows for high - resolution image synthesis. in neurips, 2025. [ 22 ] kaiming he, xiangyu zhang, shaoqing ren, and jian sun. deep residual learning for image recognition. in cvpr, 2016. [ 23 ] martin heusel, hubert ramsauer, thomas unterthiner, bernhard nessler, and sepp hochreiter. gans trained by a two time - scale update rule converge to a local nash equilibrium. in neurips, 2017. [ 24 ] jonathan ho and tim salimans. classifier - free diffusion guidance. in neurips workshop, 2021. [ 25 ] jonathan ho, ajay jain, and pieter abbeel. denoising diffusion probabilistic models. in neurips, 2020. [ 26 ] minguk kang, jun - yan zhu, richard zhang, jaesik park, eli shechtman, sylvain paris, and taesung park. scaling up gans for text - to - image synthesis. in cvpr, 2023. [ 27 ] tero karras, miika aittala, samuli laine, erik h¨ark¨onen, janne hellsten, jaakko lehtinen, and timo aila. alias - free generative adversarial networks. in neurips, 2021. [ 28 ] diederik p. kingma and jimmy ba. adam : a method for stochastic optimization. in iclr, 2015. [ 29 ] diederik p. kingma and prafulla dhariwal. glow : generative flow with invertible 1x1 convolutions. in neurips, 2018. [ 30 ] diederik p. kingma, tim salimans, rafal jozefowicz, xi chen, ilya sutskever, and max welling. improved variational inference with inverse autoregressive flow. in neurips, 2016. [ 31 ] tuomas kynk¨a¨anniemi, miika aittala, tero karras, samuli laine, timo aila, and jaakko lehtinen. applying guidance in a limited interval improves sample and distribution quality in diffusion models. in neurips, 2024. [ 32 ]", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 14, "frag_id": 4, "text": ". glow : generative flow with invertible 1x1 convolutions. in neurips, 2018. [ 30 ] diederik p. kingma, tim salimans, rafal jozefowicz, xi chen, ilya sutskever, and max welling. improved variational inference with inverse autoregressive flow. in neurips, 2016. [ 31 ] tuomas kynk¨a¨anniemi, miika aittala, tero karras, samuli laine, timo aila, and jaakko lehtinen. applying guidance in a limited interval improves sample and distribution quality in diffusion models. in neurips, 2024. [ 32 ] kyungmin lee, sihyun yu, and jinwoo shin. decoupled meanflow : turning flow models into flow maps for accelerated sampling. arxiv preprint arxiv : 2510. 24474, 2025. [ 33 ] tianhong li and kaiming he. back to basics : let denoising generative models denoise. arxiv preprint arxiv : 2511. 13720, 2025. [ 34 ] tianhong li, dina katabi, and kaiming he. return of unconditional generation : a self - supervised representation generation method. in neurips, 2024. [ 35 ] tianhong li, yonglong tian, he li, mingyang deng, and kaiming he. autoregressive image generation without vector quantization. in neurips, 2024. 14", "token_count": 330}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 15, "frag_id": 0, "text": "[ 36 ] yaron lipman, ricky t. q. chen, heli ben - hamu, maximilian nickel, and matt le. flow matching for generative modeling. in iclr, 2023. [ 37 ] xingchao liu, chengyue gong, and qiang liu. flow straight and fast : learning to generate and transfer data with rectified flow. in iclr, 2023. [ 38 ] nanye ma, mark goldstein, michael s. albergo, nicholas m. boffi, eric vanden - eijnden, and saining xie. sit : exploring flow and diffusion - based generative models with scalable interpolant transformers. in eccv, 2024. [ 39 ] chenlin meng, robin rombach, ruiqi gao, diederik p. kingma, stefano ermon, jonathan ho, and tim salimans. on distillation of guided diffusion models. in cvpr, 2023. [ 40 ] maxime oquab, timoth´ee darcet, th´eo moutakanni, huy vo, marc szafraniec, vasil khalidov, pierre fernandez, daniel haziza, francisco massa, alaaeldin el - nouby, mahmoud assran, nicolas ballas, wojciech galuba, russell howes, po - yao huang, shang - wen li, ishan misra, michael rabbat, vasu sharma, gabriel synnaeve, hu xu, herv´e jegou, julien mairal, patrick labatut, armand joulin, and piotr bojanowski. dinov2 : learning robust visual features without supervision. tmlr, 2024. [ 41 ] george papamakarios, theo pavlakou, and iain murray. masked autoregressive flow for density estimation. in neurips, 2017. [ 42 ] william peebles and saining xie. scalable diffusion models with transformers. in iccv, 2023. [ 43 ] yansong peng, kai zhu, yu liu, pingyu wu, hebei li, xiaoyan sun, and feng wu. flow - anchored consistency models. arxiv preprint arxiv : 2507. 03738, 2025. [ 44 ] sucheng ren, qihang yu, ju he, xiaohui shen, alan l", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 15, "frag_id": 1, "text": "features without supervision. tmlr, 2024. [ 41 ] george papamakarios, theo pavlakou, and iain murray. masked autoregressive flow for density estimation. in neurips, 2017. [ 42 ] william peebles and saining xie. scalable diffusion models with transformers. in iccv, 2023. [ 43 ] yansong peng, kai zhu, yu liu, pingyu wu, hebei li, xiaoyan sun, and feng wu. flow - anchored consistency models. arxiv preprint arxiv : 2507. 03738, 2025. [ 44 ] sucheng ren, qihang yu, ju he, xiaohui shen, alan l yuille, and liang - chieh chen. beyond next - token : next - x prediction for autoregressive visual generation. in iccv, 2025. [ 45 ] danilo jimenez rezende and shakir mohamed. variational inference with normalizing flows. in icml, 2015. [ 46 ] robin rombach, andreas blattmann, dominik lorenz, patrick esser, and bj¨orn ommer. high - resolution image synthesis with latent diffusion models. in cvpr, 2022. [ 47 ] olaf ronneberger, philipp fischer, and thomas brox. u - net : convolutional networks for biomedical image segmentation. in miccai, 2015. [ 48 ] tim salimans, ian goodfellow, wojciech zaremba, vicki cheung, alec radford, and xi chen. improved techniques for training gans. in neurips, 2016. [ 49 ] axel sauer, katja schwarz, and andreas geiger. stylegan - xl : scaling stylegan to large diverse datasets. in siggraph, 2022. [ 50 ] karen simonyan and andrew zisserman. very deep convolutional networks for large - scale image recognition. in iclr, 2015. [ 51 ] jiaming song, chenlin meng, and stefano ermon. denoising diffusion implicit models. in iclr, 2020. [ 52 ] yang song and prafulla dhariwal. improved techniques for training consistency models. in iclr, 2023. [ 53 ] jianlin su, yu lu, shengfeng pan, ahmed murtadha, bo wen, and yunfeng", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 15, "frag_id": 2, "text": "axel sauer, katja schwarz, and andreas geiger. stylegan - xl : scaling stylegan to large diverse datasets. in siggraph, 2022. [ 50 ] karen simonyan and andrew zisserman. very deep convolutional networks for large - scale image recognition. in iclr, 2015. [ 51 ] jiaming song, chenlin meng, and stefano ermon. denoising diffusion implicit models. in iclr, 2020. [ 52 ] yang song and prafulla dhariwal. improved techniques for training consistency models. in iclr, 2023. [ 53 ] jianlin su, yu lu, shengfeng pan, ahmed murtadha, bo wen, and yunfeng liu. roformer : enhanced transformer with rotary position embedding. neurocomputing, 2024. [ 54 ] christian szegedy, vincent vanhoucke, sergey ioffe, jon shlens, and zbigniew wojna. rethinking the inception architecture for computer vision. in cvpr, 2016. [ 55 ] zhicong tang, jianmin bao, dong chen, and baining guo. diffusion models without classifier - free guidance. arxiv preprint arxiv : 2502. 12154, 2025. [ 56 ] keyu tian, yi jiang, zehuan yuan, bingyue peng, and liwei wang. visual autoregressive modeling : scalable image generation via next - scale prediction. in neurips, 2024. [ 57 ] jakub m. tomczak and max welling. improving variational auto - encoders using householder flow. arxiv preprint arxiv : 1611. 09630, 2016. [ 58 ] ashish vaswani, noam shazeer, niki parmar, jakob uszkoreit, llion jones, aidan n. gomez, łukasz kaiser, and illia polosukhin. attention is all you need. in neurips, 2017. [ 59 ] shuai wang, zhi tian, weilin huang, and limin wang. ddt : decoupled diffusion transformer. arxiv preprint arxiv : 2504. 05741, 2025. [ 60 ] zidong wang, yiyuan zhang, xiaoyu yue, xiangyu yue", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 15, "frag_id": 3, "text": "flow. arxiv preprint arxiv : 1611. 09630, 2016. [ 58 ] ashish vaswani, noam shazeer, niki parmar, jakob uszkoreit, llion jones, aidan n. gomez, łukasz kaiser, and illia polosukhin. attention is all you need. in neurips, 2017. [ 59 ] shuai wang, zhi tian, weilin huang, and limin wang. ddt : decoupled diffusion transformer. arxiv preprint arxiv : 2504. 05741, 2025. [ 60 ] zidong wang, yiyuan zhang, xiaoyu yue, xiangyu yue, yangguang li, wanli ouyang, and lei bai. transition models : rethinking the generative learning objective. arxiv preprint arxiv : 2509. 04394, 2025. [ 61 ] sanghyun woo, shoubhik debnath, ronghang hu, xinlei chen, zhuang liu, in so kweon, and saining xie. convnext v2 : co - designing and scaling convnets with masked autoencoders. in cvpr, 2023. [ 62 ] jingfeng yao, bin yang, and xinggang wang. reconstruction vs. generation : taming optimization dilemma in latent diffusion models. in cvpr, 2025. [ 63 ] qihang yu, ju he, xueqing deng, xiaohui shen, and liangchieh chen. randomized autoregressive visual generation. in iccv, 2025. [ 64 ] sihyun yu, sangkyung kwak, huiwon jang, jongheon jeong, jonathan huang, jinwoo shin, and saining xie. representation alignment for generation : training diffusion transformers is easier than you think. in iclr, 2025. [ 65 ] shuangfei zhai, ruixiang zhang, preetum nakkiran, david berthelot, jiatao gu, huangjie zheng, tianrong chen, miguel angel bautista, navdeep jaitly, and joshua susskind. normalizing flows are capable generative models. in icml, 2024. [ 66 ] biao zhang and rico sennrich. root mean square layer normalization.", "token_count": 500}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 15, "frag_id": 4, "text": "] sihyun yu, sangkyung kwak, huiwon jang, jongheon jeong, jonathan huang, jinwoo shin, and saining xie. representation alignment for generation : training diffusion transformers is easier than you think. in iclr, 2025. [ 65 ] shuangfei zhai, ruixiang zhang, preetum nakkiran, david berthelot, jiatao gu, huangjie zheng, tianrong chen, miguel angel bautista, navdeep jaitly, and joshua susskind. normalizing flows are capable generative models. in icml, 2024. [ 66 ] biao zhang and rico sennrich. root mean square layer normalization. in neurips, 2019. [ 67 ] huijie zhang, aliaksandr siarohin, willi menapace, michael vasilkovsky, sergey tulyakov, qing qu, and ivan skorokhodov. alphaflow : understanding and improving meanflow models. arxiv preprint arxiv : 2510. 20771, 2025. [ 68 ] richard zhang, phillip isola, alexei a. efros, eli shechtman, and oliver wang. the unreasonable effectiveness of deep features as a perceptual metric. in cvpr, 2018. [ 69 ] boyang zheng, nanye ma, shengbang tong, and saining xie. diffusion transformers with representation autoencoders. arxiv preprint arxiv : 2510. 11690, 2025. [ 70 ] linqi zhou, stefano ermon, and jiaming song. inductive moment matching. in icml, 2025. 15", "token_count": 356}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 16, "frag_id": 0, "text": "class 12 : house finch, linnet, carpodacus mexicanus class 81 : ptarmigan class 207 : golden retriever class 279 : arctic fox, white fox, alopex lagopus class 309 : bee class 323 : monarch, monarch butterfly, milkweed butterfly, danaus plexippus class 327 : starfish, sea star class 417 : balloon figure 11. uncurated 1 - nfe class - conditional generation samples of biflow - b / 2 on imagenet 256×256. cfg scale : 2. 0 16", "token_count": 113}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 17, "frag_id": 0, "text": "class 425 : barn class 437 : beacon, lighthouse, beacon light, pharos class 449 : boathouse class 497 : church, church building class 538 : dome class 554 : fireboat class 562 : fountain class 616 : knot figure 12. uncurated 1 - nfe class - conditional generation samples of biflow - b / 2 on imagenet 256×256. cfg scale : 2. 0 17", "token_count": 91}
{"doc_id": "arxiv_251210953_bidirectional_normalizing_flow", "page": 18, "frag_id": 0, "text": "class 646 : maze, labyrinth class 649 : megalith, megalithic structure class 888 : viaduct class 952 : fig class 970 : alp class 973 : coral reef class 975 : lakeside, lakeshore class 985 : daisy figure 13. uncurated 1 - nfe class - conditional generation samples of biflow - b / 2 on imagenet 256×256. cfg scale : 2. 0 18", "token_count": 91}
