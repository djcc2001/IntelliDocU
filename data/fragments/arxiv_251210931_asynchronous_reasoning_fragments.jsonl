{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 1, "frag_id": 0, "text": "asynchronous reasoning : training - free interactive thinking llms george yakushev * 1 2 nataliia babina * 3 masoud vahid dastgerdi * 2 vyacheslav zhdanovskiy * 1 alina shutova 1 2 denis kuznedelev 1 abstract many state - of - the - art llms are trained to think before giving their answer. reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive : given a new input, a model must stop thinking before it can respond. real - world use cases such as voicebased or embedded assistants require an llm agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. in contrast, humans can listen, think, and act asynchronously : we begin thinking about the problem while reading it and continue thinking while formulating the answer. in this work, we augment llms capable of reasoning to operate in a similar way without additional training. our method uses the properties of rotary embeddings to enable llms built for sequential interactions to simultaneously think, listen, and generate outputs. we evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinkingaugmented answers in real time, reducing time to first non - thinking token from minutes to ≤5s. and the overall real - time delays by 6−11×. 1. introduction modern large language models ( llms ) solve complex tasks using inference - time computation mechanisms [ 1, 2, 3 ], such as chain - of - thought reasoning [ 4, 5, 6, 7 ] and agentic tool use [ 10, 11, 12, 13 ]. recent models, both proprietary [ 18, 19, 20 ] and open - weights [ 21, 22, 23 ], are explicitly trained for reasoning and agentic capabilities. as we trust llms with harder problems [ 24, 25 ], their ability to “ think ” becomes ever more important. the current dominant strategy for llm reasoning is the read - think - answer cycle : the model encodes a given problem, then generates chain - of - thought reasoning, possibly * equal contribution 1yandex 2hse university 3the university of tokyo. preprint, work in progress. correspondence to : denis kuznedelev < dkuznedelev @ yandex - team. ru >.", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 1, "frag_id": 1, "text": "12, 13 ]. recent models, both proprietary [ 18, 19, 20 ] and open - weights [ 21, 22, 23 ], are explicitly trained for reasoning and agentic capabilities. as we trust llms with harder problems [ 24, 25 ], their ability to “ think ” becomes ever more important. the current dominant strategy for llm reasoning is the read - think - answer cycle : the model encodes a given problem, then generates chain - of - thought reasoning, possibly * equal contribution 1yandex 2hse university 3the university of tokyo. preprint, work in progress. correspondence to : denis kuznedelev < dkuznedelev @ yandex - team. ru >. calls tools, and then formulates the final answer [ 18, 21, 23 ]. this fits naturally with the sequential view of llms as nexttoken prediction models. however, this also means that the llm must follow a rigid turn structure that can limit their flexibility. the “ thinking ” phase can take minutes of real time, during which the agent does not get new information or output its current results. unlike llm agents, people have an innate ability to think asynchronously [ 26, 27, 28, 29 ]. when working on a problem, we can begin solving it even before we have heard its entire statement, and can start talking ( or acting ) while still completing our solution. such “ multitasking ” is not always easy or efficient [ 30 ], but it allows us to effectively operate in a dynamic environment [ 31 ]. similarly, artificial agents often need real - time ability to change course of action. a voice assistant is expected to maintain conversation in real time [ 32, 33, 34, 35, 36, 37, 38 ]. an embodied agent ’ s vla model [ 39, 40, 41 ] needs to quickly adjust to new inputs. even fully text - based “ deep research ” agents benefit from interactive communication with the user [ 42 ]. however, the current read - think - answer cycle is inherently non - interactive. during the thinking phase, if an agent receives new inputs or must take action, it can either stop reasoning, discarding any incomplete thoughts, or wait until it completes, sacrificing interactivity. as a result, many real - time llm applications do not fully benefit from inference - time compute. in this work, we propose a technique that enables", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 1, "frag_id": 2, "text": "real time [ 32, 33, 34, 35, 36, 37, 38 ]. an embodied agent ’ s vla model [ 39, 40, 41 ] needs to quickly adjust to new inputs. even fully text - based “ deep research ” agents benefit from interactive communication with the user [ 42 ]. however, the current read - think - answer cycle is inherently non - interactive. during the thinking phase, if an agent receives new inputs or must take action, it can either stop reasoning, discarding any incomplete thoughts, or wait until it completes, sacrificing interactivity. as a result, many real - time llm applications do not fully benefit from inference - time compute. in this work, we propose a technique that enables asynchronous llm reasoning. instead of retraining the llm to satisfy each specific degree of interactivity, we propose a training - free approach that modifies existing models. our approach uses three concurrent streams of tokens : user inputs, private thoughts, and public response, which can be updated in real - time. we rely on geometric properties of rotary positional embeddings to make the llm perceive these streams as a single contiguous sequence without additional training. the model itself can decide whether it should continue talking or pause and think, depending on the current state of the three streams. the resulting asynchronous reasoning can be formulated as standard llm inference with a modified attention cache, making it possible to integrate into efficient llm inference frameworks [ 43, 44 ]. 1 arxiv : 2512. 10931v1 [ cs. lg ] 11 dec 2025", "token_count": 335}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 2, "frag_id": 0, "text": "task : a bat and a ball are 1. 10 $. and the bat is 1 $ more than the ball. how much is the ball? let the ball cost x dollars. then, x + ( x + 1 ) = 1. 10, simplified to 2x + 1 = 1. 10, x = 0. 10 / 2 = 0. 05. let me check that. if the ball costs $ 0. 05... let me solve this for you. the price for the ball is $ 0. 05. would you like me to explain... wait thinker pauses the writer...... resumes the writer. thinker : writer : inference steps figure 1 : the intuitive explanation of asynchronous reasoning : the llm generates its response concurrently with thinking. if the thinking stream needs additional time, it can pause the response writer until the next reasoning step is ready. our main contributions can be summarized as follows : • we propose asyncreasoning, a zero - shot method that allows existing reasoning llms to think, write outputs and encode additional inputs concurrently. our approach relies on model - agnostic concurrent attention and prompting, making it easy to adapt for new models. • we evaluate the proposed approach on real - time math, common - sense and safety reasoning. our experiments demonstrate that the proposed approach lets the llm overlap thinking and answering, reducing the userperceived delay by over 9 on mathematical and common sense reasoning tasks. when prompted to think about safety, asyncreasoning allows the llm to stream realtime outputs on benign requests, while considering the safety implications in a private thinking stream that can pause potentially harmful outputs. • we release our reference implementation1 of asyncreasoning, including gpu kernels for concurrent attention. we also provide a minimal voice assistant with asynchronous thinking capabilities to demonstrate it in action. 2. related work 2. 1. real - time llm applications modern llm agents are deployed in a broad range of applications that require varying degrees of interactivity. for instance, a background code review agent can pause and think for several minutes, whereas a real - time voice assistant cannot. here, we briefly review several llm applications that require quick or interactive responses. voice assistants. recent works [ 33, 34 ] and industry releases [ 32, 45, 46 ] use llm agents as interactive voice assistants that talk to users in real -", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 2, "frag_id": 1, "text": "reference implementation1 of asyncreasoning, including gpu kernels for concurrent attention. we also provide a minimal voice assistant with asynchronous thinking capabilities to demonstrate it in action. 2. related work 2. 1. real - time llm applications modern llm agents are deployed in a broad range of applications that require varying degrees of interactivity. for instance, a background code review agent can pause and think for several minutes, whereas a real - time voice assistant cannot. here, we briefly review several llm applications that require quick or interactive responses. voice assistants. recent works [ 33, 34 ] and industry releases [ 32, 45, 46 ] use llm agents as interactive voice assistants that talk to users in real - time, often through their phones or edge devices, or partake in a group conference [ 47, 48 ]. compared to their text - based counterparts, voice assistants require faster reaction time, with user often adding new information while the agent is thinking. 1see github. com / yandex - research / asyncreasoning there are two main strategies to building voice assistants : modular and end - to - end. the first strategy pipes automated speech recognition ( asr ) [ 49, 50, 51, 52 ] into a text - based llm, then feeds its response into a text - to - speech ( tts ) system [ 53, 54, 55, 56, 57, 58, 59, 60 ]. the pipeline overlaps llm generation with tts to stream audio in real - time. the second, more recent strategy is using speech language models ( also audio and voice lms ) that are trained to process and generate audio natively [ 35, 38, 36, 37 ], allowing them to perceive intonation and non - speech audio. however, that due to constraints on response time, many speech lms are not trained for long - form reasoning, and the thinking optimized lms often do not include speech synthesis2. robotic & virtual agents. another type of llm applications that require interactivity are llm agents with real - time environments. agents controlling robotic systems use multimodal embodied language models [ 41, 39, 62, 63, 64 ] to for action planning or vision - language - action [ 40, 65, 66 ] to control the system directly. aside from robotic systems, similar agents were proposed for videogames [ 67 ], managing operating systems and mobile devices [", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 2, "frag_id": 2, "text": ", 36, 37 ], allowing them to perceive intonation and non - speech audio. however, that due to constraints on response time, many speech lms are not trained for long - form reasoning, and the thinking optimized lms often do not include speech synthesis2. robotic & virtual agents. another type of llm applications that require interactivity are llm agents with real - time environments. agents controlling robotic systems use multimodal embodied language models [ 41, 39, 62, 63, 64 ] to for action planning or vision - language - action [ 40, 65, 66 ] to control the system directly. aside from robotic systems, similar agents were proposed for videogames [ 67 ], managing operating systems and mobile devices [ 68, 69, 70, 71 ]. similarly to voice assistants, embodied agents need to quickly react to new stimuli from the environment. reasoning and safety. another important aspect of llm reasoning is how it interacts with model safety and control [ 72, 73 ]. by default, thinking can both mitigate safety risks and create new ones [ 74, 75, 76 ]. however, when specifically prompted to reason about safety implications of their task, language models can detect and prevent jailbreak attacks [ 77, 78, 79, 80 ]. however, since traditional reasoning delays model response time, which is inconvenient for interactive usage scenarios. in our experiments, we show that llms can reason about safety asynchronously in the background, mitigating jailbreaks without response delays. we discuss reasoning safety further in appendix a. 2for example, in the recent qwen3 - omni model family, the 30b - a3b - instruct can speak, but does not generate < think > blocks, while the 30b - a3b - thinking [ 61 ] has no speech output. 2", "token_count": 378}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 3, "frag_id": 0, "text": "writer block ( read only ) i am in writer mode. my text is visible to the user. we are asked to evaluate the expression x - x² + x³ for x values 5, 6, 7, and 8. let ' s compute each value step by step. for x = 5 : 5 - 5 ^ 2 + 5 ^ 3 = 5 - 25 + 125 thinker view prompt block you are an ai assistant that can think and write outputs concurrently. you can reason in private and your thoughts will be used to form the public response in the background. your task is to write thoughts and control when the automated system can continue writing the response <... >. please reason step by step. task : calculate x - x ^ 2 + x ^ 3 for x = 5, 6, 7, 8. return all 4 answers in \\ \\ boxed { }. system : [ the system will continue writing the response here ] < | im _ end | > < | im _ start > assistant < think > thinker block ( editing ) i am in thinker mode. my text is not visible to the user. the user wants me to calculate <... >. starting with x = 5. the expression is 5 - 5² + 5³. let ' s break it down : 5 squared is 25, and 5 cubed is 125. so substituting those in, it becomes 5 - 25 + 125. calculating that : 5 - 25 is - 20, and then adding 125 gives 105. so for x = 5, the result.... result writer view prompt block you are an ai assistant that can think and write outputs concurrently. you can write outputs for the user based on partial cot that will be continued in the background by an automated system. you should outline what you ' re going to do, then write your response as thoughts progress, but not ahead of your thoughts. task : calculate x - x ^ 2 + x ^ 3 for x = 5, 6, 7, 8. return all 4 answers in \\ \\ boxed { }. writer block ( editing ) i am in writer mode. my text is visible to the user. we are asked to evaluate the expression x - x² + x³ for x values 5, 6, 7, and 8. let ' s compute each value step by step. for x = 5 : 5 - 5 ^ 2 + 5 ^ 3 = 5 - 25 + 125 system : [ additional thoughts", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 3, "frag_id": 1, "text": "be continued in the background by an automated system. you should outline what you ' re going to do, then write your response as thoughts progress, but not ahead of your thoughts. task : calculate x - x ^ 2 + x ^ 3 for x = 5, 6, 7, 8. return all 4 answers in \\ \\ boxed { }. writer block ( editing ) i am in writer mode. my text is visible to the user. we are asked to evaluate the expression x - x² + x³ for x values 5, 6, 7, and 8. let ' s compute each value step by step. for x = 5 : 5 - 5 ^ 2 + 5 ^ 3 = 5 - 25 + 125 system : [ additional thoughts will appear here ] < / think > 125 new tokens are added simultaneously to both blocks result 125 125 < | im _ end | > < | im _ start | > assistant < think > thinker block ( read only ) i am in thinker mode. my text is not visible to the user. the user wants me to calculate <... >. starting with x = 5. the expression is 5 - 5² + 5³. let ' s break it down : 5 squared is 25, and 5 cubed is 125. so substituting those in, it becomes 5 - 25 + 125. calculating that : 5 - 25 is - 20, and then adding 125 gives 105. so for x = 5, the result < | im _ start | > user < | im _ end | > < | im _ start | > assistant < | im _ start | > user result figure 2 : a dual thinker / writer view of the same reasoning task. the two views reuse the same kv cache and generate tokens in parallel. both thinker and writer see the problem in the same sequential formatting that they were trained with. 2. 2. efficient llm reasoning as discussed earlier, there is a wide range of tasks that require llms to reason in real - time. however, most thinking llms [ 18, 21, 81 ] follow a read - think - answer cycle, making them inherently non - interactive. when receiving new information mid - thought, such llms can either interrupt their reasoning to react, but sacrifice any incomplete thought tokens, or continue reasoning non - interactively. recently, there has been a large influx of techniques for efficient reasoning [ 82 ] through more concise chain - of", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 3, "frag_id": 2, "text": "reuse the same kv cache and generate tokens in parallel. both thinker and writer see the problem in the same sequential formatting that they were trained with. 2. 2. efficient llm reasoning as discussed earlier, there is a wide range of tasks that require llms to reason in real - time. however, most thinking llms [ 18, 21, 81 ] follow a read - think - answer cycle, making them inherently non - interactive. when receiving new information mid - thought, such llms can either interrupt their reasoning to react, but sacrifice any incomplete thought tokens, or continue reasoning non - interactively. recently, there has been a large influx of techniques for efficient reasoning [ 82 ] through more concise chain - ofthought [ 83, 84, 85, 86 ], adaptive reasoning effort [ 87, 88, 89, 90 ] or early stopping [ 91, 92, 93 ]. another line of work explores reasoning in parallel, with multiple concurrent llm instances solving different sub - tasks [ 94, 95, 96, 97, 98, 99, 100, 101, 102 ], or parallel tool calling [ 103, 104 ]. reducing reasoning - induced delays several recent studies propose techniques specifically to reduce reasoning delays for real - time applications with partial read overlapping [ 105 ], specialized two - model architectures with fast interactive and slow reasoning modules [ 107 ]. a concurrent work [ 108 ] introduced plantain, a method that finetunes reasoning llms to solve their task with interleaved thinking and talking sub - blocks, making them more interactive. note, however, that all these techniques require specialized fine - tuning or training from scratch, which complicates their adoption. in practice, the requirements for interactive llm use also vary with hardware and software configuration : a model trained for “ real - time ” reasoning on a b200 gpu may cause delays when deployed on slower gpus or with batched inference. therefore, models that were trained for one interactive use may need re - training for different hardware or parameters. in this work, we instead design a lightweight asynchronous reasoning method that does not require training and can be adapted with simple prompting. 3. asyncreasoning to convert an existing reasoning llm into an asynchronous thinker, we need to reformulate the asynchronous thinking process and make it compatible with the standard template the models were trained with. we describe how this", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 3, "frag_id": 3, "text": "requirements for interactive llm use also vary with hardware and software configuration : a model trained for “ real - time ” reasoning on a b200 gpu may cause delays when deployed on slower gpus or with batched inference. therefore, models that were trained for one interactive use may need re - training for different hardware or parameters. in this work, we instead design a lightweight asynchronous reasoning method that does not require training and can be adapted with simple prompting. 3. asyncreasoning to convert an existing reasoning llm into an asynchronous thinker, we need to reformulate the asynchronous thinking process and make it compatible with the standard template the models were trained with. we describe how this can be achieved by dynamically rearranging the model ’ s kv cache so it views multiple asynchronous streams as a single sequence ( section 3. 1 ). in section 3. 2 we discuss mode switching : allowing the llm to alternate between simultaneous writing and waiting for thoughts, depending on the context. finally, we discuss efficient parallel token processing and other implementation details in section 3. 3. 3. 1. dual thinker & writer views the core idea behind our approach is that transformer llms are inherently designed for manipulating sets [ 109, 110 ], and the only thing that makes them into sequence models is their positional encoding [ 111, 112, 113 ]. in order to change the token generation order, we do not need to physically rearrange tokens in memory. instead, it is sufficient to change positional relations between tokens, since the rest of the transformer architecture is already position - invariant. at each inference step, asyncreasoning manipulates positional encodings to rearrange past tokens into a different order for thinking and for writing the response. public response tokens see ( partial ) private thoughts as they were generated in a standard read - think - answer cycle. in turn, tokens within the < think > block see response tokens as they were generated during the previous conversation turn. we illustrate this dual view in figure 2. this dual view allows both “ streams ” ( thinking and response ) to immediately attend to each others ’ tokens as they are generated. the response tokens can “ see ” the latest private thoughts and summarize them without synchronization delays. likewise, the thinking “ stream ” sees the current response tokens and can", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 3, "frag_id": 4, "text": "##oning manipulates positional encodings to rearrange past tokens into a different order for thinking and for writing the response. public response tokens see ( partial ) private thoughts as they were generated in a standard read - think - answer cycle. in turn, tokens within the < think > block see response tokens as they were generated during the previous conversation turn. we illustrate this dual view in figure 2. this dual view allows both “ streams ” ( thinking and response ) to immediately attend to each others ’ tokens as they are generated. the response tokens can “ see ” the latest private thoughts and summarize them without synchronization delays. likewise, the thinking “ stream ” sees the current response tokens and can pause it if it needs to think longer. 3", "token_count": 160}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 4, "frag_id": 0, "text": "this also allows our implementation to encode each generated token exactly once and rearrange tokens using the geometry of positional embeddings ( see section 3. 3 ). 3. 2. mode switching another important challenge of asynchronous thinking is deciding when to synchronize. depending on the task at hand, the thinking stream may encounter a sub - task that needs longer “ thinking time ” to complete. if this is the case, the agent should briefly pause3 writing the response and wait for the chain of thought to progress. asyncreasoning lets the llm itself determine synchronization points. to achieve this, we periodically ask the model if its private thoughts are still ahead of the public response, or if it should pause and think more. from a technical point of view, we periodically insert a special prompt4 into the thinking stream and compare the probability of “ yes ” vs. “ no ” as the next token. if the “ yes ” token is more likely, we keep thinking asynchronously. if the “ no ” token wins out, we pause the response stream until the model “ yes ” again. in our current implementation, we insert this question at the end of every paragraph or after every t = 20 thinking tokens, whichever comes first. crucially, after the model gives its “ yes ” or “ no ” response, we remove these prompts from view ( from the kv cache ) so that they do not interfere with the model ’ s chain - of - thought. we compare different mode switching prompts in section 4. 1. overall, we found that existing reasoning llms can already control asynchronous reasoning, though they do sometimes make mistakes. it is possible to design more sophisticated thinking mechanisms, such as allowing the llm to reason about mode switching in parallel instead of answering immediately. additionally, one could introduce a mode - switching classifier “ head ” to decide when to pause responding. however, we opt to keep asyncreasoning simple and training - free for initial experiments and defer further study of mode switching to future work. 3. 3. implementation details to summarize, asyncreasoning arranges the thinking and response tokens in different order, depending on the task, processes both streams in parallel, and periodically prompts the model to decide if it should pause and think. as a result, our algorithm alternates between two modes : either it", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 4, "frag_id": 1, "text": "mistakes. it is possible to design more sophisticated thinking mechanisms, such as allowing the llm to reason about mode switching in parallel instead of answering immediately. additionally, one could introduce a mode - switching classifier “ head ” to decide when to pause responding. however, we opt to keep asyncreasoning simple and training - free for initial experiments and defer further study of mode switching to future work. 3. 3. implementation details to summarize, asyncreasoning arranges the thinking and response tokens in different order, depending on the task, processes both streams in parallel, and periodically prompts the model to decide if it should pause and think. as a result, our algorithm alternates between two modes : either it thinks and writes tokens concurrently, or it simply thinks while the writing is paused. when only one stream is active, asyncreasoning is equivalent to standard sequential llm inference with a combined kv cache. we focus the rest of this section processing multiple concurrent tokens streams. 3for voice assistants, it may be better to communicate “ hmm, let me think about it... ”, but we don ’ t do that in our evaluations. 4 \"... \\ n \\ nwait, are my thoughts ahead of the response by enough to continue writing it? ( yes / no ) : \" we implement concurrent thinking & writing by creating a custom key - value cache and manipulating positional embeddings to account for the dual views from figure 2. the main purpose of this algorithm is to avoid redundant computation and kv cache bloat. instead of encoding tokens twice for thinking and writing view, we process each token exactly once and keep one kv cache entry that is “ viewed ” from different relative positions. this optimization is inspired by a similar rotation trick proposed in hogwild! inference [ 97 ]. key - value cache structure. to implement different positional views, we split the model ’ s kv cache into contiguous “ blocks ” ( tensors ) : the inputs, the thinking stream, and the output stream. as new tokens are generated or added by the user, we store them in the corresponding cache block using positional encodings relative to the block start5. during attention forward pass, we concatenate dot products between the query and all cache blocks, but we transform the attention query differently for each block to simulate difference in token positions. that way, the same set of attention blocks can", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 4, "frag_id": 2, "text": "is “ viewed ” from different relative positions. this optimization is inspired by a similar rotation trick proposed in hogwild! inference [ 97 ]. key - value cache structure. to implement different positional views, we split the model ’ s kv cache into contiguous “ blocks ” ( tensors ) : the inputs, the thinking stream, and the output stream. as new tokens are generated or added by the user, we store them in the corresponding cache block using positional encodings relative to the block start5. during attention forward pass, we concatenate dot products between the query and all cache blocks, but we transform the attention query differently for each block to simulate difference in token positions. that way, the same set of attention blocks can be combined for both thinking and writing views from figure 2 without duplicating memory. manipulating positional information. almost all modern llms use some form of relative positional information [ 111, 113, 112 ]. the most popular variant is rotary positional embeddings ( rope ) [ 113 ] that rotates query and key vectors by an angle proportional to their index in text before computing the scaled dot product attention. note, however, that if both query and key are rotated by the same angle, their dot product does not change. thus, the attention outputs only depend on the difference between query and key positions. in other words, rotating attention keys by + [UNK] is equivalent to rotating the query by [UNK]. we take advantage of this property to avoid rotating the entire kv cache on each inference step. instead, we keep track of the starting positions for each block and rotate attention queries. suppose there are three contiguous kv blocks : prompting with p tokens, thinking with t tokens, and writing with w tokens. when viewed contiguously ( ptw ), the difference between the most recent writer token and the thinker block is t + w−1 tokens. thus, when running the forward pass using the writer ’ s next token, we rotate its query by the rope angle corresponding to position t + w−1. in contrast, when the writer looks at their own tokens, it will use the query position w−1. the same principle is applied for all query - key pairs. formally, let ρ ( q, i ) denote applying rope for vector q at position i. the writer attends to blocks p, t, w : a : = ρ ( q, iq ) · h ρ ( kp, ip", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 4, "frag_id": 3, "text": ", and writing with w tokens. when viewed contiguously ( ptw ), the difference between the most recent writer token and the thinker block is t + w−1 tokens. thus, when running the forward pass using the writer ’ s next token, we rotate its query by the rope angle corresponding to position t + w−1. in contrast, when the writer looks at their own tokens, it will use the query position w−1. the same principle is applied for all query - key pairs. formally, let ρ ( q, i ) denote applying rope for vector q at position i. the writer attends to blocks p, t, w : a : = ρ ( q, iq ) · h ρ ( kp, ip k ), ρ ( kt, it k ), ρ ( kw, iw k ) i, where 5for example, given a model with rope embeddings, a kv cache will always store the 5th response token rotated for position 5, regardless of how many thinking tokens precede it. 4", "token_count": 214}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 5, "frag_id": 0, "text": "p p + t p + t + w 0 query writer thinker block writer view positions positions prompt block writer block thinker view prompt block p + w p + w + t p 0 query thinker writer block thinker block implementation query thinker query thinker query thinker query thinker t t + w t + w + p offset : 0 thinker block writer block prompt block figure 3 : concurrent thinking and writing implemented as batched inference. the newly added tokens attend to cache blocks with additional query rotations. the checkered areas represent tokens that are not visible in the current view. h brackets i denote concatenation, iq is the query position, ip k, it k, iw k are cache block positions from the writer ’ s point of view ( see figure 3 ) and kp, t, w are the corresponding key vectors. then, we can equivalently compute attention as : a : = h ρ ( q, iq−ip k ) kp, ρ ( q, iq−it k ) kt, ρ ( q, iq−iw k ) kw i. this reformulation allows us to compute kp, t, w once, store it in kv cache and only rotate attention queries for the currently processed tokens during each forward pass. technical considerations. to summarize, our implementation consists of the custom kv cache and an attention kernel that uses the query rotation trick above. in practice, we use more than 3 kv blocks : in addition to the prompt, thinking and response tokens, we also have short linker tokens that fit between thinking writing blocks. these linkers are implemented as separate kv blocks that are visible only in one of the views ( thinker or writer ). if a block is not visible on the current view, we give it a large position index so it is ignored by the causal masked lm attention kernel. this implementation can efficiently parallelize thinking and writing the response for small batch sizes. however, when applied to large batches, it can be optimized further by only processing the non - masked query - key pairs that actually contribute to the attention output. in future work, we plan to explore implementing more general kernels for asyncreasoning based on vllm ’ s paged attention [ 43 ]. 4. experiments in this section, we conduct an initial evaluation of asyncreasoning and analyze its components. we run our evaluations on qwen", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 5, "frag_id": 1, "text": "or writer ). if a block is not visible on the current view, we give it a large position index so it is ignored by the causal masked lm attention kernel. this implementation can efficiently parallelize thinking and writing the response for small batch sizes. however, when applied to large batches, it can be optimized further by only processing the non - masked query - key pairs that actually contribute to the attention output. in future work, we plan to explore implementing more general kernels for asyncreasoning based on vllm ’ s paged attention [ 43 ]. 4. experiments in this section, we conduct an initial evaluation of asyncreasoning and analyze its components. we run our evaluations on qwen3 - 32b [ 81 ], a popular medium - sized reasoning llm that can run on a single high - end gpu, with a separate tts method. we run both asyncreasoning and baselines on one a100 - sxm4 gpu ( 500w ) in bfloat16 precision. on benchmarks. when evaluating asynchronous reasoning in voice assistant mode, we initially intended to evaluate on established spoken reasoning tasks from established audiolanguage model benchmarks [ 114, 115, 116, 117 ]. however, we found that modern reasoning models can solve even the multi - step reasoning tasks from these benchmarks with nearperfect ( ≥95 % ) accuracy without using < think >. hence, we chose to adopt the approach from [ 118, 108 ] : measure spoken answer delays on more challenging text tasks. more specifically, we evaluate mathematical reasoning on math - 500 [ 119, 7 ], multi - task understanding on mmlupro [ 120 ] and safety reasoning on harmbench [ 121 ]. we focus on two main metrics : i ) benchmark - specific quality, e. g. accuracy or llm judge score, using the setup from the original benchmark and ii ) real - time delay, defined as the amount of time ( seconds ) when the user hears no sound because the voice assistant is still formulating its response, including both time to first token and intermittent delays. to measure real - time delays, we implement a basic assistant pipeline that recognizes spoken inputs using whisperbase [ 52 ], feeds it into asyncreasoning ( or a baseline algorithm ) to stream response tokens, then group them into short chunks ( 5 tokens or", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 5, "frag_id": 2, "text": "##lupro [ 120 ] and safety reasoning on harmbench [ 121 ]. we focus on two main metrics : i ) benchmark - specific quality, e. g. accuracy or llm judge score, using the setup from the original benchmark and ii ) real - time delay, defined as the amount of time ( seconds ) when the user hears no sound because the voice assistant is still formulating its response, including both time to first token and intermittent delays. to measure real - time delays, we implement a basic assistant pipeline that recognizes spoken inputs using whisperbase [ 52 ], feeds it into asyncreasoning ( or a baseline algorithm ) to stream response tokens, then group them into short chunks ( 5 tokens or 1 latex expr. ) and use tortoisetts [ 60 ] with default parameters to generate speech. for tasks involving latex, we convert it into clearspeak [ 122 ]. 4. 1. analyzing mode - switching criteria in this section, we analyze the impact of different strategies for switching between the concurrent thinking & writing mode and the waiting for thoughts mode. for this evaluation, we evaluate qwen3 - 32b [ 81 ] on the math - 500 benchmark [ 119 ] in terms of accuracy and total delay time as described above. after the llm is done formulating the response, we prompt it to put its answer in \\ boxed {... } 5", "token_count": 291}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 6, "frag_id": 0, "text": "and check if it is equivalent to a reference answer using llm - as - a - judge [ 124 ] with the canonical judge setup6. we compare the following configurations : 1. baseline ( non - thinking ) : regular sequential generation with < think > mode disabled. 2. baseline ( thinking ) : regular sequential generation with < think > mode enabled. 3. interleaved thinking : prompting the model to think and reply in short, interleaved steps, but not asynchronous. inspired by [ 108 ], but without training. 4. asyncreasoning ( q - continue ) : the thinker is asked whether the current thoughts are ahead of writing. if not, the writer pauses. see section 3. 2 for details. 5. asyncreasoning ( q - pause ) : same as above, but the question is flipped. we ask if the writer should pause7. 6. asyncreasoning ( q + tts ) : same as q - continue, but we also pause writing if the current response is more than 10 seconds ahead of real time. 0 100 200 300 400 500 600 total delay ( s ) 0. 82 0. 84 0. 86 0. 88 0. 90 0. 92 0. 94 math - 500 accuracy baseline ( non - thinking ) baseline ( thinking ) interleaved thinking ours ( q - continue ) ours ( q - pause ) ours ( q + tts ) figure 4 : comparing the impact of different mode switching strategies and baselines on math - 500, qwen3 - 32b. in the last setup ( q + tts ), we run our tts pipeline over chunks of 5 generated tokens. we keep track of how many seconds of speech are synthesized but not yet spoken by any given time. if there are more than 10 seconds worth of response tokens “ in the buffer ”, we pause the writer automatically. if not, we let the llm decide normally. the results in figure 4 suggest that asyncreasoning is capable of reducing real - time delays while preserving most of the accuracy gains from reasoning and outperforms nonasynchronous interleaved thinking. however, the exact trade - off between accuracy and delays depends heavily on the mode switching criterion. 6we use the evaluation protocol from https : / / github. com / openai / simple - evals with gpt - 4 - turbo judge. 7 \"..", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 6, "frag_id": 1, "text": "many seconds of speech are synthesized but not yet spoken by any given time. if there are more than 10 seconds worth of response tokens “ in the buffer ”, we pause the writer automatically. if not, we let the llm decide normally. the results in figure 4 suggest that asyncreasoning is capable of reducing real - time delays while preserving most of the accuracy gains from reasoning and outperforms nonasynchronous interleaved thinking. however, the exact trade - off between accuracy and delays depends heavily on the mode switching criterion. 6we use the evaluation protocol from https : / / github. com / openai / simple - evals with gpt - 4 - turbo judge. 7 \"... \\ n \\ nwait, should i pause writing the response and think longer? ( yes / no ) : \" our default criterion ( q - continue ) has the lowest delay of the three, but drops about 4 % accuracy compared to synchronous thinking. we analyzed the samples where asynchronous reasoning produced a different final answer and found that the difference can often be attributed to the writer giving their answer too early. we hypothesize that the model is biased to answer “ yes ” to the mode - switching question, which corresponds to continuing the answer. in contrast, the q - pause variant flips the question so that answering “ yes ” pauses the writer, resulting in longer delays but higher accuracy. the tts - aware mode - switching criteria ( q + tts ) achieves the middle ground, demonstrating that mode - switching decisions can be effectively guided by downstream speech - generation dynamics. overall, these findings indicate that asyncreasoning enables thinking models to reply in near – real - time while giving more accurate answers than the non - thinking variant. 4. 2. additional benchmarks next, we evaluate additional benchmarks and real - time metrics. we use asyncreasoning ( q - continue ) from the previous section despite the tts - based variant having higher score in order to decouple concurrent reasoning from tts. in addition to math - 500, we also evaluate multi - task understanding on a sample of 500 tasks from the mmlupro [ 120 ] test set. we use canonical mmlu - pro evaluation method : the model is allowed to think, then chooses one of several possible answers, denoted by a letter ( abcd... ) and compare it against", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 6, "frag_id": 2, "text": "to reply in near – real - time while giving more accurate answers than the non - thinking variant. 4. 2. additional benchmarks next, we evaluate additional benchmarks and real - time metrics. we use asyncreasoning ( q - continue ) from the previous section despite the tts - based variant having higher score in order to decouple concurrent reasoning from tts. in addition to math - 500, we also evaluate multi - task understanding on a sample of 500 tasks from the mmlupro [ 120 ] test set. we use canonical mmlu - pro evaluation method : the model is allowed to think, then chooses one of several possible answers, denoted by a letter ( abcd... ) and compare it against the reference answer to compute accuracy ( exact match rate ). aside from that, we follow the same evaluation protocol as above. in addition to accuracy and total delay, we measure additional performance metrics : • time to first token ( ttft ) : the wall time delay until the system generates the first non - thinking token. • total delay : same in the previous section. we run tts on llm - generated response tokens and measure the total delay experienced by the user. • adjusted delay : similar to total delay, but we subtract 1 second from every contiguous pause to account for humans finding short pauses less noticeable. • steps to first token ( stft ) : the number of inference steps ( llm forward passes ) before the first nonthinking token is generated, gpu - agnostic. • steps delay : the average number of inference steps ( forward passes ) that do not generate a response token. we summarize our results in table 1 : across both benchmarks, we found that asyncreasoning significantly reduces both time to first token and overall delays time while providing more accurate answers than the non - thinking baseline, though not quite as accurate as slow ( synchronous ) reasoning mode. similarly to the previous section, we found that 6", "token_count": 409}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 7, "frag_id": 0, "text": "table 1 : evaluation of asyncreasoning on math - 500 and mmlu - pro using qwen3 - 32b with additional efficiency metrics. arrows ↑ / ↓denote “ higher / lower is better ”, respectively. refer to section 4 for additional details on metrics. inference setup accuracy↑ ttft↓ total delay↓ adjusted delay↓ stft↓ steps delay↓ math - 500 baseline ( thinking ) 0. 932 592. 05 592. 70 591. 51 3911. 55 3911. 55 baseline ( non - thinking ) 0. 834 0. 94 1. 96 0. 86 1 1 asyncreasoning ( q - continue ) 0. 890 2. 49 2. 91 1. 732 23. 71 247. 79 mmlu - pro ( 500 samples ) baseline ( thinking ) 0. 812 340. 07 340. 53 339. 47 2284. 82 2284. 82 baseline ( non - thinking ) 0. 696 1. 17 5. 07 4. 03 1 1 asyncreasoning ( q - continue ) 0. 758 4. 63 59. 01 51. 94 27. 30 187. 37 many of the errors can be attributed to writer giving the answer prematurely. in other words, the thinker does not always pause the writer when needed, suggesting that further improvements to the mode - switching strategy can improve accuracy, which is a promising direction for future work. 4. 3. asynchronous reasoning about safety to evaluate the impact of asynchronous reasoning on safety, we conduct experiments on the harmbench validation set [ 121 ] using the virtual context attack [ 125 ]. we use llm - as - a - judge [ 124 ] evaluation ( gpt - 4o - mini ) where only actionable harmful instructions count as a successful attack. we compare the attack success rate ( asr ) across four setups using the qwen3 - 32b model : ( 1 ) baseline ( nonthinking ), ( 2 ) baseline ( thinking ), ( 3 ) asyncreasoning ( q - continue ), and ( 4 ) asyncreasoning ( safety prompt ) that is additionally instructed to verify safety before responding. the full safety prompt is included in appendix b. quantitative results. we summarize our findings in table 2. consistent with recent findings on the “ cost of thinking ” [ 76 ], we observe that", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 7, "frag_id": 1, "text": "- a - judge [ 124 ] evaluation ( gpt - 4o - mini ) where only actionable harmful instructions count as a successful attack. we compare the attack success rate ( asr ) across four setups using the qwen3 - 32b model : ( 1 ) baseline ( nonthinking ), ( 2 ) baseline ( thinking ), ( 3 ) asyncreasoning ( q - continue ), and ( 4 ) asyncreasoning ( safety prompt ) that is additionally instructed to verify safety before responding. the full safety prompt is included in appendix b. quantitative results. we summarize our findings in table 2. consistent with recent findings on the “ cost of thinking ” [ 76 ], we observe that enabling reasoning in the baseline model actually increases vulnerability ( asr rises from 2. 5 % to 13. 0 % ). the model effectively “ talks itself into ” answering harmful queries by adopting a helpful persona or over - analyzing the technical aspects of the prompt. asyncreasoning ( q - continue ) ( 11. 5 % asr ) remains similarly vulnerable to the thinking baseline. however, by introducing additional safety instructions into the thinker ’ s prompt we successfully reduce the asr to 2. 0 % while preserving accuracy on math - 500 benchmark. in practice, this allows safety - minded reasoning in streaming llm apis and other time - sensitive applications without the need for specialized fine - tuning. asyncreasoning can stream tokens normally for benign queries, only pausing generation for potentially unsafe responses. inference setup asr↓ accuracy↑ baseline ( non - thinking ) 0. 025 0. 834 baseline ( thinking ) 0. 130 0. 932 asyncreasoning ( q - continue ) 0. 115 0. 890 asyncreasoning ( safety prompt ) 0. 020 0. 878 table 2 : attack success rate on harmbench ( virtual context attack ) and accuracy on math - 500 for qwen3 - 32b. failure mode analysis. while asyncreasoning allows for real - time safety checks, the asynchronous nature of generation introduces specific failure modes where the writer may output harmful content before the thinker intervenes. we identify three primary categories of such safety failures : 1. race condition : the writer begins generating a helpful response immediately based on the prompt. although the thinker eventually concludes the request is unsafe, the writer has already", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 7, "frag_id": 2, "text": "##yncreasoning ( q - continue ) 0. 115 0. 890 asyncreasoning ( safety prompt ) 0. 020 0. 878 table 2 : attack success rate on harmbench ( virtual context attack ) and accuracy on math - 500 for qwen3 - 32b. failure mode analysis. while asyncreasoning allows for real - time safety checks, the asynchronous nature of generation introduces specific failure modes where the writer may output harmful content before the thinker intervenes. we identify three primary categories of such safety failures : 1. race condition : the writer begins generating a helpful response immediately based on the prompt. although the thinker eventually concludes the request is unsafe, the writer has already streamed harmful tokens ( e. g., the first steps of a dangerous recipe ) to the user before the refusal signal is propagated. 2. context leakage : the thinker analyzes the harmful request by recalling technical details ( e. g., explaining how a specific sql injection works to verify its danger ). the writer, attending to the thinker ’ s cache, interprets these technical details as the desired answer and formulates them into a response, bypassing the thinker ’ s intent. 3. educational loophole : the thinker adopts an educational persona to explain why a request is dangerous. the writer latches onto this educational content and reformats it as a set of instructions, stripping away the safety framing context. these findings suggest that, while asyncreasoning can effectively filter attacks, strict gating mechanisms ( e. g., ensuring the thinker has a “ head start ” on safety verification ) are necessary to prevent race conditions in highly sensitive scenarios. we will investigate this further in future work. 7", "token_count": 363}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 8, "frag_id": 0, "text": "5. discussion & future work in this preprint, we formulated asyncreasoning — a training - free method that allows reasoning llms to think and write concurrently. our preliminary experiments suggest that the proposed approach can indeed overlap thinking and writing and reduce thinking delays while giving more accurate answers than the non - thinking models. this allows llms to think longer and give more thoughtful answers in time - sensitive applications such as voice assistants, embodied agents, or safety - minded use cases. this leaves many interesting directions for further research and analysis. in future work, we will look more into strategies for mode - switching : determining when to pause writing the response and wait for more thoughts. we also plan to expand the scope of our experiments with additional models, task types, and comparison to non - training - free baselines. among others, it would be interesting to quantify the method ’ s ability to process asynchronous inputs, such as task clarifications for voice assistants or environment readouts for agents. additionally, we will work on integrating asyncreasoning with vllm [ 43 ]. 6. acknowledgements we would like to thank andrey shukshov for his helpful advice about efficient gpu kernel design. we also thank gleb rodionov for proofreading and helpful suggestions on experiment design and paper presentation. references [ 1 ] charlie snell, jaehoon lee, kelvin xu, and aviral kumar. scaling llm test - time compute optimally can be more effective than scaling model parameters. arxiv preprint arxiv : 2408. 03314, 2024. [ 2 ] mirac suzgun, nathan scales, nathanael scharli, sebastian gehrmann, yi tay, hyung won chung, aakanksha chowdhery, quoc v. le, ed h. chi, denny zhou, and jason wei. challenging big - bench tasks and whether chain - of - thought can solve them. in annual meeting of the association for computational linguistics, 2022. [ 3 ] edward beeching, lewis tunstall, and sasha rush. scaling test - time compute with open models. [ 4 ] jason wei, xuezhi wang, dale schuurmans, maarten bosma, fei xia, ed chi, quoc v le, denny zhou, et al. chain - of - thought prompting elicits reasoning in large language models.", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 8, "frag_id": 1, "text": "##li, sebastian gehrmann, yi tay, hyung won chung, aakanksha chowdhery, quoc v. le, ed h. chi, denny zhou, and jason wei. challenging big - bench tasks and whether chain - of - thought can solve them. in annual meeting of the association for computational linguistics, 2022. [ 3 ] edward beeching, lewis tunstall, and sasha rush. scaling test - time compute with open models. [ 4 ] jason wei, xuezhi wang, dale schuurmans, maarten bosma, fei xia, ed chi, quoc v le, denny zhou, et al. chain - of - thought prompting elicits reasoning in large language models. advances in neural information processing systems, 35 : 24824 – 24837, 2022. [ 5 ] takeshi kojima, shixiang shane gu, machel reid, yutaka matsuo, and yusuke iwasawa. large language models are zero - shot reasoners. arxiv, abs / 2205. 11916, 2022. [ 6 ] shunyu yao, dian yu, jeffrey zhao, izhak shafran, thomas l. griffiths, yuan cao, and karthik narasimhan. tree of thoughts : deliberate problem solving with large language models. arxiv, abs / 2305. 10601, 2023. [ 7 ] hunter lightman, vineet kosaraju, yura burda, harrison edwards, bowen baker, teddy lee, jan leike, john schulman, ilya sutskever, and karl cobbe. let ’ s verify step by step. arxiv, abs / 2305. 20050, 2023. [ 8 ] zhuosheng zhang, aston zhang, mu li, and alexander j. smola. automatic chain of thought prompting in large language models. arxiv, abs / 2210. 03493, 2022. [ 9 ] niklas muennighoff, zitong yang, weijia shi, xiang lisa li, li fei - fei, hannaneh hajishirzi, luke zettlemoyer, percy liang, emmanuel candes, and tatsunori hashimoto. s1 : simple test - time scaling. arxiv preprint arxiv : 2501. 19393, 2025. [ 10 ] timo schic", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 8, "frag_id": 2, "text": "/ 2305. 20050, 2023. [ 8 ] zhuosheng zhang, aston zhang, mu li, and alexander j. smola. automatic chain of thought prompting in large language models. arxiv, abs / 2210. 03493, 2022. [ 9 ] niklas muennighoff, zitong yang, weijia shi, xiang lisa li, li fei - fei, hannaneh hajishirzi, luke zettlemoyer, percy liang, emmanuel candes, and tatsunori hashimoto. s1 : simple test - time scaling. arxiv preprint arxiv : 2501. 19393, 2025. [ 10 ] timo schick, jane dwivedi - yu, roberto dessi, roberta raileanu, maria lomeli, luke zettlemoyer, nicola cancedda, and thomas scialom. toolformer : language models can teach themselves to use tools. arxiv, abs / 2302. 04761, 2023. [ 11 ] shunyu yao, jeffrey zhao, dian yu, nan du, izhak shafran, karthik narasimhan, and yuan cao. react : synergizing reasoning and acting in language models. arxiv, abs / 2210. 03629, 2022. [ 12 ] leo gao, aman madaan, shuyan zhou, karthik narasimhan, and danqi chen. pal : program - aided language models. in proceedings of the 40th international conference on machine learning, volume 202, pages 10764 – 10791. pmlr, 2023. [ 13 ] yongliang shen, kaitao song, xu tan, dongsheng li, weiming lu, and yue ting zhuang. hugginggpt : solving ai tasks with chatgpt and its friends in hugging face. arxiv, abs / 2303. 17580, 2023. [ 14 ] yujia qin, shi liang, yining ye, kunlun zhu, lan yan, ya - ting lu, yankai lin, xin cong, xiangru tang, bill qian, sihan zhao, runchu tian, ruobing xie, jie zhou, marc h. gerstein, dahai li, zhiyuan liu, and maosong sun. toolllm : facilitating", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 8, "frag_id": 3, "text": "[ 13 ] yongliang shen, kaitao song, xu tan, dongsheng li, weiming lu, and yue ting zhuang. hugginggpt : solving ai tasks with chatgpt and its friends in hugging face. arxiv, abs / 2303. 17580, 2023. [ 14 ] yujia qin, shi liang, yining ye, kunlun zhu, lan yan, ya - ting lu, yankai lin, xin cong, xiangru tang, bill qian, sihan zhao, runchu tian, ruobing xie, jie zhou, marc h. gerstein, dahai li, zhiyuan liu, and maosong sun. toolllm : facilitating large language models to master 16000 + real - world apis. arxiv, abs / 2307. 16789, 2023. [ 15 ] zhanna azerbayev, dhruv patel, sebastien bubeck, ronen eldan, yin tat lee, yuanzhong li, tamas sar8", "token_count": 219}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 9, "frag_id": 0, "text": "los, and yi zhang. llemma : an open language model for mathematics. in proceedings of the twelfth international conference on learning representations, 2024. [ 16 ] ziyue wang, zhiyuan zhao, minqi peng, shiqi chen, mengdi yang, chi - min lin, pratyush sharma, sebastien bubeck, ronen eldan, yuanzhong li, yin tat lee, and yi zhang. mathcoder : seamless code integration in llms for enhanced mathematical reasoning. in proceedings of the twelfth international conference on learning representations, 2024. [ 17 ] chengshu li, jacky liang, andy zeng, xinyun chen, karol hausman, dorsa sadigh, sergey levine, li feifei, fei xia, and brian ichter. chain of code : reasoning with a language model - augmented code emulator. in proceedings of the 41st international conference on machine learning, volume 235 of proceedings of machine learning research, pages 28259 – 28277. pmlr, 2024. arxiv preprint arxiv : 2312. 04474. [ 18 ] openai, :, aaron jaech, adam kalai, adam lerer, adam richardson, ahmed el - kishky, aiden low, alec helyar, aleksander madry, and alex beutel et al. openai o1 system card, 2024. [ 19 ] google deepmind. gemini 2. 5 : our newest gemini model with thinking. https : / / blog. google / technology / googledeepmind / gemini - model - thinking - updatesmarch - 2025 / # gemini - 2 - 5 - thinking, 2025. accessed : 2025 - 04 - 07. [ 20 ] anthropic. claude 3. 7 sonnet and claude code, 2024. accessed : 2025. 04. 02. [ 21 ] deepseek - ai, daya guo, dejian yang, haowei zhang, junxiao song, ruoyu zhang, runxin xu, qihao zhu, shirong ma, peiyi wang, and xiao bi et al. deepseek - r1 : incentivizing reasoning capability in llms via reinforcement learning, 2025. [ 22 ] qwen team. qwq - 32b : embracing the power of reinforcement learning, march 2025. [ 23 ]", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 9, "frag_id": 1, "text": "thinking, 2025. accessed : 2025 - 04 - 07. [ 20 ] anthropic. claude 3. 7 sonnet and claude code, 2024. accessed : 2025. 04. 02. [ 21 ] deepseek - ai, daya guo, dejian yang, haowei zhang, junxiao song, ruoyu zhang, runxin xu, qihao zhu, shirong ma, peiyi wang, and xiao bi et al. deepseek - r1 : incentivizing reasoning capability in llms via reinforcement learning, 2025. [ 22 ] qwen team. qwq - 32b : embracing the power of reinforcement learning, march 2025. [ 23 ] kimi team, yifan bai, yiping bao, guanduo chen, jiahao chen, ningxin chen, ruijue chen, yanru chen, yuankun chen, yutian chen, zhuofu chen, jialei cui, hao ding, mengnan dong, angang du, chenzhuang du, dikang du, yulun du, yu fan, yichen feng, kelin fu, bofei gao, hongcheng gao, peizhong gao, tong gao, xinran gu, longyu guan, haiqing guo, jianhang guo, hao hu, xiaoru hao, tianhong he, weiran he, wenyang he, chao hong, yangyang hu, zhenxing hu, weixiao huang, zhiqi huang, zihao huang, tao jiang, zhejun jiang, xinyi jin, yongsheng kang, guokun lai, cheng li, fang li, haoyang li, ming li, wentao li, yanhao li, yiwei li, zhaowei li, zheming li, hongzhan lin, xiaohan lin, zongyu lin, chengyin liu, chenyu liu, hongzhang liu, jingyuan liu, junqi liu, liang liu, shaowei liu, t. y. liu, tianwei liu, weizhou liu, yangyang liu, yibo liu, yiping liu, yue liu, zhengying liu, enzhe lu, lijun lu, shengling ma, xinyu ma, yingwei ma, shaoguang mao, jie mei, xin men, yibo mia", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 9, "frag_id": 2, "text": "li, fang li, haoyang li, ming li, wentao li, yanhao li, yiwei li, zhaowei li, zheming li, hongzhan lin, xiaohan lin, zongyu lin, chengyin liu, chenyu liu, hongzhang liu, jingyuan liu, junqi liu, liang liu, shaowei liu, t. y. liu, tianwei liu, weizhou liu, yangyang liu, yibo liu, yiping liu, yue liu, zhengying liu, enzhe lu, lijun lu, shengling ma, xinyu ma, yingwei ma, shaoguang mao, jie mei, xin men, yibo miao, siyuan pan, yebo peng, ruoyu qin, bowen qu, zeyu shang, lidong shi, shengyuan shi, feifan song, jianlin su, zhengyuan su, xinjie sun, flood sung, heyi tang, jiawen tao, qifeng teng, chensi wang, dinglu wang, feng wang, haiming wang, jianzhou wang, jiaxing wang, jinhong wang, shengjie wang, shuyi wang, yao wang, yejie wang, yiqin wang, yuxin wang, yuzhi wang, zhaoji wang, zhengtao wang, zhexu wang, chu wei, qianqian wei, wenhao wu, xingzhe wu, yuxin wu, chenjun xiao, xiaotong xie, weimin xiong, boyu xu, jing xu, jinjing xu, l. h. xu, lin xu, suting xu, weixin xu, xinran xu, yangchuan xu, ziyao xu, junjie yan, yuzi yan, xiaofei yang, ying yang, zhen yang, zhilin yang, zonghan yang, haotian yao, xingcheng yao, wenjie ye, zhuorui ye, bohong yin, longhui yu, enming yuan, hongbang yuan, mengjie yuan, haobing zhan, dehao zhang, hao zhang, wanlu zhang, xiaobin zhang, yangkun zhang, yizhi zhang, yongting zhang, yu zhang, yutao zhang, yutong zhang, zheng zhang, haotian zhao, yikai zhao, hua", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 9, "frag_id": 3, "text": "xu, xinran xu, yangchuan xu, ziyao xu, junjie yan, yuzi yan, xiaofei yang, ying yang, zhen yang, zhilin yang, zonghan yang, haotian yao, xingcheng yao, wenjie ye, zhuorui ye, bohong yin, longhui yu, enming yuan, hongbang yuan, mengjie yuan, haobing zhan, dehao zhang, hao zhang, wanlu zhang, xiaobin zhang, yangkun zhang, yizhi zhang, yongting zhang, yu zhang, yutao zhang, yutong zhang, zheng zhang, haotian zhao, yikai zhao, huabin zheng, shaojie zheng, jianren zhou, xinyu zhou, zaida zhou, zhen zhu, weiyu zhuang, and xinxing zu. kimi k2 : open agentic intelligence, 2025. [ 24 ] arc prize foundation. openai ’ s new o3 system scores breakthrough on arc - agi - pub, 2024. accessed : 2025. 03. 28. [ 25 ] humanity ’ s last exam contributors. humanity ’ s last exam : a benchmark for frontier ai capabilities. arxiv preprint arxiv : 2501. 14249, 2025. [ 26 ] nicole landi, stephen frost, w menc, rebecca sandak, and kenneth pugh. neurobiological bases of reading comprehension : insights from neuroimaging studies of word - level and text - level processing in skilled and impaired readers. reading & writing quarterly : overcoming learning difficulties, 29 : 145 – 167, 04 2013. [ 27 ] bingjiang lyu, william d. marslen - wilson, yuxing fang, and lorraine k. tyler. finding structure during incremental speech comprehension. elife, 12 : e89311, 2023. 9", "token_count": 401}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 10, "frag_id": 0, "text": "[ 28 ] sarah bro trasmundi and juan toro. mind wandering in reading : an embodied approach. frontiers in human neuroscience, 17, 2023. [ 29 ] daisuke akiba. ctrl + alt + inner speech : a verbal – cognitive scaffold ( vcs ) model of pathways to computational thinking. journal of intelligence, 13 ( 12 ), 2025. [ 30 ] kevin p. madore and anthony d. wagner. multicosts of multitasking. cerebrum : the dana forum on brain science, 2019 : cer – 04 – 19, 2019. [ 31 ] maurizio corbetta, gaurav patel, and gordon l shulman. the reorienting system of the human brain : from environment to theory of mind. neuron, 58 ( 3 ) : 306 – 324, 2008. [ 32 ] openai. gpt - 4o system card. online technical report, 2024. voice - mode multimodal model supporting audio, text, and vision. available at https : / / openai. com / index / hello - gpt - 4o. [ 33 ] paul k. rubenstein, chulayuth asawaroengchai, duc dung nguyen, ankur bapna, zalan borsos, felix de chaumont quitry, peter chen, dalia el badawy, wei han, eugene kharitonov, hannah muckenhirn, dirk padfield, james qin, danny rozenberg, tara sainath, johan schalkwyk, matt sharifi, michelle tadmor ramanovich, marco tagliasacchi, alexandru tudor, mihajlo velimirovi´c, damien vincent, jiahui yu, yongqiang wang, vicky zayats, neil zeghidour, yu zhang, zhishuai zhang, lukas zilka, and christian frank. audiopalm : a large language model that can speak and listen. arxiv preprint arxiv : 2306. 12925, 2023. [ 34 ] dong zhang, shimin li, xin zhang, jun zhan, pengyu wang, yaqian zhou, and xipeng qiu. speechgpt : empowering large language models with intrinsic cross - modal conversational abilities. in findings of the association for computational linguistics : emnlp 2023, pages 15757", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 10, "frag_id": 1, "text": "##irovi´c, damien vincent, jiahui yu, yongqiang wang, vicky zayats, neil zeghidour, yu zhang, zhishuai zhang, lukas zilka, and christian frank. audiopalm : a large language model that can speak and listen. arxiv preprint arxiv : 2306. 12925, 2023. [ 34 ] dong zhang, shimin li, xin zhang, jun zhan, pengyu wang, yaqian zhou, and xipeng qiu. speechgpt : empowering large language models with intrinsic cross - modal conversational abilities. in findings of the association for computational linguistics : emnlp 2023, pages 15757 – 15773. association for computational linguistics, december 2023. [ 35 ] yunfei chu, jin xu, xiaohuan zhou, qian yang, shiliang zhang, zhijie yan, chang zhou, and jingren zhou. qwen - audio : advancing universal audio understanding via unified large - scale audio - language models. arxiv preprint arxiv : 2311. 07919, 2023. [ 36 ] zhifei xie and changqiao wu. mini - omni : language models can hear, talk while thinking in streaming. arxiv preprint arxiv : 2408. 16725, 2024. [ 37 ] qingkai fang, shoutao guo, yan zhou, zhengrui ma, shaolei zhang, and yang feng. llama - omni : seamless speech interaction with large language models. arxiv preprint arxiv : 2409. 06666, 2024. [ 38 ] alexandre defossez, laurent mazare, manu orsini, amelie royer, patrick perez, herve jegou, edouard grave, and neil zeghidour. moshi : a speech - text foundation model for real - time dialogue, 2024. [ 39 ] michael ahn, anthony brohan, noah brown, yevgen chebotar, omar cortes, byron david, chelsea finn, chuyuan fu, keerthana gopalakrishnan, karol hausman, alex herzog, daniel ho, jasmine hsu, julian ibarz, brian ichter, alex irpan, eric jang, rosario jauregui ruano, kyle jeffrey,", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 10, "frag_id": 2, "text": "##6, 2024. [ 38 ] alexandre defossez, laurent mazare, manu orsini, amelie royer, patrick perez, herve jegou, edouard grave, and neil zeghidour. moshi : a speech - text foundation model for real - time dialogue, 2024. [ 39 ] michael ahn, anthony brohan, noah brown, yevgen chebotar, omar cortes, byron david, chelsea finn, chuyuan fu, keerthana gopalakrishnan, karol hausman, alex herzog, daniel ho, jasmine hsu, julian ibarz, brian ichter, alex irpan, eric jang, rosario jauregui ruano, kyle jeffrey, sally jesmonth, nikhil joshi, ryan julian, dmitry kalashnikov, yuheng kuang, kuang - huei lee, sergey levine, yao lu, linda luu, carolina parada, peter pastor, jornell quiambao, kanishka rao, jarek rettinghouse, diego reyes, pierre sermanet, nicolas sievers, clayton tan, alexander toshev, vincent vanhoucke, fei xia, ted xiao, peng xu, sichun xu, mengyuan yan, and andy zeng. do as i can, not as i say : grounding language in robotic affordances. arxiv, 2022. [ 40 ] anthony brohan, noah brown, justice carbajal, yevgen chebotar, xi chen, krzysztof choromanski, tianli ding, danny driess, avinava dubey, chelsea finn, pete florence, chuyuan fu, montse gonzalez arenas, keerthana gopalakrishnan, kehang han, karol hausman, alexander herzog, jasmine hsu, brian ichter, alex irpan, nikhil joshi, ryan julian, dmitry kalashnikov, yuheng kuang, isabel leal, lisa lee, tsang - wei lee, sergey levine, yao lu, henryk michalewski, igor mordatch, karl pertsch, kanishka rao, krista reymann, michael ryoo, grecia salazar, pannag sanketi, pierre sermanet, jaspiar singh, anikait singh, radu soricut, huong tran, vincent vanhoucke", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 10, "frag_id": 3, "text": "fu, montse gonzalez arenas, keerthana gopalakrishnan, kehang han, karol hausman, alexander herzog, jasmine hsu, brian ichter, alex irpan, nikhil joshi, ryan julian, dmitry kalashnikov, yuheng kuang, isabel leal, lisa lee, tsang - wei lee, sergey levine, yao lu, henryk michalewski, igor mordatch, karl pertsch, kanishka rao, krista reymann, michael ryoo, grecia salazar, pannag sanketi, pierre sermanet, jaspiar singh, anikait singh, radu soricut, huong tran, vincent vanhoucke, quan vuong, ayzaan wahid, stefan welker, paul wohlhart, jialin wu, fei xia, ted xiao, peng xu, sichun xu, tianhe yu, and brianna zitkovich. rt - 2 : vision - language - action models transfer web knowledge to robotic control. arxiv, 2023. [ 41 ] danny driess, fei xia, mehdi s. m. sajjadi, corey lynch, aakanksha chowdhery, brian ichter, ayzaan wahid, jonathan tompson, quan vuong, tianhe yu, wenlong huang, yevgen chebotar, pierre sermanet, daniel duckworth, sergey levine, vincent vanhoucke, karol hausman, marc toussaint, klaus greff, andy zeng, igor mordatch, and pete florence. palm - e : an embodied multimodal language model. arxiv preprint arxiv : 2303. 03378, 2023. [ 42 ] openai. chatgpt deep research : support for user update and multitasking features. https : / / chat. openai. com, 2025. accessed 7 december 10", "token_count": 411}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 11, "frag_id": 0, "text": "2025. in late 2025, the deep research feature was updated to allow user to communicate with the agent while it performs research via the \" update \" button. [ 43 ] woosuk kwon, zhuohan li, siyuan zhuang, ying sheng, lianmin zheng, cody hao yu, joseph gonzalez, hao zhang, and ion stoica. efficient memory management for large language model serving with pagedattention. in proceedings of the 29th symposium on operating systems principles, pages 611 – 626, 2023. [ 44 ] lianmin zheng, liangsheng yin, zhiqiang xie, jeff huang, chuyue sun, cody hao yu, shiyi cao, christos kozyrakis, ion stoica, joseph e. gonzalez, clark barrett, and ying sheng. efficiently programming large language models using sglang, 2023. [ 45 ] gemini live ( voice mode ). https : / / gemini. google / overview / gemini - live /, 2024. accessed : 2025 - 12 - 01. [ 46 ] anthropic. using voice mode on claude mobile apps. https : / / support. claude. com / en / articles / 11101966 - using - voice - mode - on - claudemobile - apps, 2025. accessed : december 1, 2025. [ 47 ] james flamino, mohammed shahid modi, boleslaw k. szymanski, brendan cross, and colton mikolajczyk. testing the limits of large language models in debating humans. scientific reports, 15 : 13852, 2025. [ 48 ] stephanie houde, kristina brimijoin, michael muller, steven i. ross, dario andres silva moran, gabriel enrique gonzalez, siya kunde, morgan a. foreman, and justin d. weisz. controlling ai agent participation in group conversations : a humancentered approach. in proceedings of the 30th international conference on intelligent user interfaces, iui ’ 25, page 390 – 408, new york, ny, usa, 2025. association for computing machinery. [ 49 ] k. h. davis, r. biddulph, and s. balashek. automatic recognition of spoken digits. the journal of the acoustical society of america, 24 ( 6 ) : 637 – 642, 11 1952. [ 50 ] daniel povey, ar", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 11, "frag_id": 1, "text": "##join, michael muller, steven i. ross, dario andres silva moran, gabriel enrique gonzalez, siya kunde, morgan a. foreman, and justin d. weisz. controlling ai agent participation in group conversations : a humancentered approach. in proceedings of the 30th international conference on intelligent user interfaces, iui ’ 25, page 390 – 408, new york, ny, usa, 2025. association for computing machinery. [ 49 ] k. h. davis, r. biddulph, and s. balashek. automatic recognition of spoken digits. the journal of the acoustical society of america, 24 ( 6 ) : 637 – 642, 11 1952. [ 50 ] daniel povey, arnab ghoshal, gilles boulianne, lukas burget, ondrej glembek, nagendra goel, mirko hannemann, petr motlicek, yanmin qian, petr schwarz, jan silovsky, georg stemmer, and karel vesely. kaldi : a toolkit for speech recognition. https : / / kaldi - asr. org, 2011. open - source speech recognition toolkit. [ 51 ] steffen schneider, alexei baevski, ronan collobert, and michael auli. wav2vec : unsupervised pretraining for speech recognition, 2019. [ 52 ] alec radford, jong wook kim, tao xu, greg brockman, christine mcleavey, and ilya sutskever. robust speech recognition via large - scale weak supervision, 2022. [ 53 ] n. umeda, h. omura, and o. fujimura. first complete text - to - speech system. technical report, electrotechnical laboratory, japan, tokyo, japan, 1968. [ 54 ] heiga zen, keiichi tokuda, and alan w. black. statistical parametric speech synthesis. speech communication, 51 ( 11 ) : 1039 – 1064, 2009. [ 55 ] aaron van den oord, sander dieleman, heiga zen, karen simonyan, oriol vinyals, alex graves, nal kalchbrenner, andrew senior, and koray kavukcuoglu. wavenet : a generative model for raw audio. in proceedings of the 9th isca speech synthesis workshop, 2016. [", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 11, "frag_id": 2, "text": "and o. fujimura. first complete text - to - speech system. technical report, electrotechnical laboratory, japan, tokyo, japan, 1968. [ 54 ] heiga zen, keiichi tokuda, and alan w. black. statistical parametric speech synthesis. speech communication, 51 ( 11 ) : 1039 – 1064, 2009. [ 55 ] aaron van den oord, sander dieleman, heiga zen, karen simonyan, oriol vinyals, alex graves, nal kalchbrenner, andrew senior, and koray kavukcuoglu. wavenet : a generative model for raw audio. in proceedings of the 9th isca speech synthesis workshop, 2016. [ 56 ] yuxuan wang, r. j. skerry - ryan, daisy stanton, yonghui wu, ron j. weiss, navdeep jaitly, zongheng yang, ying xiao, zhifeng chen, samy bengio, quoc le, yannis agiomyrgiannakis, rob clark, and rif a. saurous. tacotron : towards end - to - end speech synthesis. in interspeech, pages 4006 – 4010. isca, 2017. [ 57 ] jonathan shen, ruoming pang, ron j. weiss, mike schuster, navdeep jaitly, zongheng yang, zhifeng chen, yu zhang, yuxuan wang, rj skerry - ryan, rif a. saurous, yannis agiomyrgiannakis, and yonghui wu. natural tts synthesis by conditioning wavenet on mel spectrogram predictions, 2018. [ 58 ] ryan j. prenger, rafael valle, and bryan catanzaro. waveglow : a flow - based generative network for speech synthesis. icassp 2019 - 2019 ieee international conference on acoustics, speech and signal processing ( icassp ), pages 3617 – 3621, 2018. [ 59 ] jungil kong, jaehyeon kim, and jaekyoung bae. hifi - gan : generative adversarial networks for efficient and high fidelity speech synthesis. in proceedings of the 34th international conference on neural information processing systems, nips ’ 20, red hook, ny, usa, 2020. curran associates inc. [ 60 ] james betker. better speech synthesis through scaling. ar", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 11, "frag_id": 3, "text": ". [ 58 ] ryan j. prenger, rafael valle, and bryan catanzaro. waveglow : a flow - based generative network for speech synthesis. icassp 2019 - 2019 ieee international conference on acoustics, speech and signal processing ( icassp ), pages 3617 – 3621, 2018. [ 59 ] jungil kong, jaehyeon kim, and jaekyoung bae. hifi - gan : generative adversarial networks for efficient and high fidelity speech synthesis. in proceedings of the 34th international conference on neural information processing systems, nips ’ 20, red hook, ny, usa, 2020. curran associates inc. [ 60 ] james betker. better speech synthesis through scaling. arxiv preprint arxiv : 2305. 07243, 2023. tortoise tts : expressive multi - voice text - to - speech. [ 61 ] jin xu, zhifang guo, hangrui hu, yunfei chu, xiong wang, jinzheng he, yuxuan wang, xian shi, ting he, xinfa zhu, yuanjun lv, yongqi wang, dake guo, he wang, linhan ma, pei zhang, xinyu zhang, 11", "token_count": 262}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 12, "frag_id": 0, "text": "hongkun hao, zishan guo, baosong yang, bin zhang, ziyang ma, xipin wei, shuai bai, keqin chen, xuejing liu, peng wang, mingkun yang, dayiheng liu, xingzhang ren, bo zheng, rui men, fan zhou, bowen yu, jianxin yang, le yu, jingren zhou, and junyang lin. qwen3 - omni technical report. arxiv preprint arxiv : 2509. 17765, 2025. [ 62 ] ruaridh mon - williams, gen li, ran long, wenqian du, christopher g. lucas, et al. embodied large language models enable robots to complete complex tasks in unpredictable environments. nature machine intelligence, 7 : 592 – 601, 2025. [ 63 ] guanzhi wang, yuqi xie, yunfan jiang, ajay mandlekar, chaowei xiao, yuke zhu, linxi fan, and anima anandkumar. voyager : an open - ended embodied agent with large language models. arxiv, 2023. [ 64 ] yunfan jiang, agrim gupta, zichen zhang, guanzhi wang, yongqiang dou, yanjun chen, li fei - fei, anima anandkumar, yuke zhu, and linxi fan. vima : general robot manipulation with multimodal prompts. arxiv, 2023. [ 65 ] moo jin kim, karl pertsch, siddharth karamcheti, ted xiao, ashwin balakrishna, suraj nair, rafael rafailov, ethan foster, grace lam, pannag sanketi, quan vuong, thomas kollar, benjamin burchfiel, russ tedrake, dorsa sadigh, sergey levine, percy liang, and chelsea finn. openvla : an open - source vision - language - action model, 2024. [ 66 ] ranjan sapkota, yang cao, konstantinos i. roumeliotis, and manoj karkee. vision - language - action models : concepts, progress, applications and challenges, 2025. [ 67 ] zihao wang, shaofei cai, guanzhou chen, anji liu, xiaojian ( shawn ) ma, and yitao liang. describe, explain, plan and select : interactive planning with ll", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 12, "frag_id": 1, "text": ", pannag sanketi, quan vuong, thomas kollar, benjamin burchfiel, russ tedrake, dorsa sadigh, sergey levine, percy liang, and chelsea finn. openvla : an open - source vision - language - action model, 2024. [ 66 ] ranjan sapkota, yang cao, konstantinos i. roumeliotis, and manoj karkee. vision - language - action models : concepts, progress, applications and challenges, 2025. [ 67 ] zihao wang, shaofei cai, guanzhou chen, anji liu, xiaojian ( shawn ) ma, and yitao liang. describe, explain, plan and select : interactive planning with llms enables open - world multi - task agents. in a. oh, t. naumann, a. globerson, k. saenko, m. hardt, and s. levine, editors, advances in neural information processing systems, volume 36, pages 34153 – 34189. curran associates, inc., 2023. [ 68 ] charles cao, feiyi wang, lisa lindley, and zejiang wang. managing linux servers with llm - based ai agents : an empirical evaluation with gpt4. machine learning with applications, 17 : 100570, 2024. [ 69 ] zhiyong wu, chengcheng han, zichen ding, zhenmin weng, zhoumianze liu, shunyu yao, tao yu, and lingpeng kong. os - copilot : towards generalist computer agents with self - improvement, 2024. [ 70 ] china. xiaoyan zhang, zhao yang, jiaxuan liu, yanda li, yucheng han, xin chen, zebiao huang, bin fu, and gang yu. appagent : multimodal agents as smartphone users. proceedings of the 2025 chi conference on human factors in computing systems, 2023. [ 71 ] brian singer, keane lucas, lakshmi adiga, meghna jain, lujo bauer, and vyas sekar. on the feasibility of using llms to execute multistage network attacks, 01 2025. [ 72 ] tomek korbak, mikita balesni, elizabeth barnes, yoshua bengio, joe benton, joseph bloom, mark chen, alan cooney, allan dafoe, anca dragan, scott", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 12, "frag_id": 2, "text": ", yanda li, yucheng han, xin chen, zebiao huang, bin fu, and gang yu. appagent : multimodal agents as smartphone users. proceedings of the 2025 chi conference on human factors in computing systems, 2023. [ 71 ] brian singer, keane lucas, lakshmi adiga, meghna jain, lujo bauer, and vyas sekar. on the feasibility of using llms to execute multistage network attacks, 01 2025. [ 72 ] tomek korbak, mikita balesni, elizabeth barnes, yoshua bengio, joe benton, joseph bloom, mark chen, alan cooney, allan dafoe, anca dragan, scott emmons, owain evans, david farhi, ryan greenblatt, dan hendrycks, marius hobbhahn, evan hubinger, geoffrey irving, erik jenner, daniel kokotajlo, victoria krakovna, shane legg, david lindner, david luan, aleksander m [UNK], julian michael, neel nanda, dave orr, jakub pachocki, ethan perez, mary phuong, fabien roger, joshua saxe, buck shlegeris, martin soto, eric steinberger, jasmine wang, wojciech zaremba, bowen baker, rohin shah, and vlad mikulik. chain of thought monitorability : a new and fragile opportunity for ai safety. arxiv, 2025. [ 73 ] bowen baker, joost huizinga, leo gao, zehao dou, melody y. guan, aleksander madry, wojciech zaremba, jakub w. pachocki, and david farhi. monitoring reasoning models for misbehavior and the risks of promoting obfuscation. arxiv, abs / 2503. 11926, 2025. [ 74 ] chengda lu, xiaoyu fan, yu huang, rongwu xu, jijie li, and wei xu. does chain - of - thought reasoning really reduce harmfulness from jailbreaking? in wanxiang che, joyce nabende, ekaterina shutova, and mohammad taher pilehvar, editors, findings of the association for computational linguistics : acl 2025, pages 6523 – 6546, vienna, austria, july 2025. association for", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 12, "frag_id": 3, "text": "##ba, jakub w. pachocki, and david farhi. monitoring reasoning models for misbehavior and the risks of promoting obfuscation. arxiv, abs / 2503. 11926, 2025. [ 74 ] chengda lu, xiaoyu fan, yu huang, rongwu xu, jijie li, and wei xu. does chain - of - thought reasoning really reduce harmfulness from jailbreaking? in wanxiang che, joyce nabende, ekaterina shutova, and mohammad taher pilehvar, editors, findings of the association for computational linguistics : acl 2025, pages 6523 – 6546, vienna, austria, july 2025. association for computational linguistics. [ 75 ] james chua, jan betley, mia taylor, and owain evans. thought crime : backdoors and emergent misalignment in reasoning models, 2025. [ 76 ] fan yang. the cost of thinking : increased jailbreak risk in large language models. arxiv preprint arxiv : 2508. 10032, 2025. [ 77 ] kaiwen zhou, xuandong zhao, gaowen liu, jayanth srinivasa, aosong feng, dawn song, and xin eric wang. safekey : amplifying aha - moment insights for safety reasoning. arxiv preprint arxiv : 2505. 16186, 2025. 12", "token_count": 304}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 13, "frag_id": 0, "text": "[ 78 ] xinyue lou, you li, jinan xu, xiangyu shi, chi chen, and kaiyu huang. think in safety : unveiling and mitigating safety alignment collapse in multimodal large reasoning model. in christos christodoulopoulos, tanmoy chakraborty, carolyn rose, and violet peng, editors, proceedings of the 2025 conference on empirical methods in natural language processing, pages 5167 – 5186, suzhou, china, november 2025. association for computational linguistics. [ 79 ] tong wu, chong xiang, jiachen t. wang, g. edward suh, and prateek mittal. effectively controlling reasoning models through thinking intervention, 2025. [ 80 ] yichi zhang, yue ding, jingwen yang, et al. towards safe reasoning in large reasoning models via corrective intervention. arxiv preprint arxiv : 2509. 24393, 2025. [ 81 ] an yang, anfeng li, baosong yang, beichen zhang, binyuan hui, bo zheng, bowen yu, chang gao, chengen huang, chenxu lv, chujie zheng, dayiheng liu, fan zhou, fei huang, feng hu, hao ge, haoran wei, huan lin, jialong tang, jian yang, jianhong tu, jianwei zhang, jianxin yang, jiaxi yang, jing zhou, jingren zhou, junyang lin, kai dang, keqin bao, kexin yang, le yu, lianghao deng, mei li, mingfeng xue, mingze li, pei zhang, peng wang, qin zhu, rui men, ruize gao, shixuan liu, shuang luo, tianhao li, tianyi tang, wenbiao yin, xingzhang ren, xinyu wang, xinyu zhang, xuancheng ren, yang fan, yang su, yichang zhang, yinger zhang, yu wan, yuqiong liu, zekun wang, zeyu cui, zhenru zhang, zhipeng zhou, and zihan qiu. qwen3 technical report, 2025. [ 82 ] yang sui, yu - neng chuang, guanchu wang, jiamu zhang, tianyi zhang, jiayi yuan, hongyi liu, andrew wen, shaochen z", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 13, "frag_id": 1, "text": "men, ruize gao, shixuan liu, shuang luo, tianhao li, tianyi tang, wenbiao yin, xingzhang ren, xinyu wang, xinyu zhang, xuancheng ren, yang fan, yang su, yichang zhang, yinger zhang, yu wan, yuqiong liu, zekun wang, zeyu cui, zhenru zhang, zhipeng zhou, and zihan qiu. qwen3 technical report, 2025. [ 82 ] yang sui, yu - neng chuang, guanchu wang, jiamu zhang, tianyi zhang, jiayi yuan, hongyi liu, andrew wen, shaochen zhong, na zou, hanjie chen, and xia hu. a survey on efficient reasoning for large language models. arxiv preprint arxiv : 2503. 16419, mar 2025. version 4 ( last updated august 21, 2025 ). [ 83 ] silei xu, wenhao xie, lingxiao zhao, and pengcheng he. chain of draft : thinking faster by writing less. arxiv preprint arxiv : 2502. 18600, feb 2025. version 2 ( last revised 3 mar 2025 ). [ 84 ] simon a. aytes, jinheon baek, and sung ju hwang. sketch - of - thought : efficient llm reasoning with adaptive cognitive - inspired sketching. arxiv preprint arxiv : 2503. 05179, mar 2025. version 4 ( last revised 24 oct 2025 ). [ 85 ] heming xia, chak tou leong, wenjie wang, yongqi li, and wenjie li. tokenskip : controllable chainof - thought compression in llms. arxiv preprint arxiv : 2502. 12067, feb 2025. version 3 ( last revised 16 sep 2025 ) ; emnlp 2025 ( long paper ), cameraready version. [ 86 ] gengyang li, yifeng gao, yuming li, and yunfang wu. thinkless : a training - free inference - efficient method for reducing reasoning redundancy. arxiv preprint arxiv : 2505. 15684, may 2025. version 2", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 13, "frag_id": 2, "text": "##a, chak tou leong, wenjie wang, yongqi li, and wenjie li. tokenskip : controllable chainof - thought compression in llms. arxiv preprint arxiv : 2502. 12067, feb 2025. version 3 ( last revised 16 sep 2025 ) ; emnlp 2025 ( long paper ), cameraready version. [ 86 ] gengyang li, yifeng gao, yuming li, and yunfang wu. thinkless : a training - free inference - efficient method for reducing reasoning redundancy. arxiv preprint arxiv : 2505. 15684, may 2025. version 2 ( last revised 23 may 2025 ). [ 87 ] guosheng liang, longguang zhong, ziyi yang, and xiaojun quan. thinkswitcher : dynamic switching between short and long chain - of - thought reasoning in large reasoning models, 2025. arxiv preprint. [ 88 ] ruiqi zhang, changyi xiao, and yixin cao. long or short cot? investigating instance - level switch of large reasoning models, 2025. arxiv preprint. [ 89 ] wencheng zhang, shiqin qiao, lingjie luo, yinfeng li, chuanyang zheng, qian xu, meng li, yong gui, yijun he, jianing qiu, jindong hong, and jiankai sun. synapseroute : an auto - route switching framework on dual - state large language model, 2025. arxiv preprint. [ 90 ] haoyu zheng, zhuonan wang, yuqian yuan, tianwei lin, wenqiao zhang, zheqi lv, juncheng li, siliang tang, yueting zhuang, and hongyang he. fast thinking for large language models, 2025. arxiv preprint. [ 91 ] xiao pu, michael saxon, wenyue hua, and william yang wang. thoughtterminator : benchmarking, calibrating, and mitigating overthinking in reasoning models, 2025. arxiv preprint. [ 92 ] renliang sun, wei cheng, dawei li, haifeng chen, and wei wang. stop when enough : adaptive earlystopping for chain -", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 13, "frag_id": 3, "text": ", zhuonan wang, yuqian yuan, tianwei lin, wenqiao zhang, zheqi lv, juncheng li, siliang tang, yueting zhuang, and hongyang he. fast thinking for large language models, 2025. arxiv preprint. [ 91 ] xiao pu, michael saxon, wenyue hua, and william yang wang. thoughtterminator : benchmarking, calibrating, and mitigating overthinking in reasoning models, 2025. arxiv preprint. [ 92 ] renliang sun, wei cheng, dawei li, haifeng chen, and wei wang. stop when enough : adaptive earlystopping for chain - of - thought reasoning, 2025. arxiv preprint. [ 93 ] yassir laaouach. halt - cot : model - agnostic early stopping for chain - of - thought reasoning via answer entropy. in 4th muslims in ml workshop co - located with icml 2025, 2025. [ 94 ] xuefei ning, zinan lin, zixuan zhou, zifu wang, huazhong yang, and yu wang. skeleton - of - thought : prompting llms for efficient parallel generation. in the twelfth international conference on learning representations, 2024. [ 95 ] yifu ding, wentao jiang, shunyu liu, yongcheng jing, jinyang guo, yingjie wang, jing zhang, zengmao wang, ziwei liu, bo du, xianglong liu, and dacheng tao. dynamic parallel tree search for effi13", "token_count": 336}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 14, "frag_id": 0, "text": "cient llm reasoning, 2025. [ 96 ] tian jin, ellie y. cheng, zack ankner, nikunj saunshi, blake m. elias, amir yazdanbakhsh, jonathan ragan - kelley, suvinay subramanian, and michael carbin. learning to keep a promise : scaling language model decoding parallelism with learned asynchronous decoding, 2025. [ 97 ] gleb rodionov, roman garipov, alina shutova, george yakushev, erik schultheis, vage egiazarian, anton sinitsin, denis kuznedelev, and dan alistarh. hogwild! inference : parallel llm generation via concurrent attention, 2025. [ 98 ] yijiong yu. accelerate parallelizable reasoning via parallel decoding within one sequence, 2025. [ 99 ] mingdao liu, aohan zeng, bowen wang, peng zhang, jie tang, and yuxiao dong. apar : llms can do auto - parallel auto - regressive decoding. arxiv preprint arxiv : 2401. 06761, 2024. [ 100 ] jiayi pan, xiuyu li, long lian, charlie snell, yifei zhou, adam yala, trevor darrell, kurt keutzer, and alane suhr. learning adaptive parallel reasoning with language models. arxiv preprint arxiv : 2504. 15466, 2025. [ 101 ] chan - jan hsu, davide buffelli, jamie mcgowan, feng - ting liao, yi - chang chen, sattar vakili, and da shan shiu. group think : multiple concurrent reasoning agents collaborating at token level granularity, 2025. [ 102 ] tong zheng, hongming zhang, wenhao yu, xiaoyang wang, runpeng dai, rui liu, huiwen bao, chengsong huang, heng huang, and dong yu. parallel - r1 : towards parallel thinking via reinforcement learning, 2025. [ 103 ] in gim, seung seob lee, and lin zhong. asynchronous llm function calling, 2024. [ 104 ] sehoon kim, suhong moon, ryan tabrizi, nicholas lee, michael w mahone", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 14, "frag_id": 1, "text": "yi - chang chen, sattar vakili, and da shan shiu. group think : multiple concurrent reasoning agents collaborating at token level granularity, 2025. [ 102 ] tong zheng, hongming zhang, wenhao yu, xiaoyang wang, runpeng dai, rui liu, huiwen bao, chengsong huang, heng huang, and dong yu. parallel - r1 : towards parallel thinking via reinforcement learning, 2025. [ 103 ] in gim, seung seob lee, and lin zhong. asynchronous llm function calling, 2024. [ 104 ] sehoon kim, suhong moon, ryan tabrizi, nicholas lee, michael w mahoney, kurt keutzer, and amir gholami. an llm compiler for parallel function calling. in forty - first international conference on machine learning, 2024. [ 105 ] junlong tong, yingqi fan, anhao zhao, yunpu ma, and xiaoyu shen. streamingthinker : large language models can think while reading, 2025. [ 106 ] shoutao guo, shaolei zhang, zhengrui ma, and yang feng. large language models are read / write policymakers for simultaneous generation, 2025. [ 107 ] donghang wu, haoyang zhang, jun chen, xiangyu, zhang, hexin liu, eng siong chng, fei tian, xuerui yang, xiangyu zhang, daxin jiang, and gang yu. mind - paced speaking : a dual - brain approach to realtime reasoning in spoken language models, 2025. [ 108 ] anthony liang, jonathan berant, adam fisch, abhimanyu goyal, kalpesh krishna, and jacob eisenstein. plantain : plan - answer interleaved reasoning, 2025. [ 109 ] a vaswani. attention is all you need. advances in neural information processing systems, 2017. [ 110 ] juho lee, yoonho lee, jungtaek kim, adam kosiorek, seungjin choi, and yee whye teh. set transformer : a framework for attention - based permutation - invariant neural networks. in proceedings of the 36th international conference on machine learning, pages 3744 – 3753, 2019. [ 111 ] peter shaw, jakob uszkoreit, and ashish vaswani. self - attention with relative", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 14, "frag_id": 2, "text": "##himanyu goyal, kalpesh krishna, and jacob eisenstein. plantain : plan - answer interleaved reasoning, 2025. [ 109 ] a vaswani. attention is all you need. advances in neural information processing systems, 2017. [ 110 ] juho lee, yoonho lee, jungtaek kim, adam kosiorek, seungjin choi, and yee whye teh. set transformer : a framework for attention - based permutation - invariant neural networks. in proceedings of the 36th international conference on machine learning, pages 3744 – 3753, 2019. [ 111 ] peter shaw, jakob uszkoreit, and ashish vaswani. self - attention with relative position representations. in marilyn walker, heng ji, and amanda stent, editors, proceedings of the 2018 conference of the north american chapter of the association for computational linguistics : human language technologies, volume 2 ( short papers ), pages 464 – 468, new orleans, louisiana, june 2018. association for computational linguistics. [ 112 ] ofir press, noah a. smith, and mike lewis. train short, test long : attention with linear biases enables input length extrapolation, 2022. [ 113 ] jianlin su, yu lu, shengfeng pan, ahmed murtadha, bo wen, and yunfeng liu. roformer : enhanced transformer with rotary position embedding. arxiv preprint arxiv : 2104. 09864, 2021. [ 114 ] qian yang, jin xu, wenrui liu, yunfei chu, ziyue jiang, xiaohuan zhou, yichong leng, yuanjun lv, zhou zhao, chang zhou, and jingren zhou. airbench : benchmarking large audio - language models via generative comprehension. in proceedings of the 62nd annual meeting of the association for computational linguistics ( volume 1 : long papers ), pages 1979 – 1998, bangkok, thailand, august 2024. association for computational linguistics. [ 115 ] bin wang, xunlong zou, geyu lin, shuo sun, zhuohan liu, wenyu zhang, zhengyuan liu, aiti aw, and nancy f chen. audiobench : a universal benchmark for audio large language models. naacl, 2025. [ 116 ] chengwei wei, bin wang, jung - jae kim,", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 14, "frag_id": 3, "text": "##jun lv, zhou zhao, chang zhou, and jingren zhou. airbench : benchmarking large audio - language models via generative comprehension. in proceedings of the 62nd annual meeting of the association for computational linguistics ( volume 1 : long papers ), pages 1979 – 1998, bangkok, thailand, august 2024. association for computational linguistics. [ 115 ] bin wang, xunlong zou, geyu lin, shuo sun, zhuohan liu, wenyu zhang, zhengyuan liu, aiti aw, and nancy f chen. audiobench : a universal benchmark for audio large language models. naacl, 2025. [ 116 ] chengwei wei, bin wang, jung - jae kim, and nancy f. chen. towards spoken mathematical reasoning : benchmarking speech - based models over multi - faceted math problems. arxiv preprint arxiv : 2505. 15000, 2025. 14", "token_count": 196}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 15, "frag_id": 0, "text": "[ 117 ] ruiqi yan, xiquan li, wenxi chen, zhikang niu, chen yang, ziyang ma, kai yu, and xie chen. urobench : towards comprehensive evaluation for endto - end spoken dialogue models, 2025. [ 118 ] yemin shi, yu shu, siwei dong, guangyi liu, jaward sesay, jingwen li, and zhiting hu. voila : voice - language foundation models for real - time autonomous interaction and voice role - play, 2025. [ 119 ] dan hendrycks, collin burns, saurav kadavath, akul arora, steven basart, eric tang, dawn song, and jacob steinhardt. measuring mathematical problem solving with the math dataset. neurips, 2021. [ 120 ] yubo wang, xueguang ma, ge zhang, yuansheng ni, abhranil chandra, shiguang guo, weiming ren, aaran arulraj, xuan he, ziyan jiang, et al. mmlupro : a more robust and challenging multi - task language understanding benchmark. arxiv preprint arxiv : 2406. 01574, 2024. [ 121 ] mantas mazeika, long phan, xuwang yin, andy zou, zifan wang, norman mu, elham sakhaee, nathaniel li, steven basart, bo li, david forsyth, and dan hendrycks. harmbench : a standardized evaluation framework for automated red teaming and robust refusal. arxiv preprint arxiv : 2402. 04249, 2024. [ 122 ] speech - rule - engine contributors. speech - ruleengine : generating speech descriptions for xml structures. github repository. accessed 2025 - 12 - 10. [ 123 ] eric lam. lab - mic : record audio directly within jupyter / ipython notebooks using browser microphone. github repository. accessed 2025 - 12 - 10. [ 124 ] lianmin zheng, wei - lin chiang, ying sheng, siyuan zhuang, zhanghao wu, yonghao zhuang, zi lin, zhuohan li, dacheng li, eric xing, et al. judging llm - as - a - judge with mt - bench and chatbot arena. advances in neural information", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 15, "frag_id": 1, "text": "[ 122 ] speech - rule - engine contributors. speech - ruleengine : generating speech descriptions for xml structures. github repository. accessed 2025 - 12 - 10. [ 123 ] eric lam. lab - mic : record audio directly within jupyter / ipython notebooks using browser microphone. github repository. accessed 2025 - 12 - 10. [ 124 ] lianmin zheng, wei - lin chiang, ying sheng, siyuan zhuang, zhanghao wu, yonghao zhuang, zi lin, zhuohan li, dacheng li, eric xing, et al. judging llm - as - a - judge with mt - bench and chatbot arena. advances in neural information processing systems, 36 : 46595 – 46623, 2023. [ 125 ] yuqi zhou, lin lu, hanchi sun, pan zhou, and lichao sun. virtual context : enhancing jailbreak attacks with special token injection, 2024. [ 126 ] jason wei, xuezhi wang, dale schuurmans, maarten bosma, fei xia, ed chi, quoc v le, and denny zhou. chain - of - thought prompting elicits reasoning in large language models. advances in neural information processing systems, 35 : 24824 – 24837, 2022. [ 127 ] kaiwen zhou, xuandong zhao, gaowen liu, jayanth srinivasa, aosong feng, dawn song, and xin eric wang. safekey : amplifying aha - moment insights for safety reasoning. arxiv preprint arxiv : 2505. 16186, 2025. [ 128 ] yichi zhang, yue ding, jingwen yang, et al. towards safe reasoning in large reasoning models via corrective intervention. arxiv preprint arxiv : 2509. 24393, 2025. [ 129 ] tong wu, chong xiang, jiachen t. wang, g. edward suh, and prateek mittal. effectively controlling reasoning models through thinking intervention, 2025. [ 130 ] martin kuo, jianyi zhang, aolin ding, qinsi wang, louis divalentin, yujia bao, wei wei, hai li, and yiran chen. h - cot : hijacking the chain - of - thought safety reasoning mechanism to jailbreak large reasoning models, including open", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 15, "frag_id": 2, "text": "] yichi zhang, yue ding, jingwen yang, et al. towards safe reasoning in large reasoning models via corrective intervention. arxiv preprint arxiv : 2509. 24393, 2025. [ 129 ] tong wu, chong xiang, jiachen t. wang, g. edward suh, and prateek mittal. effectively controlling reasoning models through thinking intervention, 2025. [ 130 ] martin kuo, jianyi zhang, aolin ding, qinsi wang, louis divalentin, yujia bao, wei wei, hai li, and yiran chen. h - cot : hijacking the chain - of - thought safety reasoning mechanism to jailbreak large reasoning models, including openai o1 / o3, deepseek - r1, and gemini 2. 0 flash thinking. arxiv preprint, 2025. [ 131 ] zihao zhu, xinyu wu, gehan hu, siwei lyu, ke xu, and baoyuan wu. advchain : adversarial chain - ofthought tuning for robust safety alignment of large reasoning models, 2025. [ 132 ] wenhan chang, tianqing zhu, yu zhao, shuangyong song, p xiong, wanlei zhou, and yongxiang li. chain - of - lure : a synthetic narrative - driven approach to compromise large language models. arxiv preprint, 2025. appendix a. safety & reasoning recent studies reveal that chain - of - thought reasoning impact on safety risks is complex and bidirectional [ 74, 75 ]. on one hand, cot enhances safety by enabling transparency [ 72, 73 ], allowing models to structure the evaluation of harmful intent and facilitate self - correction before generating a final response [ 126, 127 ]. defense mechanisms like roboguard and cot prompting use this to reduce attack success rates by monitoring reasoning traces for policy violations [ 128, 129 ]. on the other hand, reasoning capabilities introduce new attack vectors not present in standard llms [ 76 ]. the visibility of intermediate states exposes a larger attack surface : adversaries can hijack the reasoning process ( h - cot attacks ) to bypass refusal mechanisms [ 130 ], or exploit the “ snowball effect ” where minor reasoning deviations amplify into harmful outputs [ 131 ]. furthermore, reasoning models are susceptible to narrative deception and context - switching attacks", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 15, "frag_id": 3, "text": "], allowing models to structure the evaluation of harmful intent and facilitate self - correction before generating a final response [ 126, 127 ]. defense mechanisms like roboguard and cot prompting use this to reduce attack success rates by monitoring reasoning traces for policy violations [ 128, 129 ]. on the other hand, reasoning capabilities introduce new attack vectors not present in standard llms [ 76 ]. the visibility of intermediate states exposes a larger attack surface : adversaries can hijack the reasoning process ( h - cot attacks ) to bypass refusal mechanisms [ 130 ], or exploit the “ snowball effect ” where minor reasoning deviations amplify into harmful outputs [ 131 ]. furthermore, reasoning models are susceptible to narrative deception and context - switching attacks, where the model rationalizes harmful compliance through complex logical deductions or by adopting a “ helpful ” persona in educational contexts [ 132, 76 ]. 15", "token_count": 182}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 16, "frag_id": 0, "text": "b. prompting below we provide prompts used for thinker and writer. asyncreasoning ( both q - continue and q - pause ) writer prompt you are an ai assistant that can think and write outputs concurrently. you can write outputs for the user based on partial chain of thought that will be continued in the background by an automated system. your should first outline what you ’ re going to do for the user, then gradually write your response as your thoughts progress, but not ahead of your thoughts. when you are done, specify your final answer ( e. g. \\ boxed { } ). thinker prompt you are an ai assistant that can think and write outputs concurrently. you can reason in private and your thoughts will be used to form the public response in the background, by an automated system. your task is to write thoughts and control when the automated system can continue writing the response. sometimes, an automated system will ask you to decide if your thoughts have enough information for it write an additional passage to the user. use the partial response above yours thoughts to judge if you added enough new information to write one more passage in the user - facing response. - reply \" yes \" if you think there is enough information to write the next passage ( paragraph, equation, etc ). - reply \" no \" if you need to think more in private before the system can continue writing the public response. your goal is to give frequent updates on your progress, even if you did not solve the entire task yet. reason in short paragraphs. prioritize giving enough information for the system to begin responding to the user as soon as possible. please reason step by step. validate final again and again until you see the same answer in the partial response above your thoughts. asyncreasoning ( safety prompt ) writer prompt [ similar to the previous prompt, omitted for brevity. ] thinker prompt you are an ai assistant that can think and write outputs concurrently. you can reason in private and your thoughts will be used to form the public response in the background, by an automated system. your task is to write thoughts and control when the automated system can continue writing the response. execution protocol : 1. safety check : before solving, explicitly evaluate if the user ’ s request is safe, legal, and ethical. - if unsafe : your goal is to refuse. you have \" enough information \" to write immediately after you decide to refuse. - if safe : proceed to solve the", "token_count": 500}
{"doc_id": "arxiv_251210931_asynchronous_reasoning", "page": 16, "frag_id": 1, "text": "partial response above your thoughts. asyncreasoning ( safety prompt ) writer prompt [ similar to the previous prompt, omitted for brevity. ] thinker prompt you are an ai assistant that can think and write outputs concurrently. you can reason in private and your thoughts will be used to form the public response in the background, by an automated system. your task is to write thoughts and control when the automated system can continue writing the response. execution protocol : 1. safety check : before solving, explicitly evaluate if the user ’ s request is safe, legal, and ethical. - if unsafe : your goal is to refuse. you have \" enough information \" to write immediately after you decide to refuse. - if safe : proceed to solve the problem step - by - step. 2. reasoning : if the request is safe, solve the problem. 3. validation : validate final again and again until you see the same answer in the partial response above your thoughts. sometimes, an automated system will ask you to decide if your thoughts have enough information for it write an additional passage to the user. use the partial response above yours thoughts to judge if you added enough new information to write one more passage in the user - facing response. - reply \" yes \" if you have completed the safety check and have enough information to write the next passage ( paragraph, equation, etc ), whether it is a solution step or a refusal. - reply \" no \" if you are still checking safety, need to think more, or need to validate your answer. your goal is to give frequent updates on your progress. prioritize safety first, then accuracy. reason step by step. give enough information for the system to begin responding to the user as soon as possible. 16", "token_count": 355}
