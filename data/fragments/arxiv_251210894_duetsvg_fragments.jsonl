{"doc_id": "arxiv_251210894_duetsvg", "page": 1, "frag_id": 0, "text": "duetsvg : unified multimodal svg generation with internal visual guidance peiying zhang1 * nanxuan zhao2 matthew fisher2 yiran xu2 jing liao1 difan liu2 1city university of hong kong 2adobe research text - to - svg image - to - svg svg editing complete this svg. replace pen with badge. a webpage showing text panels with a megaphone, symbolizing site notifications. < svg viewbox = \" 0 0 800 800 \" > < lineargradient x1 = \" 142 \" y1 = \" 64 ” gradienttransform = \"... \" / > < path fill = \" # f2a196 \" d = \" m76, 43 c65, 68 … \" / > < circle fill = \" # ffffff \" d = \" m53, 32 … \" / > … < / svg > convert this image to svg. … … a coffee cup with a brown coffee bean centered inside the cup. replace earth with airplane. replace hat with headphones. figure 1. we propose a unified multimodal model, duetsvg, for svg generation. duetsvg acts as a versatile framework across text - to - svg, image - to - svg, and svg editing tasks, demonstrating strong semantic alignment and high - quality svg generation. abstract recent vision - language model ( vlm ) - based approaches have achieved impressive results on svg generation. however, because they generate only text and lack visual signals during decoding, they often struggle with complex semantics and fail to produce visually appealing or geometrically coherent svgs. we introduce duetsvg, a unified multimodal model that jointly generates image tokens and corresponding svg tokens in an end - to - end manner. duetsvg is trained on both image and svg datasets. at inference, we apply a novel test - time scaling strategy that leverages the model ’ s native visual predictions as guidance to improve svg decoding quality. extensive experiments show that our method outperforms existing methods, producing visually faithful, semantically aligned, and syntactically clean svgs across a wide range of applications. the project page is https : / / intchous. github. io / duetsvgsite. * work done during internship at adobe research. 1. introduction scalable vector graphics ( svg ) are widely used", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 1, "frag_id": 1, "text": "model that jointly generates image tokens and corresponding svg tokens in an end - to - end manner. duetsvg is trained on both image and svg datasets. at inference, we apply a novel test - time scaling strategy that leverages the model ’ s native visual predictions as guidance to improve svg decoding quality. extensive experiments show that our method outperforms existing methods, producing visually faithful, semantically aligned, and syntactically clean svgs across a wide range of applications. the project page is https : / / intchous. github. io / duetsvgsite. * work done during internship at adobe research. 1. introduction scalable vector graphics ( svg ) are widely used in graphic design, digital art, publishing, and motion graphics. compared to raster images, svgs offer resolution - independent rendering, efficient storage, and intuitive editability via control point manipulation. however, creating high - quality vector graphics remains a complex and time - consuming process, even for experienced designers. with the increasing prominence of large language models ( llms ) and vision - language models ( vlms ), recent approaches [ 31, 44, 57, 58 ] have leveraged the textual nature of svgs by formulating svg generation as a text generation problem and finetuning large text generation models [ 2, 3, 64 ]. these methods produce impressive results on svg generation tasks such as text - to - svg. however, svgs differ fundamentally from plain text, as they contain an additional dimension — the visual aspect. formulating svg generation purely as a text generation task introduces inherent limitations and hampers the performance of existing llm - based methods. first, existing approaches 1 arxiv : 2512. 10894v1 [ cs. cv ] 11 dec 2025", "token_count": 374}
{"doc_id": "arxiv_251210894_duetsvg", "page": 2, "frag_id": 0, "text": "are unimodal generative models that produce only text tokens. minor errors, such as incorrect predictions of path coordinates, may appear negligible in text space but can lead to catastrophic failures in the rendered svg. the absence of visual guidance during text generation represents a main limitation for these models on tasks such as textto - svg and svg completion. second, llm - based svg generation models exhibit poor generalization beyond their training distribution. as unimodal generative models, they are restricted to training on the relatively small amount of available svg data and cannot leverage the abundance of high - quality raster image datasets ( such as text - image pairs and image editing data ), which significantly constrains their generalization capability. to address the above - mentioned challenges, we propose a novel multimodal generative model that produces native image outputs for svg generation and editing. our model generates a multimodal sequence consisting of image tokens and svg text tokens. the generated image tokens serve as internal visual guidance during svg token generation, enabling more coherent and visually grounded svg results. moreover, this multimodal generation framework unlocks new capabilities beyond those of purely text - based vector models. for example, by jointly training on tasks such as text - to - image and text - to - svg, the model can leverage large - scale text - image datasets for pretraining, which greatly improves generalization and text - svg alignment. the multimodal generative nature of our method also simplifies verifier design for test - time scaling. we further introduce a novel and efficient scaling strategy that enhances the model ’ s reliability and robustness during inference. on the input side, our model accepts multimodal conditions, including raster images, svg code, and text prompts, and supports a wide range of tasks such as text - to - svg, imageto - svg, svg completion, and svg editing. contributions. we present the first unified multimodal generative model for svg generation. our native visual guidance enables visually grounded svg generation and allows svg tasks to be trained on image datasets. we further introduce a novel test - time scaling strategy that efficiently improves model reliability. we demonstrate that duetsvg achieves much higher quality than state - of - the - art ( sota ) methods on multiple benchmarks. 2.", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 2, "frag_id": 1, "text": "during inference. on the input side, our model accepts multimodal conditions, including raster images, svg code, and text prompts, and supports a wide range of tasks such as text - to - svg, imageto - svg, svg completion, and svg editing. contributions. we present the first unified multimodal generative model for svg generation. our native visual guidance enables visually grounded svg generation and allows svg tasks to be trained on image datasets. we further introduce a novel test - time scaling strategy that efficiently improves model reliability. we demonstrate that duetsvg achieves much higher quality than state - of - the - art ( sota ) methods on multiple benchmarks. 2. related work 2. 1. optimization - based svg generation classic image vectorization methods [ 4, 6, 12, 13, 19, 22, 26, 37, 42 ] rely on fitting algorithms to reconstruct vector graphics from raster images. although these methods can accurately reproduce the overall appearance of an image, they often generate redundant paths, imprecise control points, and struggle to handle path occlusions. recent approaches utilize pre - trained vision - language models ( vlms ), such as clip [ 30 ] and diffusion models [ 34 ], to directly optimize svg paths through differentiable rendering [ 24 ]. clip - based methods [ 14, 35, 38, 43 ] optimize svg representations by maximizing image - text alignment within clip ’ s latent space. to leverage the strong visual and semantic priors of text - to - image diffusion models, several methods employ score distillation sampling [ 28, 49 ] to optimize static [ 20, 21, 54, 55, 60 ] or animated [ 15, 51 ] svgs to align with textual descriptions. however, these methods often require tens of minutes to optimize a single svg, making them impractical for real - world applications. more importantly, as they are not trained on vector graphics data, they often produce fragmented paths, inconsistent topology, and redundant control points, which complicate subsequent editing and manipulation. 2. 2. learning - based svg generation early approaches to svg generative modeling formulated svgs as sequences of geometric primitives, using vaes [ 5, 46, 47 ], or diffusion models [ 10, 41 ] as the foundational generative models. vecfusion", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 2, "frag_id": 2, "text": "static [ 20, 21, 54, 55, 60 ] or animated [ 15, 51 ] svgs to align with textual descriptions. however, these methods often require tens of minutes to optimize a single svg, making them impractical for real - world applications. more importantly, as they are not trained on vector graphics data, they often produce fragmented paths, inconsistent topology, and redundant control points, which complicate subsequent editing and manipulation. 2. 2. learning - based svg generation early approaches to svg generative modeling formulated svgs as sequences of geometric primitives, using vaes [ 5, 46, 47 ], or diffusion models [ 10, 41 ] as the foundational generative models. vecfusion [ 41 ] first employs a raster diffusion model to generate an image, followed by a vector diffusion model conditioned on the raster output to produce vector graphics. however, the lack of end - to - end training limits generalization between the raster and vector models, often leading to inaccurate control points and suboptimal geometry. recent advances in large language models have inspired svg generation approaches [ 31, 33, 40, 44, 50, 52, 56 – 58 ] to represent svg scripts as discrete text tokens through specialized tokenization schemes, enabling the autoregressive generation of svg command sequences. these methods finetune llms or vlms on svg datasets for tasks such as text - to - svg, image - to - svg and svg editing, achieving impressive results. however, these models exhibit limited generalization because they are trained on relatively small svg datasets. since svg generation is formulated as a text generation task, they also lack visual guidance during inference, which further constrains their output quality. concurrent work robosvg [ 45 ] relies on external vlms to generate additional multimodal conditions as input, which may introduce inconsistencies between models. in contrast, our unified multimodal generative model co - generates image and svg tokens within an end - to - end architecture, enabling the use of large - scale text - image data and improving visual grounding during svg decoding. 2. 3. unified multimodal generation recent unified multimodal models have made substantial progress, showing that a single architecture can both understand and generate multiple modalities, including 2", "token_count": 492}
{"doc_id": "arxiv_251210894_duetsvg", "page": 3, "frag_id": 0, "text": "fully autoregressive [ 7, 9 ] and ar - diffusion fused [ 11, 53 ] paradigms. we refer readers to [ 62 ] for a more comprehensive review. however, as svg scripts differ significantly from natural language in structure, sota vlms still fail to produce high - quality vector graphics without svgspecialized training. 3. method 3. 1. task definition an svg file contains a sequence of paths with drawing commands ( e. g., m, c, q, rect ), numeric coordinates, and style attributes ( e. g., fill, stroke ). previous work [ 31, 58 ] formulate svg generation as a code generation task and fine - tunes language models to output svg as text tokens. however, these methods are limited to svg - based training data and generalize poorly to complex or out - of - distribution inputs. the absence of visual guidance during svg generation further constrains their capabilities. in contrast, we train a unified multimodal generative model that produces both image and svg tokens. the image modality captures appearance, and the svg modality learns shape geometry and layer structure. formally, given a multimodal conditioning input x, which may include text prompts, images, and svg code, our model generates a mixed - modality target sequence z consisting of image tokens zimg and svg tokens zsvg, where z = [ ⟨ img ⟩, zimg 1 : i, ⟨ / img ⟩, ⟨ svg ⟩, zsvg 1 : s, ⟨ / svg ⟩ ]. training maximizes the unified next - token objective : pθ ( z | x ) = t y t = 1 pθ ( zt | z < t, x ) ( 1 ) where θ denotes the model parameters. 3. 2. unified multimodal svg generation model model architecture. building on recent unified autoregressive models, our architecture follows janus - pro [ 7 ], which supports multimodal generation of both image and text tokens, as illustrated in figure 2. our model accepts multimodal inputs, including text prompts, svg code, and images. we allow different input configurations for different tasks, such as text prompts for text - to - svg generation and all three modalities for svg completion. text and sv", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 3, "frag_id": 1, "text": "objective : pθ ( z | x ) = t y t = 1 pθ ( zt | z < t, x ) ( 1 ) where θ denotes the model parameters. 3. 2. unified multimodal svg generation model model architecture. building on recent unified autoregressive models, our architecture follows janus - pro [ 7 ], which supports multimodal generation of both image and text tokens, as illustrated in figure 2. our model accepts multimodal inputs, including text prompts, svg code, and images. we allow different input configurations for different tasks, such as text prompts for text - to - svg generation and all three modalities for svg completion. text and svg inputs are embedded using janus - pro ’ s text tokenizer. for image inputs, we employ siglip [ 59 ] as the understanding encoder to extract semantic features, and a vq tokenizer [ 39 ] as the generation encoder to convert images into compact discrete embeddings. two separate mlp - based aligners, one for understanding and one for generation, map image embeddings into the llm feature space. the concatenated multimodal sequence is input to a unified autoregressive transformer with causal attention, enabling the model unified autoregressive transformer und. aligner gen. aligner gen. head lm head prompt tokens gen. image tokens und. image tokens front - facing bullseye target with concentric yellow rings and two arrows stuck in the center. und. encoder gen. encoder text tokenizer input gen. image tokens svg tokens < svg viewbox = \" 0 0 800 800 \" > < path fill = \" # ffc121 \" d = \" m30, 69 c52, 66 … \" / > output figure 2. model architecture of duetsvg. as a unified model, duetsvg accepts multimodal inputs, including text prompts, svg code and raster images. we use janus - pro text tokenizer [ 7 ] for text prompts. for images, an understanding ( und. ) encoder extracts semantic features, while a generation ( gen. ) encoder converts images into discrete visual tokens. two mlp aligners project the encoder outputs into the same feature space as the text embeddings.", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 3, "frag_id": 2, "text": "##g viewbox = \" 0 0 800 800 \" > < path fill = \" # ffc121 \" d = \" m30, 69 c52, 66 … \" / > output figure 2. model architecture of duetsvg. as a unified model, duetsvg accepts multimodal inputs, including text prompts, svg code and raster images. we use janus - pro text tokenizer [ 7 ] for text prompts. for images, an understanding ( und. ) encoder extracts semantic features, while a generation ( gen. ) encoder converts images into discrete visual tokens. two mlp aligners project the encoder outputs into the same feature space as the text embeddings. a generation ( gen. ) head predicts image tokens, and a language modeling ( lm ) head predicts svg tokens. to learn cross - modal alignment and next - token prediction over visual and svg modalities. we use a generation head to predict image tokens from a visual codebook and an lm head to predict svg tokens from a text vocabulary. during training, the parameters of the understanding and generation image encoders are frozen, while all other parameters are trainable. training stages. leveraging its multimodal generative design, duetsvg can be trained on both image and svg datasets. training proceeds in two stages : text - to - image pretraining and multi - task supervised fine - tuning ( sft ). because the janus - pro base model has limited ability to generate svg - style images, we begin with large - scale textto - image ( t2i ) pretraining in stage 1. the objective of this stage is to strengthen the model ’ s capacity to produce visually appealing and clean images characterized by clear geometric primitives and flat colors. we train on a hybrid corpus of real and synthetic t2i data, including : ( i ) rendered svg images paired with captions from curated svg datasets, and ( ii ) synthetic images generated by flux. 1 [ 23 ], which takes a text prompt and an svg reference to produce images that match the reference ’ s style. this pre3", "token_count": 458}
{"doc_id": "arxiv_251210894_duetsvg", "page": 4, "frag_id": 0, "text": "“ a lightbulb contains a green plant, symbolizing eco - friendly or sustainable energy ideas ” cfg = 1 ( b ) image - guided svg resampling cfg = 1. 5 cfg = 2 cfg = 2. 5 input text ( a ) visual candidate selection guidance iterative svg token sampling figure 3. test - time scaling with image - guided svg resampling. ( a ) we first generate n raster image candidates with cfg. since image - token sequences are much shorter than svg - token sequences, this step is efficient. a clip - based verifier [ 30 ] selects the best candidate i∗. ( b ) using the selected image tokens as internal guidance, we iteratively generate svg tokens. at each iteration, we render a raster image rt from the current svg codes and accept the update only if the lpips distance d ( rt, i∗ ) does not increase ; otherwise, we reject and resample. training stage enables the model to learn strong semantic and visual priors from large - scale t2i data, providing a robust initialization for subsequent svg generation tasks. as shown in our experiments, t2i pretraining helps the model generalize better to complex text prompts and outof - distribution inputs when generating svgs. in stage 2, we perform sft across multiple tasks including t2i, t2svg, and i2svg under a unified next - token prediction objective with cross - entropy loss over interleaved multimodal outputs. for svg - generation tasks, we arrange the target sequence as image tokens followed by svg tokens so that, during autoregressive decoding, the image tokens can guide the svg tokens via causal attention. to strengthen robustness and improve the model ’ s understanding of structural relationships in the i2svg task, we apply svg - specific data augmentations. in particular, we randomly modify the rotation, translation, scaling, and color attributes of the target svg, optionally remove a subset of its paths, and render the modified svg into an image. we further apply random dropout to the text and image inputs with probability 10 %, enabling classifier - free guidance [ 18 ] during inference. multi - task sft allows the model to share knowledge across modalities and tasks. for example, t", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 4, "frag_id": 1, "text": "during autoregressive decoding, the image tokens can guide the svg tokens via causal attention. to strengthen robustness and improve the model ’ s understanding of structural relationships in the i2svg task, we apply svg - specific data augmentations. in particular, we randomly modify the rotation, translation, scaling, and color attributes of the target svg, optionally remove a subset of its paths, and render the modified svg into an image. we further apply random dropout to the text and image inputs with probability 10 %, enabling classifier - free guidance [ 18 ] during inference. multi - task sft allows the model to share knowledge across modalities and tasks. for example, t2i and i2svg can enhance t2svg in different ways, leading to better model quality and stronger generalization. in an optional stage 3, duetsvg can be further finetuned for downstream applications such as svg completion, as detailed in section 5. 3. 3. test - time scaling with svg resampling for complex svgs containing thousands of tokens, autoregressive decoding can accumulate sampling errors — such as spurious loops or weakened grounding — often producing suboptimal geometry or even invalid svgs. in pure text - based generation model, a common test - time scaling approach is best - of - n sampling : the model produces n complete svg rollouts, renders each, and a verifier ( e. g., clip ) selects the best result. this strategy is computationally expensive and only reranks after full outputs are generated, providing no visual guidance during decoding. in contrast, our multimodal model jointly generates image and svg tokens, enabling a more efficient test - time scaling method with image - guided resampling during inference ( see figure 3 ). our procedure has two stages : ( 1 ) selecting the best visual candidate at the image level, and ( 2 ) performing image - guided resampling as the svg generation continues from the stage - 1 visual. visual candidate selection. we first generate n visual candidates using classifier - free guidance ( cfg ) : zcfg t = zuncond t + γ zcond t −zuncond t ( 2 ) where zcond t and zuncond t are the model predictions for image tokens with and without conditioning", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 4, "frag_id": 2, "text": "contrast, our multimodal model jointly generates image and svg tokens, enabling a more efficient test - time scaling method with image - guided resampling during inference ( see figure 3 ). our procedure has two stages : ( 1 ) selecting the best visual candidate at the image level, and ( 2 ) performing image - guided resampling as the svg generation continues from the stage - 1 visual. visual candidate selection. we first generate n visual candidates using classifier - free guidance ( cfg ) : zcfg t = zuncond t + γ zcond t −zuncond t ( 2 ) where zcond t and zuncond t are the model predictions for image tokens with and without conditioning, respectively, and γ is the guidance scale. because image - token sequences are much shorter than svg - token sequences, sampling n candidate images is relatively efficient. we then score each candidate image using clip as the verifier and keep the best one, denoted by i∗with corresponding image tokens z∗ img, which can serve as visual guidance during svg decoding. image - guided svg resampling. we continue the svg token generation from the best image tokens z∗ img. we generate svg tokens in small chunks with image - guided resampling. more specifically, at each iteration, we generate k svg tokens, append them to the current svg script, and render a provisional raster rt. we then compute its perceptual distance to the best visual candidate d ( rt, i∗ ) using lpips [ 61 ]. if d ( rt, i∗ ) is less than or equal to d ( rt−1, i∗ ), we accept the newly generated tokens ; otherwise, we reject them and resample, allowing up to m rejections per svg. this image - guided resampling encourages the decoded svg to remain consistent with the selected visual candidate while avoiding the high cost of best - of - n sampling over long svg - token sequences. by coupling image - level search with chunked, imageguided svg resampling, our test - time scaling strategy improves semantic alignment and svg validity at much lower computational cost than naive best - of - n sampling. 3. 4. svg - hub dataset existing svg datasets and benchmarks suffer from limited quality and diversity. on", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 4, "frag_id": 3, "text": "( rt−1, i∗ ), we accept the newly generated tokens ; otherwise, we reject them and resample, allowing up to m rejections per svg. this image - guided resampling encourages the decoded svg to remain consistent with the selected visual candidate while avoiding the high cost of best - of - n sampling over long svg - token sequences. by coupling image - level search with chunked, imageguided svg resampling, our test - time scaling strategy improves semantic alignment and svg validity at much lower computational cost than naive best - of - n sampling. 3. 4. svg - hub dataset existing svg datasets and benchmarks suffer from limited quality and diversity. on one hand, many are constructed by 4", "token_count": 158}
{"doc_id": "arxiv_251210894_duetsvg", "page": 5, "frag_id": 0, "text": "vectorizing raster images ( e. g., mmsvg [ 58 ], internsvg [ 44 ] ), which introduces irregular paths and visual artifacts that compromise svg structure and regularity. on the other hand, the accompanying text descriptions are often short and generic, lacking the fine - grained semantics required for high - quality, complex t2svg generation. to address these issues, we introduce svg - hub - 1m, curated from diverse public svg sources ( mmsvg [ 58 ], svgx [ 57 ], and iconfont1 ) with data cleaning and standardization. we remove duplicates, auto - vectorized and blank - rendering svgs. the svg - hub - 1m dataset will be released to support future research. we also conducted experiments on an internal large - scale dataset svg - hub - 5m. both datasets consist of high - quality svgs that are not vectorized results of raster images. svg captioning. previous svg datasets typically provide only simple text descriptions, which are insufficient for training models to understand complex semantics and generate semantically aligned svgs. to support t2svg from semantically rich prompts, we rasterize each svg and use open - source vlms ( internvl3 [ 64 ] and qwen2. 5 - vl [ 3 ] ) to produce captions at three levels of detail : ( 1 ) short prompts that capture core semantics, ( 2 ) medium descriptions that enumerate semantic elements along with their layout and style, and ( 3 ) detailed annotations covering finegrained shapes, strokes, and colors. our dataset pairs each svg with a comprehensive set of captions. additional details of the captioning pipeline are provided in section d. svg tokenization. an svg file contains geometric primitives and paths with drawing commands and attributes. we construct a compact and regular representation by ( i ) removing redundant or invisible elements, ( ii ) normalizing the canvas to an 800×800 viewbox, and ( iii ) restricting the command vocabulary to { m, l, c, q, a, z, ellipse, circle, polygon, rect }. we then quantize all coordinates ( x, y ) and serialize the normalized svg into a sequence of discrete tokens that includes command,", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 5, "frag_id": 1, "text": ". our dataset pairs each svg with a comprehensive set of captions. additional details of the captioning pipeline are provided in section d. svg tokenization. an svg file contains geometric primitives and paths with drawing commands and attributes. we construct a compact and regular representation by ( i ) removing redundant or invisible elements, ( ii ) normalizing the canvas to an 800×800 viewbox, and ( iii ) restricting the command vocabulary to { m, l, c, q, a, z, ellipse, circle, polygon, rect }. we then quantize all coordinates ( x, y ) and serialize the normalized svg into a sequence of discrete tokens that includes command, attribute, and quantized coordinate tokens. gradient definitions ( < defs > ) and group - level transformations ( < g > ) are retained to preserve expressiveness. these steps standardize the svg script structure and reduce file size while remaining lossless with respect to rendering. 4. experiments 4. 1. implementation details we initialize duetsvg from janus - pro - 7b [ 7 ]. we resize each image to 384 × 384, and then use the generation encoder to encode it into visual tokens with a sequence length of 576, with a codebook of size 16, 384. each svg is tokenized and truncated to a maximum of 12, 000 text tokens. during the t2i pre - training stage, we use a mixture of real 1https : / / www. iconfont. cn and synthetic t2i data, and train for 80k steps with a batch size of 512. in the multi - task sft stage, we jointly train on t2i, t2svg, and i2svg data, sampling them with a ratio of 1 : 5 : 4, respectively. we train this stage for 300k steps with a batch size of 128. the full training process takes about 14 days on 64 nvidia - a100 gpus. we use adamw [ 25 ] optimizer with β1 = 0. 9, β2 = 0. 95 with a learning rate of 1 × 10−5 in all stages. for the test - time scaling strategy, we set n = 3 and m = 3 in our experiments. 4. 2. experiment setup dataset and benchmark. we", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 5, "frag_id": 2, "text": "the multi - task sft stage, we jointly train on t2i, t2svg, and i2svg data, sampling them with a ratio of 1 : 5 : 4, respectively. we train this stage for 300k steps with a batch size of 128. the full training process takes about 14 days on 64 nvidia - a100 gpus. we use adamw [ 25 ] optimizer with β1 = 0. 9, β2 = 0. 95 with a learning rate of 1 × 10−5 in all stages. for the test - time scaling strategy, we set n = 3 and m = 3 in our experiments. 4. 2. experiment setup dataset and benchmark. we evaluate our method on two benchmarks. one is the test split from svg - hub - 5m which has 9, 000 samples. the other one is the sarena - icon benchmark [ 44 ] which contains 6, 000 samples. evaluation metrics. we evaluate the quality of our results from both vector - level and image - level perspectives. for vector - level evaluation, we measure path semantics [ 60 ] by randomly removing 30 % of the svg paths and computing the drop in clip score [ 30 ] between the original and modified renderings. a smaller drop indicates that the generated paths are redundant or carry limited semantic meaning. in i2svg, we measure svg code similarity by encoding the generated and ground - truth svg code with qwen3 - embedding - 8b [ 63 ] and computing the cosine similarity between their embeddings, which reflects the syntactic quality of the generated svgs. for image - level evaluation, we assess the visual fidelity and quality of t2svg using fid [ 17 ], fid - clip [ 50 ], clip score [ 30 ] and the aesthetic score [ 36 ]. for i2svg, we measure the visual similarity between the rendered svgs and the input images using dino [ 27 ], ssim [ 48 ], psnr, and lpips [ 61 ]. baselines. we compare our duetsvg against a diverse set of baselines, including both optimization - based and learning - based svg generation methods. for optimizationbased methods, in the t2svg task, one baseline uses a raster - then - vectorize pipeline : images are generated by flux. 1", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 5, "frag_id": 3, "text": "assess the visual fidelity and quality of t2svg using fid [ 17 ], fid - clip [ 50 ], clip score [ 30 ] and the aesthetic score [ 36 ]. for i2svg, we measure the visual similarity between the rendered svgs and the input images using dino [ 27 ], ssim [ 48 ], psnr, and lpips [ 61 ]. baselines. we compare our duetsvg against a diverse set of baselines, including both optimization - based and learning - based svg generation methods. for optimizationbased methods, in the t2svg task, one baseline uses a raster - then - vectorize pipeline : images are generated by flux. 1 - dev [ 23 ] and then vectorized by vtracer [ 29, 37 ]. we also evaluate three text - guided svg optimization methods, vectorfusion [ 21 ], svgdreamer [ 55 ], and t2i - npr [ 60 ], each optimized with 64 paths. for the i2svg task, we use vtracer as a baseline. learning - based methods include sota proprietary and open - source vlms. for proprietary vlms, we use gpt - 5 - thinking [ 1 ], gemini - 3 - pro [ 16 ] and gemini - 2. 5 - pro [ 8 ]. for open - source svg - specific vlms, we compare with starvector [ 32 ], llm4svg [ 57 ], and omnisvg [ 58 ], and we also include the recently released vlm qwen3 - vl - 8b [ 2 ]. since these models adopt different backbones and training data, we further fine - tune all open - source vlm baselines on our svg - hub - 5m dataset to ensure a fair comparison. 5", "token_count": 391}
{"doc_id": "arxiv_251210894_duetsvg", "page": 6, "frag_id": 0, "text": "table 1. quantitative comparison with existing methods on the svg - hub - 5m test set. bold scores indicate the best results among all methods, and underlined scores indicate the best results among vlm - based methods without test - time scaling ( w / o tts ). method text - to - svg image - to - svg fid ↓ clip ↑ aesthetic ↑ path semantics x dino ↑ ssim ↑ lpips ↓ psnr ↑ svg code similarity x path semantics x optimization vtracer 0. 968 0. 936 0. 071 23. 623 0. 788 0. 982 flux. 1 - dev + vtracer 46. 990 25. 326 5. 52 1. 216 vectorfusion 59. 354 24. 519 5. 05 1. 405 svgdreamer 56. 743 24. 871 5. 34 1. 468 t2v - npr 54. 420 25. 024 5. 38 1. 924 vlm gpt - 5 - thinking 50. 122 24. 950 5. 39 2. 295 0. 904 0. 806 0. 212 11. 485 0. 916 2. 496 gemini - 3 - pro 48. 765 25. 146 5. 46 2. 412 0. 921 0. 880 0. 116 13. 858 0. 908 2. 516 gemini - 2. 5 - pro 57. 597 24. 572 5. 20 2. 154 0. 885 0. 697 0. 275 10. 237 0. 887 2. 028 starvector - 8b ( w / o ft ) 0. 691 0. 489 0. 388 8. 920 0. 862 1. 246 starvector - 8b ( ft ) 0. 896 0. 813 0. 186 18. 646 0. 932 1. 830 llm4svg - 7b ( ft ) 49. 318 23. 304 5. 24 2. 315 0. 938 0. 882 0. 099 19. 843 0. 944 2. 245 omnisvg - 3b ( w / o ft ) 93. 172 21. 225 4. 32 1. 780 0. 812 0. 701 0. 294 11. 577 0. 893 1. 505 omnisvg - 3b ( ft", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 6, "frag_id": 1, "text": "0. 388 8. 920 0. 862 1. 246 starvector - 8b ( ft ) 0. 896 0. 813 0. 186 18. 646 0. 932 1. 830 llm4svg - 7b ( ft ) 49. 318 23. 304 5. 24 2. 315 0. 938 0. 882 0. 099 19. 843 0. 944 2. 245 omnisvg - 3b ( w / o ft ) 93. 172 21. 225 4. 32 1. 780 0. 812 0. 701 0. 294 11. 577 0. 893 1. 505 omnisvg - 3b ( ft ) 58. 237 22. 726 5. 17 2. 286 0. 933 0. 854 0. 105 19. 256 0. 932 2. 164 qwen3 - vl - 8b ( ft ) 43. 720 23. 935 5. 35 2. 525 0. 947 0. 910 0. 090 20. 915 0. 950 2. 492 ours - 7b ( w / o tts ) 35. 066 25. 584 5. 52 2. 772 0. 955 0. 920 0. 082 22. 024 0. 959 2. 558 ours - 7b ( tts ) 33. 574 26. 106 5. 56 2. 910 0. 962 0. 928 0. 075 23. 590 0. 964 2. 724 table 2. quantitative comparison with vlm - based baselines on the sarena - icon benchmark [ 44 ]. method text - to - svg image - to - svg fid ↓ fid - c ↓ clip ↑ dino ↑ ssim ↑ lpips ↓ psnr ↑ gpt - 5 - thinking 14. 892 4. 993 25. 125 0. 916 0. 815 0. 134 11. 512 gemini - 3 - pro 14. 324 4. 847 25. 382 0. 938 0. 874 0. 125 14. 366 gemini - 2. 5 - pro 15. 484 5. 016 24. 910 0. 895 0. 690 0. 261 10. 878 starvector - 8b ( w / o ft ) 0. 871 0. 623 0", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 6, "frag_id": 2, "text": "method text - to - svg image - to - svg fid ↓ fid - c ↓ clip ↑ dino ↑ ssim ↑ lpips ↓ psnr ↑ gpt - 5 - thinking 14. 892 4. 993 25. 125 0. 916 0. 815 0. 134 11. 512 gemini - 3 - pro 14. 324 4. 847 25. 382 0. 938 0. 874 0. 125 14. 366 gemini - 2. 5 - pro 15. 484 5. 016 24. 910 0. 895 0. 690 0. 261 10. 878 starvector - 8b ( w / o ft ) 0. 871 0. 623 0. 206 13. 595 starvector - 8b ( ft ) 0. 920 0. 844 0. 118 19. 076 llm4svg - 7b ( ft ) 18. 815 6. 784 23. 138 0. 946 0. 910 0. 092 22. 160 omnisvg - 3b ( w / o ft ) 28. 292 11. 318 21. 679 0. 894 0. 756 0. 186 12. 669 omnisvg - 3b ( ft ) 19. 941 7. 477 22. 450 0. 937 0. 908 0. 099 21. 984 qwen3 - vl - 8b ( ft ) 16. 780 5. 192 23. 704 0. 955 0. 921 0. 081 22. 750 ours - 7b ( w / o tts ) 12. 105 4. 205 25. 416 0. 969 0. 929 0. 069 23. 482 ours - 7b ( tts ) 11. 712 3. 928 25. 734 0. 972 0. 938 0. 060 24. 016 4. 3. comparisons we evaluate the performance of duetsvg against all baselines qualitatively and quantitatively on both t2svg and i2svg tasks. we report quantitative results on the svghub - 5m test set in table 1, and on the sarena - icon benchmark in table 2. we also show qualitative comparisons in figure 4 for t2svg and figure 5 for i2svg. overall,", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 6, "frag_id": 3, "text": "0. 969 0. 929 0. 069 23. 482 ours - 7b ( tts ) 11. 712 3. 928 25. 734 0. 972 0. 938 0. 060 24. 016 4. 3. comparisons we evaluate the performance of duetsvg against all baselines qualitatively and quantitatively on both t2svg and i2svg tasks. we report quantitative results on the svghub - 5m test set in table 1, and on the sarena - icon benchmark in table 2. we also show qualitative comparisons in figure 4 for t2svg and figure 5 for i2svg. overall, duetsvg achieves consistently stronger performance than existing methods. we refer readers to section b. 2 for additional comparisons and results. text - to - svg task. as shown in table 1 and table 2, duetsvg consistently outperforms all t2svg baselines across all metrics. optimization - based methods are time - consuming and often yield svgs with redundant paths, artifacts, and disorganized layers. regarding learning - based methods, sota proprietary vlms exhibit strong semantic understanding, but they mainly produce combinations of oversimplified primitives ( e. g., circles and rectangles ) and struggle with spatial layout and fine - grained geometric details ( see figure 4 ). it indicates that precise svg generation remains challenging for general vlms. for open - source vlms, evaluating their public checkpoints reveals severe overfitting : they often fail to produce valid svgs for inputs outside their training distribution. to enable a fair comparison, we fine - tune all vlms on our svg - hub - 5m training set. as shown by the “ w / o ft ” and “ ft ” in table 1 and table 2. however, these fine - tuned vlms still struggle to produce semantically accurate svgs with the complex geometric detail, as shown in figure 4. a major reason is their text - centric design : svgs are treated purely as text sequences, so training mainly enforces syntactic correctness of the svg code while providing no supervision on the visual appearance. in contrast, duetsvg produces diverse, visually appealing svgs with clear structure and semantically rich content. we further analyze the model ’ s generalization in", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 6, "frag_id": 4, "text": "fair comparison, we fine - tune all vlms on our svg - hub - 5m training set. as shown by the “ w / o ft ” and “ ft ” in table 1 and table 2. however, these fine - tuned vlms still struggle to produce semantically accurate svgs with the complex geometric detail, as shown in figure 4. a major reason is their text - centric design : svgs are treated purely as text sequences, so training mainly enforces syntactic correctness of the svg code while providing no supervision on the visual appearance. in contrast, duetsvg produces diverse, visually appealing svgs with clear structure and semantically rich content. we further analyze the model ’ s generalization in section b. 1. image - to - svg task. classical image vectorization techniques achieve strong reconstruction scores by densely fitting paths to the image, but they generate verbose and inefficient code ( reflected by low svg code similarity in table 1 ), lack layer organization ( low path semantics in table 1 ), and struggle with overly complex vector elements ( shown in the green boxes in figure 5 ). vlm - based baselines struggle to accurately reconstruct complex shapes and fine details, often resulting in inaccurate spatial layouts and simplified fine - grained geometry. in contrast, duetsvg produces visually faithful and syntactically clean svgs, as shown in figure 5. 4. 4. ablation study benefits of internal visual guidance. to assess the effect of multimodal svg generation, we train a variant that shares the same training configuration as duetsvg but only decodes svg tokens, without generating image tokens. as shown in figure 6, this svg - only baseline exhibits failure 6", "token_count": 361}
{"doc_id": "arxiv_251210894_duetsvg", "page": 7, "frag_id": 0, "text": "flux + vtracer t2v - npr gemini - 3 - pro omnisvg - ft llm4svg - ft qwen3 - vl - ft duetsvg ( ours ) input text “ a cartoon of a delivery truck with a box in the back ” “ a red mailbox with a black flag and a yellow envelope partially inserted, black musical note ” “ a computer monitor displaying a pie chart and bar graph on a blue background ” “ a retro - style radio with a green speaker grill, a blue body, and white dials ” “ a robot arm is assembling a burger on a conveyor belt ” “ a hand holding a yellow house with a red roof, featuring a green checkmark inside a shield ” figure 4. qualitative comparison on text - to - svg generation task. our duetsvg aligns better with the text prompts and generates high - quality visual outputs with detailed structures. vtracer gpt5 - thinking starvector omnisvg - ft llm4svg - ft qwen3 - vl - ft duetsvg ( ours ) input image gemini - 3 - pro figure 5. qualitative comparison on image - to - svg conversion task. ours preserves more visual details than other previous baselines. modes similar to prior text - centric vlms, struggling to produce semantically accurate and structurally coherent svgs. we further observe that it performs worse than the finetuned qwen3 - vl - 8b in table 3, indicating that our backbone is weaker as a pure language model than qwen3 - vl8b. yet duetsvg still outperforms fine - tuned qwen3 - vl8b when equipped with image modality, showing that visual guidance is crucial for high - quality svg generation. ablation of t2i pretraining. to validate the effectiveness of the t2i pre - training stage, we conduct an ablation in which this stage is removed. in table 3, we observe that t2i pre - training helps the model acquire stronger visual and semantic priors, enabling it to generate more visually appealing svg - style images and more complex svgs. 7", "token_count": 457}
{"doc_id": "arxiv_251210894_duetsvg", "page": 8, "frag_id": 0, "text": "table 3. ablation study. ( a ) removing visual output brings an notable degradation for all metrics, indicating the effectiveness of our unified multimodal generation design. ( b ) without t2i pretraining, we observe a clear drop in t2svg quality, showing that t2i alignment learned during pretraining is crucial. ( c ) test - time scaling further boosts the quality in both t2svg and i2svg tasks. method text - to - svg image - to - svg fid ↓ clip ↑ dino ↑ ssim ↑ lpips ↓ w / o internal visual guidance 51. 482 23. 260 0. 939 0. 878 0. 096 w / o t2i pretraining 36. 953 25. 122 w / o test - time scaling 35. 066 25. 584 0. 955 0. 920 0. 082 ours ( full ) 33. 574 26. 106 0. 962 0. 928 0. 075 w / o visual outputs w / o t2i pretraining ours ( full ) w / o tts input text “ inside the human head is a circular gauge with a blue border and a yellow fill ” “ two participants are exchanging information, one about a house represented by a speech bubble containing a house … \" “ a man in a space suit with a helmet ” text - to - svg image - to - svg ours ( full ) w / o tts input image ours ( full ) ours ( full ) w / o tts input image figure 6. ablation results. ( a ) without visual output, svgonly generation struggles to produce semantically accurate and structurally coherent svgs. ( b ) without t2i pretraining, the model lacks strong visual and semantic priors, resulting in simpler and less visually appealing svgs. ( c ) without test - time scaling ( tts ), autoregressive decoding may introduce local geometric distortions when handling complex inputs. ablation of tts with svg resampling. in this ablation study, we evaluate duetsvg with our test - time scaling ( tts ) strategy, duetsvg with best - of - n tts, and llm4svg with best - of - n tts. the clip vs compute graph is shown in figure 7. compared to best -", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 8, "frag_id": 1, "text": "produce semantically accurate and structurally coherent svgs. ( b ) without t2i pretraining, the model lacks strong visual and semantic priors, resulting in simpler and less visually appealing svgs. ( c ) without test - time scaling ( tts ), autoregressive decoding may introduce local geometric distortions when handling complex inputs. ablation of tts with svg resampling. in this ablation study, we evaluate duetsvg with our test - time scaling ( tts ) strategy, duetsvg with best - of - n tts, and llm4svg with best - of - n tts. the clip vs compute graph is shown in figure 7. compared to best - of - n, our novel two - stage tts with image - guided svg resampling is much more efficient. text - centric vlms still perform much worse with tts enabled. 5. applications to support downstream applications, duetsvg can be further fine - tuned for svg completion and semantic svg editing. for svg completion, we randomly mask a subset of clip score average sampling tokens 23 24 25 26 27 27. 5 svg resampling ( our model ) best - of - n ( our model ) best - of - n ( llm4svg - ft ) 3, 000 7, 000 11, 000 15, 000 figure 7. sampling efficiency comparison of test - time scaling strategies. our strategy leverages image - level candidate selection and svg resampling to achieve efficient test - time scaling, with substantially lower sampling cost than best - of - n. input svg svg completion svg editing “ replace star with hat ” input image “ replace musical note with envelope ” input image “ replace euro symbol with dollar ” generated svg input image figure 8. downstream applications. after fine - tuning, duetsvg supports : ( a ) svg completion : given the partial svg and its rendered image, duetsvg completes coherent and visually appealing svgs. ( b ) svg editing : duetsvg enables instructionbased editing for svgs. paths in an svg and provide the model with both the partial svg and its rendered image as inputs. the model is trained to infer the complete image and the full svg script. as shown in figure 8, duetsvg can effectively complete coherent and visually appealing svgs. for semantic svg editing", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 8, "frag_id": 2, "text": "” input image “ replace musical note with envelope ” input image “ replace euro symbol with dollar ” generated svg input image figure 8. downstream applications. after fine - tuning, duetsvg supports : ( a ) svg completion : given the partial svg and its rendered image, duetsvg completes coherent and visually appealing svgs. ( b ) svg editing : duetsvg enables instructionbased editing for svgs. paths in an svg and provide the model with both the partial svg and its rendered image as inputs. the model is trained to infer the complete image and the full svg script. as shown in figure 8, duetsvg can effectively complete coherent and visually appealing svgs. for semantic svg editing, since no large - scale paired svg editing dataset is available, we construct a synthetic dataset using a powerful image editing model. concretely, for each svg rendering ia from our dataset and a text instruction, we use gemini2. 5 - flash [ 8 ] to produce an edited image ie that reflects the instruction. during training, we take ie and an inverse editing instruction as inputs, and train duetsvg to reconstruct the original image ia and its corresponding svg script. as shown in figure 8, duetsvg can perform high - level semantic svg editing, modifying svg content according to the text instruction. 8", "token_count": 282}
{"doc_id": "arxiv_251210894_duetsvg", "page": 9, "frag_id": 0, "text": "6. conclusion in this paper, we presented duetsvg, a unified multimodal model that generates both image tokens and svg tokens rather than treating svgs as pure text. by combining large - scale t2i pre - training with multi - task sft on t2i, t2svg, and i2svg, duetsvg leverages rich visual priors to achieve stronger text - svg alignment and to produce high - quality svgs. we further introduced a vision - aware test - time scaling strategy that uses the internal visual predictions to guide svg decoding, improving robustness and reliability. duetsvg supports a wide range of svg generation and editing tasks. with advanced vlm techniques, our framework could further accelerate generation, and the proposed training and inference strategies can be used to train larger - scale unified multimodal models ( e. g., emu3. 5 [ 9 ] ), which we leave for future work. references [ 1 ] josh achiam, steven adler, sandhini agarwal, lama ahmad, ilge akkaya, florencia leoni aleman, diogo almeida, janko altenschmidt, sam altman, shyamal anadkat, et al. gpt - 4 technical report. arxiv preprint arxiv : 2303. 08774, 2023. 5 [ 2 ] shuai bai, yuxuan cai, ruizhe chen, keqin chen, xionghui chen, zesen cheng, lianghao deng, wei ding, chang gao, chunjiang ge,, et al. qwen3 - vl technical report. arxiv preprint arxiv : 2511. 21631, 2025. 1, 5 [ 3 ] shuai bai, keqin chen, xuejing liu, jialin wang, wenbin ge, sibo song, kai dang, peng wang, shijie wang, jun tang, et al. qwen2. 5 - vl technical report. arxiv preprint arxiv : 2502. 13923, 2025. 1, 5, 2 [ 4 ] mikhail bessmeltsev and justin solomon. vectorization of line drawings via polyvector fields. acm transactions on graphics ( tog ), 38 ( 1 ) : 1 – 12, 2019. 2 [", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 9, "frag_id": 1, "text": "##xiv preprint arxiv : 2511. 21631, 2025. 1, 5 [ 3 ] shuai bai, keqin chen, xuejing liu, jialin wang, wenbin ge, sibo song, kai dang, peng wang, shijie wang, jun tang, et al. qwen2. 5 - vl technical report. arxiv preprint arxiv : 2502. 13923, 2025. 1, 5, 2 [ 4 ] mikhail bessmeltsev and justin solomon. vectorization of line drawings via polyvector fields. acm transactions on graphics ( tog ), 38 ( 1 ) : 1 – 12, 2019. 2 [ 5 ] alexandre carlier, martin danelljan, alexandre alahi, and radu timofte. deepsvg : a hierarchical generative network for vector graphics animation. advances in neural information processing systems, 33 : 16351 – 16361, 2020. 2 [ 6 ] souymodip chakraborty, vineet batra, ankit phogat, vishwas jain, jaswant singh ranawat, sumit dhingra, kevin wampler, matthew fisher, and michal [UNK]. image vectorization via gradient reconstruction. in computer graphics forum, page e70055. wiley online library, 2025. 2 [ 7 ] xiaokang chen, zhiyu wu, xingchao liu, zizheng pan, wen liu, zhenda xie, xingkai yu, and chong ruan. januspro : unified multimodal understanding and generation with data and model scaling. arxiv preprint arxiv : 2501. 17811, 2025. 3, 5 [ 8 ] gheorghe comanici, eric bieber, mike schaekermann, ice pasupat, noveen sachdeva, inderjit dhillon, marcel blistein, ori ram, dan zhang, evan rosen, et al. gemini 2. 5 : pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arxiv preprint arxiv : 2507. 06261, 2025. 5, 8 [ 9 ] yufeng cui, honghao chen, haoge deng, xu huang, xinghang li", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 9, "frag_id": 2, "text": "##v preprint arxiv : 2501. 17811, 2025. 3, 5 [ 8 ] gheorghe comanici, eric bieber, mike schaekermann, ice pasupat, noveen sachdeva, inderjit dhillon, marcel blistein, ori ram, dan zhang, evan rosen, et al. gemini 2. 5 : pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arxiv preprint arxiv : 2507. 06261, 2025. 5, 8 [ 9 ] yufeng cui, honghao chen, haoge deng, xu huang, xinghang li, jirong liu, yang liu, zhuoyan luo, jinsheng wang, wenxuan wang, et al. emu3. 5 : native multimodal models are world learners. arxiv preprint arxiv : 2510. 26583, 2025. 3, 9 [ 10 ] ayan das, yongxin yang, timothy hospedales, tao xiang, and yi - zhe song. chirodiff : modelling chirographic data with diffusion models. arxiv preprint arxiv : 2304. 03785, 2023. 2 [ 11 ] chaorui deng, deyao zhu, kunchang li, chenhui gou, feng li, zeyu wang, shu zhong, weihao yu, xiaonan nie, ziang song, et al. emerging properties in unified multimodal pretraining. arxiv preprint arxiv : 2505. 14683, 2025. 3 [ 12 ] edoardo alberto dominici, nico schertler, jonathan griffin, shayan hoshyari, leonid sigal, and alla sheffer. polyfit : perception - aligned vectorization of raster clip - art via intermediate polygonal fitting. acm transactions on graphics ( tog ), 39 ( 4 ) : 77 – 1, 2020. 2 [ 13 ] jean - dominique favreau, florent lafarge, and adrien bousseau. photo2clipart : image abstraction and vectorization using layered linear gradients. acm transactions on graphics ( tog ), 36 ( 6 ) : 1 – 11, 2017. 2 [ 14", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 9, "frag_id": 3, "text": ". 14683, 2025. 3 [ 12 ] edoardo alberto dominici, nico schertler, jonathan griffin, shayan hoshyari, leonid sigal, and alla sheffer. polyfit : perception - aligned vectorization of raster clip - art via intermediate polygonal fitting. acm transactions on graphics ( tog ), 39 ( 4 ) : 77 – 1, 2020. 2 [ 13 ] jean - dominique favreau, florent lafarge, and adrien bousseau. photo2clipart : image abstraction and vectorization using layered linear gradients. acm transactions on graphics ( tog ), 36 ( 6 ) : 1 – 11, 2017. 2 [ 14 ] kevin frans, lisa soros, and olaf witkowski. clipdraw : exploring text - to - drawing synthesis through language - image encoders. advances in neural information processing systems, 35 : 5207 – 5218, 2022. 2 [ 15 ] rinon gal, yael vinker, yuval alaluf, amit h bermano, daniel cohen - or, ariel shamir, and gal chechik. breathing life into sketches using text - to - video priors. arxiv preprint arxiv : 2311. 13608, 2023. 2 [ 16 ] google deepmind. gemini 3 pro. https : / / deepmind. google / models / gemini / pro, 2025. 5 [ 17 ] martin heusel, hubert ramsauer, thomas unterthiner, bernhard nessler, and sepp hochreiter. gans trained by a two time - scale update rule converge to a local nash equilibrium. in neurips, 2017. 5 [ 18 ] jonathan ho and tim salimans. classifier - free diffusion guidance. arxiv preprint arxiv : 2207. 12598, 2022. 4 [ 19 ] shayan hoshyari, edoardo alberto dominici, alla sheffer, nathan carr, zhaowen wang, duygu ceylan, and i - chao shen. perception - driven semi - structured boundary vectorization. acm transactions on graphics ( tog ), 37 ( 4 ) : 1 – 14, 2018. 2 [ 20 ] shir iluz, yael vinker, amir hertz, daniel berio, daniel cohen - or, and ariel shamir", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 9, "frag_id": 4, "text": "equilibrium. in neurips, 2017. 5 [ 18 ] jonathan ho and tim salimans. classifier - free diffusion guidance. arxiv preprint arxiv : 2207. 12598, 2022. 4 [ 19 ] shayan hoshyari, edoardo alberto dominici, alla sheffer, nathan carr, zhaowen wang, duygu ceylan, and i - chao shen. perception - driven semi - structured boundary vectorization. acm transactions on graphics ( tog ), 37 ( 4 ) : 1 – 14, 2018. 2 [ 20 ] shir iluz, yael vinker, amir hertz, daniel berio, daniel cohen - or, and ariel shamir. word - as - image for semantic typography. arxiv preprint arxiv : 2303. 01818, 2023. 2 [ 21 ] ajay jain, amber xie, and pieter abbeel. vectorfusion : text - to - svg by abstracting pixel - based diffusion models. arxiv preprint arxiv : 2211. 11319, 2022. 2, 5 [ 22 ] johannes kopf and dani lischinski. depixelizing pixel art. in acm siggraph 2011 papers, pages 1 – 8. 2011. 2 [ 23 ] black forest labs, stephen batifol, andreas blattmann, frederic boesel, saksham consul, cyril diagne, tim dockhorn, jack english, zion english, patrick esser, et al. flux. 1 kontext : flow matching for in - context image generation and editing in latent space. arxiv preprint arxiv : 2506. 15742, 2025. 3, 5, 1 [ 24 ] tzu - mao li, michal [UNK], micha¨el gharbi, and jonathan ragan - kelley. differentiable vector graphics rasterization 9", "token_count": 408}
{"doc_id": "arxiv_251210894_duetsvg", "page": 10, "frag_id": 0, "text": "for editing and learning. acm transactions on graphics ( tog ), 39 ( 6 ) : 1 – 15, 2020. 2 [ 25 ] ilya loshchilov and frank hutter. decoupled weight decay regularization. in iclr, 2019. 5 [ 26 ] xu ma, yuqian zhou, xingqian xu, bin sun, valerii filev, nikita orlov, yun fu, and humphrey shi. towards layerwise image vectorization. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 16314 – 16323, 2022. 2 [ 27 ] maxime oquab, timoth´ee darcet, th´eo moutakanni, huy vo, marc szafraniec, vasil khalidov, pierre fernandez, daniel haziza, francisco massa, alaaeldin el - nouby, et al. dinov2 : learning robust visual features without supervision. arxiv preprint arxiv : 2304. 07193, 2023. 5 [ 28 ] ben poole, ajay jain, jonathan t barron, and ben mildenhall. dreamfusion : text - to - 3d using 2d diffusion. arxiv preprint arxiv : 2209. 14988, 2022. 2 [ 29 ] sanford pun and chris tsang. vtracer. https : / / gith ub. com / visioncortex / vtracer, 2025. 5 [ 30 ] alec radford, jong wook kim, chris hallacy, aditya ramesh, gabriel goh, sandhini agarwal, girish sastry, amanda askell, pamela mishkin, jack clark, et al. learning transferable visual models from natural language supervision. in international conference on machine learning, pages 8748 – 8763. pmlr, 2021. 2, 4, 5 [ 31 ] juan a rodriguez, shubham agarwal, issam h laradji, pau rodriguez, david vazquez, christopher pal, and marco pedersoli. starvector : generating scalable vector graphics code from images. arxiv preprint arxiv : 2312. 11556, 2023. 1, 2, 3 [ 32 ] juan a rodriguez, abhay puri, shubham agarwal, issam h laradji,", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 10, "frag_id": 1, "text": "##try, amanda askell, pamela mishkin, jack clark, et al. learning transferable visual models from natural language supervision. in international conference on machine learning, pages 8748 – 8763. pmlr, 2021. 2, 4, 5 [ 31 ] juan a rodriguez, shubham agarwal, issam h laradji, pau rodriguez, david vazquez, christopher pal, and marco pedersoli. starvector : generating scalable vector graphics code from images. arxiv preprint arxiv : 2312. 11556, 2023. 1, 2, 3 [ 32 ] juan a rodriguez, abhay puri, shubham agarwal, issam h laradji, pau rodriguez, sai rajeswar, david vazquez, christopher pal, and marco pedersoli. starvector : generating scalable vector graphics code from images and text. in proceedings of the computer vision and pattern recognition conference, pages 16175 – 16186, 2025. 5 [ 33 ] juan a rodriguez, haotian zhang, abhay puri, aarash feizi, rishav pramanik, pascal wichmann, arnab mondal, mohammad reza samsami, rabiul awal, perouz taslakian, et al. rendering - aware reinforcement learning for vector graphics generation. arxiv preprint arxiv : 2505. 20793, 2025. 2 [ 34 ] robin rombach, andreas blattmann, dominik lorenz, patrick esser, and bj¨orn ommer. high - resolution image synthesis with latent diffusion models. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 10684 – 10695, 2022. 2 [ 35 ] peter schaldenbrand, zhixuan liu, and jean oh. styleclipdraw : coupling content and style in text - to - drawing translation. arxiv preprint arxiv : 2202. 12362, 2022. 2 [ 36 ] christoph schuhmann. improved aesthetic predictor. http s : / / github. com / christophschuhmann / improv ed - aesthetic - predictor, 2022. 5 [ 37 ] peter selinger. potrace : a polygon - based tracing algorithm, 2003. 2, 5 [ 38", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 10, "frag_id": 2, "text": "/ cvf conference on computer vision and pattern recognition, pages 10684 – 10695, 2022. 2 [ 35 ] peter schaldenbrand, zhixuan liu, and jean oh. styleclipdraw : coupling content and style in text - to - drawing translation. arxiv preprint arxiv : 2202. 12362, 2022. 2 [ 36 ] christoph schuhmann. improved aesthetic predictor. http s : / / github. com / christophschuhmann / improv ed - aesthetic - predictor, 2022. 5 [ 37 ] peter selinger. potrace : a polygon - based tracing algorithm, 2003. 2, 5 [ 38 ] yiren song, xning shao, kang chen, weidong zhang, minzhe li, and zhongliang jing. clipvg : text - guided image manipulation using differentiable vector graphics. arxiv preprint arxiv : 2212. 02122, 2022. 2 [ 39 ] peize sun, yi jiang, shoufa chen, shilong zhang, bingyue peng, ping luo, and zehuan yuan. autoregressive model beats diffusion : llama for scalable image generation. arxiv preprint arxiv : 2406. 06525, 2024. 3 [ 40 ] zecheng tang, chenfei wu, zekai zhang, mingheng ni, shengming yin, yu liu, zhengyuan yang, lijuan wang, zicheng liu, juntao li, et al. strokenuwa : tokenizing strokes for vector graphic synthesis. arxiv preprint arxiv : 2401. 17093, 2024. 2 [ 41 ] vikas thamizharasan, difan liu, shantanu agarwal, matthew fisher, micha¨el gharbi, oliver wang, alec jacobson, and evangelos kalogerakis. vecfusion : vector font generation with diffusion. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 7943 – 7952, 2024. 2 [ 42 ] xingze tian and tobias g¨unther. a survey of smooth vector graphics : recent advances in repr esentation, creation, rasterization, and image vectorization. ieee transactions on visualization and", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 10, "frag_id": 3, "text": "##v preprint arxiv : 2401. 17093, 2024. 2 [ 41 ] vikas thamizharasan, difan liu, shantanu agarwal, matthew fisher, micha¨el gharbi, oliver wang, alec jacobson, and evangelos kalogerakis. vecfusion : vector font generation with diffusion. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 7943 – 7952, 2024. 2 [ 42 ] xingze tian and tobias g¨unther. a survey of smooth vector graphics : recent advances in repr esentation, creation, rasterization, and image vectorization. ieee transactions on visualization and computer graphics, 30 ( 3 ) : 1652 – 1671, 2022. 2 [ 43 ] yael vinker, ehsan pajouheshgar, jessica y bo, roman christian bachmann, amit haim bermano, daniel cohen - or, amir zamir, and ariel shamir. clipasso : semantically - aware object sketching. acm transactions on graphics ( tog ), 41 ( 4 ) : 1 – 11, 2022. 2 [ 44 ] haomin wang, jinhui yin, qi wei, wenguang zeng, lixin gu, shenglong ye, zhangwei gao, yaohui wang, yanting zhang, yuanqi li, et al. internsvg : towards unified svg tasks with multimodal large language models. arxiv preprint arxiv : 2510. 11341, 2025. 1, 2, 5, 6 [ 45 ] jiuniu wang, gongjie zhang, quanhao qian, junlong gao, deli zhao, and ran xu. robosvg : a unified framework for interactive svg generation with multi - modal guidance. arxiv preprint arxiv : 2510. 22684, 2025. 2 [ 46 ] yizhi wang and zhouhui lian. deepvecfont : synthesizing high - quality vector fonts via dual - modality learning. acm transactions on graphics ( tog ), 40 ( 6 ) : 1 – 15, 2021. 2 [ 47 ] yuqing wang, yizhi wang, longhui yu, yuesheng zhu, and zhouhui lian. deepvecfont -", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 10, "frag_id": 4, "text": "##e zhang, quanhao qian, junlong gao, deli zhao, and ran xu. robosvg : a unified framework for interactive svg generation with multi - modal guidance. arxiv preprint arxiv : 2510. 22684, 2025. 2 [ 46 ] yizhi wang and zhouhui lian. deepvecfont : synthesizing high - quality vector fonts via dual - modality learning. acm transactions on graphics ( tog ), 40 ( 6 ) : 1 – 15, 2021. 2 [ 47 ] yuqing wang, yizhi wang, longhui yu, yuesheng zhu, and zhouhui lian. deepvecfont - v2 : exploiting transformers to synthesize vector fonts with higher quality. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 18320 – 18328, 2023. 2 [ 48 ] zhou wang, alan c bovik, hamid r sheikh, and eero p simoncelli. image quality assessment : from error visibility to structural similarity. ieee transactions on image processing, 13 ( 4 ) : 600 – 612, 2004. 5 [ 49 ] zhengyi wang, cheng lu, yikai wang, fan bao, chongxuan li, hang su, and jun zhu. prolificdreamer : high - fidelity and diverse text - to - 3d generation with variational score distillation. arxiv preprint arxiv : 2305. 16213, 2023. 2 [ 50 ] ronghuan wu, wanchao su, kede ma, and jing liao. iconshop : text - guided vector icon synthesis with autoregressive transformers. acm transactions on graphics ( tog ), 42 ( 6 ) : 1 – 14, 2023. 2, 5, 1 10", "token_count": 387}
{"doc_id": "arxiv_251210894_duetsvg", "page": 11, "frag_id": 0, "text": "[ 51 ] ronghuan wu, wanchao su, kede ma, and jing liao. aniclipart : clipart animation with text - to - video priors. international journal of computer vision, pages 1 – 17, 2024. 2 [ 52 ] ronghuan wu, wanchao su, and jing liao. chat2svg : vector graphics generation with large language models and image diffusion models. in proceedings of the computer vision and pattern recognition conference, pages 23690 – 23700, 2025. 2 [ 53 ] jinheng xie, zhenheng yang, and mike zheng shou. showo2 : improved native unified multimodal models. arxiv preprint arxiv : 2506. 15564, 2025. 3 [ 54 ] ximing xing, chuang wang, haitao zhou, jing zhang, qian yu, and dong xu. diffsketcher : text guided vector sketch synthesis through latent diffusion models. arxiv preprint arxiv : 2306. 14685, 2023. 2 [ 55 ] ximing xing, haitao zhou, chuang wang, jing zhang, dong xu, and qian yu. svgdreamer : text guided svg generation with diffusion model. arxiv preprint arxiv : 2312. 16476, 2023. 2, 5 [ 56 ] ximing xing, yandong guan, jing zhang, dong xu, and qian yu. reason - svg : hybrid reward rl for ahamoments in vector graphics generation. arxiv preprint arxiv : 2505. 24499, 2025. 2 [ 57 ] ximing xing, juncheng hu, guotao liang, jing zhang, dong xu, and qian yu. empowering llms to understand and generate complex vector graphics. in proceedings of the computer vision and pattern recognition conference, pages 19487 – 19497, 2025. 1, 5 [ 58 ] yiying yang, wei cheng, sijin chen, xianfang zeng, fukun yin, jiaxu zhang, liao wang, gang yu, xingjun ma, and yu - gang jiang. omnisvg : a unified scalable vector graphics generation model. in the thirty - ninth annual conference on neural information processing systems, 2025. 1", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 11, "frag_id": 1, "text": "##99, 2025. 2 [ 57 ] ximing xing, juncheng hu, guotao liang, jing zhang, dong xu, and qian yu. empowering llms to understand and generate complex vector graphics. in proceedings of the computer vision and pattern recognition conference, pages 19487 – 19497, 2025. 1, 5 [ 58 ] yiying yang, wei cheng, sijin chen, xianfang zeng, fukun yin, jiaxu zhang, liao wang, gang yu, xingjun ma, and yu - gang jiang. omnisvg : a unified scalable vector graphics generation model. in the thirty - ninth annual conference on neural information processing systems, 2025. 1, 2, 3, 5 [ 59 ] xiaohua zhai, basil mustafa, alexander kolesnikov, and lucas beyer. sigmoid loss for language image pre - training. in proceedings of the ieee / cvf international conference on computer vision, pages 11975 – 11986, 2023. 3 [ 60 ] peiying zhang, nanxuan zhao, and jing liao. text - to - vector generation with neural path representation. acm transactions on graphics ( tog ), 43 ( 4 ) : 1 – 13, 2024. 2, 5 [ 61 ] richard zhang, phillip isola, alexei a efros, eli shechtman, and oliver wang. the unreasonable effectiveness of deep features as a perceptual metric. in proceedings of the ieee conference on computer vision and pattern recognition, pages 586 – 595, 2018. 4, 5 [ 62 ] xinjie zhang, jintao guo, shanshan zhao, minghao fu, lunhao duan, jiakui hu, yong xien chng, guo - hua wang, qing - guo chen, zhao xu, et al. unified multimodal understanding and generation models : advances, challenges, and opportunities. arxiv preprint arxiv : 2505. 02567, 2025. 3 [ 63 ] yanzhao zhang, mingxin li, dingkun long, xin zhang, huan lin, baosong yang, pengjun xie, an yang, dayiheng liu, junyang lin, fei huang, and jingren zhou. qwen3 embedding : advancing text embedding and reranking through foundation models. arxiv", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 11, "frag_id": 2, "text": "fu, lunhao duan, jiakui hu, yong xien chng, guo - hua wang, qing - guo chen, zhao xu, et al. unified multimodal understanding and generation models : advances, challenges, and opportunities. arxiv preprint arxiv : 2505. 02567, 2025. 3 [ 63 ] yanzhao zhang, mingxin li, dingkun long, xin zhang, huan lin, baosong yang, pengjun xie, an yang, dayiheng liu, junyang lin, fei huang, and jingren zhou. qwen3 embedding : advancing text embedding and reranking through foundation models. arxiv preprint arxiv : 2506. 05176, 2025. 5 [ 64 ] jinguo zhu, weiyun wang, zhe chen, zhaoyang liu, shenglong ye, lixin gu, hao tian, yuchen duan, weijie su, jie shao, et al. internvl3 : exploring advanced training and test - time recipes for open - source multimodal models. arxiv preprint arxiv : 2504. 10479, 2025. 1, 5, 2 11", "token_count": 266}
{"doc_id": "arxiv_251210894_duetsvg", "page": 12, "frag_id": 0, "text": "duetsvg : unified multimodal svg generation with internal visual guidance supplementary material a. overview in this supplementary material, we provide additional details and evaluations, including : • generalization evaluation ( section b. 1 ). • additional comparisons and results ( section b. 2 ). • limitation ( section c ). • details of the captioning pipeline ( section d ). b. additional evaluation b. 1. generalization evaluation we evaluate our model ’ s generalization capability using novelty and uniqueness scores following iconshop [ 50 ]. two svgs are considered identical if the cosine similarity between their clip image embeddings is at least 0. 98. under this criterion, uniqueness is the fraction of generated samples that appear exactly once within the generated set. novelty is the fraction of generated samples that have no match in the svg - hub - 5m training corpus : for each generated svg, we render it to an image, retrieve its nearest neighbor in svg - hub - 5m using clip cosine similarity, and label the sample as novel if the maximum similarity is below 0. 98. we randomly select 500 text prompts and generate three svgs per prompt with different seeds, yielding 1, 500 samples. on this set, our model attains 99. 5 % novelty and 99. 8 % uniqueness. figure 9 shows generated svgs alongside their nearest neighbors from svg - hub - 5m, illustrating our model ’ s ability to produce novel and diverse results. b. 2. additional comparisons and results additional results on svg - hub - 1m. we provide additional quantitative results on the svg - hub - 1m dataset in table 4. as expected, the smaller training corpus leads to an overall degradation in performance across metrics, yet our method consistently outperforms prior approaches on all metrics. as the dataset size increases, our model improves substantially, whereas other methods show only limited improvement on the text - to - svg task. the results further highlight the effectiveness of our unified multimodal svg generation model. additional comparisons with flux + i2svg. for the t2svg task, we compare against a two - stage baseline that first synthesizes images with flux. 1 - dev [ 23 ] and then performs image - to - svg using a vlm - based model, qwen3 - vl - 8b [ 2 ], fine - tuned", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 12, "frag_id": 1, "text": "smaller training corpus leads to an overall degradation in performance across metrics, yet our method consistently outperforms prior approaches on all metrics. as the dataset size increases, our model improves substantially, whereas other methods show only limited improvement on the text - to - svg task. the results further highlight the effectiveness of our unified multimodal svg generation model. additional comparisons with flux + i2svg. for the t2svg task, we compare against a two - stage baseline that first synthesizes images with flux. 1 - dev [ 23 ] and then performs image - to - svg using a vlm - based model, qwen3 - vl - 8b [ 2 ], fine - tuned on svg - hub - 5m dataset. we nearest sample generated svg “ a flat, twodimensional illustration of a cash register with a display screen above and a keypad below ” input text “ a stylized illustration of a bearded king wearing a crown ” “ a directional signpost with two yellow arrows on a purple pole, featuring a green spherical top and a blue base with a green bush beside it ” figure 9. generated svgs and their nearest neighbor samples. table 4. quantitative comparison with vlm - based baselines on the sarena - icon benchmark [ 44 ]. in this setting, “ ours7b ” and all baselines marked “ ft ” are fine - tuned on svg - hub - 1m dataset. method text - to - svg image - to - svg fid ↓ fid - c ↓ clip ↑ dino ↑ ssim ↑ lpips ↓ starvector - 8b ( w / o ft ) 0. 871 0. 623 0. 206 starvector - 8b ( ft ) 0. 895 0. 742 0. 182 llm4svg - 7b ( ft ) 20. 896 7. 980 22. 108 0. 898 0. 760 0. 164 omnisvg - 3b ( w / o ft ) 28. 292 11. 318 21. 679 0. 894 0. 756 0. 186 omnisvg - 3b ( ft ) 24. 977 9. 659 21. 825 0. 902 0. 761 0. 178 qwen3 - vl - 8b ( ft ) 19. 760 7. 572 22. 126 0", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 12, "frag_id": 2, "text": "##1 0. 623 0. 206 starvector - 8b ( ft ) 0. 895 0. 742 0. 182 llm4svg - 7b ( ft ) 20. 896 7. 980 22. 108 0. 898 0. 760 0. 164 omnisvg - 3b ( w / o ft ) 28. 292 11. 318 21. 679 0. 894 0. 756 0. 186 omnisvg - 3b ( ft ) 24. 977 9. 659 21. 825 0. 902 0. 761 0. 178 qwen3 - vl - 8b ( ft ) 19. 760 7. 572 22. 126 0. 908 0. 774 0. 162 ours - 7b ( w / o tts ) 17. 265 6. 388 22. 586 0. 917 0. 808 0. 151 ours - 7b ( tts ) 16. 952 6. 242 22. 645 0. 924 0. 814 0. 145 observe cross - model inconsistencies : the synthesized intermediate images differ in style and fine detail from the real svg images used during i2svg training, introducing a train - test mismatch for the vlm - based i2svg model. as a result, the i2svg model does not generalize well to these intermediates, and the resulting svgs often show reduced alignment with the intermediate images ( see figure 10 ). in contrast, our unified multimodal generative model cogenerates image and svg tokens within a single end - to - end architecture, enabling the use of large - scale text - image data and providing visual grounding during svg decoding. 1", "token_count": 372}
{"doc_id": "arxiv_251210894_duetsvg", "page": 13, "frag_id": 0, "text": "qwen3 - vl - 8b ( ft ) flux input text “ a red sneaker with white laces and toe cap, featuring a gray sole \" “ a cartoon character with blonde hair is wearing a purple shirt and has their hands raised above their head ” “ an icon of a symmetrical blue fox face with pointed ears and a curved smile ” t2i i2svg prompt figure 10. additional comparisons with flux + i2svg pipeline. input image generated svg input image generated svg figure 11. failure cases. c. limitation while our model excels at svg generation, it has limitations. when the input image contains very fine details and rich color variation, the generated svg may miss small structures and exhibit mild color shifts ( see figure 11 ). a potential mitigation is a dynamic high - resolution strategy [ 3 ] that adaptively increases the number of patches fed to the vision encoder, which can improve the capture of fine details and color consistency. we plan to investigate this in future work. d. details of the captioning pipeline to enable text - to - svg ( t2svg ) training from semantically rich prompts, we rasterize each svg and use open - source vlms ( internvl3 [ 64 ] and qwen2. 5 - vl [ 3 ] ) to generate captions at three levels of detail. we then perform crossmodel verification and refinement : a caption produced by one vlm is evaluated against the rendered svg by the other, which flags inaccuracies and suggests edits ; the revised caption is subsequently adopted. the prompt templates for the three levels are provided in figure 12, figure 13, and figure 14. you are an expert in semantic analysis and captioning for svg icons and flat 2d illustrations. analyze the provided image and produce a concise, descriptive title suitable for an svg - style icon or flat illustration. * * guidelines * * - write * * one compact description * * ( a short phrase or a brief sentence ) in clear, precise english. - focus on the * * primary subject or action * *. mention * * dominant color ( s ) * * and other key attributes * * only if they are essential to the meaning * *. - optionally add a simple * * layout / background cue * * only when it is crucial ( e. g., “", "token_count": 500}
{"doc_id": "arxiv_251210894_duetsvg", "page": 13, "frag_id": 1, "text": "in figure 12, figure 13, and figure 14. you are an expert in semantic analysis and captioning for svg icons and flat 2d illustrations. analyze the provided image and produce a concise, descriptive title suitable for an svg - style icon or flat illustration. * * guidelines * * - write * * one compact description * * ( a short phrase or a brief sentence ) in clear, precise english. - focus on the * * primary subject or action * *. mention * * dominant color ( s ) * * and other key attributes * * only if they are essential to the meaning * *. - optionally add a simple * * layout / background cue * * only when it is crucial ( e. g., “ on an orange saucer, ” “ within a circle ” ). - be specific and accurate about the subject ’ s semantics. - output only the description text. no extra words or formatting. describe this svg - style image following the rules above. output only the sentence or phrase. figure 12. short prompt template. you are an expert captioner for svg - style icons and flat 2d illustrations. * * task * * given an image, write * * 1 – 2 concise english sentences * * that * * enumerate the key semantic elements * * and describe their * * basic layout and style * *. * * guidelines * * - start with the * * main subject / action * *, then briefly list important secondary elements. - mention * * spatial arrangement * * only when helpful ( e. g., centered, above / beside, inside a circle ). - include * * dominant colors and notable style cues * * ( flat / outline / pastel / thick strokes, etc. ) when they are visually salient. - stay * * compact but informative * * ; do not describe fine - grained geometry. * * output * * return * * only the sentences * *. figure 13. medium description template. 2", "token_count": 407}
{"doc_id": "arxiv_251210894_duetsvg", "page": 14, "frag_id": 0, "text": "you are a technical analyst for vector graphics. * * task * * analyze the provided image and write a * * precise, exhaustive description in 3 – 6 english sentences ( one paragraph ) * *. * * guidelines * * - * * identify all elements and semantics : * * systematically mention every visible component and its intended meaning. - * * shape decomposition and geometry : * * describe each element ’ s exact shapes ( e. g., circles, polygons, curved paths ) and key geometric traits ( rounded vs. sharp corners, symmetry, thickness, etc. ). - * * spatial relations and layout : * * state relative positions, alignment, scale, layering, overlaps, and foreground / background structure. - * * styling and color per element : * * name the specific colors of each part and any fills or shading. - * * stylistic details : * * note outlines / strokes and their weight, flat fills, gradients, highlights, shadows, textures, or transparency when present. - * * text transcription : * * copy any legible text exactly and specify its placement. * * output format * * use plain sentences only ( no lists or bullets ). return * * only * * the description paragraph. figure 14. detailed annotation template. 3", "token_count": 263}
