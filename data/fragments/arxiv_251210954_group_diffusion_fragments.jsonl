{"doc_id": "arxiv_251210954_group_diffusion", "page": 1, "frag_id": 0, "text": "group diffusion : enhancing image generation by unlocking cross - sample collaboration sicheng mo1 thao nguyen2 richard zhang3 nick kolkin3 siddharth srinivasan iyer3 eli shechtman3 krishna kumar singh3 yong jae lee3 bolei zhou1 yuheng li3 1university of california, los angeles 2university of wisconsin – madison 3adobe research https : / / sichengmo. github. io / groupdiff / abstract in this work, we explore an untapped signal in diffusion model inference. while all previous methods generate images independently at inference, we instead ask if samples can be generated collaboratively. we propose group diffusion, unlocking the attention mechanism to be shared across images, rather than limited to just the patches within an image. this enables images to be jointly denoised at inference time, learning both intra and inter - image correspondence. we observe a clear scaling effect – larger group sizes yield stronger cross - sample attention and better generation quality. furthermore, we introduce a qualitative measure to capture this behavior and show that its strength closely correlates with fid. built on standard diffusion transformers, our groupdiff achieves up to 32. 2 % fid improvement on imagenet - 256×256. our work reveals cross - sample inference as an effective, previously unexplored mechanism for generative modeling. 1. introduction “ alone we can do so little ; together we can do so much. ” helen keller during generative model training, network weights are optimized using batches of images to learn an underlying image distribution [ 6, 12, 16, 22, 36, 44 ]. however, at inference time, images are typically generated independently. while patches within an image can interact to produce a coherent output, patches across different images are processed separately. this raises an intriguing, unexplored question – can images and patches across a batch collaborate to enhance generation quality collectively? following our inquiry, we introduce group diffusion, which jointly denoises a group of samples with the same example generation co - generations within group group diffusion ( ours ) 1 fid : 3. 50 fid : 2. 92 fid : 2. 42 fid : 2. 14 increasing group size 2 4 8 individual diffusion ( baseline ) figure 1. in standard diffusion ( top row ), samples are generated independently. our groupdiff uses cross - sample attention, enabling", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 1, "frag_id": 1, "text": "time, images are typically generated independently. while patches within an image can interact to produce a coherent output, patches across different images are processed separately. this raises an intriguing, unexplored question – can images and patches across a batch collaborate to enhance generation quality collectively? following our inquiry, we introduce group diffusion, which jointly denoises a group of samples with the same example generation co - generations within group group diffusion ( ours ) 1 fid : 3. 50 fid : 2. 92 fid : 2. 42 fid : 2. 14 increasing group size 2 4 8 individual diffusion ( baseline ) figure 1. in standard diffusion ( top row ), samples are generated independently. our groupdiff uses cross - sample attention, enabling samples within a batch to collaborate on a generation. we show selected examples of class - conditional imagenet generation, using group sizes { 1, 2, 4, 8 }. we find that the average generation quality improves with larger group size. conditioning. this is enabled using bidirectional attention across samples. during training, we construct each group by querying semantically or visually similar samples from the training dataset, allowing the attention mechanism to see all patches from within the group. then, at test time, we generate images in a batch, allowing images within the batch to aid one another in the diffusion process. we observe a clear scaling effect, where increasing the group size strengthens cross - sample attention and consistently improves generation quality, as illustrated in figure 1. we further analyze the attention patterns across images. as shown in figure 2, the group - wise denoising enables each patch to attend to others within the group, allowing the 1 arxiv : 2512. 10954v1 [ cs. cv ] 11 dec 2025", "token_count": 360}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 2, "frag_id": 0, "text": "class 555 : fire engine, fire truck class 980 : volcano class 88 : macaw class 208 : golden retriever figure 2. attention map visualization. we show the attention map, using the query point starred on the left, across samples ( group size 4 ), from the second layer. the star refers to the anchor patch. high attention score patches are denoted in red. during the generation process, each image patch is encouraged to attend to similar patches from other images, which enhances the generation quality. model to learn both intra and inter - image correspondence. interestingly, we show that generation quality is largely determined by how attention is distributed across samples, with the model assigning higher weights to semantically relevant samples that exert a stronger influence on the final output. we additionally identify a qualitative measure of cross - sample attention whose strength correlates closely with generation quality, providing deeper insight into how group - wise interaction governs the generation process. we summarize our contribution as follows : ( 1 ) we present groupdiff, a simple yet effective framework that jointly denoise a group of samples with the same condition rather than individual images, enabling cross - sample interaction through attention. ( 2 ) a systematic study on groupdiff training and inference behavior, offering insights for better leveraging inter - sample correspondence in image generation. ( 3 ) our framework improves generation quality and flexibility over traditional systems ; e. g., integrating groupdiff with sit yields 20. 9 % and 32. 2 % better fid when trained from scratch and resumed from a pre - trained checkpoint, respectively. 2. related work diffusion models. powered by their ability to model complex distributions via iterative denoising, diffusion models have become the leading paradigm for high - fidelity image [ 9, 16, 42, 44, 45 ], video [ 17, 24, 32, 37, 58, 66, 71 ] and multi - modal concept [ 34, 47, 72 ] generation. besides relying solely on the diffusion objective, recent literature [ 26, 62, 67, 70 ] explores the alignment between generative modeling and representation learning. repa [ 67 ] accelerates diffusion model training by aligning its representation with the pretrained ssl models. repa - e [ 26 ] further leverages the pretrained model ’ s knowledge with additional learnable parameters from the latent encoder. meanwhile, another line of work addresses this potential limitation from the pre - trained", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 2, "frag_id": 1, "text": "leading paradigm for high - fidelity image [ 9, 16, 42, 44, 45 ], video [ 17, 24, 32, 37, 58, 66, 71 ] and multi - modal concept [ 34, 47, 72 ] generation. besides relying solely on the diffusion objective, recent literature [ 26, 62, 67, 70 ] explores the alignment between generative modeling and representation learning. repa [ 67 ] accelerates diffusion model training by aligning its representation with the pretrained ssl models. repa - e [ 26 ] further leverages the pretrained model ’ s knowledge with additional learnable parameters from the latent encoder. meanwhile, another line of work addresses this potential limitation from the pre - trained vision encoder by aligning cross - layer features to each other ( sra [ 20 ] ) or explicitly applying ssl object function on generative model representation ( dispersive loss [ 59 ] ). in contrast, groupdiff learns a stronger representation implicitly by allowing group attention to learn both inter and intra - image correspondence. this novel approach offers a fresh perspective on integrating diffusion modeling with representation learning. semantic correspondence in diffusion models. semantic correspondence maps semantically related regions across images, enabling alignment despite changes in appearance or pose. in addition to its state - of - the - art generation capability, a large - scale pre - trained text - to - image diffusion model [ 9, 43, 45 ] naturally captures such semantic correspondence robustly, which unlocks promising applications in classification [ 27 ] and segmentation [ 51, 54, 63 ] with such features. meanwhile, a line of works [ 30, 69 ] extract high - quality representation from the denoiser by adding different levels of noise and enabling robust crossimage point matching. follow - up work leverages the global level dense semantic correspondence for image - to - image translation [ 8, 29, 33, 35, 56 ] method without additional training. furthermore, there is another line of work that goes beyond single - image generation to multi - view generation [ 18 ], style - controlled group generation [ 48 ], and video generation [ 21 ], by modeling inter - image correspondence with mutual attention. different from the aforementioned literature, our method explicitly leverages cross - sample relationships to enhance individual sample ’ s quality by jointly denoising all images within a group together, instead of implicitly learning it from individual samples. unified transformer models. transformer models [", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 2, "frag_id": 2, "text": "levels of noise and enabling robust crossimage point matching. follow - up work leverages the global level dense semantic correspondence for image - to - image translation [ 8, 29, 33, 35, 56 ] method without additional training. furthermore, there is another line of work that goes beyond single - image generation to multi - view generation [ 18 ], style - controlled group generation [ 48 ], and video generation [ 21 ], by modeling inter - image correspondence with mutual attention. different from the aforementioned literature, our method explicitly leverages cross - sample relationships to enhance individual sample ’ s quality by jointly denoising all images within a group together, instead of implicitly learning it from individual samples. unified transformer models. transformer models [ 57 ] have unified domain - specific architecture design across language, vision, and audio. it first showcased its strong capability on encoder - decoder and later decoder - only language models in the language domain. vit [ 7 ] proposed to con2", "token_count": 202}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 3, "frag_id": 0, "text": "individual diffusion ( baseline ) group diffusion ( ours ) reshape multi - head self - attention pointwise feedforward [UNK] input tokens conditioning patches batch reshape group attention operation mlp output tokens figure 3. approach. ( left ) previous approaches generate images independently. we explore group diffusion, which allows a set of images to collaborate together during inference time. ( right ) group attention can be implemented simply by reshaping the tokens within a batch, before and after the attention operation. vert images to a series of smaller patches to adapt the transformer model to the vision field and find its remarkable scaling capabilities under increasing data, training compute, and data. in the image generative model field, diffusion transformer [ 40 ] firstly verified the outstanding scalability of such an architecture, and a similar model design has been further extended to video diffusion models in [ 2, 24, 58 ]. moreover, multi - modal models [ 34, 47, 53, 72 ] with unified transformer again verified the generaizability of such architecture. groupdiff benefits from the flexibility of the unified transformer model design by adding multi - image generation capability to the image generative model. 3. group diffusion 3. 1. preliminary diffusion models gradually reverse the process of adding noise to an image, starting from a noise vector xt and progressively generating less noisy samples xt −1, xt −2,..., x0 with learned denoising function eθ. the training objective aims to minimize the difference between the predicted and true noise. specifically, for each time step t, the objective is to solve the following denoising problem on the image data x : ldm = ex, [UNK] ( 0, i ), t [UNK] −eθ ( xt ; t, c ) [UNK] 2, ( 1 ) where xt is the noisy image at time step t, uniformly sampled from { 1,..., t }, and eθ ( xt, t, c ) is the denoising function that predicts the noise added to xt conditioned on the time step t and context c ( often a text prompt or class label ). classifier - free diffusion guidance [ 15 ] enables controlling the trade - off between sample quality and diversity in diffusion models. it shifts pθ ( c | xt ) to assign a higher likelihood to the condition c without additional classifier. this is implemented by training the diffusion model for both", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 3, "frag_id": 1, "text": "i ), t [UNK] −eθ ( xt ; t, c ) [UNK] 2, ( 1 ) where xt is the noisy image at time step t, uniformly sampled from { 1,..., t }, and eθ ( xt, t, c ) is the denoising function that predicts the noise added to xt conditioned on the time step t and context c ( often a text prompt or class label ). classifier - free diffusion guidance [ 15 ] enables controlling the trade - off between sample quality and diversity in diffusion models. it shifts pθ ( c | xt ) to assign a higher likelihood to the condition c without additional classifier. this is implemented by training the diffusion model for both conditional and unconditional denoising and combining the two score estimates at inference time. specifically, at inference time, the modified score estimate [UNK] ( xt, c ) is extrapolated in the direction towards the conditional eθ ( xt, c ) and away from the unconditional eθ ( xt, ∅ ). [UNK] ( xt ; t, c ) = eθ ( xt ; t, c ) + s · eθ ( xt ; t, c ) −eθ ( x ; t, ∅ ) ( 2 ) 3. 2. approach at the core of our method is the idea of generating multiple images together, so each sample can enhance its generation by selectively learning from other samples, as illustrated in figure 2. in our groupdiff, we construct a group with related image data, thus allowing the diffusion model to learn a better representation that can be aided by other samples. at test time, we generate multiple images, conditioned on the same conditioning c, a setup that aligns well with modern applications, where users typically expect several outputs under the same condition. we follow best practices, adopting the diffusion transformer ( dit [ 40 ] ) model architecture, which uses an attention mechanism between patches within an image. we simply modify the attention by concatenating the group of image patches together, so that each patch can take other samples into consideration. to ensure that the diffusion model can recognize different image samples, we add the same learnable sample embedding to all patches from a given image. we formally define the groupdiff method as follows. query method. our hypothesis for groupdiff is that images in the same group are related either semantically or visually,", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 3, "frag_id": 2, "text": "the same conditioning c, a setup that aligns well with modern applications, where users typically expect several outputs under the same condition. we follow best practices, adopting the diffusion transformer ( dit [ 40 ] ) model architecture, which uses an attention mechanism between patches within an image. we simply modify the attention by concatenating the group of image patches together, so that each patch can take other samples into consideration. to ensure that the diffusion model can recognize different image samples, we add the same learnable sample embedding to all patches from a given image. we formally define the groupdiff method as follows. query method. our hypothesis for groupdiff is that images in the same group are related either semantically or visually, and can be used to aid in the denoising process. 3", "token_count": 164}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 4, "frag_id": 0, "text": "thus, we must construct sets of images that are related during training time. given the image x ∈rh×w ×3 and the entire image dataset d, we define the query function q ( x ) as the following : q ( x ; d ; τimg ) = { xi ∈d | sim ( x, xi ) ≥τimg }, ( 3 ) where sim ( · ) returns the image similarity between two images, and τimg is a similarity threshold. in practice, we compute the sim ( · ) by cosine similarity between image embeddings from pre - trained models like clip [ 41 ] or dino [ 38 ]. groupdiff training. at each training step, we first construct a group of related images x ∈rn×h×w ×3, including the original image x, by randomly sampling n −1 images from the images returned by query function q ( x ; d ; τ ). we use threshold τimg = 0. 7 in our experiments, which retrieves a sufficient number of related samples. for such image group, we first extract their latent with a pre - trained vae from stable diffusion [ 45 ]. to obtain the noisy latent, we sample the timestep independently for each sample but ensure that the variance of the timestep within each group is under the threshold of timestep variation σtv. to compute the group attention, we first extract the hidden states h from the input x, and then reshape them from rn×l×c →r1× ( nl ) ×c, where l is the image patch sequence length and c is the channel. after the attention ( · ) operation, we reshape the hidden states back. in particular, groupdiff enables generating multiple samples in a group by using lgroup as the loss function as follows : lgroup = ex, [UNK] ( 0, i ), t \" n x i = 1 [UNK] −eθ ( x ; t, c ) [UNK] 2 #, ( 4 ) where c is the condition and t is the denoising timestep. groupdiff inference. groupdiff enables generating n dependent images following the condition c together at the inference time, instead of n independent image as in previous systems [ 40 ]. at each timestep, the denoiser predicts two scores : conditional and unconditional. we introduce two variations of our method, groupdiff", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 4, "frag_id": 1, "text": "the hidden states back. in particular, groupdiff enables generating multiple samples in a group by using lgroup as the loss function as follows : lgroup = ex, [UNK] ( 0, i ), t \" n x i = 1 [UNK] −eθ ( x ; t, c ) [UNK] 2 #, ( 4 ) where c is the condition and t is the denoising timestep. groupdiff inference. groupdiff enables generating n dependent images following the condition c together at the inference time, instead of n independent image as in previous systems [ 40 ]. at each timestep, the denoiser predicts two scores : conditional and unconditional. we introduce two variations of our method, groupdiff - f and groupdiff - l, by flexibly deciding whether to predict the conditional score with group attention or not. for groupdiff - f, we obtain both scores from group attention and apply the cfg guidance to combine those scores as follows : [UNK] ( xt ; t, c ) = eθ ( xt ; t, c ) + s · ( eθ ( xt ; t, c ) −eθ ( xt ; t, ∅ ) ). ( 5 ) for groupdiff - l, only the unconditional score is predicted from group attention. in this case, we obtain [UNK] as follows : [UNK] ( xt ; t, c ) = { eθ ( xi t ; t, c ) } n i = 1 + s · ( { eθ ( xi t ; t, c ) } n i = 1 −eθ ( xt ; t, ∅ ) ), ( 6 ) where xi t is the ith element in group x. by convention, only 10 % of the data is used to train the unconditional model for generation with cfg [ 15 ]. since groupdiff - l applies the large group size only to this unconditional model, the remaining 90 % is trained with a group size of one. thus, most of the training remains identical to standard diffusion, making groupdiff - l computationally lightweight compared to groupdiff - f and close to baseline systems [ 31, 40 ]. empirically, we find that groupdiffl strikes a good balance between generation quality and computational cost. throughout the paper, we refer to groupdiff as groupdiff - l unless otherwise specified. 4. experiments we now analyze", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 4, "frag_id": 2, "text": "element in group x. by convention, only 10 % of the data is used to train the unconditional model for generation with cfg [ 15 ]. since groupdiff - l applies the large group size only to this unconditional model, the remaining 90 % is trained with a group size of one. thus, most of the training remains identical to standard diffusion, making groupdiff - l computationally lightweight compared to groupdiff - f and close to baseline systems [ 31, 40 ]. empirically, we find that groupdiffl strikes a good balance between generation quality and computational cost. throughout the paper, we refer to groupdiff as groupdiff - l unless otherwise specified. 4. experiments we now analyze our proposed groupdiff, beginning with the introduction of the experiment setup and a series of ablation studies on the group settings, followed by observations of the intriguing property and behavior of groupdiff. lastly, we benchmark with previous leading systems. 4. 1. setup implementation details. we strictly follow the dit [ 40 ] and sit [ 31 ] model architecture / configuration and data process. we train the groupdiff with adamw optimizer, a constant learning rate of 1 × 10−4, and weight decay 0. 01 on a100 gpus. sampling is performed using the sde euler - maruyama sampler and the iddpm [ 36 ] sampler with nfe = 250 when sit [ 31 ] and dit [ 40 ] are selected as the baseline model, respectively. we consistently use a global batch size of 256 when adjusting the group size to ensure a fair comparison across variations and baseline methods. additional implementation details and baseline introduction are provided in the supplementary. datasets and metrics. following dit [ 40 ], we conduct experiments on imagenet [ 5 ] and use a pretrained stable diffusion vae with a compression ratio of 8 to encode each 256 × 256 image into a compressed vector x ∈r32×32×4. and we report the fid [ 14 ], inception score [ 46 ], precision and recall [ 25 ] for measuring the generation quality. 4. 2. main properties as shown in table 1, we discover that groupdiff consistently provides a substantially improved generation performance across various design choices, achieving a much better fid score than the vanilla model. below, we provide a detailed analysis of the impact of each component. group model. the", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 4, "frag_id": 3, "text": "supplementary. datasets and metrics. following dit [ 40 ], we conduct experiments on imagenet [ 5 ] and use a pretrained stable diffusion vae with a compression ratio of 8 to encode each 256 × 256 image into a compressed vector x ∈r32×32×4. and we report the fid [ 14 ], inception score [ 46 ], precision and recall [ 25 ] for measuring the generation quality. 4. 2. main properties as shown in table 1, we discover that groupdiff consistently provides a substantially improved generation performance across various design choices, achieving a much better fid score than the vanilla model. below, we provide a detailed analysis of the impact of each component. group model. the leading diffusion systems usually benefit from classifier - free guidance [ 15 ], which takes the joint effect with the conditional model and unconditional model. in practice, those two models usually share most model weights besides the condition embedding. we begin 4", "token_count": 201}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 5, "frag_id": 0, "text": "groupdiff settings w / o cfg w / cfg cross - sample linear prob iter model query method noise var. fid ↓ fid ↓ cfg - scale attn. score ↑ acc. ↑ 800k c = 1, uc = 1 0 14. 38 3. 50 1. 5 49. 48 800k c = 4, uc = 4 class 0 14. 27 3. 08 1. 6 62. 15 800k c = 1, uc = 4 class 0 13. 22 2. 81 1. 6 64. 44 800k c = 1, uc = 2 clip - l 0 13. 47 2. 92 1. 5 0. 00 % 55. 33 800k c = 1, uc = 4 clip - l 0 13. 93 2. 42 2. 0 19. 95 % 58. 83 800k c = 1, uc = 8 clip - l 0 13. 08 2. 14 2. 2 51. 13 % 67. 93 800k c = 1, uc = 16 clip - l 0 13. 84 1. 86 2. 5 56. 47 % 72. 91 800k c = 1, uc = 4 random 0 13. 28 3. 57 1. 5 23. 17 % 800k c = 1, uc = 4 class 0 13. 22 2. 81 1. 6 22. 51 % 64. 44 800k c = 1, uc = 4 clip - b 0 13. 47 2. 51 1. 9 19. 20 % 61. 14 800k c = 1, uc = 4 clip - l 0 13. 93 2. 42 2. 0 19. 95 % 58. 83 800k c = 1, uc = 4 siglip 0 13. 83 2. 45 2. 0 19. 98 % 63. 32 800k c = 1, uc = 4 dinov2 - b 0 14. 40 2. 51 1. 9 18. 45 % 63. 32 800k c = 1, uc = 4 dinov2 - l 0 13. 35 2. 51 1. 9 22. 85 % 59. 16 800k c = 1, uc = 4 i - jepa 0 13. 08 2. 44 1. 8 18. 50 % 60. 50 800k c = 1, uc = 4 clip - l 0 13. 93 2. 42 2. 0 19. 95 % 58. 83 800k c = 1, uc = 4 clip -", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 5, "frag_id": 1, "text": "##p 0 13. 83 2. 45 2. 0 19. 98 % 63. 32 800k c = 1, uc = 4 dinov2 - b 0 14. 40 2. 51 1. 9 18. 45 % 63. 32 800k c = 1, uc = 4 dinov2 - l 0 13. 35 2. 51 1. 9 22. 85 % 59. 16 800k c = 1, uc = 4 i - jepa 0 13. 08 2. 44 1. 8 18. 50 % 60. 50 800k c = 1, uc = 4 clip - l 0 13. 93 2. 42 2. 0 19. 95 % 58. 83 800k c = 1, uc = 4 clip - l 20 13. 50 2. 42 2. 0 21. 37 % 60. 48 800k c = 1, uc = 4 clip - l 50 12. 81 2. 34 2. 0 26. 23 % 68. 91 800k c = 1, uc = 4 clip - l 100 12. 78 2. 32 1. 9 23. 33 % 62. 82 800k c = 1, uc = 4 clip - l 150 13. 70 2. 25 2. 0 24. 31 % 63. 74 800k c = 1, uc = 4 clip - l 200 13. 26 2. 32 1. 8 24. 46 % 60. 03 table 1. component - wise analysis on imagenet 256 × 256 with dit - xl / 2 [ 40 ] trained for 800k iterations. all metrics except accuracy ( acc. ) are measured with the iddpm [ 36 ] sampler with nfe = 250. for generation results with classifier - free guidance, we search for the optimal guidance scale using an interval of 0. 1 and report the one with the optimal fid score. ↑and ↓indicate whether higher or lower values are better, respectively. c and uc referring to the conditional model and unconditional model, respectively. the ablation by analyzing the model behavior when applying the group attention operation on one or both models. in this analysis, we use the imagenet [ 5 ] class label as the query method to build each group. we observe that groupdiff consistently outperforms the individual diffusion baseline. notably, when only running the uc model in the groupdiff mode, our system further achieves higher generation quality when both the cfg is disabled or enabled, reflected by", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 5, "frag_id": 2, "text": ", we search for the optimal guidance scale using an interval of 0. 1 and report the one with the optimal fid score. ↑and ↓indicate whether higher or lower values are better, respectively. c and uc referring to the conditional model and unconditional model, respectively. the ablation by analyzing the model behavior when applying the group attention operation on one or both models. in this analysis, we use the imagenet [ 5 ] class label as the query method to build each group. we observe that groupdiff consistently outperforms the individual diffusion baseline. notably, when only running the uc model in the groupdiff mode, our system further achieves higher generation quality when both the cfg is disabled or enabled, reflected by lower fid. under this setting, we observe that the condition model ’ s generation capability has also improved when we train only the unconditional model with group attention. we hypothesize that the stronger representation in the uc model implicitly enhances the c model via weight sharing. in later experiments, we set c = 1, uc = n as the default choice to balance training and inference. group size. we also study the impact of group size in groupdiff. larger groups generally yield better generation results, as reflected by consistent improvements in fid and feature quality. we hypothesize that larger groups offer greater flexibility for finding better patch - level matches, thereby enhancing generation and internal representations. detailed pattern analysis is provided in sec. 4. 3. in the following experiments, we choose 4 as the group size for fair comparison with baseline methods. group construction method. we then investigate the imclass label clip - l dinov2 - l query image figure 4. comparison of group candidates from different query methods. the difference in pretraining settings lead each query method to form distinct groups. we show nearest samples from the imagenet [ 5 ] training split, with the class label row showing random same - class samples. pact of different group construction methods, including random sampling, class - based grouping, and similarity - based retrieval via pre - trained vision encoders. quantitatively, similarity - based grouping yields the best generation quality, followed by class - based grouping, while random sampling performs the worst ( on par with the baseline ). this indicates that group attention does not degrade the baseline diffusion model ’ s performance, even without any bells and whistles. meanwhile, we hypothesize that image", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 5, "frag_id": 3, "text": "comparison of group candidates from different query methods. the difference in pretraining settings lead each query method to form distinct groups. we show nearest samples from the imagenet [ 5 ] training split, with the class label row showing random same - class samples. pact of different group construction methods, including random sampling, class - based grouping, and similarity - based retrieval via pre - trained vision encoders. quantitatively, similarity - based grouping yields the best generation quality, followed by class - based grouping, while random sampling performs the worst ( on par with the baseline ). this indicates that group attention does not degrade the baseline diffusion model ’ s performance, even without any bells and whistles. meanwhile, we hypothesize that image similarity within a 5", "token_count": 154}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 6, "frag_id": 0, "text": "fid cross - sample score ( % ) cross sample attention weight layer index figure 5. cross - sample attention in groupdiff. ( a ) fid vs cross - sample score ( left ). our groupdiff shows a strong correlation ( 0. 95 ) between cross - attention to other samples and generation quality. ( b ) cross - sample attention visualization ( right ). group is crucial for strengthening cross - sample interaction. random groups often contain unrelated samples and thus lack meaningful mutual information, whereas similaritybased retrieval retrieves semantically coherent images, reducing the fid ( with cfg ) from 3. 57 to around 2. 4. interestingly, figure 4 shows that different pre - trained encoders form visually distinct groups. for instance, clipl [ 41 ] tends to cluster semantically similar samples, while dinov2 - b [ 38 ] captures alternative aspects of visual similarity. nevertheless, their resulting generation quality remains comparable, suggesting that the benefit primarily arises from semantic consistency rather than the specific encoder style. overall, groupdiff demonstrates strong flexibility and generalization, showing that the quality of the pre - trained encoders does not limit its performance. group noise - level variation. lastly, we explore the effect of introducing noise - level variance within each group. instead of applying the same noise level to the entire group, we restrict the noise levels of the other samples to differ from that of the first sample by up to a specified range, e. g. 50 or 200. prior works [ 4, 65 ] verified that adding different level of noise could be an effective augmentation method for improving representations learning and generation quality. in our setting, we hypothesize that noisier samples benefit from cleaner ones within the same group, further encouraging cross - sample attention. we find that setting the noiselevel variation in the range of 50 to 200 yields the best performance, improving both fid and linear probe accuracy while strengthening cross - sample attention. 4. 3. groupdiff generation pattern analysis after validating the effectiveness of different group settings, we now analyze why and how groupdiff improves generation quality and investigate its unique generation patterns. cross - sample attention. to understand why groupdiff improves generation, we examine how cross - sample interaction influences the diffusion process. at the core of groupdiff is cross - sample attention, enabling each patch 0. 2 0. 4 0. 8 0. 6 figure 6. controlling", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 6, "frag_id": 1, "text": "samples benefit from cleaner ones within the same group, further encouraging cross - sample attention. we find that setting the noiselevel variation in the range of 50 to 200 yields the best performance, improving both fid and linear probe accuracy while strengthening cross - sample attention. 4. 3. groupdiff generation pattern analysis after validating the effectiveness of different group settings, we now analyze why and how groupdiff improves generation quality and investigate its unique generation patterns. cross - sample attention. to understand why groupdiff improves generation, we examine how cross - sample interaction influences the diffusion process. at the core of groupdiff is cross - sample attention, enabling each patch 0. 2 0. 4 0. 8 0. 6 figure 6. controlling groupdiff denoising steps. we show generated sample examples when groupdiff is turned off after different denoising stages. stable quality after denoising with groupdiff at early steps. to establish intra - image and inter - image correspondence across the group. figure 2 shows that a patch corresponding to a “ dog ’ s ear ” attends to both the same region of its own instance and to similar “ ear ” regions in other dog images. to quantitatively measure the cross - sample attention, we define the image - level self - attention as attention assigned to its own patches, and cross - attention as attention assigned to patches from other images in the group. formally, let image xi contain patch indices ii. for a query patch q ∈ii and any key patch k, let the attention weight be αqk. we define the image - level cross - attention weight for image xi as p xi cross = pxi→xj j = i, where pxi→xj = x q∈ii x k∈ij αqk. ( 7 ) furthermore, we introduce the mean cross - attention score and the max cross - attention score of image xi by tak6", "token_count": 389}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 7, "frag_id": 0, "text": "ing the mean and maximum over p xi cross : p xi cross - mean = mean ( p xi cross ), p xi cross - max = max ( p xi cross ). attention over denoising steps. to further quantify this effect, we measure cross - sample attention across different denoising steps using the image - level cross - attention score, p xi cross. for each image, we compute its mean and maximum cross - attention scores, p xi cross - mean and p xi cross - max, and average these statistics over all images in the group. as shown in figure 5 ( right ), both the mean and maximum cross - attention scores gradually decrease as the noise level reduces, indicating that inter - sample information exchange is most active at the early stages of denoising when global structure and semantics are being formed. to validate this observation, we conduct an intervention experiment by turning off groupdiff after a certain number of denoising steps and continuing the process using the baseline dit model. as illustrated in figure 6, disabling groupdiff in the middle or late stages yields little quality degradation, confirming our aforementioned hypothesis. table 2 shows that groupdiff could be faster without degraded quality by only applying group attention in the early and middle stages. attention over denoiser layers. we also examine the layer - wise distribution of cross - sample attention. groupdiff shows stronger cross - sample attention in the early and final layers, suggesting that it uses other samples to form global context and later refine details. table 3 shows that early layers are essential, while late layers have much less impact on groupdiff. these results indicate groupdiff strengthens cross - sample interaction in the early timesteps and shallow layers, leading to improved generation quality. methods fid - 10k baseline 4. 21 w / t 0. 0 - 0. 2 4. 04 w / t 0. 0 - 0. 4 3. 92 w / t 0. 0 - 0. 6 4. 63 table 2. ablation on group attention timestep. methods fid - 10k baseline 4. 21 w / o layer 1 - 9 294. 38 w / o layer 10 - 19 5. 49 w / o layer 20 - 27 4. 49 table 3. ablation on group attention layers. cross - sample attention score. under a setting that encourages cross - image attention, we hypothesize two possible operating modes : ( i )", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 7, "frag_id": 1, "text": "shallow layers, leading to improved generation quality. methods fid - 10k baseline 4. 21 w / t 0. 0 - 0. 2 4. 04 w / t 0. 0 - 0. 4 3. 92 w / t 0. 0 - 0. 6 4. 63 table 2. ablation on group attention timestep. methods fid - 10k baseline 4. 21 w / o layer 1 - 9 294. 38 w / o layer 10 - 19 5. 49 w / o layer 20 - 27 4. 49 table 3. ablation on group attention layers. cross - sample attention score. under a setting that encourages cross - image attention, we hypothesize two possible operating modes : ( i ) an evenly distributed mode, where an image spreads attention across all others, and ( ii ) a neighbor - focused mode, where it primarily attends to its most similar counterpart. we focus on the latter behavior and quantify its strength using an image - level cross - sample attention score defined as scross = pcross - max −pcross - mean pcross - max, ( 8 ) where pcross - max and pcross - mean denote the maximum and mean cross - sample attention from one image to the others reference replace 2nd replace 3rd replace 4th lower attention weights to the first sample figure 7. controlling conditions. the reference group uses class 89, and in each row, one sample ’ s ( red ) condition is changed to class 360. in the group. intuitively, this score measures how strongly the attention distribution concentrates on the most similar image, normalized by the overall attention magnitude. a score close to 0 indicates a uniform, distributed attention pattern, while a score close to 1 reflects a highly peaked, neighbor - focused attention on a single image. by varying the query method, noise range, and group size across groupdiff variants, we compare their crosssample attention scores with their fid. we observe a strong correlation ( r = 0. 95 ; fig. 5 left ), showing that more neighbor - focused cross - sample attention leads to higher generation quality. upon closer inspection, several distinct clusters emerge in the plot, primarily corresponding to different group sizes. we find that increasing the group size effectively encourages stronger cross - sample attention behavior, further improving generation quality. moreover, even within each cluster, higher cross - sample attention scores still correlate with lower fid, showing that this interaction reliably reflects generation quality.", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 7, "frag_id": 2, "text": "highly peaked, neighbor - focused attention on a single image. by varying the query method, noise range, and group size across groupdiff variants, we compare their crosssample attention scores with their fid. we observe a strong correlation ( r = 0. 95 ; fig. 5 left ), showing that more neighbor - focused cross - sample attention leads to higher generation quality. upon closer inspection, several distinct clusters emerge in the plot, primarily corresponding to different group sizes. we find that increasing the group size effectively encourages stronger cross - sample attention behavior, further improving generation quality. moreover, even within each cluster, higher cross - sample attention scores still correlate with lower fid, showing that this interaction reliably reflects generation quality. cross - condition generation. to further validate the role of cross - sample attention, we conduct a controlled experiment by replacing one image in the group with a sample from a different class while keeping the latent variables fixed. we first generate a group of reference images and rank them by their cross - attention weights to the first sample, pxi→x1. then, we gradually replace the condition of one sample with another class during the entire denoising process and show the results in figure 7. we observed that the generation of the reference ( green box ) image is highly sensitive to which sample is replaced. when we replace a sample that originally receives high attention weights, the reference image changes significantly. in contrast, replacing a low - attention sample results in almost no visual difference. 7", "token_count": 307}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 8, "frag_id": 0, "text": "figure 8. qualitative results. examples of class - conditional generation on imagenet 256×256 using groupdiff - 4 with sit - xl / 2. method epoch w / cfg fid is pre. rec. with semantic feature distillation ddt - xl [ 61 ] 800 1. 26 310. 6 0. 79 0. 65 sit - xl / 2 + repa - e [ 26 ] 800 1. 26 314. 9 0. 79 0. 66 sit - xl / 2 + repa [ 67 ] 800 1. 42 305. 7 0. 80 0. 64 + our groupdiff - 4 * 800 + 100 1. 14 315. 3 0. 77 0. 66 without semantic feature distillation adm [ 6 ] 400 4. 59 186. 7 0. 82 0. 52 vdm + + [ 23 ] 2. 12 267. 7 ldm - 4 [ 43 ] 1400 3. 60 247. 7 0. 87 0. 48 mdtv2 - xl / 2 [ 11 ] 900 1. 58 314. 7 0. 79 0. 65 var - d30 [ 55 ] 350 1. 92 323. 1 0. 82 0. 59 llamagen - 3b [ 50 ] 2. 18 263. 3 0. 81 0. 58 randar - xxl [ 39 ] 300 2. 15 322. 0 0. 79 0. 62 maskdit [ 70 ] 1600 2. 28 276. 6 0. 89 0. 61 dit - xl / 2 [ 40 ] 1400 2. 27 278. 2 0. 83 0. 57 + our groupdiff - 4 800 1. 66 279. 4 0. 83 0. 57 + our groupdiff - 4∗ 1400 + 100 1. 55 285. 4 0. 80 0. 63 sit - xl / 2 [ 31 ] 1400 2. 06 270. 3 0. 82 0. 59 + sra [ 20 ] 800 1. 58 311. 4 0. 80 0. 63 + dispersive loss [ 59 ] 1200 1. 97 + our groupdiff - 4 800 1. 63 283. 2 0. 81 0. 64 + our groupdiff - 4∗ 1400 + 100 1. 40 290. 7 0. 79 0. 64 table 4. system - level performance comparison on imagenet 256 × 256. our groupdiff enables the dit / sit model to achieve state - of - the -", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 8, "frag_id": 1, "text": "groupdiff - 4∗ 1400 + 100 1. 55 285. 4 0. 80 0. 63 sit - xl / 2 [ 31 ] 1400 2. 06 270. 3 0. 82 0. 59 + sra [ 20 ] 800 1. 58 311. 4 0. 80 0. 63 + dispersive loss [ 59 ] 1200 1. 97 + our groupdiff - 4 800 1. 63 283. 2 0. 81 0. 64 + our groupdiff - 4∗ 1400 + 100 1. 40 290. 7 0. 79 0. 64 table 4. system - level performance comparison on imagenet 256 × 256. our groupdiff enables the dit / sit model to achieve state - of - the - art performance both with / without semantic feature distillation. ∗ : continue training from pre - trained checkpoint for an additional 100 epochs. this indicates that cross - sample attention controls the interimage correspondence within the group, with high - attention samples contributing more to the final generation, consistent with our earlier observations of cross - sample attention patterns. furthermore, we believe this property points to a promising future direction. when the group size is sufficiently large, the generation process of groupdiff could be extended to handle diverse or cross - conditioned inputs, enabling more flexible inter - image correspondence within the generation process. 4. 4. benchmarking with previous systems we compare against leading generative systems in table 4. for this experiment, we train groupdiff in two settings : from scratch and from the pre - trained weights, denoted as groupdiff - 4 and groupdiff - 4∗, respectively. when training from scratch, groupdiff improves dit - xl / 2 with 29 % lower fid and sit - xl / 2 with 30 % lower fid while only using 57 % of original training iterations. for the second setting, we only use the lgroup as the training objective, no matter if other objectives, e. g. lrepa from repa [ 67 ], exist in the previous stages. notably, groupdiff - 4 with dit - xl / 2 achieves an fid of 1. 55 ( from 2. 27 ) and groupdiff - 4 with sit - xl / 2 further improves to 1. 40 ( from 2. 06 ) with only 100 additional training epochs, outperforming all other state - of - the - art methods when no semantic feature distillation has", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 8, "frag_id": 2, "text": "fid and sit - xl / 2 with 30 % lower fid while only using 57 % of original training iterations. for the second setting, we only use the lgroup as the training objective, no matter if other objectives, e. g. lrepa from repa [ 67 ], exist in the previous stages. notably, groupdiff - 4 with dit - xl / 2 achieves an fid of 1. 55 ( from 2. 27 ) and groupdiff - 4 with sit - xl / 2 further improves to 1. 40 ( from 2. 06 ) with only 100 additional training epochs, outperforming all other state - of - the - art methods when no semantic feature distillation has been applied. moreover, when using pre - trained weights from the semantic feature distillation method, groupdiff again obtains a significant improvement, achieving an fid of 1. 14 ( down from 1. 42 ). qualitative samples are provided in figure 8. 5. discussion and conclusion limitations. while groupdiff demonstrates strong improvements in generation quality, its increased training cost remains a challenge. when the group size is n, groupdifff and groupdiff - l require approximately ( n −1 ) × and ( 0. 1n ) × longer training time in every iteration, and ( n − 1 ) × and 0. 5 ( n −1 ) × longer inference time, respectively. nevertheless, ( a ) this design opens a new avenue for exploring the trade - off between computational cost and generation quality, and ( b ) a high - quality model can serve as a teacher to distill faster and lighter students. we leave the study for a more efficient method for future exploration. conclusion. we introduce group diffusion, a simple yet effective framework that reshapes diffusion training into a group - wise denoising process. by enabling crosssample attention among related instances, the model implicitly learns relational structures that enhance representation quality and generation fidelity. experiments on imagenet demonstrate consistent fid improvements across architectures with minimal computational overhead. beyond boosting performance, group diffusion provides a new lens connecting representation learning and generative modeling, suggesting that cross - sample interactions can serve as an implicit form of supervision for stronger and more generalizable diffusion models. 8", "token_count": 469}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 9, "frag_id": 0, "text": "references [ 1 ] fan bao, shen nie, kaiwen xue, yue cao, chongxuan li, hang su, and jun zhu. all are worth words : a vit backbone for diffusion models. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 22669 – 22679, 2023. 21 [ 2 ] tim brooks, bill peebles, connor holmes, will depue, yufei guo, li jing, david schnurr, joe taylor, troy luhman, eric luhman, clarence ng, ricky wang, and aditya ramesh. video generation models as world simulators. 2024. 3 [ 3 ] shoufa chen, chongjian ge, shilong zhang, peize sun, and ping luo. pixelflow : pixel - space generative models with flow. arxiv preprint arxiv : 2504. 07963, 2025. 13 [ 4 ] ting chen, simon kornblith, mohammad norouzi, and geoffrey hinton. a simple framework for contrastive learning of visual representations. in international conference on machine learning, pages 1597 – 1607. pmlr, 2020. 6 [ 5 ] jia deng, wei dong, richard socher, li - jia li, kai li, and li fei - fei. imagenet : a large - scale hierarchical image database. in computer vision and pattern recognition, 2009. cvpr 2009. ieee conference on, pages 248 – 255. ieee, 2009. 4, 5 [ 6 ] prafulla dhariwal and alexander nichol. diffusion models beat gans on image synthesis. advances in neural information processing systems, 34 : 8780 – 8794, 2021. 1, 8, 12, 13 [ 7 ] alexey dosovitskiy. an image is worth 16x16 words : transformers for image recognition at scale. arxiv preprint arxiv : 2010. 11929, 2020. 2 [ 8 ] dave epstein, allan jabri, ben poole, alexei efros, and aleksander holynski. diffusion self - guidance for controllable image generation. advances in neural information processing systems, 36 : 16222 – 16239, 2023. 2 [ 9 ] patrick esser, sumith kulal, andreas blattmann, rahim entezari, jonas m¨uller, harry", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 9, "frag_id": 1, "text": ": 8780 – 8794, 2021. 1, 8, 12, 13 [ 7 ] alexey dosovitskiy. an image is worth 16x16 words : transformers for image recognition at scale. arxiv preprint arxiv : 2010. 11929, 2020. 2 [ 8 ] dave epstein, allan jabri, ben poole, alexei efros, and aleksander holynski. diffusion self - guidance for controllable image generation. advances in neural information processing systems, 36 : 16222 – 16239, 2023. 2 [ 9 ] patrick esser, sumith kulal, andreas blattmann, rahim entezari, jonas m¨uller, harry saini, yam levi, dominik lorenz, axel sauer, frederic boesel, et al. scaling rectified flow transformers for high - resolution image synthesis. in forty - first international conference on machine learning, 2024. 2, 21 [ 10 ] wan - cyuan fan, yen - chun chen, dongdong chen, yu cheng, lu yuan, and yu - chiang frank wang. frido : feature pyramid diffusion for complex scene image synthesis. in proceedings of the aaai conference on artificial intelligence, pages 579 – 587, 2023. 21 [ 11 ] shanghua gao, pan zhou, ming - ming cheng, and shuicheng yan. mdtv2 : masked diffusion transformer is a strong image synthesizer. arxiv preprint arxiv : 2303. 14389, 2023. 8, 12 [ 12 ] ian goodfellow, jean pouget - abadie, mehdi mirza, bing xu, david warde - farley, sherjil ozair, aaron courville, and yoshua bengio. generative adversarial networks. communications of the acm, 63 ( 11 ) : 139 – 144, 2020. 1 [ 13 ] shuyang gu, dong chen, jianmin bao, fang wen, bo zhang, dongdong chen, lu yuan, and baining guo. vector quantized diffusion model for text - to - image synthesis. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 10696 – 10706, 2022. 21 [ 14 ] martin heusel, hubert ramsauer, thomas unterthiner, bernhard nessler, and sep", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 9, "frag_id": 2, "text": "david warde - farley, sherjil ozair, aaron courville, and yoshua bengio. generative adversarial networks. communications of the acm, 63 ( 11 ) : 139 – 144, 2020. 1 [ 13 ] shuyang gu, dong chen, jianmin bao, fang wen, bo zhang, dongdong chen, lu yuan, and baining guo. vector quantized diffusion model for text - to - image synthesis. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 10696 – 10706, 2022. 21 [ 14 ] martin heusel, hubert ramsauer, thomas unterthiner, bernhard nessler, and sepp hochreiter. gans trained by a two time - scale update rule converge to a local nash equilibrium. advances in neural information processing systems, 30, 2017. 4, 12 [ 15 ] jonathan ho and tim salimans. classifier - free diffusion guidance. arxiv preprint arxiv : 2207. 12598, 2022. 3, 4 [ 16 ] jonathan ho, ajay jain, and pieter abbeel. denoising diffusion probabilistic models. advances in neural information processing systems, 33 : 6840 – 6851, 2020. 1, 2 [ 17 ] wenyi hong, ming ding, wendi zheng, xinghan liu, and jie tang. cogvideo : large - scale pretraining for text - to - video generation via transformers. arxiv preprint arxiv : 2205. 15868, 2022. 2 [ 18 ] zehuan huang, yuan - chen guo, haoran wang, ran yi, lizhuang ma, yan - pei cao, and lu sheng. mv - adapter : multi - view consistent image generation made easy. in proceedings of the ieee / cvf international conference on computer vision, pages 16377 – 16387, 2025. 2 [ 19 ] allan jabri, david fleet, and ting chen. scalable adaptive computation for iterative generation. arxiv preprint arxiv : 2212. 11972, 2022. 13 [ 20 ] dengyang jiang, mengmeng wang, liuzhuozheng li, lei zhang, haoyu wang, wei wei, guang dai, yanning zhang, and jingdong wang.", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 9, "frag_id": 3, "text": "guo, haoran wang, ran yi, lizhuang ma, yan - pei cao, and lu sheng. mv - adapter : multi - view consistent image generation made easy. in proceedings of the ieee / cvf international conference on computer vision, pages 16377 – 16387, 2025. 2 [ 19 ] allan jabri, david fleet, and ting chen. scalable adaptive computation for iterative generation. arxiv preprint arxiv : 2212. 11972, 2022. 13 [ 20 ] dengyang jiang, mengmeng wang, liuzhuozheng li, lei zhang, haoyu wang, wei wei, guang dai, yanning zhang, and jingdong wang. no other representation component is needed : diffusion transformers can provide representation guidance by themselves. arxiv preprint arxiv : 2505. 02831, 2025. 2, 8, 12 [ 21 ] ozgur kara, bariscan kurtkaya, hidir yesiltepe, james m rehg, and pinar yanardag. rave : randomized noise shuffling for fast and consistent video editing with diffusion models. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 6507 – 6516, 2024. 2 [ 22 ] tero karras, samuli laine, and timo aila. a style - based generator architecture for generative adversarial networks, 2019. 1 [ 23 ] diederik kingma and ruiqi gao. understanding diffusion objectives as the elbo with simple data augmentation. advances in neural information processing systems, 36 : 65484 – 65516, 2023. 8 [ 24 ] w kong, q tian, z zhang, r min, z dai, j zhou, j xiong, x li, b wu, j zhang, et al. hunyuanvideo : a systematic framework for large video generative models, 2025. url https : / / arxiv. org / abs / 2412. 03603. 2, 3 [ 25 ] tuomas kynk¨a¨anniemi, tero karras, samuli laine, jaakko lehtinen, and timo aila. improved precision and recall metric for assessing generative models. advances in neural information processing systems, 32, 2019. 4, 12 [ 26 ] xing", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 9, "frag_id": 4, "text": "8 [ 24 ] w kong, q tian, z zhang, r min, z dai, j zhou, j xiong, x li, b wu, j zhang, et al. hunyuanvideo : a systematic framework for large video generative models, 2025. url https : / / arxiv. org / abs / 2412. 03603. 2, 3 [ 25 ] tuomas kynk¨a¨anniemi, tero karras, samuli laine, jaakko lehtinen, and timo aila. improved precision and recall metric for assessing generative models. advances in neural information processing systems, 32, 2019. 4, 12 [ 26 ] xingjian leng, jaskirat singh, yunzhong hou, zhenchang xing, saining xie, and liang zheng. repa - e : unlocking vae for end - to - end tuning with latent diffusion transformers. arxiv preprint arxiv : 2504. 10483, 2025. 2, 8, 12 [ 27 ] alexander c li, mihir prabhudesai, shivam duggal, ellis brown, and deepak pathak. your diffusion model is secretly a zero - shot classifier. in proceedings of the ieee / cvf international conference on computer vision, pages 2206 – 2217, 2023. 2 9", "token_count": 294}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 10, "frag_id": 0, "text": "[ 28 ] tianhong li and kaiming he. back to basics : let denoising generative models denoise. arxiv preprint arxiv : 2511. 13720, 2025. 13 [ 29 ] kuan heng lin, sicheng mo, ben klingher, fangzhou mu, and bolei zhou. ctrl - x : controlling structure and appearance for text - to - image generation without guidance. advances in neural information processing systems, 37 : 128911 – 128939, 2024. 2 [ 30 ] grace luo, lisa dunlap, dong huk park, aleksander holynski, and trevor darrell. diffusion hyperfeatures : searching through time and space for semantic correspondence. advances in neural information processing systems, 36 : 47500 – 47510, 2023. 2 [ 31 ] nanye ma, mark goldstein, michael s albergo, nicholas m boffi, eric vanden - eijnden, and saining xie. sit : exploring flow and diffusion - based generative models with scalable interpolant transformers. in european conference on computer vision, pages 23 – 40. springer, 2024. 4, 8, 12 [ 32 ] xin ma, yaohui wang, gengyun jia, xinyuan chen, ziwei liu, yuan - fang li, cunjian chen, and yu qiao. latte : latent diffusion transformer for video generation. arxiv preprint arxiv : 2401. 03048, 2024. 2 [ 33 ] sicheng mo, fangzhou mu, kuan heng lin, yanli liu, bochen guan, yin li, and bolei zhou. freecontrol : training - free spatial control of any text - to - image diffusion model with any condition. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 7465 – 7475, 2024. 2 [ 34 ] sicheng mo, thao nguyen, xun huang, siddharth srinivasan iyer, yijun li, yuchen liu, abhishek tandon, eli shechtman, krishna kumar singh, yong jae lee, et al. xfusion : introducing new modality to frozen large language models. arxiv preprint arxiv : 2504. 20996, 2025.", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 10, "frag_id": 1, "text": "##an, yin li, and bolei zhou. freecontrol : training - free spatial control of any text - to - image diffusion model with any condition. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 7465 – 7475, 2024. 2 [ 34 ] sicheng mo, thao nguyen, xun huang, siddharth srinivasan iyer, yijun li, yuchen liu, abhishek tandon, eli shechtman, krishna kumar singh, yong jae lee, et al. xfusion : introducing new modality to frozen large language models. arxiv preprint arxiv : 2504. 20996, 2025. 2, 3 [ 35 ] thao nguyen, yuheng li, utkarsh ojha, and yong jae lee. visual instruction inversion : image editing via image prompting. advances in neural information processing systems, 36 : 9598 – 9613, 2023. 2 [ 36 ] alexander quinn nichol and prafulla dhariwal. improved denoising diffusion probabilistic models. in international conference on machine learning, pages 8162 – 8171. pmlr, 2021. 1, 4, 5 [ 37 ] openai. sora : creating video from text — openai. com. https : / / openai. com / index / sora /, 2025. [ accessed 30 - 04 - 2025 ]. 2 [ 38 ] maxime oquab, timoth´ee darcet, th´eo moutakanni, huy q. vo, marc szafraniec, vasil khalidov, pierre fernandez, daniel haziza, francisco massa, alaaeldin el - nouby, mahmoud assran, nicolas ballas, wojciech galuba, russ howes, po - yao ( bernie ) huang, shang - wen li, ishan misra, michael g. rabbat, vasu sharma, gabriel synnaeve, huijiao xu, herv´e j´egou, julien mairal, patrick labatut, armand joulin, and piotr bojanowski. dinov2 : learning robust visual features without supervision. arxiv, abs / 2304. 07193, 2023. 4, 6 [ 39 ] ziqi pang, tianyuan zhang", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 10, "frag_id": 2, "text": "pierre fernandez, daniel haziza, francisco massa, alaaeldin el - nouby, mahmoud assran, nicolas ballas, wojciech galuba, russ howes, po - yao ( bernie ) huang, shang - wen li, ishan misra, michael g. rabbat, vasu sharma, gabriel synnaeve, huijiao xu, herv´e j´egou, julien mairal, patrick labatut, armand joulin, and piotr bojanowski. dinov2 : learning robust visual features without supervision. arxiv, abs / 2304. 07193, 2023. 4, 6 [ 39 ] ziqi pang, tianyuan zhang, fujun luan, yunze man, hao tan, kai zhang, william t freeman, and yu - xiong wang. randar : decoder - only autoregressive visual generation in random orders. in proceedings of the computer vision and pattern recognition conference, pages 45 – 55, 2025. 8, 12 [ 40 ] william peebles and saining xie. scalable diffusion models with transformers. in proceedings of the ieee / cvf international conference on computer vision, pages 4195 – 4205, 2023. 3, 4, 5, 8, 12, 21 [ 41 ] alec radford, jong wook kim, chris hallacy, aditya ramesh, gabriel goh, sandhini agarwal, girish sastry, amanda askell, pamela mishkin, jack clark, et al. learning transferable visual models from natural language supervision. arxiv preprint arxiv : 2103. 00020, 2021. 4, 6 [ 42 ] aditya ramesh, prafulla dhariwal, alex nichol, casey chu, and mark chen. hierarchical text - conditional image generation with clip latents. arxiv preprint arxiv : 2204. 06125, 2022. 2 [ 43 ] robin rombach, a. blattmann, dominik lorenz, patrick esser, and bj¨orn ommer. high - resolution image synthesis with latent diffusion models. cvpr, 2022. 2, 8, 12 [ 44 ] robin rombach, andreas blattmann, dominik lorenz, patrick esser, and bj¨orn ommer. high", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 10, "frag_id": 3, "text": ", 2021. 4, 6 [ 42 ] aditya ramesh, prafulla dhariwal, alex nichol, casey chu, and mark chen. hierarchical text - conditional image generation with clip latents. arxiv preprint arxiv : 2204. 06125, 2022. 2 [ 43 ] robin rombach, a. blattmann, dominik lorenz, patrick esser, and bj¨orn ommer. high - resolution image synthesis with latent diffusion models. cvpr, 2022. 2, 8, 12 [ 44 ] robin rombach, andreas blattmann, dominik lorenz, patrick esser, and bj¨orn ommer. high - resolution image synthesis with latent diffusion models. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 10684 – 10695, 2022. 1, 2 [ 45 ] robin rombach, andreas blattmann, dominik lorenz, patrick esser, and bj¨orn ommer. high - resolution image synthesis with latent diffusion models, 2022. 2, 4 [ 46 ] tim salimans, ian goodfellow, wojciech zaremba, vicki cheung, alec radford, and xi chen. improved techniques for training gans. advances in neural information processing systems, 29, 2016. 4, 12 [ 47 ] weijia shi, xiaochuang han, chunting zhou, weixin liang, xi victoria lin, luke zettlemoyer, and lili yu. llamafusion : adapting pretrained language models for multimodal generation. arxiv preprint arxiv : 2412. 15188, 2024. 2, 3 [ 48 ] kihyuk sohn, nataniel ruiz, kimin lee, daniel castro chin, irina blok, huiwen chang, jarred barber, lu jiang, glenn entis, yuanzhen li, et al. styledrop : text - to - image generation in any style. arxiv preprint arxiv : 2306. 00983, 2023. 2 [ 49 ] peize sun, yi jiang, shoufa chen, shilong zhang, bingyue peng, ping luo, and zehuan yuan. autoregressive model beats diffusion : llama for scalable image generation", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 10, "frag_id": 4, "text": "##xiv : 2412. 15188, 2024. 2, 3 [ 48 ] kihyuk sohn, nataniel ruiz, kimin lee, daniel castro chin, irina blok, huiwen chang, jarred barber, lu jiang, glenn entis, yuanzhen li, et al. styledrop : text - to - image generation in any style. arxiv preprint arxiv : 2306. 00983, 2023. 2 [ 49 ] peize sun, yi jiang, shoufa chen, shilong zhang, bingyue peng, ping luo, and zehuan yuan. autoregressive model beats diffusion : llama for scalable image generation. arxiv preprint arxiv : 2406. 06525, 2024. 12 [ 50 ] peize sun, yi jiang, shoufa chen, shilong zhang, bingyue peng, ping luo, and zehuan yuan. autoregressive model beats diffusion : llama for scalable image generation. arxiv preprint arxiv : 2406. 06525, 2024. 8 [ 51 ] raphael tang, linqing liu, akshat pandey, zhiying jiang, gefei yang, karun kumar, pontus stenetorp, jimmy lin, and ferhan ture. what the daam : interpreting stable diffusion using cross attention. arxiv preprint arxiv : 2210. 04885, 2022. 2 [ 52 ] ming tao, hao tang, fei wu, xiao - yuan jing, bing - kun bao, and changsheng xu. df - gan : a simple and effective baseline for text - to - image synthesis. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 16515 – 16525, 2022. 21 10", "token_count": 400}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 11, "frag_id": 0, "text": "[ 53 ] chameleon team. chameleon : mixed - modal early - fusion foundation models. arxiv preprint arxiv : 2405. 09818, 2024. 3 [ 54 ] junjiao tian, lavisha aggarwal, andrea colaco, zsolt kira, and mar gonzalez - franco. diffuse attend and segment : unsupervised zero - shot segmentation using stable diffusion. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 3554 – 3563, 2024. 2 [ 55 ] keyu tian, yi jiang, zehuan yuan, bingyue peng, and liwei wang. visual autoregressive modeling : scalable image generation via next - scale prediction. advances in neural information processing systems, 37 : 84839 – 84865, 2024. 8, 12 [ 56 ] narek tumanyan, michal geyer, shai bagon, and tali dekel. plug - and - play diffusion features for text - driven image - to - image translation. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 1921 – 1930, 2023. 2 [ 57 ] ashish vaswani, noam shazeer, niki parmar, jakob uszkoreit, llion jones, aidan n gomez, łukasz kaiser, and illia polosukhin. attention is all you need. advances in neural information processing systems, 30, 2017. 2 [ 58 ] team wan, ang wang, baole ai, bin wen, chaojie mao, chen - wei xie, di chen, feiwu yu, haiming zhao, jianxiao yang, et al. wan : open and advanced large - scale video generative models. arxiv preprint arxiv : 2503. 20314, 2025. 2, 3 [ 59 ] runqian wang and kaiming he. diffuse and disperse : image generation with representation regularization. arxiv preprint arxiv : 2506. 09027, 2025. 2, 8, 12 [ 60 ] shuai wang, ziteng gao, chenhui zhu, weilin huang, and limin wang. pixnerd : pixel neural field diffusion. arxiv preprint arxiv : 2507. 23268, 2025", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 11, "frag_id": 1, "text": ", haiming zhao, jianxiao yang, et al. wan : open and advanced large - scale video generative models. arxiv preprint arxiv : 2503. 20314, 2025. 2, 3 [ 59 ] runqian wang and kaiming he. diffuse and disperse : image generation with representation regularization. arxiv preprint arxiv : 2506. 09027, 2025. 2, 8, 12 [ 60 ] shuai wang, ziteng gao, chenhui zhu, weilin huang, and limin wang. pixnerd : pixel neural field diffusion. arxiv preprint arxiv : 2507. 23268, 2025. 13 [ 61 ] shuai wang, zhi tian, weilin huang, and limin wang. ddt : decoupled diffusion transformer. arxiv preprint arxiv : 2504. 05741, 2025. 8, 12 [ 62 ] ge wu, shen zhang, ruijing shi, shanghua gao, zhenyuan chen, lei wang, zhaowei chen, hongcheng gao, yao tang, jian yang, et al. representation entanglement for generation : training diffusion transformers is much easier than you think. arxiv preprint arxiv : 2507. 01467, 2025. 2 [ 63 ] jiarui xu, sifei liu, arash vahdat, wonmin byeon, xiaolong wang, and shalini de mello. open - vocabulary panoptic segmentation with text - to - image diffusion models. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 2955 – 2966, 2023. 2 [ 64 ] tao xu, pengchuan zhang, qiuyuan huang, han zhang, zhe gan, xiaolei huang, and xiaodong he. attngan : finegrained text to image generation with attentional generative adversarial networks. in proceedings of the ieee conference on computer vision and pattern recognition, pages 1316 – 1324, 2018. 21 [ 65 ] jiawei yang, tianhong li, lijie fan, yonglong tian, and yue wang. latent denoising makes good visual tokenizers. arxiv preprint arxiv : 2507. 15856, 2025. 6 [", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 11, "frag_id": 2, "text": "and pattern recognition, pages 2955 – 2966, 2023. 2 [ 64 ] tao xu, pengchuan zhang, qiuyuan huang, han zhang, zhe gan, xiaolei huang, and xiaodong he. attngan : finegrained text to image generation with attentional generative adversarial networks. in proceedings of the ieee conference on computer vision and pattern recognition, pages 1316 – 1324, 2018. 21 [ 65 ] jiawei yang, tianhong li, lijie fan, yonglong tian, and yue wang. latent denoising makes good visual tokenizers. arxiv preprint arxiv : 2507. 15856, 2025. 6 [ 66 ] zhuoyi yang, jiayan teng, wendi zheng, ming ding, shiyu huang, jiazheng xu, yuanming yang, wenyi hong, xiaohan zhang, guanyu feng, et al. cogvideox : text - to - video diffusion models with an expert transformer. arxiv preprint arxiv : 2408. 06072, 2024. 2 [ 67 ] sihyun yu, sangkyung kwak, huiwon jang, jongheon jeong, jonathan huang, jinwoo shin, and saining xie. representation alignment for generation : training diffusion transformers is easier than you think. arxiv, abs / 2410. 06940, 2024. 2, 8, 12 [ 68 ] han zhang, jing yu koh, jason baldridge, honglak lee, and yinfei yang. cross - modal contrastive learning for text - toimage generation. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 833 – 842, 2021. 21 [ 69 ] junyi zhang, charles herrmann, junhwa hur, luisa polania cabrera, varun jampani, deqing sun, and ming - hsuan yang. a tale of two features : stable diffusion complements dino for zero - shot semantic correspondence. advances in neural information processing systems, 36 : 45533 – 45547, 2023. 2 [ 70 ] hongkai zheng, weili nie, arash vahdat, and anima anandkumar. fast training of diffusion models with masked transformers. arxiv preprint arxiv : 2306", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 11, "frag_id": 3, "text": "generation. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 833 – 842, 2021. 21 [ 69 ] junyi zhang, charles herrmann, junhwa hur, luisa polania cabrera, varun jampani, deqing sun, and ming - hsuan yang. a tale of two features : stable diffusion complements dino for zero - shot semantic correspondence. advances in neural information processing systems, 36 : 45533 – 45547, 2023. 2 [ 70 ] hongkai zheng, weili nie, arash vahdat, and anima anandkumar. fast training of diffusion models with masked transformers. arxiv preprint arxiv : 2306. 09305, 2023. 2, 8, 12 [ 71 ] zangwei zheng, xiangyu peng, tianji yang, chenhui shen, shenggui li, hongxin liu, yukun zhou, tianyi li, and yang you. open - sora : democratizing efficient video production for all. arxiv preprint arxiv : 2412. 20404, 2024. 2 [ 72 ] chunting zhou, lili yu, arun babu, kushal tirumala, michihiro yasunaga, leonid shamis, jacob kahn, xuezhe ma, luke zettlemoyer, and omer levy. transfusion : predict the next token and diffuse images with one multi - modal model. 2024. 2, 3 [ 73 ] mingyuan zhou, huangjie zheng, zhendong wang, mingzhang yin, and hai huang. score identity distillation : exponentially fast distillation of pretrained diffusion models for one - step generation. in forty - first international conference on machine learning, 2024. 13 [ 74 ] y zhou, r zhang, c chen, c li, c tensmeyer, t yu, j gu, j xu, and t sun. lafite : towards language - free training for text - to - image generation. arxiv. arxiv preprint arxiv : 2111. 13792, 3, 2022. 21 [ 75 ] minfeng zhu, pingbo pan, wei chen, and yi yang. dm - gan : dynamic memory generative adversarial networks for textto - image synthesis. in proceedings of the ieee / cvf", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 11, "frag_id": 4, "text": "##llation of pretrained diffusion models for one - step generation. in forty - first international conference on machine learning, 2024. 13 [ 74 ] y zhou, r zhang, c chen, c li, c tensmeyer, t yu, j gu, j xu, and t sun. lafite : towards language - free training for text - to - image generation. arxiv. arxiv preprint arxiv : 2111. 13792, 3, 2022. 21 [ 75 ] minfeng zhu, pingbo pan, wei chen, and yi yang. dm - gan : dynamic memory generative adversarial networks for textto - image synthesis. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 5802 – 5810, 2019. 21 11", "token_count": 169}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 12, "frag_id": 0, "text": "supplementary a. implementation details 12 a. 1. baselines................... 12 a. 2. evaluation metric.............. 12 a. 3. hyperparameter............... 12 b. experiment 12 b. 1. ablations................... 12 b. 2. extending to pixel diffusion......... 13 b. 3. additional qualitative results........ 13 b. 4. cross - sample score visualization...... 21 b. 5. text - to - image generation.......... 21 a. implementation details a. 1. baselines we introduce the baselines of the leading generative systems as follows : • adm [ 6 ] leverages classifier for guiding diffusion sampling to improve generation. • ldm [ 43 ] presents latent diffusion, enabling fast, highresolution generation by training diffusion models in a latent space. • mdtv2 [ 11 ] combines masked token modeling with diffusion transformers to learn visual representations. • var [ 55 ] introduces next - scale prediction to autoregressive generative models. • llamagen [ 49 ] shows vanilla autoregressive models could achieve strong generation performance at scale, outperforming diffusion baselines. • randar [ 39 ] proposes a decoder - only autoregressive model that utilizes position instruction tokens to generate image tokens in arbitrary orders. • maskdit [ 70 ] uses masked input patches and an asymmetric encoder - decoder to achieve faster diffusion model training. • dit [ 40 ] proposes a scalable transformer architecture based on adain - zero for diffusion model training. • sit [ 31 ] further improves the efficiency and scalability on dit by introducing flow matching. • repa [ 67 ] analyzes the alignment between feature quality and generation fidelity of diffusion backbone and accelerates diffusion model training by aligning diffusion feature with pre - trained vision encoders. • repa - e [ 26 ] enables representation learning inside diffusion backbones by unlocking the latent encoder. • ddt [ 61 ] proposes a diffusion", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 12, "frag_id": 1, "text": "##s in arbitrary orders. • maskdit [ 70 ] uses masked input patches and an asymmetric encoder - decoder to achieve faster diffusion model training. • dit [ 40 ] proposes a scalable transformer architecture based on adain - zero for diffusion model training. • sit [ 31 ] further improves the efficiency and scalability on dit by introducing flow matching. • repa [ 67 ] analyzes the alignment between feature quality and generation fidelity of diffusion backbone and accelerates diffusion model training by aligning diffusion feature with pre - trained vision encoders. • repa - e [ 26 ] enables representation learning inside diffusion backbones by unlocking the latent encoder. • ddt [ 61 ] proposes a diffusion architecture that separates semantic encoding from high - frequency decoding to accelerate convergence during training. • sra [ 20 ] introduces a simple approach to align crosslayer diffusion backbone features to improve training efficiency without a pre - trained vision encoder. • dispersive loss [ 59 ] introduces a simple regularization loss that encourages internal representations to disperse in the hidden space to improve diffusion model training. a. 2. evaluation metric we use the conventional evaluation pipelines for classconditional generative models, following adm [ 6 ]. specifically, we introduce the focusing concept of each metric : • fr´echet inception distance ( fid ) [ 14 ] evaluates the feature distance of generated images and the reference samples. lower fid usually suggests better generation fidelity and diversity. • inception score ( is ) [ 46 ] measures image quality and diversity based on how confidently a classifier recognizes each image and how varied the generated classes are. a higher inception score indicates a more meaningful image within each class. • precision and recall [ 25 ]. precision captures the realism of generated images, while recall captures their diversity relative to real data. a. 3. hyperparameter in table 5, we introduce the hyperparameter setting for models reported at table 3. b. experiment b. 1. ablations groupdiff - f : group size. we additionally investigate into the group size in groupdiff - f setting. figure 9 shows the which images shares the same group during inference. we compare the uncurated samples from groupdifff - { 1, 2, 3, 4 } in figure 10 and figure 11. our observation on groupdiff - f aligns that of groupdiff - l, where increasing the group size", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 12, "frag_id": 2, "text": "• precision and recall [ 25 ]. precision captures the realism of generated images, while recall captures their diversity relative to real data. a. 3. hyperparameter in table 5, we introduce the hyperparameter setting for models reported at table 3. b. experiment b. 1. ablations groupdiff - f : group size. we additionally investigate into the group size in groupdiff - f setting. figure 9 shows the which images shares the same group during inference. we compare the uncurated samples from groupdifff - { 1, 2, 3, 4 } in figure 10 and figure 11. our observation on groupdiff - f aligns that of groupdiff - l, where increasing the group size considerably improves the generation fidelity. groupdiff - f - 1 groupdiff - f - 2 groupdiff - f - 4 groupdiff - f - 8 sample - 1 sample - 2 sample - 6 sample - 8 sample - 3 sample - 4 sample - 5 sample - 7 figure 9. group attention illustration. in each row, samples in the sample group shares the same color block. groupdiff - l * : query method. beyond training from scratch, resuming from individual diffusion offers an efficient solution to adding groupdiff over existing pipelines. thus, we also explore different query methods under this 12", "token_count": 275}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 13, "frag_id": 0, "text": "dit - xl / 2 sit - xl / 2 sit - xl / 2 - repa groupdiff - 4 groupdiff - 4 * groupdiff - 4 groupdiff - 4 * groupdiff - 4 * architecture input dim. 32 × 32× 4 32 × 32 × 4 32 × 32 × 4 32 × 32 × 4 32 × 32 × 4 num. layers 28 28 28 28 28 hidden dim. 1, 152 1, 152 1, 152 1, 152 1, 152 num. heads 16 16 16 16 16 optimization resume dit - xl / 2 - 7m sit - xl / 2 - 7m repa - 4m training iteration 4m 500k 4m 500k 500k batch size 256 256 256 256 256 optimzier adamw adamw adamw adamw adamw lr 0. 0001 0. 0001 0. 0001 0. 0001 0. 0001 betas ( 0. 9, 0. 999 ) ( 0. 9, 0. 999 ) ( 0. 9, 0. 999 ) ( 0. 9, 0. 999 ) ( 0. 9, 0. 999 ) weight decay 0. 01 0. 01 0. 01 0. 01 0. 01 groupdiff mode groupdiff - l groupdiff - l groupdiff - l groupdiff - l groupdiff - l query method clip - l clip - l clip - l clip - l clip - l τimg 0. 7 0. 7 0. 7 0. 7 0. 7 group size 4 4 4 4 4 noise var. 50 50 50 50 0 inference steps 250 250 250 250 250 guidance scale 1. 70 1. 60 2. 35 1. 85 2. 575 guidance interval ( 0, 1 ) ( 0, 1 ) ( 0. 25, 1. 0 ) ( 0. 15, 1. 0 ) ( 0. 25, 0. 75 ) table 5. hyperparameter setup. method query method fid ↓ is ↑ pre. ↑ rec. ↑ sit - xl / 2 2. 06 270. 3 0. 82 0. 59 + groupdiff - 4 * class 1. 76 283. 5 0. 81 0. 61 + groupdiff - 4 * clip - l 1. 40 290. 7 0. 79 0. 64 table 6. ablation : query method. ∗ : continue training from pre - trained checkpoint for an additional 100", "token_count": 500}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 13, "frag_id": 1, "text": "35 1. 85 2. 575 guidance interval ( 0, 1 ) ( 0, 1 ) ( 0. 25, 1. 0 ) ( 0. 15, 1. 0 ) ( 0. 25, 0. 75 ) table 5. hyperparameter setup. method query method fid ↓ is ↑ pre. ↑ rec. ↑ sit - xl / 2 2. 06 270. 3 0. 82 0. 59 + groupdiff - 4 * class 1. 76 283. 5 0. 81 0. 61 + groupdiff - 4 * clip - l 1. 40 290. 7 0. 79 0. 64 table 6. ablation : query method. ∗ : continue training from pre - trained checkpoint for an additional 100 epochs. setting. table 6 shows clip - l yields the optimality performance while the simplest groupdiff - 4∗obtains a considerable improvement ( 14. 5 % ) over the baseline, highlighting the effectiveness of cross - sample attention. b. 2. extending to pixel diffusion. we further validate groupdiff on pixel diffusion systems. as shown in table 7, groupdiff - 4 with jit - b / 16 delivers a substantial 15. 8 % improvement with only 100 additional training steps when resumed from a pre - trained model. this again highlights the effectiveness of cross - sample collaboration in pixel diffusion and its strong potential for broader applicability. method params fid is adm - g [ 6 ] 559m 7. 72 172. 7 rin [ 19 ] 320m 3. 95 216 sid [ 73 ], uvit / 2 2b 2. 44 256. 3 pixelflow [ 3 ], xl / 4 677m 1. 98 282. 1 pixnerd [ 60 ], xl / 16 700m 2. 15 297 jit - h / 16 [ 28 ] 953m 1. 86 303. 4 jit - b / 16 [ 28 ] 131m 3. 66 275. 1 + our groupdiff - 4 * 131m 3. 08 245. 6 table 7. system - level performance of pixel diffusion models evaluated on imagenet 256×256. ∗ : continue training from pretrained checkpoint for an additional 100 epochs. b. 3. additional qualitative results. we provide additional uncurated samples generated by groupdiff - 4 in figures 14 – 26. 13", "token_count": 488}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 14, "frag_id": 0, "text": "groupdiff - f - 1 groupdiff - f - 2 groupdiff - f - 4 groupdiff - f - 8 groupdiff - f - 1 groupdiff - f - 2 groupdiff - f - 4 groupdiff - f - 8 groupdiff - f - 1 groupdiff - f - 2 groupdiff - f - 4 groupdiff - f - 8 figure 10. uncurated generation results of groupdiff - f without classifier - free guidance. examples of class - conditional generation on imagenet 256×256. groupdiff with a larger group size consistently obtains better generation fidelity. 14", "token_count": 135}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 15, "frag_id": 0, "text": "groupdiff - f - 1 groupdiff - f - 2 groupdiff - f - 4 groupdiff - f - 8 groupdiff - f - 1 groupdiff - f - 2 groupdiff - f - 4 groupdiff - f - 8 groupdiff - f - 1 groupdiff - f - 2 groupdiff - f - 4 groupdiff - f - 8 figure 11. uncurated generation results of groupdiff - f without classifier - free guidance. examples of class - conditional generation on imagenet 256×256. groupdiff with a larger group size consistently obtains better generation fidelity. 15", "token_count": 135}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 16, "frag_id": 0, "text": "figure 12. uncurated generation results of groupdiff - 4. we use classifier - free guidance with w = 3. 5. class label = “ loggerhead sea turtle ” ( 33 ). figure 13. uncurated generation results of groupdiff - 4. we use classifier - free guidance with w = 3. 5. class label = “ macaw ” ( 88 ). figure 14. uncurated generation results of groupdiff - 4. we use classifier - free guidance with w = 3. 5. class label = “ sulphur - crested cockatoo, kakatoe galerita, cacatua galerita ” ( 89 ). 16", "token_count": 142}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 17, "frag_id": 0, "text": "figure 15. uncurated generation results of groupdiff - 4. we use classifier - free guidance with w = 3. 5. class label = “ golden retriever ” ( 207 ). figure 16. uncurated generation results of groupdiff - 4. we use classifier - free guidance with w = 3. 5. class label = “ siberian husky ” ( 250 ). figure 17. uncurated generation results of groupdiff - 4. we use classifier - free guidance with w = 3. 5. class label = “ white wolf, arctic wolf, canis lupus tundrarum ” ( 270 ). 17", "token_count": 133}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 18, "frag_id": 0, "text": "figure 18. uncurated generation results of groupdiff - 4. we use classifier - free guidance with w = 3. 5. class label = “ arctic fox, white fox, alopex lagopus ” ( 279 ). figure 19. uncurated generation results of groupdiff - 4. we use classifier - free guidance with w = 3. 5. class label = “ acoustic guitar ” ( 402 ). figure 20. uncurated generation results of groupdiff - 4. we use classifier - free guidance with w = 3. 5. class label = “ balloon ” ( 417 ). 18", "token_count": 130}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 19, "frag_id": 0, "text": "figure 21. uncurated generation results of groupdiff - 4. we use classifier - free guidance with w = 3. 5. class label = “ baseball ” ( 429 ). figure 22. uncurated generation results of groupdiff - 4. we use classifier - free guidance with w = 3. 5. class label = “ fire engine, fire truck ” ( 555 ). figure 23. uncurated generation results of groupdiff - 4. we use classifier - free guidance with w = 3. 5. class label = “ space shuttle ” ( 812 ). 19", "token_count": 125}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 20, "frag_id": 0, "text": "figure 24. uncurated generation results of groupdiff - 4. we use classifier - free guidance with w = 3. 5. class label = “ cheeseburger ” ( 933 ). figure 25. uncurated generation results of groupdiff - 4. we use classifier - free guidance with w = 3. 5. class label = “ coral reef ” ( 973 ). figure 26. uncurated generation results of groupdiff - 4. we use classifier - free guidance with w = 3. 5. class label = “ volcano ” ( 980 ). 20", "token_count": 123}
{"doc_id": "arxiv_251210954_group_diffusion", "page": 21, "frag_id": 0, "text": "method type fid attngan [ 64 ] gan 35. 49 dm - gan [ 75 ] gan 32. 64 vq - diffusion [ 13 ] diffusion 19. 75 df - gan [ 52 ] gan 19. 32 xmc - gan [ 68 ] gan 9. 33 frido [ 10 ] diffusion 8. 97 lafite [ 74 ] gan 8. 12 u - net [ 1 ] diffusion 7. 32 u - vit - s / 2 [ 1 ] diffusion 5. 95 u - vit / s / 2 ( deep ) [ 1 ] diffusion 5. 45 mmdit [ 9 ] diffusion 5. 3 dit - xl / 2 w / cross - attention [ 40 ] diffusion 6. 95 + our groupdiff - 4 diffusion 6. 65 table 8. quantitative comparison on text - to - image generation ( ms - coco ). b. 4. cross - sample score visualization additionally, we show the relation between fid and crosssample score computed by the group - level mean and max of the attention score in figure 27. fid cross - sample score ( % ) figure 27. fid vs cross - sample score ( group - level ) our groupdiff shows a strong correlation ( 0. 94 ) between crossattention to other samples and generation quality. b. 5. text - to - image generation we also validate groupdiff in text - to - image generation. we mostly follow the experimental setup used in u - vit [ 1 ] unless otherwise specified : we train the model from scratch on a train split of the ms - coco dataset and use a validation split for evaluation. we use dit - xl / 2 with cross - attention and train it for 150k iterations with a batch size of 256. we use the frozen clip text encoder to extract text prompts from captions. table 8 shows that groupdiff remains effective in the t2i generation setting without bells and whistles, highlighting the importance of applying cross - sample attention even with text conditions. 21", "token_count": 418}
