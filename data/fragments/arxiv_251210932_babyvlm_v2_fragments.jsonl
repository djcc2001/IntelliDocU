{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 1, "frag_id": 0, "text": "babyvlm - v2 : toward developmentally grounded pretraining and benchmarking of vision foundation models shengao wang * †, wenqi wang∗, zecheng wang∗, max whitton∗ michael wakeham, arjun chandra, joey huang, pengyue zhu helen chen ‡, david li ‡, jeffrey li ‡, shawn l. li ‡, andrew zagula ‡, amy zhao ‡, andrew zhu ‡ sayaka nakamura2, yuki yamamoto2, jerry jun yokono2 aaron mueller, bryan a. plummer, kate saenko, venkatesh saligrama, boqing gong boston university, 2sony group corporation, { wsashawn, wqwang, vicwang0, maxwh, bgong } @ bu. edu https : / / shawnking98. github. io / babyvlm - v2 / abstract early children ’ s developmental trajectories set up a natural goal for sample - efficient pretraining of vision foundation models. we introduce babyvlm - v2, a developmentally grounded framework for infant - inspired vision - language modeling that extensively improves upon babyvlm - v1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, devcv toolbox for cognitive evaluation. the pretraining set maximizes coverage while minimizing curation of a longitudinal, infant - centric audiovisual corpus, yielding video - utterance, image - utterance, and multi - turn conversational data that mirror infant experiences. devcv toolbox adapts all vision - related measures of the recently released nih baby toolbox® into a benchmark suite of ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children ’ s capabilities. experimental results show that a compact model pretrained from scratch can achieve competitive performance on devcv toolbox, outperforming gpt - 4o on some tasks. we hope the principled, unified babyvlm - v2 framework will accelerate research in developmentally plausible pretraining of vision foundation models. 1. introduction we formalize our objective : given a longitudinal, infantcentric audiovisual sample of early children ’ s sensory experiences ( e. g., figure 1a ), can we learn a foundation model * equal contribution. † project lead. ‡ equal contribution ; work done as interns at boston university", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 1, "frag_id": 1, "text": "ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children ’ s capabilities. experimental results show that a compact model pretrained from scratch can achieve competitive performance on devcv toolbox, outperforming gpt - 4o on some tasks. we hope the principled, unified babyvlm - v2 framework will accelerate research in developmentally plausible pretraining of vision foundation models. 1. introduction we formalize our objective : given a longitudinal, infantcentric audiovisual sample of early children ’ s sensory experiences ( e. g., figure 1a ), can we learn a foundation model * equal contribution. † project lead. ‡ equal contribution ; work done as interns at boston university. figure 1. babyvlm - v2 : an extensive, versatile, and developmentally plausible framework for research in vision foundation models. its ( a ) pretraining set is diverse in format ( video, imageutterance, and multiple turns ), enabling ( b ) a flexible model. its ( c ) benchmark developmentally aligns with the pretraining set ’ s age span by grounding on the newly released nih baby toolbox®. ( fm ) that is as versatile and capable as the early children ’ s perception? as a further challenge, can we leverage principles of developmental psychology to create a benchmark as an initial step toward artificial developmental intelligence ( adi ), in both what it is and how to achieve it within the constraints of early children ’ s limited sensory intake? we consider a resultant model and benchmark developmentally plausible if the training data and desired model performance closely mirror those of early children. we envision that our answer to this objective, babyvlm - v2, will have a threefold impact. first, by making the limited training data accessible to independent researchers and friendly to university resources, we will broaden research engagement in developing fms [ 18, 59 ] in a time when the scaling law [ 23 ] causes research on fms 1 arxiv : 2512. 10932v1 [ cs. cv ] 11 dec 2025", "token_count": 431}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 2, "frag_id": 0, "text": "table 1. babyvlm - v2 extensively extends babyvlm - v1 [ 56 ]. babyvlm - v1 babyvlm - v2 ( ours ) pretraining 67k img - utterance 768k img - utterance 181k video - utterance 63k interleaved instruction none 150k examples benchmarks 4 tasks, intuitive 10 tasks, grounded on nih baby toolbox® visual vocabulary, captioning visual vocabulary, counting, memory, attention, spatial reasoning, localization, spatiotemporal reasoning, executive function models input : text, single img input : text, img, multiimg, video, multi - turn output : logits output : language to be dominated by industry. second, we envision that adi could advance studies in cognitive science and psychology by allowing scientists to read into early children ’ s minds in an unprecedented way. lastly, we believe that the broadened engagement in fms will improve public understanding, trust, and safe use of fms and ai in general. previously, wang et al. proposed babyvlm - v1 [ 56 ], a scaffold for studying adi from the lens of vision - language models ( vlms ). it consists of 1 ) an image - text pretraining set extracted from saycam ’ s head - mounted camera recordings from three children for approximately two hours per week from age 6 to 32 months [ 50 ], 2 ) four intuitive and developmentally inspired benchmark tasks, and 3 ) a public codebase for pretraining and evaluation. babyvlmv1 pretrained a baseline vlm from scratch, whose performance, unfortunately, fell far behind the remarkable capabilities of early children [ 7, 34 ]. similarly, vong et al. [ 54 ] trained a clip - style [ 44 ] contrastive model using saycam, but with a narrower focus on word - referent mappings rather than general perception. more related work is in section 2. while babyvlm - v1 sets up a basic framework, it lacks crucial elements. its pretraining set only leverages about a third of saycam ’ s recordings, causing it to cover only a tiny portion of the total visual intake time of a three - year - old since birth [ 38 ]. it does not support instruction tuning [ 64 ], which is crucial for a pretrained model to articulate its capabilities to user instructions. importantly,", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 2, "frag_id": 1, "text": "children [ 7, 34 ]. similarly, vong et al. [ 54 ] trained a clip - style [ 44 ] contrastive model using saycam, but with a narrower focus on word - referent mappings rather than general perception. more related work is in section 2. while babyvlm - v1 sets up a basic framework, it lacks crucial elements. its pretraining set only leverages about a third of saycam ’ s recordings, causing it to cover only a tiny portion of the total visual intake time of a three - year - old since birth [ 38 ]. it does not support instruction tuning [ 64 ], which is crucial for a pretrained model to articulate its capabilities to user instructions. importantly, its evaluation benchmarks are not based on any established psychology tests. finally, the models trained in babyvlm - v1 have near - zero open - set performance, and one has to postprocess their logits for evaluation. this work extends babyvlm - v1 to a comprehensive, extensive, and developmentally plausible framework, babyvlm - v2 ( see figure 1 ), for studying the objective posed at the beginning of the paper. table 1 contrasts the two frameworks in pretraining, instruction tuning, benchmarks, and baseline models. notably, we provide devcv toolbox ( see figure 3 ), a benchmark of ten tasks designed using the nih baby toolbox® [ 11, 16 ], which was publicly released in february 2025 as a “ universal assessment for developmental and pediatric communities ”. we make minimal changes while adapting all of its vision - related measures to devcv toolbox in order to maintain developmental fidelity. interestingly, the devcv toolbox tasks are naturally diverse in format, desiring fms to understand individual videos and images, reason across multiple images, and solve a task in multiple turns. to account for these requirements in the pretraining data, we compile video, imageutterance, and multi - turn data from the longitudinal, infantcentric videos in saycam [ 50 ]. as in babyvlm - v1, we include a minimal curation process to bring our pretraining data as close to the children ’ s sensory intake as possible. we validate babyvlm - v2 through extensive experiments and human performance surveys. a model trained from scratch within our babyvlm - v2 framework outperforms gpt - 4", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 2, "frag_id": 2, "text": "the devcv toolbox tasks are naturally diverse in format, desiring fms to understand individual videos and images, reason across multiple images, and solve a task in multiple turns. to account for these requirements in the pretraining data, we compile video, imageutterance, and multi - turn data from the longitudinal, infantcentric videos in saycam [ 50 ]. as in babyvlm - v1, we include a minimal curation process to bring our pretraining data as close to the children ’ s sensory intake as possible. we validate babyvlm - v2 through extensive experiments and human performance surveys. a model trained from scratch within our babyvlm - v2 framework outperforms gpt - 4o in math tasks, highlighting the potential of developmentally grounded pretraining. 2. related work vision fms refer to general - purpose models [ 3 ] often pretrained on massive visual data [ 4, 37, 46, 58 ]. they can tackle many vision tasks via a unified interface, such as clip [ 44 ], align [ 20 ], blip [ 26, 27 ], sams [ 24, 45 ], and vision llms [ 1, 6, 28, 40 ]. the development of these powerful models hinges critically on pretraining [ 3, 5, 10 ], a process that trains a model on a large, generic dataset before tuning it to any downstream tasks. sample - efficient pretraining. while fms have been relying on the scaling law, sample - efficient pretraining has gained momentum recently in the language [ 59 ] and medical [ 51 ] domains. to the best of our knowledge, babyvlmv1 was the first of this kind in vision, and we further their effort with a more comprehensive and extensive framework. cognitively plausible benchmarking. babyvlm - v1 [ 56 ] designs four developmentally plausible tasks, which unfortunately lack grounding on established psychological tests. devbench [ 52 ] and kiva [ 62 ] draw inspiration from kidoriented tests, yet they are more age - advanced than our pretraining data. other cognitively plausible benchmarks have a narrower focus, such as zorro [ 19 ], lrs [ 25 ], inflevel [ 60 ], corecognition [ 29 ], and mewl [ 21 ], and modelvsbaby [ 48 ]. table 2 summarizes the differences.", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 2, "frag_id": 3, "text": "of this kind in vision, and we further their effort with a more comprehensive and extensive framework. cognitively plausible benchmarking. babyvlm - v1 [ 56 ] designs four developmentally plausible tasks, which unfortunately lack grounding on established psychological tests. devbench [ 52 ] and kiva [ 62 ] draw inspiration from kidoriented tests, yet they are more age - advanced than our pretraining data. other cognitively plausible benchmarks have a narrower focus, such as zorro [ 19 ], lrs [ 25 ], inflevel [ 60 ], corecognition [ 29 ], and mewl [ 21 ], and modelvsbaby [ 48 ]. table 2 summarizes the differences. tools assessing neurodevelopment in children. our benchmark tasks are grounded on the nih baby toolbox® [ 11 ], a standardized tool released in february 2025 for assessing neurodevelopment in children. it is not 2", "token_count": 200}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 3, "frag_id": 0, "text": "table 2. comparison of existing developmentally inspired benchmarks. benchmark developmental task diversity multimodal train val test in - domain ood human data model devbench [ 52 ] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] labeled - s [ 54 ] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] modelvsbaby [ 48 ] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] mewl [ 21 ] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] zorro [ 19 ] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] inflevel [ 60 ] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] lrs [ 25 ] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] corecognition [ 29 ] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] babyvlm [ 56 ] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] devcv toolbox ( ours ) [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] only more recent but also more comprehensive and normed than alternatives, such as the bayley scales of infant and toddler development [ 2 ], mullen scales of early learning [ 9 ], and battelle developmental inventory [ 39 ]. besides, its design for clinical use validates its credibility over the psychological tests used in research settings. 3. babyvlm - v2 3. 1. data source & the pretraining set we describe saycam, the developmental data source, followed by our minimal process to curate the pretraining set. saycam : the developmental plausibility of our work hinges on the use of a visual - audio - text corpus that faithfully samples what early children have seen and heard by a certain age, which requires the corpus to be 1 ) longitudinal and 2 ) infant - centric. to accomplish this, we use the saycam dataset [ 50 ], which is accessible to all nonprofit institutes, and will include babyview [ 32 ] in future work. saycam contains egocentric recordings from three infants ( left of figure 1a ) taken once every week from roughly 6 to 32 months old. each recording is approximately two hours, and the recordings total 478 hours ( see bottom of figure 2 for the recorded time vs. wake and sleep time [ 38 ] ). notably, the utterances found in saycam are mostly from caregivers providing simple verbal instructions and descriptions to the infants ( top of figure 2 ). babyview [ 32 ] is an ongoing effort in the same", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 3, "frag_id": 1, "text": ") longitudinal and 2 ) infant - centric. to accomplish this, we use the saycam dataset [ 50 ], which is accessible to all nonprofit institutes, and will include babyview [ 32 ] in future work. saycam contains egocentric recordings from three infants ( left of figure 1a ) taken once every week from roughly 6 to 32 months old. each recording is approximately two hours, and the recordings total 478 hours ( see bottom of figure 2 for the recorded time vs. wake and sleep time [ 38 ] ). notably, the utterances found in saycam are mostly from caregivers providing simple verbal instructions and descriptions to the infants ( top of figure 2 ). babyview [ 32 ] is an ongoing effort in the same spirit as saycam, but at a larger scale and with extra gyroscope / accelerometer sensors. data split & the pretraining set. to maximize our use of the saycam corpus, we designate all video clips containing speech to the pretraining split, and evenly divide the remaining clips into validation and test splits. their relative sizes are approximately 3 : 1 : 1, respectively. we then apply minimal processing to facilitate model pretraining while observing the children ’ s sensory intake as much as possible. specifically, we transcribe all utterances, which are almost all from caregivers, using azure speech recognition [ 36 ]. we then construct three types of pretraining data. • video – utterance pairs. we segment the camera recordings into short clips based on transcript boundaries, with each clip corresponding to exactly one utterance. we then drop the video clips shorter than 0. 5 seconds or with a transcript confidence score below 0. 3. further, we compute video - utterance similarities using x - clip [ 33 ] and only retain the video - utterance pairs with similarities greater than 0. 1. this process leaves approximately 181k video clips in our pretraining set, a total of 138 ( out of 478 ) hours. we pad 1 second to either side of the clips. • image – utterance pairs. following babyvlm - v1, we sample at 1 fps from the video - utterance pairs and compute the clip similarity [ 44 ] between each frame and its utterance. only frames with clip similarities > 0. 2 are retained, resulting in 768k image - utterance pairs in total. • interleaved text and images. we create sequences of inter", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 3, "frag_id": 2, "text": "compute video - utterance similarities using x - clip [ 33 ] and only retain the video - utterance pairs with similarities greater than 0. 1. this process leaves approximately 181k video clips in our pretraining set, a total of 138 ( out of 478 ) hours. we pad 1 second to either side of the clips. • image – utterance pairs. following babyvlm - v1, we sample at 1 fps from the video - utterance pairs and compute the clip similarity [ 44 ] between each frame and its utterance. only frames with clip similarities > 0. 2 are retained, resulting in 768k image - utterance pairs in total. • interleaved text and images. we create sequences of interleaved images and utterance from consecutive video segments, aiming to enable downstream capabilities that involve conversations. for each video segment, we pair the frame with the highest clip similarity with its associated utterance and use a sliding window over the resulting image - utterance pairs to construct the interleaved sequences. we randomly choose a window size between 4 and 8 and employ a stride of half the window size, resulting in 63k interleaved sequences. unlike babyvlm - v1 ’ s image - utterance pairs, the mixing of three pretraining data formats prepares models for diverse downstream tasks, which can involve videos, multiple or single images, and even multi - turn conversations. 3. 2. pretraining & fine - tuning babyllava - v2 using our pretraining split, we pretrain babyllava - v2, which uses a language model ( llama - 1. 1b [ 53, 63 ] ) as a versatile interface to probe various capabilities of a visual encoder ( vit - l - 16 [ 8 ], 300m parameters ). a lightweight mlp connector [ 30 ] projects visual features into the language space. this model architecture ( figure 1b ) is the same babyvlm - v1 ’ s babyllava - llama. we pretrain the entire model from scratch using the three - stage pipeline described in appendix a. finally, we fine - tune the model using a small, curated instruction set consisting of the tasks as in devcv toolbox, which we describe next. 3", "token_count": 462}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 4, "frag_id": 0, "text": "time : 15 : 22 utterance : it ’ s really good with yogurt. time : 17 : 03 utterance : want to try a little piece of this? time : 18 : 46 utterance : how about another blueberry? time : 18 : 59 utterance : oh that ’ s a very sweet one. time : 00 : 40 utterance : you won ’ t? time : 00 : 45 utterance : which one are you choosing? time : 00 : 48 utterance : hard, huh? time : 00 : 50 utterance : don ’ t use your hand. 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 month sleep wake recorded figure 2. top : video frames and utterances recorded from the infants ’ view. bottom : recorded wake time vs. wake / sleep time for the ages of 6 months to 32 months in saycam [ 50 ]. 3. 3. age - appropriate benchmarking our objective with babyvlm - v2 is to design benchmark tasks that test age - appropriate visual skills given our pretraining data ’ s age span. however, we acknowledge that developmental benchmarking is an ongoing and rapidly evolving field of research. early children ’ s growth rates vary significantly, and among psychologists and cognitive scientists, substantial conceptual and methodological disagreements exist regarding the notion of developmental intelligence and how to properly probe, measure, and benchmark it [ 16 ]. how can we properly define adi, then, given the inconsistent measurement techniques in human developmental research? to answer this, we consult with two experienced psychologists specializing in development and learning. numerous meetings with them led us to the timely nih baby toolbox®, over which we ground the design of our benchmark, devcv toolbox. 3. 3. 1. background : nih baby toolbox® in february 2025, a multi - institutional team solicited by the nih publicly released the nih baby toolbox®, envisioning it as a standardized evaluation of neurodevelopmental intelligence in infants [ 15 ]. the nih baby toolbox® divides developmental function into three domains : cognition, motor, and social - emotional, where the cognition domain includes the subdomains of language, executive function / memory, and math, each consisting of some number of specific tests, known in the toolbox as measures. see table 3 for a summary", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 4, "frag_id": 1, "text": "toolbox®, over which we ground the design of our benchmark, devcv toolbox. 3. 3. 1. background : nih baby toolbox® in february 2025, a multi - institutional team solicited by the nih publicly released the nih baby toolbox®, envisioning it as a standardized evaluation of neurodevelopmental intelligence in infants [ 15 ]. the nih baby toolbox® divides developmental function into three domains : cognition, motor, and social - emotional, where the cognition domain includes the subdomains of language, executive function / memory, and math, each consisting of some number of specific tests, known in the toolbox as measures. see table 3 for a summary of these measures and appendix b for technical details. 3. 3. 2. devcv toolbox in this section, we develop a computer vision counterpart, called task for clarity, for every vision - related measure in the nih baby toolbox®, leading to ten tasks in our devcv toolbox, which are summarized in table 3 and illustrated in figure 3. the need to adapt measures to tasks. unlike the practice in computer vision, most of the measures originally found in the nih baby toolbox® 1 ) have only a couple of test examples and 2 ) are human - oriented but not accessible to ai models. additionally, the cartoon stimuli in nih baby toolbox® are out - of - domain from our pretraining set, preventing their direct use. hence, we adapt the measures to computer vision tasks by standardizing their format and equipping each task with thousands of naturalistic examples ( see table 3 ), separated into instruction and test sets according to the split defined in the pretraining stage. we construct the tasks using saycam to ensure that the examples are in the same domain as the pretraining data, thereby focusing the benchmarking on the models ’ in - domain cognitive capabilities. to provide an additional tool to evaluate models ’ generalizability, we also compile an out - of - domain test set using ego4d [ 14 ] with the same techniques. below, we detail the construction of picture vocabulary as a representative example, and briefly describe the rest. see appendix b for more details on the construction of devcv toolbox. picture vocabulary ( ≥25 months ) : the top right of figure 3 shows the original picture vocabulary ( pv ) measure found in the nih", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 4, "frag_id": 2, "text": "split defined in the pretraining stage. we construct the tasks using saycam to ensure that the examples are in the same domain as the pretraining data, thereby focusing the benchmarking on the models ’ in - domain cognitive capabilities. to provide an additional tool to evaluate models ’ generalizability, we also compile an out - of - domain test set using ego4d [ 14 ] with the same techniques. below, we detail the construction of picture vocabulary as a representative example, and briefly describe the rest. see appendix b for more details on the construction of devcv toolbox. picture vocabulary ( ≥25 months ) : the top right of figure 3 shows the original picture vocabulary ( pv ) measure found in the nih baby toolbox®, which assesses the receptive language of children aged 25 months and older. participants are presented with four clipart images on an ipad, and an audio prompt instructs them to touch the named image. we adapt pv to devcv toolbox using the pipeline in figure 4, to replace the clipart in the nih baby toolbox® manual with objects and actions detected from saycam video frames. concretely, we sample frames at 1fps, label all objects and actions present using manual transcripts and gpt - 4o, and then crop out regions for each label using grounding - dino [ 31 ]. low quality crops and labels beyond the child - oriented mab - cdi vocabulary [ 35 ] are removed. each pv example ( e. g., the top left of figure 3 ) consists of a language prompt, a target image corresponding 4", "token_count": 330}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 5, "frag_id": 0, "text": "figure 3. devcv toolbox tasks and their corresponding nih baby toolbox® measures to the prompt, and three distractor images, and we construct the examples in a round - robin manner for diversity. the target and distractor images are related either semantically or phonologically in nih baby toolbox® ; therefore, we derive a distractor distribution over phonology and semantics from the toolbox and then sample distractor images accordingly. we manually screen the process to ensure quality and diversity. appendix b presents more details. other tasks. we describe the other tasks in devcv toolbox briefly. construction details are in appendix b. 1. looking while listening ( 6 – 24 months ) shows infants two clipart objects, and plays an audio prompt describing one of them. eye tracking is used to detect the participant ’ s response. we replace clipart with natural objects from saycam, and eye tracking with multiple choice. 2. localization / mullen visual receptive language # 19 5", "token_count": 205}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 6, "frag_id": 0, "text": "table 3. devcv toolbox tasks and their corresponding nih baby toolbox® measures ( ef / m stands for executive function / memory ). devcv toolbox tasks # instruct / # test model input nih baby toolbox® measures months subdomain looking while listening 0 / 1. 2k 2 images looking while listening 6 - 24 language picture vocabulary 63. 9k / 1. 2k 4 images picture vocabulary 25 + language localization 12. 3k / 2. 1k 1 image mullen receptive language # 19 1 - 42 language left / right 12. 3k / 2. 3k 4 images mullen visual reception # 29 1 - 42 ef / m spatial details 11. 8k / 1. 2k 4 image mullen visual reception # 20 1 - 42 ef / m visual delayed response 5. 2k / 0. 9k 5 - 8 images visual delayed response 22 - 42 ef / m memory 10. 0k / 0. 5k 29 images delayed memory 22 - 42 ef / m who has more ( synthetic ) 11. 2k / 1. 8k 2 images who has more 25 - 42 math who has more ( naturalistic ) 6. 9k / 2. 2k 2 images who has more 25 - 42 math subitizing ( synthetic ) 0 / 1. 9k 3 images subitizing 25 - 42 math subitizing ( naturalistic ) 0 / 0. 2k 3 images subitizing 25 - 42 math object counting 13. 7k / 3. 0k 1 image object counting 25 - 42 math figure 4. pipeline to adapt the picture vocabulary measure in nih baby toolbox® to devcv toolbox. ( 1 – 42 months ) tests an infant ’ s ability to point at sketched objects as they are named. we task a model with localizing an object in a natural video frame. 3. left / right / mullen visual reception # 29 ( 1 – 42 months ) measures an infant ’ s attention to detail by instructing them to match objects by orientation. 4. spatial details / mullen visual reception # 29 ( 1 – 42 months ) measures attention to detail in identical objects among distractors of the same type. 5. visual delayed response ( 22 – 42 months ) shows infants a creature moving behind one of two occluders, and after a short pause, instructs them to tap the target occlu", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 6, "frag_id": 1, "text": "##box. ( 1 – 42 months ) tests an infant ’ s ability to point at sketched objects as they are named. we task a model with localizing an object in a natural video frame. 3. left / right / mullen visual reception # 29 ( 1 – 42 months ) measures an infant ’ s attention to detail by instructing them to match objects by orientation. 4. spatial details / mullen visual reception # 29 ( 1 – 42 months ) measures attention to detail in identical objects among distractors of the same type. 5. visual delayed response ( 22 – 42 months ) shows infants a creature moving behind one of two occluders, and after a short pause, instructs them to tap the target occluder. we use video clips with prominent objects moving out of the field of view. 6. ( delayed ) memory ( 22 – 42 months ) involves multiple turns, each presenting a pair of animals. participants are asked to “ feed ” the new animal appearing for the first time, and they receive corrective feedback during the early rounds. 7. who has more ( 25 – 42 months ) shows two images with the same shape in different quantities and asks which image has more. we replace the shape with natural objects as one sub - task, and use entire natural video frames for the other sub - task. 8. subitizing ( 25 – 42 months ) refers to the rapid identification of the number of items in a small set. an infant sees one to four identical shapes for one second, and then an audio prompt requests the count. 9. object counting ( 25 – 42 months ) evaluates a child ’ s ability to count up to 12 colored shapes on a screen. during evaluation, we employ accuracy as the metric. these tasks cover all cognitive measures in nih baby toolbox® except the non - visual macarthur - bates language ( 9 – 30 months, 7 – 18 months ), familiarization ( 6 – 21 months ), verbal counting ( 25 – 42 months ), and verbal arithmetic ( 37 – 42 months ). adult performance data on these tasks confirms the validity of our devcv toolbox ( see human performance in tables 4 and appendix c for details ). in future work, we hope to complete a survey of children ’ s performance. 4. experiments we design experiments about the key elements of babyvlm - v2 framework, aiming to validate the quality of the devcv toolbox, as well as", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 6, "frag_id": 2, "text": "employ accuracy as the metric. these tasks cover all cognitive measures in nih baby toolbox® except the non - visual macarthur - bates language ( 9 – 30 months, 7 – 18 months ), familiarization ( 6 – 21 months ), verbal counting ( 25 – 42 months ), and verbal arithmetic ( 37 – 42 months ). adult performance data on these tasks confirms the validity of our devcv toolbox ( see human performance in tables 4 and appendix c for details ). in future work, we hope to complete a survey of children ’ s performance. 4. experiments we design experiments about the key elements of babyvlm - v2 framework, aiming to validate the quality of the devcv toolbox, as well as illustrate the effectiveness of our training data and training recipe. meanwhile, the exper6", "token_count": 167}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 7, "frag_id": 0, "text": "figure 5. task - specific supervised finetuning of llavaonevision - 7b and qwen2. 5 - vl - 7b. iments position our babyllava - v2 in context across three cognitive subdomains and ten tasks. note that we exclude two tasks, subitizing and looking while listening, from the majority of the experiments to test our models ’ generalization on unseen tasks near the end. implementation details are in appendix d. 4. 1. examining devcv toolbox overall quality. we validate the quality of devcv toolbox by conducting human surveys, detailed in appendix c. as shown in table 4, the human volunteers recruited in our home institute achieved near - perfect accuracy on the the executive functioning / memory subdomain ( spatial details, memory, visual delayed response ) and the math tasks of object counting and who has more. their accuracy on localization is slightly low ( 87. 3 % ), and a follow - up revealed that it could improve when the volunteers were instructed to spend more time on the task. differentiating capability. table 4 also demonstrates that, between human performance and random guess, there is a sufficiently big room for differentiating various models. indeed, the proprietary gpt and gemini models are on the upper end, while our babyllava - v2 and the open - source models of about the same size as ours are on the lower end, indicating that the tasks in devcv toolbox are challenging but solvable. developmental fidelity. devcv toolbox should developmentally align with the pretraining data ’ s age span ( 6 – 32 months ). hence, we are in the process of performing a large - scale children survey about devcv toolbox using the children helping science platform [ 49 ], though this survey will take a couple of years per our estimation. 4. 2. validating the instruction tuning dataset instruction tuning addresses the mismatch between pretraining and downstream tasks, steering models towards the downstream. to validate the effectiveness of our instruction tuning data, we use it to supervise the fine - tuning of three models under two strategies. figure 5 fine - tunes llavaonevision - 7b and qwen2. 5 - vl - 7b on each task separately ( see appendix a for the experiment setup ). the consistent and relatively big gains from the fine - tuning are highlighted in the red top bars, signifying that the instruction data", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 7, "frag_id": 1, "text": "devcv toolbox using the children helping science platform [ 49 ], though this survey will take a couple of years per our estimation. 4. 2. validating the instruction tuning dataset instruction tuning addresses the mismatch between pretraining and downstream tasks, steering models towards the downstream. to validate the effectiveness of our instruction tuning data, we use it to supervise the fine - tuning of three models under two strategies. figure 5 fine - tunes llavaonevision - 7b and qwen2. 5 - vl - 7b on each task separately ( see appendix a for the experiment setup ). the consistent and relatively big gains from the fine - tuning are highlighted in the red top bars, signifying that the instruction data can effectively guide the models to the downstream tasks in devcv toolbox. furthermore, we experiment with the second fine - tuning strategy that combines the instruction data into a single set. table 5 contrasts it against the first strategy, fine - tuning a model for each task separately, over our babyllava - v2. the results show that the overall difference between the two strategies is marginal. the results on most tasks decrease under the mixed - tuning setting, which produces a single unified model rather than multiple per - task models, but some tasks, such as memory and spatial details, can actually benefit from the mixed fine - tuning, implying knowledge transfer or regularization from other tasks. 4. 3. ablating the pretraining data the speech transcripts in our pretraining set could be noisy because the naturalistic child - directed utterances are often misaligned with the children ’ s visual intake. we study their impact on the pretrained models by replacing the transcripts with video captions generated by gpt - 4o ( see appendix d for how we prompt gpt - 4o ). we train babyllava - v2synthetic on this altered pretraining dataset and present the results in table 6. overall, the synthetic captions improve performance, especially on tasks that demand semantic reasoning ( picture vocabulary ) and a long attention window ( memory ). however, the gains are modest, suggesting that our minimally curated pretraining set already provides strong supervision. in future work, novel pretraining algorithms can likely mine stronger supervision from this organic pretraining set. 4. 4. inspecting babyllava - v2 our babyllava - v2 ’ s overall performance in table 4 is", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 7, "frag_id": 2, "text": "captions generated by gpt - 4o ( see appendix d for how we prompt gpt - 4o ). we train babyllava - v2synthetic on this altered pretraining dataset and present the results in table 6. overall, the synthetic captions improve performance, especially on tasks that demand semantic reasoning ( picture vocabulary ) and a long attention window ( memory ). however, the gains are modest, suggesting that our minimally curated pretraining set already provides strong supervision. in future work, novel pretraining algorithms can likely mine stronger supervision from this organic pretraining set. 4. 4. inspecting babyllava - v2 our babyllava - v2 ’ s overall performance in table 4 is encouraging, on par with the open - source models whose size is about the same as ours. of course, one could argue that those models are not fine - tuned under the babyvlm - v2 framework, but they are probably trained on much larger datasets than ours. to further stretch babyllava - v2, we study its generalization along two axes : 1 ) out - of - domain generalization and 2 ) performance over previously unseen tasks. out - of - domain generalization. we have created a sibling of devcv toolbox by replacing saycam with ego4d. both are about egocentric videos, but ego4d is from the perspective of grown - ups. babyllava - v2 ’ s overall accuracy on this sibling benchmark is 41. 1 % ( vs. 31. 8 % of random guess ), significantly lower than its in - domain performance ( 55. 2 % ) on devcv toolbox. we conclude that babyllava - v2 can generalize beyond its training domain to some degree, but it is far from human infants ’ remarkable generalization capabilities. appendix d further 7", "token_count": 382}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 8, "frag_id": 0, "text": "table 4. performance comparison of different models on devcv toolbox ( in - domain ). different background colors denote different model families. we report accuracy ( % ) for all tasks ; the higher, the better. model overall count leftright spatial pv memory localization visual delay response who has more binary multi - exact multi - adjacent synthetic naturalistic upper bound human performance 93. 0 99. 1 94. 5 100 91. 8 97. 9 87. 3 98. 2 63. 6 95. 5 98. 2 96. 4 proprietary models gemini - 2. 5 - flash 72. 7 71. 1 34. 9 73. 8 91. 2 96. 9 84. 8 75. 9 42. 4 70. 3 87. 5 70. 7 gpt - 4o 74. 6 39. 0 89. 8 92. 6 93. 7 99. 7 81. 7 64. 2 29. 3 62. 9 87. 9 79. 3 gemini - 2. 5 - pro 82. 5 77. 2 68. 8 90. 5 93. 8 97. 8 88. 8 86. 9 54. 0 87. 7 90. 6 71. 7 gpt - 5 87. 6 69. 1 96. 0 94. 5 95. 0 99. 9 85. 2 95. 1 62. 9 90. 1 88. 9 86. 6 open - source models llava - onevision - 0. 5b 33. 2 43. 5 33. 7 28. 7 23. 5 24. 0 12. 3 58. 9 7. 31 49. 2 37. 3 46. 2 internvl3. 5 - 1b 37. 2 27. 9 32. 2 34. 6 34. 4 25. 8 44. 8 64. 1 11. 6 36. 8 47. 8 49. 1 qwen2. 5 - vl - 3b 47. 0 29. 2 33. 7 40. 0 71. 7 36. 5 85. 8 66. 7 17. 0 32. 7 51. 7 52. 3 baby models ( ours ) babyllava - v2 55. 2 44. 6 42. 3 91. 3 27. 4 75. 3 38. 8 57. 6 33. 1 45. 6 98. 4 52. 8 lower bound random guess 31. 8 8. 33 33. 3 33. 3 25. 0 25. 0 25. 0 50. 0 12. 5 37. 5 50", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 8, "frag_id": 1, "text": "25. 8 44. 8 64. 1 11. 6 36. 8 47. 8 49. 1 qwen2. 5 - vl - 3b 47. 0 29. 2 33. 7 40. 0 71. 7 36. 5 85. 8 66. 7 17. 0 32. 7 51. 7 52. 3 baby models ( ours ) babyllava - v2 55. 2 44. 6 42. 3 91. 3 27. 4 75. 3 38. 8 57. 6 33. 1 45. 6 98. 4 52. 8 lower bound random guess 31. 8 8. 33 33. 3 33. 3 25. 0 25. 0 25. 0 50. 0 12. 5 37. 5 50. 0 50. 0 table 5. two supervised fine - tuning strategies. babyllava - v2 - separate denotes models fine - tuned on each task ’ s instruction dataset separately, and babyllava - v2 - mixed is a single model fine - tuned on the mixed instruction set. model overall count leftright spatial pv memory localization visual delay response who has more binary multi - exact multi - adjacent synthetic naturalistic babyllava - v2 - separate 56. 0 45. 2 42. 5 87. 1 28. 4 70. 7 43. 3 55. 7 37. 0 49. 9 98. 6 56. 4 babyllava - v2 - mixed 55. 2 44. 6 42. 3 91. 3 27. 4 75. 3 38. 8 57. 6 33. 1 45. 6 98. 4 52. 8 figure 6. gpt - 4o and our model ’ s counting performance by different object numbers. tests babyllava - v2 ’ s out - of - domain generalization on the original nih baby toolbox®. unseen tasks. we have excluded looking while listening and subitizing from the instruction tuning, which are thus unseen by babyllava - v2. while the two tasks are in spirit similar to picture vocabulary and object counting, respectively, babyllava - v2 yields near - random - guess results on them. we will address this issue in future work by improving the instruction tuning algorithm. 4. 5. intriguing findings finally, we draw some intriguing “ byproduct ” findings from table 4, which can improve our understanding of the proprietary gpt and gemini models. gpt models struggle to count. object counting requires a model to count objects", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 8, "frag_id": 2, "text": "##2 ’ s out - of - domain generalization on the original nih baby toolbox®. unseen tasks. we have excluded looking while listening and subitizing from the instruction tuning, which are thus unseen by babyllava - v2. while the two tasks are in spirit similar to picture vocabulary and object counting, respectively, babyllava - v2 yields near - random - guess results on them. we will address this issue in future work by improving the instruction tuning algorithm. 4. 5. intriguing findings finally, we draw some intriguing “ byproduct ” findings from table 4, which can improve our understanding of the proprietary gpt and gemini models. gpt models struggle to count. object counting requires a model to count objects in an image ( between 1 and 12 ), and gpt - 4o can hardly count beyond 5 ( see figure 6 ). babyllava - v2 can match or outperform gpt - 4o on some cognitive tasks. on spatial details and who has more, babyllava - v2 is on par with the four latest gpt and gemini models. moreover, it even outperforms gpt4o on the math tasks of object counting and who has more. figure 6 shows that babyllava - v2 counts better than gpt - 4o given six or more objects. gpt vs. gemini. in general, the proprietary models give rise to similar results on devcv toolbox. however, when we zoom into the individual tasks, gpt - 5 is significantly better than the rest on spatial details, while gemini models are better at object counting than the gpt models. 5. conclusion we introduced babyvlm - v2, a framework that features a developmentally plausible pretraining set derived from the longitudinal saycam corpus, a compact vlm ( babyllava - v2 ) trained from scratch, and comprehensive developmental benchmarks ( devcv toolbox ). devcv toolbox adapts all vision - related measures from the newly published nih baby toolbox®. it contains ten measures spanning three subdomains ( language, executive function / memory, and math ) and requires a flexible model interface that can process image, video, and multi - turn dialogue. we demonstrate the potential of developmentally plausible vision fms through extensive experiments on our pretraining and instruction tuning datasets, and we confirm the quality of devcv toolbox through extensive benchmarking", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 8, "frag_id": 3, "text": "features a developmentally plausible pretraining set derived from the longitudinal saycam corpus, a compact vlm ( babyllava - v2 ) trained from scratch, and comprehensive developmental benchmarks ( devcv toolbox ). devcv toolbox adapts all vision - related measures from the newly published nih baby toolbox®. it contains ten measures spanning three subdomains ( language, executive function / memory, and math ) and requires a flexible model interface that can process image, video, and multi - turn dialogue. we demonstrate the potential of developmentally plausible vision fms through extensive experiments on our pretraining and instruction tuning datasets, and we confirm the quality of devcv toolbox through extensive benchmarking with proprietary and open - source models. this framework will serve as a principled platform to broaden research 8", "token_count": 172}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 9, "frag_id": 0, "text": "table 6. two language sources for pretraining. babyllava - v2 - original is pretrained on our pretraining set whose language is mainly caregivers ’ speech transcripts, while babyllava - v2 - synthetic is pretrained on synthetic utterances generated by gpt - 4o. model overall count leftright spatial pv memory localization visual delay response who has more binary multi - exact multi - adjacent synthetic naturalistic babyllava - v2 - original 55. 2 44. 6 42. 3 91. 3 27. 4 75. 3 38. 8 57. 6 33. 1 45. 6 98. 4 52. 8 babyllava - v2 - synthetic 57. 4 46. 7 35. 3 92. 0 30. 7 87. 7 36. 9 57. 8 38. 1 49. 0 99. 2 57. 6 engagement in vision fms and accelerate progress toward developmentally plausible learning. acknowledgements special thanks to chen yu, jessica sullivan, and michel c. frank for their wholehearted support and feedback throughout the project! references [ 1 ] shuai bai, keqin chen, xuejing liu, jialin wang, wenbin ge, sibo song, kai dang, peng wang, shijie wang, jun tang, humen zhong, yuanzhi zhu, mingkun yang, zhaohai li, jianqiang wan, pengfei wang, wei ding, zheren fu, yiheng xu, jiabo ye, xi zhang, tianbao xie, zesen cheng, hang zhang, zhibo yang, haiyang xu, and junyang lin. qwen2. 5 - vl technical report, 2025. arxiv : 2502. 13923 [ cs ]. 2 [ 2 ] palanikumar balasundaram and indirapriya darshini avulakunta. bayley scales of infant and toddler development. in statpearls. statpearls publishing, treasure island ( fl ), 2025. 3 [ 3 ] rishi bommasani, drew a. hudson, ehsan adeli, russ altman, simran arora, sydney von arx, michael s. bernstein, jeannette bohg, antoine bosselut, and emma brunskill et al. on the opportunities and risks of foundation models, 2022. arxiv : 2108.", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 9, "frag_id": 1, "text": "report, 2025. arxiv : 2502. 13923 [ cs ]. 2 [ 2 ] palanikumar balasundaram and indirapriya darshini avulakunta. bayley scales of infant and toddler development. in statpearls. statpearls publishing, treasure island ( fl ), 2025. 3 [ 3 ] rishi bommasani, drew a. hudson, ehsan adeli, russ altman, simran arora, sydney von arx, michael s. bernstein, jeannette bohg, antoine bosselut, and emma brunskill et al. on the opportunities and risks of foundation models, 2022. arxiv : 2108. 07258 [ cs ]. 2 [ 4 ] mathilde caron, alireza fathi, cordelia schmid, and ahmet iscen. web - scale visual entity recognition : an llm - driven data approach. in proceedings of the 38th international conference on neural information processing systems, red hook, ny, usa, 2024. curran associates inc. 2 [ 5 ] fei - long chen, du - zhen zhang, ming - lun han, xiu - yi chen, jing shi, shuang xu, and bo xu. vlp : a survey on vision - language pre - training. machine intelligence research, 20 ( 1 ) : 38 – 56, 2023. 2 [ 6 ] gheorghe comanici, eric bieber, mike schaekermann, ice pasupat, noveen sachdeva, inderjit dhillon, marcel blistein, ori ram, dan zhang, and evan rosen et al. gemini 2. 5 : pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. arxiv : 2507. 06261 [ cs ]. 2 [ 7 ] gil diesendruck and paul bloom. how specific is the shape bias? child development, 74 ( 1 ) : 168 – 178, 2003. 2 [ 8 ] alexey dosovitskiy, lucas beyer, alexander kolesnikov, dirk weissenborn, xiaohua zhai, thomas unterthiner, mostafa dehghani, matthias minderer, georg heigold, sylvain gelly, jakob uszkoreit, and neil houlsby. an", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 9, "frag_id": 2, "text": "gemini 2. 5 : pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. arxiv : 2507. 06261 [ cs ]. 2 [ 7 ] gil diesendruck and paul bloom. how specific is the shape bias? child development, 74 ( 1 ) : 168 – 178, 2003. 2 [ 8 ] alexey dosovitskiy, lucas beyer, alexander kolesnikov, dirk weissenborn, xiaohua zhai, thomas unterthiner, mostafa dehghani, matthias minderer, georg heigold, sylvain gelly, jakob uszkoreit, and neil houlsby. an image is worth 16x16 words : transformers for image recognition at scale. in international conference on learning representations, 2021. 3, 13 [ 9 ] ron dumont, john o. willis, kathleen viezel, and jamie zibulsky. mullen scales of early learning, ags edition, 1995. in encyclopedia of special education. john wiley & sons, ltd, 2014. 3 [ 10 ] dumitru erhan, aaron courville, yoshua bengio, and pascal vincent. why does unsupervised pre - training help deep learning? in proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 201 – 208. jmlr workshop and conference proceedings, 2010. issn : 1938 - 7228. 2 [ 11 ] richard gershon, miriam a. novack, and aaron j. kaat. the nih infant and toddler toolbox : a new standardized tool for assessing neurodevelopment in children ages 1 – 42 months. child development, 95 ( 6 ) : 2252 – 2254, 2024. 2, 15 [ 12 ] richard c. gershon, molly v. wagster, hugh c. hendrie, nathan a. fox, karon f. cook, and cindy j. nowinski. nih toolbox for assessment of neurological and behavioral function. neurology, 80 ( 11 _ supplement _ 3 ) : s2 – s6, 2013. 15 [ 13 ] kristen grauman, andrew westbury, eugene byrne, zachary chavis, antonino furnari, rohit girdhar, jackson hamburger, hao jiang, miao liu, xingyu liu, et al. ego4", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 9, "frag_id": 3, "text": "months. child development, 95 ( 6 ) : 2252 – 2254, 2024. 2, 15 [ 12 ] richard c. gershon, molly v. wagster, hugh c. hendrie, nathan a. fox, karon f. cook, and cindy j. nowinski. nih toolbox for assessment of neurological and behavioral function. neurology, 80 ( 11 _ supplement _ 3 ) : s2 – s6, 2013. 15 [ 13 ] kristen grauman, andrew westbury, eugene byrne, zachary chavis, antonino furnari, rohit girdhar, jackson hamburger, hao jiang, miao liu, xingyu liu, et al. ego4d : around the world in 3, 000 hours of egocentric video. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 18995 – 19012, 2022. 24 [ 14 ] kristen grauman, andrew westbury, eugene byrne, zachary chavis, antonino furnari, rohit girdhar, jackson hamburger, hao jiang, miao liu, and xingyu liu et al. ego4d : around the world in 3, 000 hours of egocentric video. in proceedings of the ieee / cvf conference on computer vision and pattern recognition ( cvpr ), pages 18995 – 19012, 2022. 4 [ 15 ] y. catherine han, courtney k. blackwell, elizabeth m. dworak, rachel m. flynn, maxwell a. mansolf, miriam a. novack, sarah pila, and aaron j. kaat. nih baby toolbox® technical manual. northwestern university, evanston, il, version 1. 1 edition, 2025. 4, 15, 16 [ 16 ] y. catherine han, elizabeth m. dworak, maxwell mansolf, hubert adam, lihua yao, miriam a. novack, sarah pila, rachel m. flynn, amanda m. flagg, vitali ustsinovich, kay savio, greg j. byrne, richard c. gershon, and aaron j. kaat. nih baby toolbox® methodology and norms development. infant behavior and development, 80 : 102117, 2025. 2, 4 [ 17 ] jordan hoffmann, sebastian borgeaud, arthur mensch, elena buchatskaya, trevor cai, eliza rutherford", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 9, "frag_id": 4, "text": "evanston, il, version 1. 1 edition, 2025. 4, 15, 16 [ 16 ] y. catherine han, elizabeth m. dworak, maxwell mansolf, hubert adam, lihua yao, miriam a. novack, sarah pila, rachel m. flynn, amanda m. flagg, vitali ustsinovich, kay savio, greg j. byrne, richard c. gershon, and aaron j. kaat. nih baby toolbox® methodology and norms development. infant behavior and development, 80 : 102117, 2025. 2, 4 [ 17 ] jordan hoffmann, sebastian borgeaud, arthur mensch, elena buchatskaya, trevor cai, eliza rutherford, diego de las casas, lisa anne hendricks, johannes welbl, aidan clark, tom hennigan, eric noland, katie millican, george van den driessche, bogdan damoc, aurelia guy, simon 9", "token_count": 202}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 10, "frag_id": 0, "text": "osindero, karen simonyan, erich elsen, jack w. rae, oriol vinyals, and laurent sifre. training compute - optimal large language models, 2022. 25 [ 18 ] michael y. hu, aaron mueller, candace ross, adina williams, tal linzen, chengxu zhuang, ryan cotterell, leshem choshen, alex warstadt, and ethan gotlieb wilcox. findings of the second babylm challenge : sampleefficient pretraining on developmentally plausible corpora, 2024. arxiv : 2412. 05149 [ cs ] version : 1. 1 [ 19 ] philip a. huebner, elior sulem, fisher cynthia, and dan roth. babyberta : learning more grammar with smallscale child - directed language. in proceedings of the 25th conference on computational natural language learning, pages 624 – 646, online, 2021. association for computational linguistics. 2, 3 [ 20 ] chao jia, yinfei yang, ye xia, yi - ting chen, zarana parekh, hieu pham, quoc le, yun - hsuan sung, zhen li, and tom duerig. scaling up visual and vision - language representation learning with noisy text supervision. in proceedings of the 38th international conference on machine learning, pages 4904 – 4916. pmlr, 2021. issn : 2640 - 3498. 2 [ 21 ] guangyuan jiang, manjie xu, shiji xin, wei liang, yujia peng, chi zhang, and yixin zhu. mewl : few - shot multimodal word learning with referential uncertainty. in proceedings of the 40th international conference on machine learning, pages 15144 – 15169. pmlr, 2023. issn : 26403498. 2, 3 [ 22 ] jared kaplan, sam mccandlish, tom henighan, tom b. brown, benjamin chess, rewon child, scott gray, alec radford, jeffrey wu, and dario amodei. scaling laws for neural language models, 2020. 25 [ 23 ] jared kaplan, sam mccandlish, tom henighan, tom b. brown, benjamin chess, rewon child, scott gray, alec radford, jeffrey wu, and dario amodei. scaling laws for neural language models, 2020. arxiv : 2001", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 10, "frag_id": 1, "text": "in proceedings of the 40th international conference on machine learning, pages 15144 – 15169. pmlr, 2023. issn : 26403498. 2, 3 [ 22 ] jared kaplan, sam mccandlish, tom henighan, tom b. brown, benjamin chess, rewon child, scott gray, alec radford, jeffrey wu, and dario amodei. scaling laws for neural language models, 2020. 25 [ 23 ] jared kaplan, sam mccandlish, tom henighan, tom b. brown, benjamin chess, rewon child, scott gray, alec radford, jeffrey wu, and dario amodei. scaling laws for neural language models, 2020. arxiv : 2001. 08361 [ cs ]. 1 [ 24 ] alexander kirillov, eric mintun, nikhila ravi, hanzi mao, chloe rolland, laura gustafson, tete xiao, spencer whitehead, alexander c. berg, wan - yen lo, piotr dollar, and ross girshick. segment anything. in 2023 ieee / cvf international conference on computer vision ( iccv ), pages 3992 – 4003, 2023. 2 [ 25 ] eliza kosoy, emily rose reagan, leslie lai, alison gopnik, and danielle krettek cobb. comparing machines and children : using developmental psychology experiments to assess the strengths and weaknesses of lamda responses. ssrn electronic journal, 2024. available at ssrn : https : / / ssrn. com / abstract = 4696693 or http : / / dx. doi. org / 10. 2139 / ssrn. 4696693. 2, 3 [ 26 ] junnan li, dongxu li, caiming xiong, and steven hoi. blip : bootstrapping language - image pre - training for unified vision - language understanding and generation. in proceedings of the 39th international conference on machine learning, pages 12888 – 12900. pmlr, 2022. issn : 2640 - 3498. 2 [ 27 ] junnan li, dongxu li, silvio savarese, and steven hoi. blip - 2 : bootstrapping language - image pre - training with frozen image encoders and large language models. in proceedings of the 40th international conference on machine learning, pages 19730 – 19742. pmlr,", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 10, "frag_id": 2, "text": ". 2, 3 [ 26 ] junnan li, dongxu li, caiming xiong, and steven hoi. blip : bootstrapping language - image pre - training for unified vision - language understanding and generation. in proceedings of the 39th international conference on machine learning, pages 12888 – 12900. pmlr, 2022. issn : 2640 - 3498. 2 [ 27 ] junnan li, dongxu li, silvio savarese, and steven hoi. blip - 2 : bootstrapping language - image pre - training with frozen image encoders and large language models. in proceedings of the 40th international conference on machine learning, pages 19730 – 19742. pmlr, 2023. issn : 2640 - 3498. 2 [ 28 ] songtao li and hao tang. multimodal alignment and fusion : a survey, 2025. arxiv : 2411. 17040 [ cs ] version : 2. 2 [ 29 ] yijiang li, qingying gao, tianwei zhao, bingyang wang, haoran sun, haiyun lyu, robert d. hawkins, nuno vasconcelos, tal golan, dezhi luo, and hokin deng. core knowledge deficits in multi - modal language models. in forty - second international conference on machine learning, 2025. 2, 3 [ 30 ] haotian liu, chunyuan li, qingyang wu, and yong jae lee. visual instruction tuning. in proceedings of the 37th international conference on neural information processing systems, red hook, ny, usa, 2023. curran associates inc. 3, 13 [ 31 ] shilong liu, zhaoyang zeng, tianhe ren, feng li, hao zhang, jie yang, qing jiang, chunyuan li, jianwei yang, hang su, jun zhu, and lei zhang. grounding dino : marrying dino with grounded pre - training for open - set object detection. in computer vision – eccv 2024 : 18th european conference, milan, italy, september 29 – october 4, 2024, proceedings, part xlvii, pages 38 – 55, berlin, heidelberg, 2024. springer - verlag. 4, 14 [ 32 ] bria long, robert z. sparks, violet xiang, stefan stojanov, zi yin, grace e. keene, alvin w. m. tan, steven y", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 10, "frag_id": 3, "text": "##yang zeng, tianhe ren, feng li, hao zhang, jie yang, qing jiang, chunyuan li, jianwei yang, hang su, jun zhu, and lei zhang. grounding dino : marrying dino with grounded pre - training for open - set object detection. in computer vision – eccv 2024 : 18th european conference, milan, italy, september 29 – october 4, 2024, proceedings, part xlvii, pages 38 – 55, berlin, heidelberg, 2024. springer - verlag. 4, 14 [ 32 ] bria long, robert z. sparks, violet xiang, stefan stojanov, zi yin, grace e. keene, alvin w. m. tan, steven y. feng, chengxu zhuang, virginia a. marchman, daniel l. k. yamins, and michael c. frank. the babyview dataset : high - resolution egocentric videos of infants ’ and young children ’ s everyday experiences, 2025. arxiv : 2406. 10447 [ cs ]. 3 [ 33 ] yiwei ma, guohai xu, xiaoshuai sun, ming yan, ji zhang, and rongrong ji. x - clip : end - to - end multi - grained contrastive learning for video - text retrieval. in proceedings of the 30th acm international conference on multimedia, page 638 – 647, new york, ny, usa, 2022. association for computing machinery. 3 [ 34 ] maya malaviya, ilia sucholutsky, kerem oktar, and { thomas l. } griffiths. can humans do less - than - one - shot learning? pages 997 – 1003, 2022. publisher copyright : © 2022 the author ( s ). this work is licensed under a creative commons attribution 4. 0 international license ( cc by ) ; 44th annual meeting of the cognitive science society : cognitive diversity, cogsci 2022 ; conference date : 27 - 07 - 2022 through 30 - 07 - 2022. 2 [ 35 ] virginia a. marchman and philip s. dale. the macarthurbates communicative development inventories : updates from the cdi advisory board. frontiers in psychology, 14, 2023. publisher : frontiers. 4, 14, 15 [ 36 ] microsoft. azure ai speech | microsoft azure. [ accessed 13 - 11 - 2025 ] https : / / azure", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 10, "frag_id": 4, "text": ", 2022. publisher copyright : © 2022 the author ( s ). this work is licensed under a creative commons attribution 4. 0 international license ( cc by ) ; 44th annual meeting of the cognitive science society : cognitive diversity, cogsci 2022 ; conference date : 27 - 07 - 2022 through 30 - 07 - 2022. 2 [ 35 ] virginia a. marchman and philip s. dale. the macarthurbates communicative development inventories : updates from the cdi advisory board. frontiers in psychology, 14, 2023. publisher : frontiers. 4, 14, 15 [ 36 ] microsoft. azure ai speech | microsoft azure. [ accessed 13 - 11 - 2025 ] https : / / azure. microsoft. com / enus / products / ai - services / ai - speech. 3 [ 37 ] antoine miech, dimitri zhukov, jean - baptiste alayrac, makarand tapaswi, ivan laptev, and josef sivic. howto100m : learning a text - video embedding by watching hundred million narrated video clips. in 2019 10", "token_count": 228}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 11, "frag_id": 0, "text": "ieee / cvf international conference on computer vision ( iccv ), pages 2630 – 2640, 2019. issn : 2380 - 7504. 2 [ 38 ] national heart, lung, and blood institute. how sleep works - how much sleep is enough? nhlbi, national institutes of health ( nih ) website, 2022. 2, 3 [ 39 ] jean. newborg and riverside publishing company. battelle developmental inventory. bdi - 2, 2005. edition : 2nd ed. place : itasca, ill publisher : riverside pub. 3 [ 40 ] openai, aaron hurst, adam lerer, adam p. goucher, adam perelman, aditya ramesh, aidan clark, a. j. ostrow, akila welihinda, alan hayes, and alec radford et al. gpt - 4o system card, 2024. arxiv : 2410. 21276 [ cs ]. 2 [ 41 ] maxime oquab, timothee darcet, theo moutakanni, huy vo, marc szafraniec, vasil khalidov, pierre fernandez, daniel haziza, francisco massa, alaaeldin el - nouby, mahmoud assran, nicolas ballas, wojciech galuba, russell howes, po - yao huang, shang - wen li, ishan misra, michael rabbat, vasu sharma, gabriel synnaeve, hu xu, herve jegou, julien mairal, patrick labatut, armand joulin, and piotr bojanowski. dinov2 : learning robust visual features without supervision, 2024. arxiv : 2304. 07193 [ cs ]. 13 [ 42 ] alec radford, karthik narasimhan, tim salimans, and ilya sutskever. improving language understanding by generative pre - training.. 13 [ 43 ] alec radford, jeffrey wu, rewon child, david luan, dario amodei, and ilya sutskever. language models are unsupervised multitask learners.. 13 [ 44 ] alec radford, jong wook kim, chris hallacy, aditya ramesh, gabriel goh, sandhini agarwal, girish sastry, amanda askell, pamela mishkin, jack clark, gretchen", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 11, "frag_id": 1, "text": "arxiv : 2304. 07193 [ cs ]. 13 [ 42 ] alec radford, karthik narasimhan, tim salimans, and ilya sutskever. improving language understanding by generative pre - training.. 13 [ 43 ] alec radford, jeffrey wu, rewon child, david luan, dario amodei, and ilya sutskever. language models are unsupervised multitask learners.. 13 [ 44 ] alec radford, jong wook kim, chris hallacy, aditya ramesh, gabriel goh, sandhini agarwal, girish sastry, amanda askell, pamela mishkin, jack clark, gretchen krueger, and ilya sutskever. learning transferable visual models from natural language supervision. in proceedings of the 38th international conference on machine learning, pages 8748 – 8763. pmlr, 2021. issn : 2640 - 3498. 2, 3, 15 [ 45 ] nikhila ravi, valentin gabeur, yuan - ting hu, ronghang hu, chaitanya ryali, tengyu ma, haitham khedr, roman radle, chloe rolland, laura gustafson, eric mintun, junting pan, kalyan vasudev alwala, nicolas carion, chaoyuan wu, ross girshick, piotr dollar, and christoph feichtenhofer. sam 2 : segment anything in images and videos. in the thirteenth international conference on learning representations, 2025. 2 [ 46 ] christoph schuhmann, romain beaumont, richard vencu, cade gordon, ross wightman, mehdi cherti, theo coombes, aarush katta, clayton mullis, mitchell wortsman, patrick schramowski, srivatsa kundurthy, katherine crowson, ludwig schmidt, robert kaczmarczyk, and jenia jitsev. laion - 5b : an open large - scale dataset for training next generation image - text models. in proceedings of the 36th international conference on neural information processing systems, pages 25278 – 25294, red hook, ny, usa, 2022. curran associates inc. 2 [ 47 ] rico sennrich, barry haddow, and alexandra birch. neural machine translation of rare words with subword units. in proceedings", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 11, "frag_id": 2, "text": "##man, mehdi cherti, theo coombes, aarush katta, clayton mullis, mitchell wortsman, patrick schramowski, srivatsa kundurthy, katherine crowson, ludwig schmidt, robert kaczmarczyk, and jenia jitsev. laion - 5b : an open large - scale dataset for training next generation image - text models. in proceedings of the 36th international conference on neural information processing systems, pages 25278 – 25294, red hook, ny, usa, 2022. curran associates inc. 2 [ 47 ] rico sennrich, barry haddow, and alexandra birch. neural machine translation of rare words with subword units. in proceedings of the 54th annual meeting of the association for computational linguistics ( volume 1 : long papers ), pages 1715 – 1725, berlin, germany, 2016. association for computational linguistics. 13 [ 48 ] saber sheybani, sahaj singh maini, aravind dendukuri, zoran tiganj, and linda b. smith. modelvsbaby : a developmentally motivated benchmark of out - of - distribution object recognition, 2024. 2, 3 [ 49 ] melissa kline struhl, laura schulz, and mark sheskin et al. children helping science. [ accessed 13 - 11 - 2025 ] https : / / childrenhelpingscience. com /. 7, 24 [ 50 ] jessica sullivan, michelle mei, andrew perfors, erica wojcik, and michael c. frank. saycam : a large, longitudinal audiovisual dataset recorded from the infant ’ s perspective. open mind, 5 : 20 – 29, 2021. 2, 3, 4 [ 51 ] yuqi sun, weimin tan, zhuoyao gu, ruian he, siyuan chen, miao pang, and bo yan. a data - efficient strategy for building high - performing medical foundation models. nature biomedical engineering, 9 ( 4 ) : 539 – 551, 2025. publisher : nature publishing group. 2 [ 52 ] alvin wei ming tan, chunhua yu, bria lorelle long, wanjing anya ma, tonya murray, rebecca d. silverman, jason d yeatman, and michael frank. devbench : a multimodal developmental benchmark for language learning. in the thirty - eight conference on neural information processing systems data", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 11, "frag_id": 3, "text": "29, 2021. 2, 3, 4 [ 51 ] yuqi sun, weimin tan, zhuoyao gu, ruian he, siyuan chen, miao pang, and bo yan. a data - efficient strategy for building high - performing medical foundation models. nature biomedical engineering, 9 ( 4 ) : 539 – 551, 2025. publisher : nature publishing group. 2 [ 52 ] alvin wei ming tan, chunhua yu, bria lorelle long, wanjing anya ma, tonya murray, rebecca d. silverman, jason d yeatman, and michael frank. devbench : a multimodal developmental benchmark for language learning. in the thirty - eight conference on neural information processing systems datasets and benchmarks track, 2024. 2, 3 [ 53 ] hugo touvron, thibaut lavril, gautier izacard, xavier martinet, marie - anne lachaux, timothee lacroix, baptiste roziere, naman goyal, eric hambro, faisal azhar, aurelien rodriguez, armand joulin, edouard grave, and guillaume lample. llama : open and efficient foundation language models, 2023. arxiv : 2302. 13971 [ cs ]. 3, 13 [ 54 ] wai keen vong, wentao wang, a. emin orhan, and brenden m. lake. grounded language acquisition through the eyes and ears of a single child. science ( new york, n. y. ), 383 ( 6682 ) : 504 – 511, 2024. 2, 3 [ 55 ] ao wang, lihao liu, hui chen, zijia lin, jungong han, and guiguang ding. yoloe : real - time seeing anything, 2025. 20 [ 56 ] shengao wang, arjun chandra, aoming liu, venkatesh saligrama, and boqing gong. babyvlm : data - efficient pretraining of vlms inspired by infant learning. in proceedings of the ieee / cvf international conference on computer vision ( iccv ), pages 1380 – 1390, 2025. 2, 3, 13 [ 57 ] wenhui wang, furu wei, li dong, hangbo bao, nan yang, and ming zhou. minilm : deep self - attention distillation for task - ag", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 11, "frag_id": 4, "text": ", hui chen, zijia lin, jungong han, and guiguang ding. yoloe : real - time seeing anything, 2025. 20 [ 56 ] shengao wang, arjun chandra, aoming liu, venkatesh saligrama, and boqing gong. babyvlm : data - efficient pretraining of vlms inspired by infant learning. in proceedings of the ieee / cvf international conference on computer vision ( iccv ), pages 1380 – 1390, 2025. 2, 3, 13 [ 57 ] wenhui wang, furu wei, li dong, hangbo bao, nan yang, and ming zhou. minilm : deep self - attention distillation for task - agnostic compression of pre - trained transformers. in proceedings of the 34th international conference on neural information processing systems, red hook, ny, usa, 2020. curran associates inc. 15 [ 58 ] yi wang, yinan he, yizhuo li, kunchang li, jiashuo yu, xin ma, xinhao li, guo chen, xinyuan chen, yaohui wang, ping luo, ziwei liu, yali wang, limin wang, and yu qiao. internvid : a large - scale video - text dataset for multimodal understanding and generation. in the twelfth international conference on learning representations, 2024. 2 [ 59 ] alex warstadt, aaron mueller, leshem choshen, ethan wilcox, chengxu zhuang, juan ciro, rafael mosquera, bhargavi paranjabe, adina williams, tal linzen, and ryan cotterell. findings of the babylm challenge : sampleefficient pretraining on developmentally plausible corpora. in proceedings of the babylm challenge at the 27th conference on computational natural language learning, pages 11", "token_count": 382}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 12, "frag_id": 0, "text": "1 – 34, singapore, 2023. association for computational linguistics. 1, 2 [ 60 ] luca weihs, amanda yuile, renee baillargeon, cynthia fisher, gary marcus, roozbeh mottaghi, and aniruddha kembhavi. benchmarking progress to infant - level physical reasoning in ai. transactions on machine learning research, 2022. 2, 3 [ 61 ] saining xie, ross girshick, piotr dollar, zhuowen tu, and kaiming he. aggregated residual transformations for deep neural networks. in proceedings of the ieee conference on computer vision and pattern recognition, pages 1492 – 1500, 2017. 13 [ 62 ] eunice yiu, maan qraitem, anisa noor majhi, charlie wong, yutong bai, shiry ginosar, alison gopnik, and kate saenko. kiva : kid - inspired visual analogies for testing large multimodal models. in the thirteenth international conference on learning representations, 2025. 2 [ 63 ] peiyuan zhang, guangtao zeng, tianduo wang, and wei lu. tinyllama : an open - source small language model, 2024. arxiv : 2401. 02385 [ cs ]. 3, 13 [ 64 ] shengyu zhang, linfeng dong, xiaoya li, sen zhang, xiaofei sun, shuhe wang, jiwei li, runyi hu, tianwei zhang, fei wu, and guoyin wang. instruction tuning for large language models : a survey, 2025. arxiv : 2308. 10792 [ cs ]. 2 [ 65 ] yifu zhang, peize sun, yi jiang, dongdong yu, fucheng weng, zehuan yuan, ping luo, wenyu liu, and xinggang wang. bytetrack : multi - object tracking by associating every detection box. 2022. 20 12", "token_count": 410}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 13, "frag_id": 0, "text": "babyvlm - v2 : toward developmentally grounded pretraining and benchmarking of vision foundation models supplementary material a. model training a. 1. babyllava - v2 architecture we build upon the original babyllava - llama model introduced in babyvlm - v1 [ 56 ], by giving it the capability to process multiple images as input and conduct multiturn visual – linguistic interactions. the model architecture consists of a compact language backbone, a visual encoder, and a lightweight multilayer perceptron ( mlp ) connector that projects visual features into the language space. unlike babyvlm - v1, which also experimented with smaller backbones ( gpt - 2 [ 43 ] + resnext - 50 [ 61 ] ), we only adopt the larger variant composed of a llama - 1. 1b [ 53, 63 ] language model and a vit - l - 16 [ 8 ] visual encoder ( 300m params ). we find that the smaller variant often struggles to complete complex downstream tasks such as memory, primarily due to its limited model capacity, whereas the larger configuration achieves a better balance between developmental plausibility and expressive capability. a. 2. babyllava - v2 training paradigm we train the entire model from scratch using a four - stage pipeline, as summarized in table 7. stage 0 : unimodal training. in the first stage, the language and vision backbones are trained independently to acquire the basic representational abilities for each modality. the language backbone is trained on all transcribed utterances using a standard autoregressive loss [ 42 ]. its tokenizer is initialized via byte - pair encoding ( bpe ) [ 47 ] trained on the same corpus, with a fixed vocabulary size of 6000. the vision backbone is trained using a dinov2 [ 41 ] objective on saycam frames. we do not apply any filtering during this stage — except restricting samples to the training split — since the filtering procedures are primarily designed to enforce image – utterance alignment, which is irrelevant to unimodal representation learning. stage 1 : feature alignment. this stage corresponds to phase 1 training in llava [ 30 ]. both the vision and language backbones are frozen, and only the mlp connector is optimized using an autoregressive loss. the objective is to align visual features with the language embedding space, effectively bridging the two modalities.", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 13, "frag_id": 1, "text": "trained on the same corpus, with a fixed vocabulary size of 6000. the vision backbone is trained using a dinov2 [ 41 ] objective on saycam frames. we do not apply any filtering during this stage — except restricting samples to the training split — since the filtering procedures are primarily designed to enforce image – utterance alignment, which is irrelevant to unimodal representation learning. stage 1 : feature alignment. this stage corresponds to phase 1 training in llava [ 30 ]. both the vision and language backbones are frozen, and only the mlp connector is optimized using an autoregressive loss. the objective is to align visual features with the language embedding space, effectively bridging the two modalities. to maintain training stability, we use only the image – utterance subset of the pretraining data in this stage, postponing exposure to multiimage inputs until later phases. 1the training dataset, the model checkpoints, the training scripts and the evaluation samples will be released to the public in the near future. stage 2 : joint pretraining. in this stage, the vision backbone remains frozen, while the mlp connector and language backbone are trained jointly on the full mixed - format pretraining dataset, as described in section 3. 1. this allows the model to learn multimodal grounding over diverse input structures. stage 3 : instruction fine - tuning. finally, we fine - tune the model using the mixed instruction dataset, which is a combination of all the instruction samples mentioned in table 3. this step enables the model to perform various downstream tasks through natural - language prompts. the vision backbone, mlp connector and language backbone are all updated to learn instruction - following behavior and contextdependent reasoning. we apply two different learning rates for different modules in this stage : the learning rate of the vision backbone is 1e - 5, while that of the mlp connector and language backbone is 5e - 5. main hyperparameters of all 4 stages are summarized in table 7. all experiments are conducted on four nvidia a6000 gpus with 48 gb of vram each. language backbone training completes in less than one hour, while the vision backbone completes in 4 days. next, training the mlp connector requires approximately five hours. joint pretraining on the mixed - format dataset takes roughly 34 hours to converge. finally, instruction tuning takes 60 hours. a. 3", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 13, "frag_id": 2, "text": "instruction - following behavior and contextdependent reasoning. we apply two different learning rates for different modules in this stage : the learning rate of the vision backbone is 1e - 5, while that of the mlp connector and language backbone is 5e - 5. main hyperparameters of all 4 stages are summarized in table 7. all experiments are conducted on four nvidia a6000 gpus with 48 gb of vram each. language backbone training completes in less than one hour, while the vision backbone completes in 4 days. next, training the mlp connector requires approximately five hours. joint pretraining on the mixed - format dataset takes roughly 34 hours to converge. finally, instruction tuning takes 60 hours. a. 3. open - source model fine - tuning we conduct lora finetuning experiments on two opensource models, llava - onevision - 7b and qwen2. 5 - vl7b, to evaluate the effectiveness of our instructionfinetuning dataset. each task is finetuned separately. we set the lora rank to 64, use a scaling factor of 64, and apply a dropout rate of 0. 05. training is performed for 5 epochs with a global batch size of 128, a learning rate of 1e - 4, a weight decay of 0. 1, a warmup ratio of 0. 03, and a cosine learning - rate schedule. b. developmentally aligned benchmarks in appendix b, we adopt the following organization : subsection b. 1 describes general implementation details that are shared by several tasks, including details on the vocabulary used in devcv toolbox, acquisition of saycam annotations, acquisition of ego4d annotations, and important distinction between saycam and ego4d. then, each subsection between 2 and 11 describes how these annotations are used to construct one task in devcv toolbox each, and are each broken up into original toolbox task, adaptation, 13", "token_count": 409}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 14, "frag_id": 0, "text": "table 7. training stage specification of babyllava - v2. note that for stage 3, different modules have different learning rate, as mentioned in section a. 2. stage trained modules frozen modules dataset loss learning rate epoch global batch size 0 - language language backbone n / a 283k utterance only autoregressive 2e - 4 10 16 0 - vision vision backbone n / a 1085k image only dinov2 1e - 4 100 64 1 mlp connector language backbone + vision backbone 768k image - utterance autoregressive 3e - 3 5 128 2 mlp connector + language backbone vision backbone 768k image - utterance + 181k video - utterance + 63k multi - turn autoregressive 2e - 4 5 128 3 mlp connector + language backbone + vision backbone none 150k instruction finetune autoregressive 5e - 5 | 1e - 5 5 128 data collection, and example prompt. some of these also include information on evaluation or data composition. b. 1. data collection procedures common to all tasks vocabulary filtering to ensure that all benchmarks in this work focus on developmentally appropriate vocabulary, we draw on the macarthur – bates communicative development inventories ( mab – cdi ) : words and gestures [ 35 ]. the mab – cdi is a standardized instrument assessing early vocabulary comprehension and production in infants and toddlers, covering familiar words across core semantic categories ( e. g., animals, foods, body parts, actions ). because it is widely regarded as a gold - standard reference for early lexical development, we restrict our benchmark vocabulary to words that appear in — or are closely aligned with — those in the mab – cdi. accordingly, during visual concept mining from saycam and ego4d, we retain only crops whose labels fall within this developmentally grounded lexical domain, ensuring that every keyword used across tasks reflects concepts young children could plausibly understand. saycam annotations to support all saycam - based benchmarks in this work, we build the following unified preprocessing pipeline that extracts high - quality image crops for every object and action concept appearing in the corpus. this pipeline is reused ( with task - specific modifications described in the corresponding benchmark sections ) across tasks and provides consistent visual grounding for all downstream datasets. • frame - level detection and indexing : we first sample saycam videos at", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 14, "frag_id": 1, "text": "in the mab – cdi. accordingly, during visual concept mining from saycam and ego4d, we retain only crops whose labels fall within this developmentally grounded lexical domain, ensuring that every keyword used across tasks reflects concepts young children could plausibly understand. saycam annotations to support all saycam - based benchmarks in this work, we build the following unified preprocessing pipeline that extracts high - quality image crops for every object and action concept appearing in the corpus. this pipeline is reused ( with task - specific modifications described in the corresponding benchmark sections ) across tasks and provides consistent visual grounding for all downstream datasets. • frame - level detection and indexing : we first sample saycam videos at 1 fps and run an open - vocabulary detector ( grounding – dino [ 31 ] ) using the gpt - annotated labels associated with each frame as the open set. let s denote the set of all such saycam labels. for each label s ∈s, we construct an index index ( s ) that maps s to all frames in which it is detected, together with its proportionally buffered bounding boxes and gpt - derived blurriness scores. this index ( s ) structure serves as the master lookup table for retrieving visual instances of any concept. • normalizing label variants : raw saycam labels s ∈s often include plural forms, paraphrases, or compositional descriptions. to ensure consistent visual grounding, we cluster lexically or semantically equivalent labels into small groups based on lexical similarity, plural equivalence, and phrase containment heuristics. each label s is assigned to its cluster m ( s ). this allows us to treat variants of s such as “ shoes ”, “ a shoe ”, or “ pair of shoes ” as a single underlying concept by retrieving visual instances from { index ( s ′ ) | s ′ ∈m ( s ) }. • quality filtering : because saycam contains naturalistic video frames from children ’ s head - cam footage, many detections are of low - quality due to motion blur, wrong / irrelevant detector predictions, small / partial bounding boxes. therefore, we score each detection result using four broad signals : ( 1 ) detector confidence, ( 2 ) clip image – text alignment, ( 3 ) crop size, and ( 4 ) spatial clarity ( e. g., centeredness ). these signals are normal", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 14, "frag_id": 2, "text": "to treat variants of s such as “ shoes ”, “ a shoe ”, or “ pair of shoes ” as a single underlying concept by retrieving visual instances from { index ( s ′ ) | s ′ ∈m ( s ) }. • quality filtering : because saycam contains naturalistic video frames from children ’ s head - cam footage, many detections are of low - quality due to motion blur, wrong / irrelevant detector predictions, small / partial bounding boxes. therefore, we score each detection result using four broad signals : ( 1 ) detector confidence, ( 2 ) clip image – text alignment, ( 3 ) crop size, and ( 4 ) spatial clarity ( e. g., centeredness ). these signals are normalized per - concept and combined into a single quality measure. we also employ additional light - weight adjustments to ensure that within m ( s ), rare labels are not overwhelmed by frequent ones and that exact label matches are preferred over looser variants. • ensuring lexical and visual diversity : to avoid selecting many near - duplicate frames of the same scene, we apply simple diversity controls. we first ensure that different lexical variants of a concept are represented, and then enforce a minimal temporal spacing between chosen frames. from this diversified pool, we keep only a small number ( ≤10 ) of final crops per concept, prioritizing clarity and representativeness. • final output : the result is a compact, high - quality set of image crops for every object or action concept in saycam. these curated crops act as the visual foundation for the majority of benchmarks built from saycam in this paper. they guarantee concept fidelity, diversity of visual 14", "token_count": 339}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 15, "frag_id": 0, "text": "table 8. comparison between saycam and ego4d. object size is reported in terms of the average % of the frame ’ s area filled. data source participants number of pixels object size saycam infants 307k ( fixed ) 57 % ego4d adults over 2m ( average ) 4 % contexts, and consistent quality standards across tasks. ego4d annotations for ego4d, we do not perform any heavy data cleaning or processing due to the native, high - quality annotations of the dataset. for most of our benchmarks, we use image data from the egotracks split, which contains densely annotated egocentric video tracks. in addition, picture vocabulary ( see section b. 2 ) also draws image crops from fho _ lta — a subset of ego4d focused on future hand – object interactions — providing additional objectcentric visual diversity. overall differences between saycam and ego4d here, we analyze the differences between saycam and ego4d which result in babyllava - v2 ’ s very poor generalization to ego4d. specifically, saycam was filmed by 3 babies across 4 homes, while ego4d was filmed by 923 participants across 74 sites. there ’ s also a significant domain shift in the size of the frames and the sizes of the objects relative to the frames. see table 8 for a summary. in addition, although all of the ego4d examples constructed in devcv toolbox are directly based on objects listed in saycam ’ s vocabulary, their backgrounds may still include objects that babyllava - v2 never saw in its training, and thus detract from its ’ overall understanding of the scene. for example, we might construct an example from ego4d that asks about the location of a hand, and although babyllava - v2 saw examples of hand during training, the frame is full of other objects to which babyllava - v2 can attribute no meaning. in such a case, context clues learned by babyllava - v2 about where gloves are usually found relative to their scene, such as at the end of an arm or holding onto a known object, are lost, and performance drops correspondingly. further, we conjecture that this lack of generalization stems from not only the explicit action categories included in ego4d that a baby would never have seen ( like fixing a car or performing a laboratory experiment ), but also from the inherently wider field of", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 15, "frag_id": 1, "text": "an example from ego4d that asks about the location of a hand, and although babyllava - v2 saw examples of hand during training, the frame is full of other objects to which babyllava - v2 can attribute no meaning. in such a case, context clues learned by babyllava - v2 about where gloves are usually found relative to their scene, such as at the end of an arm or holding onto a known object, are lost, and performance drops correspondingly. further, we conjecture that this lack of generalization stems from not only the explicit action categories included in ego4d that a baby would never have seen ( like fixing a car or performing a laboratory experiment ), but also from the inherently wider field of view captured adult demonstrators relative to babies. further, we argue that even if ego4d had been filmed of the same locations and actions as saycam, we would still observe a domain shift caused solely because the demonstrators are adults, perceiving the world from a higher point of view than babies. this point reinforces the uniqueness of the baby domain in the space of egocentric computer vision. b. 2. picture vocabulary original toolbox task our task is directly adapted from the nih baby toolbox® picture vocabulary test ( pvt ), which evaluates a participant ’ s receptive vocabulary by presenting a spoken target word alongside four images ( one correct, three distractors ) [ 11 ]. the goal is to touch the picture matching the target word. distractors in the original pvt are designed to be plausible but incorrect, typically encompassing coarse - categorical, fine - categorical, or phonological similarity. while the full pvt, taken directly from the nih toolbox® [ 12, 15 ], includes 373 examples, we identify 52 examples intended for early childhood receptive vocabulary evaluation through combining all - minilm - l6 - v2 embedding similarity [ 57 ] comparison to vocabulary in [ 35 ] and manual inspection. while the baby toolbox pvt uses an irt - based computer - adaptive score that converts response patterns into age - normed ability estimates [ 15 ], our adaptation simplifies this to straightforward 4 - way accuracy since all items in the benchmark are evaluated rather than adaptively selected. our adaptation preserves the original developmental intent while replacing controlled illustrations with naturalistic egocentric visual inputs ( saycam / ego4d ), providing a grounded benchmark for modeling baby -", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 15, "frag_id": 2, "text": "##box® [ 12, 15 ], includes 373 examples, we identify 52 examples intended for early childhood receptive vocabulary evaluation through combining all - minilm - l6 - v2 embedding similarity [ 57 ] comparison to vocabulary in [ 35 ] and manual inspection. while the baby toolbox pvt uses an irt - based computer - adaptive score that converts response patterns into age - normed ability estimates [ 15 ], our adaptation simplifies this to straightforward 4 - way accuracy since all items in the benchmark are evaluated rather than adaptively selected. our adaptation preserves the original developmental intent while replacing controlled illustrations with naturalistic egocentric visual inputs ( saycam / ego4d ), providing a grounded benchmark for modeling baby - level vocabulary comprehension in realistic developmental environments. adaptation to adapt the original pvt design to naturalistic corpora, we first map mab - cdi words r ∈r to corpus vocabularies s : gpt - annotated labels for saycam and native objects / actions labels for ego4d. this produces a set of visually grounded targets gr ⊂s for each cdi anchor r, forming a one - to - many mapping r→gr. we then analyze the 52 baby - level nih pvt items to quantify the original distractor structure. we define three categories : fine - categorical, coarse - categorical, and phonological. we manually annotate every nih distractor to one or more of these types accordingly. note that the original pvt includes unrelated distractors and we exclude those given the difficulty in controlling the quality of unrelated distractors in naturalistic imagery. we obtain the unnormalized distractor - type weights : wcoarse = 0. 5643, wfine = 0. 1472, wphon = 0. 0321. using these proportions, we construct corpus - specific distractor pools from the entire corpus s ( because the model is only required to identify the correct target concept, not to correctly recognize or label the distractors ) : • fine - categorical : we use similarity scoring based on clip text embeddings [ 44 ]. for saycam, candidates 15", "token_count": 441}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 16, "frag_id": 0, "text": "above a similarity threshold of 0. 7 is considered belonging to the same fine - grained category while for ego4d we use a quantile band [ 0. 997, 0. 99973 ] to also filter out overly similar and thus indistinguishable words. • coarse - categorical : we use kmeans clustering based on clip text embeddings ( saycam : k = 100 ; ego4d : k = 150 ). • phonological : we use soundex - based string similarity for both datasets. for each cdi anchor r, we select a ground - truth label g ∈gr and sample three distinct distractors from these pools using the weights. for saycam examples, we perform a final round of manual screening to filter out the infeasible examples, while ego4d examples are filtered with a hybrid procedure combining gemini2. 5 - flash checks with a lightweight manual review. data collection to produce high - quality 4 - way visual choices, we collect image crops corresponding to every target and distractor label. for saycam, because picture vocabulary requires extremely precise, semantically clear images, we modify the fully automated pipeline in section b. 1 with the following changes : 1. candidates come directly from index ( g ) where g ∈gr given anchor r. 2. human annotators manually filter irrelevant, ambiguous, or blurry crops and refine bounding boxes, replacing automated quality scores. this process yields a compact, high - precision crop inventory used for all saycam examples. for ego4d, for the objects, we use the bounding boxes from the visual _ crop field of the egotracks benchmark, applying a deterministic buffer ( 1. 2× + 8px margin ) and requiring a post - buffer normalized area > 0. 03. for actions, we use the fho _ lta benchmark which contains abundant action annotations. as there are no explicit bounding box annotations, we sample frames from the middle 25 % of each action frame interval and apply minimal center - biased cropping to maintain clarity. for each label, we keep 10 candidates while preserving diversity and visual fidelity, and we apply a gemini2. 5 - flash pass to eliminate unusable crops. dataset composition. as shown in figure 7, we obtain 1181 saycam examples, covering 344 unique gt labels, 1311 unique distrator labels, and 1660", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 16, "frag_id": 1, "text": "( 1. 2× + 8px margin ) and requiring a post - buffer normalized area > 0. 03. for actions, we use the fho _ lta benchmark which contains abundant action annotations. as there are no explicit bounding box annotations, we sample frames from the middle 25 % of each action frame interval and apply minimal center - biased cropping to maintain clarity. for each label, we keep 10 candidates while preserving diversity and visual fidelity, and we apply a gemini2. 5 - flash pass to eliminate unusable crops. dataset composition. as shown in figure 7, we obtain 1181 saycam examples, covering 344 unique gt labels, 1311 unique distrator labels, and 1660 unique crops. due to manual filtering, its distractor distribution only loosely follows nih proportions ( shown in figure 8 ). similarly, we obtain 346 ego4d examples over 124 unique gt labels, 343 unique distractor labels, and 633 unique images ( shown in figure 7 ) with the corresponding distractor distribution figure 7. label / image crop uniqueness comparison between picture vocabulary test in nih baby toolbox®, saycam, and ego4d. figure 8. distractor type composition for the picture vocabulary test in nih baby toolbox®, saycam, and ego4d. the original pvt contains multi - type overlaps, while our sampling assigns each distractor a single type even though some satisfy multiple cues. ego4d uses unrelated distractors only as a rare fallback. shown in figure 8. example prompt each finalized example is a prompt embedded with 4 image choices for which the following is an example : \" touch the image of ’ foot ’ ( a ) < image > ( b ) < image > ( c ) < image > ( d ) < image > \" the model needs to output one of a, b, c, or d to be evaluated. b. 3. looking while listening original toolbox task the looking while listening test ( lwl ) from nih baby toolbox®aims to evaluate comprehension for object labeling and receptive language [ 15 ]. the infant is shown two clipart images which is followed by an audio prompt describing one of them. eye tracking is used to detect whether the participant is looking at the ground - truth image. similar to pvt, we simplify the original metric to accuracy only. adaptation to adapt lwl to our benchmark in saycam", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 16, "frag_id": 2, "text": ") < image > ( b ) < image > ( c ) < image > ( d ) < image > \" the model needs to output one of a, b, c, or d to be evaluated. b. 3. looking while listening original toolbox task the looking while listening test ( lwl ) from nih baby toolbox®aims to evaluate comprehension for object labeling and receptive language [ 15 ]. the infant is shown two clipart images which is followed by an audio prompt describing one of them. eye tracking is used to detect whether the participant is looking at the ground - truth image. similar to pvt, we simplify the original metric to accuracy only. adaptation to adapt lwl to our benchmark in saycam, we replace clipart with naturalistic image crops from saycam, and eye tracking with multiple choice, similar to picture vocabulary. 16", "token_count": 177}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 17, "frag_id": 0, "text": "data collection examples for looking while listening are taken directly from picture vocabulary examples. example prompt each finalized example is a prompt embedded with 2 image choices for which the following is an example : \" touch the image of ’ foot ’ ( a ) < image > ( b ) < image > \" the model needs to output one of a or b to be evaluated. b. 4. localization original toolbox task much like picture vocabulary, the mullen receptive language test # 19 tests infants on their ability to point at sketched target objects as they are named, avoiding confusing them with the distractor objects. specifically, after gesturing to a group of sketched objects, the psychologist asks : look at these. where is the cat? if the child points in the direction of the cat, they pass the test. adaptation localization makes a significant modification to the original nih baby toolbox® measure - in devcv toolbox, we find it meaningful to test pointing to objects in their naturalistic environments, namely, we treat the objects naturally occurring in the background of the frame as distractors rather than inserting unrelated objects. additionally, because if is infeasible to ask a model to ’ point ’, the answer choices are always top left, top right, bottom left, bottom right. again, the objects in this task are real objects from saycam and ego4d rather than the sketches used in the nih baby toolbox®, and just like in the nih baby toolbox®, the prompt is the full frame and the name of the object to be localized. data collection the examples for both saycam and ego4d are generated using the centers of the bounding boxes annotated in b. 1 and ego4d ’ s egotracks, respectively. to avoid including test examples where a bounding box stretches across two answers ambiguously ( for example, an object in the bottom middle that could reasonably be called either bottom left or bottom right ), we 1 ) crop each frame so that its closest corner is flush with the edges of the object ’ s bounding box, and 2 ) enforce a maximum bounding box area of 1 / 4 of the frame ’ s area ( see figure 9 ), which filters out 5. 2k of the 7. 3k possible test examples. in practice, we find that both of these steps are needed to ensure fair, reasonably unambiguous examples. we enforce no minimum confidence in the saycam", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 17, "frag_id": 1, "text": "ego4d ’ s egotracks, respectively. to avoid including test examples where a bounding box stretches across two answers ambiguously ( for example, an object in the bottom middle that could reasonably be called either bottom left or bottom right ), we 1 ) crop each frame so that its closest corner is flush with the edges of the object ’ s bounding box, and 2 ) enforce a maximum bounding box area of 1 / 4 of the frame ’ s area ( see figure 9 ), which filters out 5. 2k of the 7. 3k possible test examples. in practice, we find that both of these steps are needed to ensure fair, reasonably unambiguous examples. we enforce no minimum confidence in the saycam object annotations figure 9 and use all object names generated in b. 1. example prompt each finalized example is a prompt embedded with one image and the same four choices, for which the following is an example : \" < image > point at the cup. is it in ( a ) the top left of the image, ( b ) the top right, ( c ) the bottom left, or ( d ) the bottom right? \" the model needs to output one of top left, top right, bottom left, or bottom right to be evaluated. b. 5. left / right original toolbox task left / right is adapted directly from mullen visual reception test # 29, in which a psychologist shows a child an object, then instructs the child to match it with the identical one. if the child correctly points to the identical object, 17", "token_count": 322}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 18, "frag_id": 0, "text": "figure 10 avoiding confusing it with its own mirror image, they pass the test. adaptation the only modification made while adapting vr test # 29 to devcv toolbox is replacing the clipart objects with real objects from saycam and ego4d. in devcv toolbox, the basic format is preserved : a prompt image, followed by a correct answer and two distractor choices in some random order, are presented to the model. the target image is a duplicate of the prompt, and the incorrect answers are the mirror image of the target. some examples in left / right are harder than others ; we conjecture that difficulty in this task can result from either 1 ) naturally symmetric objects, or 2 ) low resolution objects ( see figure 10 ). naturally symmetric objects are difficult because they require an encoding of fine - grained details. however, low resolution objects are difficult because even though there might be some spatial clues to discriminate the target from its mirror image, if have models can ’ t ascribe any semantic meaning to the image, they won ’ t encode any semantic meaning to its details. by filtering out small bounding boxes, we aim to remove the examples that are difficult solely due to low resolution. data collection for the saycam variant, use the object names and bounding boxes generated in b. 1. we enforce no minimum or maximum object size, and for the val and test splits, we enforce a minimum confidence in the bounding box of. 85. in both variants, all object crops are zero - padded to ( 640, 480 ). for the ego4d variant, we use object names and bounding boxes from the published ego4d egotracks annotations and include only objects that belong to the vocabulary defined in b. 1. to remove examples with poor resolution, we require either a minimum bounding box height or width of one fifth of the frame, which filters out about half of the otherwise qualifying examples. example prompt each finalized example is a prompt embedded with 1 image prompt and 3 image choices for which the following is an example : \" < image > which of the following is the same as this? ( a ) < image > ( b ) < image >, or ( c ) < image >? \" the model needs to output one of a, b, or c to be evaluated. b. 6. spatial details original toolbox task similarly, mullen visual reception test # 25 also tests understanding of details in images.", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 18, "frag_id": 1, "text": "to the vocabulary defined in b. 1. to remove examples with poor resolution, we require either a minimum bounding box height or width of one fifth of the frame, which filters out about half of the otherwise qualifying examples. example prompt each finalized example is a prompt embedded with 1 image prompt and 3 image choices for which the following is an example : \" < image > which of the following is the same as this? ( a ) < image > ( b ) < image >, or ( c ) < image >? \" the model needs to output one of a, b, or c to be evaluated. b. 6. spatial details original toolbox task similarly, mullen visual reception test # 25 also tests understanding of details in images. in this test, the child is presented with a sketch of a tulip, and the psychologist asks : see this flower. find one just like this. look for it here, while tracing their finger along a page filled with sketches of a tulip, a sunflower, a clover, and a daisy. the child is allowed to refer back to the tulip while choosing their answer. if the child points to the tulip, they pass the test. adaptation again, the objects in our benchmark are real, cropped objects from saycam and ego4d rather than clipart, and they come from more categories than just flower. additionally, because the models cannot \" point \" to the choices, the choices are passed as separate images and the correct answer is the index ( a, b, or c ) of the matching image. our final modification to the original measure is that to make it more difficult for a computer, we present the answer choices in their naturalistic backgrounds rather than cropped as in the nih baby toolbox®. in practice, we find the final modification necessary to make spatial details require a fine - grained understanding of detail, as matching identical images is trivial even for a small vision model. data collection to construct examples from both saycam and ego4d, we match objects with the label, but require that they come from different videos. the labels for each come from b. 1 and egotracks, respectively. note that the same object can appear multiple times within an example - for instance, the same chair, captured in two separate videos, can show up as two of the choices. in such cases, the model is forced to rely on spatial details such as orientation, perspective, and lighting,", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 18, "frag_id": 2, "text": "##ped as in the nih baby toolbox®. in practice, we find the final modification necessary to make spatial details require a fine - grained understanding of detail, as matching identical images is trivial even for a small vision model. data collection to construct examples from both saycam and ego4d, we match objects with the label, but require that they come from different videos. the labels for each come from b. 1 and egotracks, respectively. note that the same object can appear multiple times within an example - for instance, the same chair, captured in two separate videos, can show up as two of the choices. in such cases, the model is forced to rely on spatial details such as orientation, perspective, and lighting, to match identical occurrences. to ensure quality, we enforce a minimum object confidence of. 92 in the saycam annotations. to increase difficulty, we also require that objects have an area of less than half of the frame ’ s area. 18", "token_count": 200}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 19, "frag_id": 0, "text": "figure 11. comparison between sources of occlusion. left : object occlusion from a static camera and moving object. right : object occlusion from a moving camera and static object. each panel shows a top - down view of the scene along with the corresponding projected 2d video depicting the occlusion event. example prompt each finalized example is a prompt embedded with 1 image prompt and 3 image choices for which the following is an example : \" < image > which of the following is the same as this? ( a ) < image > ( b ) < image >, or ( c ) < image >? \" the model needs to output one of a, b, or c to be evaluated. b. 7. visual delayed response original toolbox task inspired by visual delayed response in the nih baby toolbox, we introduce an evaluation task designed to assess the spatiotemporal reasoning capabilities of visionlanguage models. more specifically, our task focuses on object tracking and spatial localization over time, requiring models to process multi - frame / video input to infer spatial trajectory and disappearance of a designated object. adaptation in the original nih baby toolbox task, a cartoon creature is placed in a frame with grey walls to its left and right. the creature moves from the center of the frame to behind either wall, and the child must identify which wall the creature hid behind. ( see figure 12 ) translating this task to real - world videos is challenging, as the synthetic examples from the toolbox portray an unrealistically ideal scenario. each toolbox example depicts a moving object observed from a static camera perspective, with simplified backgrounds and perfectly smooth motion trajectories. such controlled scenarios are rare in real - world footage, especially in egocentric videos captured from a toddler ’ s perspective. to address this challenge we exploit the frequent head movements captured in saycam footage, together with the figure 12. example of visual delayed response task, taken directly from the nih baby toolbox. fact that many objects in real - world scenes are largely stationary. by inverting the source of 2d object motion from a static camera with moving objects to a moving camera with stationary objects ( see figure 11 ), we are able to expand the dataset by over an order of magnitude. formally, the model is provided a video v = { f1, f2,... ft } and designated key object k. the video depicts the key object k moving within the", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 19, "frag_id": 1, "text": "footage, especially in egocentric videos captured from a toddler ’ s perspective. to address this challenge we exploit the frequent head movements captured in saycam footage, together with the figure 12. example of visual delayed response task, taken directly from the nih baby toolbox. fact that many objects in real - world scenes are largely stationary. by inverting the source of 2d object motion from a static camera with moving objects to a moving camera with stationary objects ( see figure 11 ), we are able to expand the dataset by over an order of magnitude. formally, the model is provided a video v = { f1, f2,... ft } and designated key object k. the video depicts the key object k moving within the field of view and eventually exiting the visible frame at time t∗≤t. the model ’ s objective is to predict the exit region r ∈r, where r denotes the set of possible frame boundaries through which the object may leave. we define two variants of this task, which differ in the set of selectable exit regions r provided to the model : • multi - choice setting : rm = { left, right, top, bottom, top - left, top - right, bottom - left, bottom - right } • binary setting : rb = { correct, opposite }, rb ⊆rm the multi - choice variant provides a comprehensive set of possible exit regions, where the model is given eight regions as selectable options. the binary variant is a simplified version of the task, where the model only chooses between two options : the correct exit region or the region directly opposite to it. the overall task can be summarized as a mapping fv dr ( v, k ) →r, where r ∈r. here, fv dr represents the function that, given a video v and designated key object k, predicts the exit region r ∈r through which the object leaves the frame. data collection saycam. collecting examples for the saycam variant of visual delayed response can be split into 3 stages : fil19", "token_count": 414}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 20, "frag_id": 0, "text": "tering with gpt annotations, filtering with object tracking, and manual labeling. stage 1. we first use the 1 fps annotations provided by gpt in b. 1, where each frame is labeled with a \" key object \" and \" objects \" attribute. the \" key object \" denotes a singular object being attended to in a particular frame ( if any ), and \" object \" denotes a list of all visible objects within the frame of view. we do an initial filtering for candidate clips by using a sliding window over the 1 fps frames of each longrange video. for a clip to pass the filter, the first half of frames in the window must have the same \" key object \", k. in addition, the second half of frames must not have k listed as a \" key object \" or be present in the \" objects \" list. from the 422990 initial clips, 17443 are passed as candidate clips to the next stage. stage 2. we then perform open - set object detection [ 55 ] over the 1 fps frames sampled from each candidate clip, where the only object class to be detected is the \" key object \" itself. an object tracking algorithm [ 65 ] is also used to track the \" key object \" over the full fps video. the clips are filtered according to the object tracks, where each track must satisfy all of the following : – start within the middle 70 % of the frame – appear in at least 10 consecutive frames – disappear for at least 10 consecutive frames before the full clip ends to help account for errors in the object detection / tracking, we purposefully loosen the filters and add additional measures for sporadic / false detections. from the 17443 initial clips, 3908 are passed as candidate clips to the next stage. stage 3. the final stage involves manually reviewing and hand - labeling each candidate example from the previous stage. we label not only for the ground truth exit direction, but also for a variety of annotations related to overall quality of the clip. in total, we annotate for camera motion, scene visibility, camera stability, occlusion, exit direction, and presence of multiple objects. a breakdown for each is provided as follows : – occlusion : { fully occluded, partially occluded, remains in view } – camera motion : { static, moving } – direction of exit : [ up, down, left, right ] – scene visibility : {", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 20, "frag_id": 1, "text": "3908 are passed as candidate clips to the next stage. stage 3. the final stage involves manually reviewing and hand - labeling each candidate example from the previous stage. we label not only for the ground truth exit direction, but also for a variety of annotations related to overall quality of the clip. in total, we annotate for camera motion, scene visibility, camera stability, occlusion, exit direction, and presence of multiple objects. a breakdown for each is provided as follows : – occlusion : { fully occluded, partially occluded, remains in view } – camera motion : { static, moving } – direction of exit : [ up, down, left, right ] – scene visibility : { excellent, good, fair, poor } – camera stability : { very stable, stable, shaky, very shaky } – multiple objects : { true, false } we then filter for valid high - quality clips according to the following criteria : – object must become fully occluded – direction of exit cannot be contradicting ( both left & right, or both up & down ) – scene visibility better than \" poor \" – camera stability better than \" very shaky \" figure 13. visualization of evaluation methods for visual delayed response task. left : binary evaluation for the binary setting, where there is only a correct and opposite incorrect option. right : exact and adjacent evaluation for the multi - region setting, where the correct region for exact is defined by only the green region, and the correct region for adjacent is defined by both the green and yellow regions. from the 3908 initial clips, 2380 are passed as final clips for the dataset. ego4d. data collection for ego4d follows a very similar structure to the saycam process, with the addition of tracked object annotations being already provided by the ego4d dataset. we use a sliding window over each longrange video ’ s object tracks and filter for all of the following : – object is present in first half of window and disappears in second half – object bounding box ≥40000 pixels ( 13 % of screen ) – starts within the middle 50 % frame each clip is also manually reviewed / labelled according to the same procedure as saycam data collection multi - frame versions. since the average clip can range from 100 - 150 frames, we manually create multiframe counterparts to each example. more specifically, we look to obtain 1 representative object frame and 3 -", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 20, "frag_id": 2, "text": "follows a very similar structure to the saycam process, with the addition of tracked object annotations being already provided by the ego4d dataset. we use a sliding window over each longrange video ’ s object tracks and filter for all of the following : – object is present in first half of window and disappears in second half – object bounding box ≥40000 pixels ( 13 % of screen ) – starts within the middle 50 % frame each clip is also manually reviewed / labelled according to the same procedure as saycam data collection multi - frame versions. since the average clip can range from 100 - 150 frames, we manually create multiframe counterparts to each example. more specifically, we look to obtain 1 representative object frame and 3 - 9 linearly sampled frames that best showcase the object motion / disappearance in a given clip. to do this, we first find the specific frame for three different fields : full object frame, start occlusion frame, and end occlusion frame. the full object frame is always used as the first frame in the multi - frame sequence, and shows the key object in clear view. the start / end occlusion frames mark the interval with which the key object becomes occluded. a random number of frames ( 3 - 9 ) are linearly sampled along this interval to complete the multi - frame sequence for a given clip. evaluation evaluation is performed over three separate variants : exact and adjacent in the multi - choice setting, and binary in the binary setting ( see figure 13 ). accuracy is used as the metric for evaluation, defined as the fraction of predictions considered correct across all trials for a given variant. in the multi - choice setting : 20", "token_count": 342}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 21, "frag_id": 0, "text": "• exact : only the labelled ground truth region is counted as correct. • adjacent : both the labelled ground truth region and its two adjacent regions are counted as correct. this helps account for small ambiguities in the ground truth label. in the binary setting : • binary : a prediction is correct if it matches the \" correct \" region rather than the \" opposite \" region. example prompt each finalized example includes a series of < image > tags or singular < video > tag, followed by the prompt. to be properly evaluated, the model must output exactly one option from the choices given in the prompt. example from binary setting with multi - frame input : \" < image > < image > < image > < image > does the bottle leave the frame through the right side of the frame or the left side of the frame? respond only with ’ right ’ or ’ left ’. \" example from multi - choice setting with video input : \" < video > which part of the frame do the toys leave from? respond only with one of : ’ top ’, ’ bottom ’, ’ left ’, ’ right ’, ’ top right ’, ’ top left ’, ’ bottom right ’, or ’ bottom left ’. \" b. 8. memory original toolbox task the memory task in the nih toolbox is designed to measure how well toddlers ( 22 – 42 months old ) learn and remember new information using a touchscreen. children play a short game where they “ feed ” hungry cartoon animals by touching them on the screen. the test is divided into the learning phase and the test phase. • learning phase : children see pairs of animals and are told to touch the new animal — the one they have not fed before. they complete 10 trials and receive feedback so they can learn the rules and memorize the animals seen in this phase. • testing phase : children again see pairs of animals and told to touch the new animal, where each old animal from the learning phase appears twice, each time paired with a different new animal. they complete 20 trials and receive no feedback so correct responses reflect their memory for animals in learning phase. the animals were selected based on how many 24 - month old infants were familiar with them according to data from the mb - cdi wordbank. performance is scored based on whether the child touches the correct animal in the testing phase, along with optional reaction time measures to show how quickly they respond. adaptation to simplify the problem and enlarge the potential", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 21, "frag_id": 1, "text": "10 trials and receive feedback so they can learn the rules and memorize the animals seen in this phase. • testing phase : children again see pairs of animals and told to touch the new animal, where each old animal from the learning phase appears twice, each time paired with a different new animal. they complete 20 trials and receive no feedback so correct responses reflect their memory for animals in learning phase. the animals were selected based on how many 24 - month old infants were familiar with them according to data from the mb - cdi wordbank. performance is scored based on whether the child touches the correct animal in the testing phase, along with optional reaction time measures to show how quickly they respond. adaptation to simplify the problem and enlarge the potential dataset size, we define the set of word labels used in the learning phase as wlearn = { w1, w2,..., wk }, where each wi corresponds to an image xi ∈xlearn. these image – label pairs ( xi, wi ) serve as the stimuli to be memorized during the learning phase. we further sample 2k additional word labels for the testing phase, wtest = { wk + 1, wk + 2,..., w3k }, each associated with a novel image xj ∈xtest. at each round t, the vision – language model ( vlm ) receives an input consisting of two images and a text prompt : it = { xpt, xqt, pt }, where xpt, xqt are the image inputs and pt is the corresponding prompt. • learning phase : the learning phase contains k rounds : ilearn t = ( { x1, p1 }, t = 1, { xt−1, xt, pt }, 2 ≤t ≤k, where the two images in the second case are presented in random order. this setup enables the model to incrementally associate visual concepts across consecutive rounds within a single context window. • testing phase : the testing phase consists of 2k rounds, each comparing a learned stimulus with a new one : itest t = { xi ( t ), xj ( t ), p test t }, xi ( t ) ∈xlearn, xj ( t ) ∈xtest. here, xi ( t ) is a previously seen image and xj ( t ) a novel one. the model must identify which image corresponds to", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 21, "frag_id": 2, "text": "}, t = 1, { xt−1, xt, pt }, 2 ≤t ≤k, where the two images in the second case are presented in random order. this setup enables the model to incrementally associate visual concepts across consecutive rounds within a single context window. • testing phase : the testing phase consists of 2k rounds, each comparing a learned stimulus with a new one : itest t = { xi ( t ), xj ( t ), p test t }, xi ( t ) ∈xlearn, xj ( t ) ∈xtest. here, xi ( t ) is a previously seen image and xj ( t ) a novel one. the model must identify which image corresponds to the new concept described in p test t. evaluation each learned concept wi ∈wlearn is paired with two distinct new concepts : ( wi, wa ( i ) ), ( wi, wb ( i ) ), a ( i ), b ( i ) ∈ { k + 1,..., 3k }, a ( i ) = b ( i ), ( 1 ) forming two dyads per old stimulus and a total of 2k dyads in the testing phase. to mitigate the influence of random guessing, an old stimulus wi is considered successfully remembered only if both of its dyads are answered correctly : ri = ( 1, if both dyads for wi are correct, 0, otherwise. 21", "token_count": 301}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 22, "frag_id": 0, "text": "figure 14. a sample of our memory task adaptation. we use the mab - cdi words detected in saycam as the images to be memorized. the overall memory accuracy is then computed as accmem = 1 k k x i = 1 ri. in all experiments, we set k = 5, resulting in a total of 3k = 15 distinct word – image pairs. this design preserves the spirit of the original toolbox while adapting the procedure to the vlm ’ s limited context window. when designing the evaluation metric, we follow the structure of the original toolbox with appropriate simplifications. specifically, we remove the original intermediate 6 – 8 min delay settings between the learning and testing phases in our benchmark design. future extensions may incorporate external memory mechanisms such as retrieval - augmented generation ( rag ), or introduce irrelevant contexts between the two phases to simulate real - world temporal gaps. in this work, however, we focus exclusively on assessing the model ’ s in - context retrieval ability. data collection for the scalability of the memory task, we expanded the image set from the cartoon animals in the original toolbox to the objects in the saycam dataset, which also ensures that the items are familiar to children. we used a combination of annotation - based search scripts and automated vision models, including clip for object – text similarity and sam for object segmentation as shown in b. 1, to find and isolate frames where these objects appeared clearly. manual screening was also done after auto - filtering. this process allowed us to gather real - world visual examples of common objects seen by young children, supporting the creation of new learning and memory trials for our benchmark. the visual objects collected from saycam dataset will serve as our stimuli in the memory task. example prompt each finalized example is a list of prompts each embedded with 2 image choices, for which the following is an example : \" let ’ s try more. touch the new image. ( a ) < image > or ( b ) < image >. \" the model needs to output one of a or b to be evaluated. b. 9. who has more original toolbox task in the nih baby toolbox®, the who has more measure is poised as a simple narrative : there are two animals ; each of them is pictured with some number of the same object. which animal has more? adaptation in devcv toolbox, we remove the narrative aspect and replace the clipart objects with", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 22, "frag_id": 1, "text": "as our stimuli in the memory task. example prompt each finalized example is a list of prompts each embedded with 2 image choices, for which the following is an example : \" let ’ s try more. touch the new image. ( a ) < image > or ( b ) < image >. \" the model needs to output one of a or b to be evaluated. b. 9. who has more original toolbox task in the nih baby toolbox®, the who has more measure is poised as a simple narrative : there are two animals ; each of them is pictured with some number of the same object. which animal has more? adaptation in devcv toolbox, we remove the narrative aspect and replace the clipart objects with naturalistic saycam and ego4d objects. in the naturalistic adaptation, the objects are not necessarily identical and appear in their naturalistic backgrounds ; in the synthetic adaptation, the objects are perfectly identical, cropped, and pasted onto black backgrounds in matching layouts. the model is prompted to identify whether the first or second has more. data collection in the synthetic variants, to pick the two quantities to compare, we first sample a number between one and ten. then, from the numbers remaining that are lower than the first one, we sample the second quantity. we do this to ensure 22", "token_count": 270}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 23, "frag_id": 0, "text": "a balanced distribution in the differences in numbers being compared for each answer. the objects being compared come from the annotations in b. 1 and egotracks for saycam and ego4d, respectively. for the test sets in the naturalistic adaptations, each example is hand annotated by two separate human experts to cross - validate annotation quality. specifically, the first human expert labels video frames with an object type and the number of that object. next, for each the frames that the first annotator labeled, the second annotator labels the number of the named object in each, without access to the first ’ s annotation. with both labels for each frame, we construct an example for every pair of frames of with objects of the same type for which both annotators would have arrived at the same answer answer as to which has more had they based their decision solely on their count annotation. as an example, say the first annotator labels frame a as having 5 cups, and frame b as having 6 cups. if the second annotator labels 5 cups in frame a and 7 cups in frame b, we construct a who has more example from frames a and b ( despite the annotators giving frame b two different labels ) because 5 < 7 and 6 < 7. however, if the second annotator instead labeled frame b as having 5 cups, we do not construct a who has more example from frames a and b, because the two annotators would have given different answers for such an example. in constructing who has more, we observe that some objects occur in multiples more than others, and each object follows a unique ( and usually nonuniform ) distribution of quantity - for example, the number of hands visible in a frame is usually one or two and rarely another number, while an object like books could reasonably be seen in any quantity between one and ten. additionally, we observe that given the differences in settings and scene perspective, the distributions of object types as well as quantity per object is inherently different for saycam and ego4d. example prompt each finalized example is a prompt embedded with 2 image choices for which the following is an example : which of the following has more of shoe? ( a ) < image >, or ( b ) < image >? \" the model needs to output one of a or b to be evaluated. b. 10. subitizing original toolbox task in the ni", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 23, "frag_id": 1, "text": "quantity - for example, the number of hands visible in a frame is usually one or two and rarely another number, while an object like books could reasonably be seen in any quantity between one and ten. additionally, we observe that given the differences in settings and scene perspective, the distributions of object types as well as quantity per object is inherently different for saycam and ego4d. example prompt each finalized example is a prompt embedded with 2 image choices for which the following is an example : which of the following has more of shoe? ( a ) < image >, or ( b ) < image >? \" the model needs to output one of a or b to be evaluated. b. 10. subitizing original toolbox task in the nih baby toolbox®, the infant sees one to four colored dots for only one second, then an audio prompt requests the number of dots. importantly, the dots are not shown for long enough to be counted one at a time - subitize is intended to measure the ability to quickly identify small quantities, without counting. adaptation to construct subitizing in devcv toolbox, we paste objects onto random locations on black frames, in random quantities between one and four. to simulate the \" one second flash \", we insert empty frames before and after the frame including the objects. data collection in the saycam variant, the objects being pasted come from frames cropped by the bounding boxes obtained in section b. 1, subjected to a minimum confidence of. 95. in the ego4d variant, the bounding boxes come from egotracks, and only objects in the mab - cdi vocabulary are included. example prompt each finalized example is a prompt embedded with 1 blank frame, 1 image prompt, and 1 blank frame for which the following is an example : < image > < image > < image > how many of apple did you see? answer with 1, 2, 3, or 4. \" the model needs to output one of 1, 2, 3, or 4 to be evaluated. b. 11. object counting original toolbox task in the nih baby toolbox®, infants are shown some number of an object on a screen, and asked to count them. unlike the subitize measure, there is no time limit - participants have time to count each item individually. adaptation in devcv toolbox, the examples are constructed in the same way as the subitizingexamples, except", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 23, "frag_id": 2, "text": ", 1 image prompt, and 1 blank frame for which the following is an example : < image > < image > < image > how many of apple did you see? answer with 1, 2, 3, or 4. \" the model needs to output one of 1, 2, 3, or 4 to be evaluated. b. 11. object counting original toolbox task in the nih baby toolbox®, infants are shown some number of an object on a screen, and asked to count them. unlike the subitize measure, there is no time limit - participants have time to count each item individually. adaptation in devcv toolbox, the examples are constructed in the same way as the subitizingexamples, except the quantities are between one and twelve, and there are no blank frames corresponding with the lack of a time limit. data collection the data collection for object countingis the same as for subitizing. example prompt each finalized example is a prompt embedded with 1 image prompt, for which the following is an example : < image > how many of chair did you see? answer with a number 1 - 12. \" the model needs to output a number between 1 and 12 to be evaluated. 23", "token_count": 248}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 24, "frag_id": 0, "text": "c. human survey c. 1. small - scale human adult test to confirm the validity of devcv toolbox, we collect small - scale adult performance data on eight of the ten tasks. we omit looking while listening and subitizing as their examples are directly taken from picture vocabulary and object counting, respectively. in total, we have data from n = 11 adult participants, each completing 10 trials per task for the saycam variants of picture vocabulary, localization, left / right, spatial details, visual delayed response, and object counting, and 5 trials per task for the saycam variants of naturalistic who has more and synthetic who has more, and as well as the ego4d variants of all tasks other than memory. participants completed 30 consecutive rounds of each memory variant, requiring a maximum memory of 29 distinct images. results for each task can be found in the human performance rows of tables 4 and 9. in summary, our participants achieved an average accuracy of 93. 0 on all saycam tasks and 93. 5 on all ego4d tasks, for both of which they far outperform any model. from this, we conclude 1 ) devcv toolbox is a valid discriminator of vision fms with adult performance as a strong upper bound, and 2 ) the saycam and ego4d variants have roughly similar complexity and ambiguity for humans. c. 2. children helping science tests to further examine the developmental fidelity of devcv toolbox, an irb review process is currently underway to extend this survey to a large scale children survey, where we plan to collect response data for each task from children of the ages recommended for the corresponding nih baby toolbox® measure. to this end, we collaborated with expert psychologists to develop child - friendly web interfaces for selected tasks and prepared them for deployment on the online developmental research platform children helping science ( chs ) [ 49 ]. chs is a widely used, home - based platform through which families can participate in browser - based developmental studies run by researchers worldwide. by adapting our saycam - based tasks ( pv, vdr and memory ) to chs, we aim to collect performance from young children under conditions analogous to the nih baby toolbox®. at the time of writing, the studies are under review and not yet live. we show two examples of our task ui design in figures15 and 16. taking pv as an example ( figure 15 ), to approximate the modality of", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 24, "frag_id": 1, "text": "collaborated with expert psychologists to develop child - friendly web interfaces for selected tasks and prepared them for deployment on the online developmental research platform children helping science ( chs ) [ 49 ]. chs is a widely used, home - based platform through which families can participate in browser - based developmental studies run by researchers worldwide. by adapting our saycam - based tasks ( pv, vdr and memory ) to chs, we aim to collect performance from young children under conditions analogous to the nih baby toolbox®. at the time of writing, the studies are under review and not yet live. we show two examples of our task ui design in figures15 and 16. taking pv as an example ( figure 15 ), to approximate the modality of the original nih baby toolbox®task, which relies on audio - visual interaction with spoken prompts and observed child responses, we design an audio & video test page to verify that instructions and target words can be delivered clearly via audio and that the child ’ s webcam setup is functioning for basic participation monitoring. the instruction page provides caregiver - friendly guidance in both text and spoken form. finally, the trial pages present each example in a clean 2×2 grid of four large image options, paired with an audio prompt of the target word, optimizing engagement and accessibility for infants and toddlers while staying faithful to the original task format. following the pv setup, vdr also has an initial audio & video test page, along with an instruction page to provide context of the experiment to the caregiver. the trial page for this task ( see figure 16 ) displays the object that should be tracked, along with the video clip itself and two selectable arrows to submit an answer. since mp4 with interactive display is not yet supported on the website, a gif is created in its place. the beginning 5 seconds of the gif show the first frame with a countdown, then the clip is played as normal and followed with another 5 second buffer to show that the video has ended. to help the caregiver and child understand the experiment, an interactive demo is played as the first 3 trials to showcase how each one should be properly done. d. additional experiments & details d. 1. out - of - domain evaluation to test babyllava - v2 ’ s capability of generalizing to unseen data domain, we further evaluate it on a set of out - ofdomain ( ood ) tasks that share the", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 24, "frag_id": 2, "text": ". since mp4 with interactive display is not yet supported on the website, a gif is created in its place. the beginning 5 seconds of the gif show the first frame with a countdown, then the clip is played as normal and followed with another 5 second buffer to show that the video has ended. to help the caregiver and child understand the experiment, an interactive demo is played as the first 3 trials to showcase how each one should be properly done. d. additional experiments & details d. 1. out - of - domain evaluation to test babyllava - v2 ’ s capability of generalizing to unseen data domain, we further evaluate it on a set of out - ofdomain ( ood ) tasks that share the same structure as the in - domain benchmarks but differ in their visual domains. we consider two ood settings : ( 1 ) ego4d - based tasks use egocentric videos from the ego4d dataset [ 13 ], which remain first - person and naturalistic but introduce distinct environments and contexts. ( 2 ) babytoolbox - based tasks correspond directly to standardized developmental psychology and clinical assessments, where the visual stimuli are abstract, non - egocentric cartoon images. the detailed test results are reflected in table 9 and table 10. d. 2. importance of the pretraining stage to evaluate the contribution of the pretraining stage, we compare two variants of babyllava - v2 : ( 1 ) the full model trained with stage 0 – 2 pretraining before instruction tuning, and ( 2 ) a randomly initialized model that skips pretraining and is trained only with stage 3. for both variants, we finetune using different fractions of the instruction dataset and evaluate each model on in - domain tasks. as shown in figure 17, the pretrained model consistently outperforms the non - pretrained variant across all data fractions. the gap is especially pronounced when the instruction data is limited, demonstrating that pretraining provides a strong and sample - efficient initialization for downstream instruction tuning. as the instruction data fraction increases, both models improve, reflecting a clear scaling - like trend qualitatively consistent with observations in large24", "token_count": 449}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 25, "frag_id": 0, "text": "figure 15. user interface design for our chs - adapted picture vocabulary task. figure 16. user interface design for the trial page of visual delayed response task on chs. figure 17. devcv toolbox overall performance on different instruction tuning data fraction. scale model studies [ 17, 22 ]. this suggests that datadependent performance gains also exist in compact, developmentally inspired models, while pretraining remains a crucial component for achieving data - efficient learning. d. 3. synthetic caption generation we study the impact of noisy visual - alignment in the naturalistic child - directed utterances transcribed in the pretraining dataset by replacing them with video captions generated by gpt - 4o. to encourage diversity in the generated captions and ensure they remain close to the style of the original dataset, we include 10 randomly sampled transcriptions in each prompt. the transcriptions are sampled from a pool of the 1, 000 highest confidence transcriptions in the original dataset that contain at least one noun and more than three words. these heuristic filters help ensure that the sampled transcriptions contain stylistic information rather than simple phrases that are common in the dataset like \" wow \" or \" let ’ s go \". the pool of 1, 000 transcriptions are manually screened to remove uninformative transcriptions that passed the filtering step. the full prompt to gpt - 4o is shown in figure 18 and an example of a generated caption is shown in figure 19. figure 18. full prompt for pretraining data ablation 25", "token_count": 315}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 26, "frag_id": 0, "text": "table 9. performance comparison across models on devcv toolbox out - of - domain tasks ( ego4d ). different background colors denote different model families. we report accuracy ( % ) for all tasks. model overall count leftright spatial pv memory localization visual delay response who has more binary multi - exact multi - adjacent synthetic naturalistic upper bound human performance 93. 5 96. 4 98. 2 96. 4 96. 4 98. 8 90. 9 100 58. 2 100 100 92. 7 proprietary models gpt - 4o 67. 6 62. 1 45. 1 94. 7 85. 3 100 80. 4 45. 5 13. 2 48. 3 84. 3 84. 6 gpt - 5 86. 7 77. 5 88. 0 96. 8 91. 9 100 88. 7 94. 4 50. 3 82. 6 94. 6 88. 5 gemini - 2. 5 - flash 77. 7 72. 9 49. 6 86. 7 92. 5 99. 2 88. 4 80. 6 37. 1 70. 2 97. 8 80. 1 gemini - 2. 5 - pro 88. 2 81. 9 88. 0 94. 8 91. 9 100 90. 2 91. 3 50. 3 87. 9 96. 5 97. 8 open - source models llava - onevision - 0. 5b 39. 4 43. 9 32. 6 33. 3 27. 7 22. 6 21. 6 73. 0 15. 2 67. 7 46. 8 49. 4 internvl3. 5 - 1b 43. 7 34. 7 34. 0 34. 1 33. 8 24. 9 60. 7 73. 9 16. 9 68. 5 49. 0 49. 9 qwen2. 5 - vl - 3b 48. 1 35. 7 32. 6 44. 1 41. 9 25. 7 86. 7 79. 8 28. 9 51. 1 50. 2 53. 4 baby models ( ours ) babyllava - v2 41. 1 33. 9 32. 9 42. 4 29. 8 40. 7 30. 0 55. 3 17. 7 37. 1 86. 0 45. 8 lower bound random guess 31. 8 8. 33 33. 3 33. 3 25. 0 25. 0 25. 0 50. 0 12. 5 37. 5 50. 0 50. 0 table 10. performance on ni", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 26, "frag_id": 1, "text": "9 68. 5 49. 0 49. 9 qwen2. 5 - vl - 3b 48. 1 35. 7 32. 6 44. 1 41. 9 25. 7 86. 7 79. 8 28. 9 51. 1 50. 2 53. 4 baby models ( ours ) babyllava - v2 41. 1 33. 9 32. 9 42. 4 29. 8 40. 7 30. 0 55. 3 17. 7 37. 1 86. 0 45. 8 lower bound random guess 31. 8 8. 33 33. 3 33. 3 25. 0 25. 0 25. 0 50. 0 12. 5 37. 5 50. 0 50. 0 table 10. performance on nih baby toolbox out - of - domain tasks. we report the # correct / # total for all tasks. model who has more count mullen visual reception babyllava - v2 13 / 24 2 / 6 3 / 12 figure 19. example of caption generated by gpt - 4o table 11. comparison between gemini - 2. 5 - flash performance with different prompting strategies prompt type count leftright standard 69 55 one - shot 66 82 alternate prompt 1 55 54 alternate prompt 2 67 56 d. 4. prompting experiment finally, we complete a prompting experiment to show the stability of devcv toolbox examples with respect to commercial models, the results of which are shown in table 11. we select left / right and object counting for this experiment, as we found that commercial models had the lowest and most variable performance on these. for both tasks, 100 examples are randomly selected and presented to gemini - 2. 5 - flash with a standard prompt, a one - shot prompt, and two variations of the standard prompt, called alternate prompt 1 and alternate prompt 2. the standard prompt is the one used in all other experiments, and the one shot - prompt is a prompt that includes one other example, with its correct answer, prepended to the standard prompt. for object counting, alternate prompt 1 does not give the object ’ s name to be counted, e. g. \" < image > how many objects do you see? \", which we see drops performance, which is intuitive because large models thrive on context, in this case the name of the object to be counted. alternate prompt 2 gives more detail, e. g. < image > count the flora very closely, starting from one. keep track of which ones have already", "token_count": 500}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 26, "frag_id": 2, "text": "two variations of the standard prompt, called alternate prompt 1 and alternate prompt 2. the standard prompt is the one used in all other experiments, and the one shot - prompt is a prompt that includes one other example, with its correct answer, prepended to the standard prompt. for object counting, alternate prompt 1 does not give the object ’ s name to be counted, e. g. \" < image > how many objects do you see? \", which we see drops performance, which is intuitive because large models thrive on context, in this case the name of the object to be counted. alternate prompt 2 gives more detail, e. g. < image > count the flora very closely, starting from one. keep track of which ones have already been counted and what number you ’ ve counted to thus far. then, report how many flora you counted. \". unsurprisingly, alternate prompt 2 does not improve performance, showing that 1 ) the standard prompt was sufficient and 2 ) gemini - 2. 5 - flash has capable instruction - following capabilities. for object counting, we find that a one - shot prompt does not boost performance. for left / right, the standard prompt gives each image token interleaved with their answer labels, e. g. \" < image > which of the following 26", "token_count": 264}
{"doc_id": "arxiv_251210932_babyvlm_v2", "page": 27, "frag_id": 0, "text": "is the same as this? ( a ) < image > ( b ) < image > ( c ) < image > \". in alternate prompt 1, we undo this interleaving, resulting in \" < image > < image > < image > < image > which of the following is the same as the first one? ( a ) the second one, ( b ) the third one, or ( c ) the fourth one? \". in alternate prompt 2, we interleave even more, by giving some descriptive text before the prompt image, e. g. \" here is an image : < image >. which of the following is the same as it? ( a ) < image >, ( b ) < image >, or ( c ) < image >? \". intuitively we expect alternate prompt 2 to be the easiest, alternate prompt 1 to be the hardest, and the standard prompt to fall in between. however, we find that none of these prompts elicits significantly different performance, however, the one - shot prompt significantly boosts performance. these two findings show the robustness of gemini - 2. 5 - flash, and the complexity of left / right, respectively. 27", "token_count": 244}
