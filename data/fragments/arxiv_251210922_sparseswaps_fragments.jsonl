{"doc_id": "arxiv_251210922_sparseswaps", "page": 1, "frag_id": 0, "text": "sparseswaps : tractable llm pruning mask refinement at scale max zimmer, christophe roux, moritz wagner, deborah hendrych, sebastian pokutta department for ai in society, science, and technology, zuse institute berlin, germany institute of mathematics, technische universit¨at berlin, germany { zimmer, roux, wagner, hendrych, pokutta } @ zib. de abstract the resource requirements of neural networks can be significantly reduced through pruning – the removal of seemingly less important parameters. however, with the rise of large language models ( llms ), full retraining to recover pruning - induced performance degradation is often prohibitive and classical approaches such as global magnitude pruning are suboptimal on transformer architectures. state - of - the - art methods hence solve a layer - wise mask selection problem, the problem of finding a pruning mask which minimizes the per - layer pruning error on a small set of calibration data. exactly solving this problem to optimality using integer programming ( ip ) solvers is computationally infeasible due to its combinatorial nature and the size of the search space, and existing approaches therefore rely on approximations or heuristics. in this work, we demonstrate that the mask selection problem can be made drastically more tractable at llm scale. to that end, we decouple the rows by enforcing equal sparsity levels per row. this allows us to derive optimal 1 - swaps ( exchanging one kept and one pruned weight ) that can be computed efficiently using the gram matrix of the calibration data. using these observations, we propose a tractable and simple 1 - swap algorithm that warm starts from any pruning mask, runs efficiently on gpus at llm scale, and is essentially hyperparameter - free. we demonstrate that our approach reduces per - layer pruning error by up to 60 % over wanda ( sun et al., 2023 ) and consistently improves perplexity and zero - shot accuracy across state - of - the - art gpt architectures. 1 introduction pruning after training ( han et al., 2015 ; gale et al., 2019 ; lin et al., 2020 ; hoefler et al., 2021 ; zimmer et al., 2025 ) is a state - of -", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 1, "frag_id": 1, "text": "these observations, we propose a tractable and simple 1 - swap algorithm that warm starts from any pruning mask, runs efficiently on gpus at llm scale, and is essentially hyperparameter - free. we demonstrate that our approach reduces per - layer pruning error by up to 60 % over wanda ( sun et al., 2023 ) and consistently improves perplexity and zero - shot accuracy across state - of - the - art gpt architectures. 1 introduction pruning after training ( han et al., 2015 ; gale et al., 2019 ; lin et al., 2020 ; hoefler et al., 2021 ; zimmer et al., 2025 ) is a state - of - the - art technique to reduce the resource requirements of neural networks. a simple yet effective approach to obtain such sparse models starts from a pretrained dense model, removes seemingly unimportant parameters based on their magnitude, and requires retraining to compensate for pruning - induced performance degradation. however, while the inexpensive, data - free magnitude criterion has often achieved strong performance on traditional architectures ( gale et al., 2019 ; zimmer et al., 2023b ), pruning has undergone a paradigm shift with the rise of large pretrained foundation models, particularly llms. first, the size of the models has shifted the focus toward retraining - free pruning criteria, as retraining is often computationally expensive if not infeasible, with parameter - efficient fine - tuning ( lialin et al., 2023 ; zimmer et al., 2023a ) being an exception. secondly, systematic activation outliers ( dettmers et al., 2022 ) and highly important super - weights ( yu et al., 2025 ) in sufficiently large transformers ( vaswani et al., 2017 ) have rendered magnitude pruning no better than random pruning for llms ( sun et al., 2023 ; yin et al., 2023 ). lastly, state - of - the - art methods ( frantar & alistarh, 2023 ; sun et al., 2023 ; zhang et al., 2024 ) prune layer - wise : they split the pruning problem into per - layer subproblems, pruning layers sequentially and independently using a small calibration dataset to", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 1, "frag_id": 2, "text": "( dettmers et al., 2022 ) and highly important super - weights ( yu et al., 2025 ) in sufficiently large transformers ( vaswani et al., 2017 ) have rendered magnitude pruning no better than random pruning for llms ( sun et al., 2023 ; yin et al., 2023 ). lastly, state - of - the - art methods ( frantar & alistarh, 2023 ; sun et al., 2023 ; zhang et al., 2024 ) prune layer - wise : they split the pruning problem into per - layer subproblems, pruning layers sequentially and independently using a small calibration dataset to estimate parameter importance. rather than optimizing the global loss, such approaches minimize a per - layer local pruning loss. specifically, for a single layer with calibration input matrix 1 arxiv : 2512. 10922v1 [ cs. lg ] 11 dec 2025", "token_count": 210}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 2, "frag_id": 0, "text": "x ∈rdin×b and weights w ∈rdout×din, the objective becomes min m [UNK] − ( m [UNK] ) [UNK] f, ( 1 ) where m ∈ { 0, 1 } dout×din is a binary pruning mask achieving a desired level of sparsity, e. g., [UNK] ≤k for unstructured sparsity, and [UNK] the element - wise multiplication or hadamard product. here, b = n · l with n being the number of samples in the calibration batch and l being the sequence length. solving this combinatorial mask selection problem to optimality is np - hard due to feature correlations : selecting k of dout · din weights yields a cardinality - constrained binary quadratic program ( a best - subset selection variant ). even for a single row i, the problem reduces to min mi [UNK] i x − ( mi [UNK] ) [UNK] 2 f = min mi b x k = 1 din x j = 1 ( 1 −mij ) wijxjk 2, where wi ∈rdin and mi ∈ { 0, 1 } din denote the i - th row of w and m, respectively. while ip solvers could theoretically provide optimal solutions, the combinatorial search over mask entries makes this infeasible for llms. in practice, existing methods therefore relax equation 1 or approximate it. however, with deployed llms now serving millions of users, it becomes increasingly worthwhile to invest substantial resources to obtain pruned models that reach high performance, because the pruning cost is paid once during training whereas inference costs scale with the number of requests. in this work, we revisit the per - layer mask selection problem and demonstrate that it can be operationalized at llm scale, enabling monotone improvements with each optimization step rather than relying on proxy importance scores. to that end, we observe that enforcing equal sparsitylevel across rows ensures row - wise separability that yields independent objectives. this makes the problem drastically more tractable and leads to good practical performance for llms. instead of trying to obtain exact solutions via ip solvers, we instead propose a gpu - accelerated local optimization algorithm based on 1 - swaps ( exchanging one kept and one pruned weight ) that perform exact and efficient local refinement with incremental cost updates using the gram matrix g = [UNK] monotonically decrease the objective from any warm start. the resulting method", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 2, "frag_id": 1, "text": "demonstrate that it can be operationalized at llm scale, enabling monotone improvements with each optimization step rather than relying on proxy importance scores. to that end, we observe that enforcing equal sparsitylevel across rows ensures row - wise separability that yields independent objectives. this makes the problem drastically more tractable and leads to good practical performance for llms. instead of trying to obtain exact solutions via ip solvers, we instead propose a gpu - accelerated local optimization algorithm based on 1 - swaps ( exchanging one kept and one pruned weight ) that perform exact and efficient local refinement with incremental cost updates using the gram matrix g = [UNK] monotonically decrease the objective from any warm start. the resulting method, which we term sparseswaps, can start from any warm - start mask, evaluates the exact per - row quadratic loss, and is scalable, parallelizable across rows, almost hyperparameterfree, and deterministic for a fixed warm start. with only few 1 - swap iterations, it can reduce the perlayer pruning error by up to 60 % compared to wanda and improves final perplexity and zero - shot accuracy across architectures. our approach is a post - hoc refinement of existing pruning methods that can significantly improve upon the state of the art for unstructured, per - row, or n : m sparsity. contributions. our contributions are as follows : 1. making the mask selection problem tractable. we observe that a ) enforcing equal sparsity levels per row decouples the rows, and that b ) optimal 1 - swaps ( exchanging one kept and one pruned weight ) can be evaluated efficiently using the gram matrix g = [UNK] the calibration data, ensuring efficient lookups when determining the most beneficial swap. 2. sparseswaps : a practical post - hoc pruning algorithm. building on these observations, we propose sparseswaps, a plug - and - play 1 - swap refinement that starts from any warmstart mask and monotonically decreases the exact per - row objective under per - row or n : m constraints. in particular, sparseswaps is almost hyperparameter - free, completely parallelizable across rows and scalable to llms. 3. computational study. we verify our hypotheses on state - of - the - art generative pretrained transform", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 2, "frag_id": 2, "text": "evaluated efficiently using the gram matrix g = [UNK] the calibration data, ensuring efficient lookups when determining the most beneficial swap. 2. sparseswaps : a practical post - hoc pruning algorithm. building on these observations, we propose sparseswaps, a plug - and - play 1 - swap refinement that starts from any warmstart mask and monotonically decreases the exact per - row objective under per - row or n : m constraints. in particular, sparseswaps is almost hyperparameter - free, completely parallelizable across rows and scalable to llms. 3. computational study. we verify our hypotheses on state - of - the - art generative pretrained transformer ( gpt ) architectures and demonstrate that sparseswaps delivers large reductions in local pruning error ( up to 60 % per - layer error reduction over wanda ) and strong perplexity and zero - shot gains across a wide range of different llms. we conduct a series of ablations highlighting the advantages and drawbacks of the proposed approach. 2", "token_count": 225}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 3, "frag_id": 0, "text": "further related work. post - training pruning has a long history, and while magnitude pruning ( janowsky, 1989 ; han et al., 2015 ) is among the most popular criteria, it is not the only one ( cf. lecun et al., 1989 ; hassibi & stork, 1993 ; molchanov et al., 2016 ; yeom et al., 2019 ) ; see hoefler et al. ( 2021 ) for a comprehensive review. despite their simplicity, magnitude - based methods have been shown to produce sparse models competitive with far more complex algorithms for convolutional architectures ( gale et al., 2019 ; zimmer et al., 2023b ). for llms, however, magnitude pruning is argued to be unsuitable ( yin et al., 2023 ). consequently, there is growing interest in criteria beyond magnitude that achieve high performance on llms, and do so without requiring an expensive retraining procedure ( kwon et al., 2022 ; frantar & alistarh, 2023 ; sun et al., 2023 ; zhang et al., 2024 ). in this work, we develop a post - hoc refinement of existing methods, rather than proposing a new criterion. a related approach, dsnot ( zhang et al., 2023 ), also performs iterative weight swaps but differs significantly in its optimization strategy. inspired by dynamic sparse training ( cf. evci et al., 2020 ), dsnot prunes and regrows weights based on expected reconstruction - error improvements, using feature means and variances as surrogates. while effective, it does not guarantee a monotonic decrease in the true pruning error, whereas our method does. we compare the two empirically and find that sparseswaps consistently outperforms dsnot. subset selection and ip approaches. to solve equation 1 to global optimality, which can be formulated as a mixed - integer nonlinear program ( minlp ), several efficient open - source solvers are available, including scip ( bolusani et al., 2024 ), bonmin ( bonami et al., 2008 ), shot ( lundell et al., 2022 ) and boscia ( hendrych et al., 2025 ), among others. while we demonstrate how the problem can be made drastically more tractable, explicit solution", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 3, "frag_id": 1, "text": "a monotonic decrease in the true pruning error, whereas our method does. we compare the two empirically and find that sparseswaps consistently outperforms dsnot. subset selection and ip approaches. to solve equation 1 to global optimality, which can be formulated as a mixed - integer nonlinear program ( minlp ), several efficient open - source solvers are available, including scip ( bolusani et al., 2024 ), bonmin ( bonami et al., 2008 ), shot ( lundell et al., 2022 ) and boscia ( hendrych et al., 2025 ), among others. while we demonstrate how the problem can be made drastically more tractable, explicit solution remains very time - consuming for large instances ; we therefore opt for a gpu - friendly 1 - swap approach that avoids moving large tensors to the cpu for ip solvers. we leave such an extension for future work. 2 methodology in the following, we use uppercase letters for matrices ( w, x, m ) and lowercase letters for scalars and vectors. matrix entries are denoted wij for the element in row i, column j. rows of matrices are denoted with lowercase subscripts : wi represents the i - th row of matrix w. row and column slices use colon notation : xj, : for the j - th row and x :, k for the k - th column. we use [UNK] element - wise multiplication, [UNK] · [UNK] for frobenius norm, and [UNK] · [UNK] for ℓ2 norm. 2. 1 preliminaries before describing our proposed method, we make several assumptions and observations that make the problem tractable. 2. 1. 1 equal sparsity - level across rows does not need to be detrimental first, note that the objective in equation 1 decomposes into a sum of dout row - wise quadratics, [UNK] − ( m [UNK] ) [UNK] f = dout x i = 1 [UNK] i x − ( mi [UNK] ) [UNK] 2 2 where wi ∈rdin and mi ∈ { 0, 1 } din denote the i - th row of w and m, respectively. this alone does not make the corresponding minimization problem row - separable under unstructured sparsity, since the matrix cardinality constraint couples rows. in contrast, semi - structured patterns like perrow sparsity ( keep k per row ) or n : m ( pr", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 3, "frag_id": 2, "text": "1 equal sparsity - level across rows does not need to be detrimental first, note that the objective in equation 1 decomposes into a sum of dout row - wise quadratics, [UNK] − ( m [UNK] ) [UNK] f = dout x i = 1 [UNK] i x − ( mi [UNK] ) [UNK] 2 2 where wi ∈rdin and mi ∈ { 0, 1 } din denote the i - th row of w and m, respectively. this alone does not make the corresponding minimization problem row - separable under unstructured sparsity, since the matrix cardinality constraint couples rows. in contrast, semi - structured patterns like perrow sparsity ( keep k per row ) or n : m ( prune m −n per block of m weights ) enforce equal per - row sparsity, meaning that the rows are fully decoupled by definition. we therefore focus on the decoupled case, allowing to treat each row separately and reducing the problem to min mi [UNK] i x − ( mi [UNK] ) [UNK] 2 f = min mi b x k = 1 din x j = 1 ( 1 −mij ) wijxjk 2 ( 2 ) for each row i ∈ { 1,..., dout }. note that, for llms, sun et al. ( 2023 ) observe that row - wise sparsity benefits performance for both wanda and magnitude pruning. we therefore argue that enforcing per - row sparsity rather than unstructured sparsity is justified and need not harm final performance, at least for llms. 3", "token_count": 327}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 4, "frag_id": 0, "text": "2. 1. 2 avoiding intermediate value caching through the gram matrix formulation naively caching all b · din intermediate products wijxjk in equation 2 to evaluate candidate masks is prohibitive. to illustrate the scale, consider a single row of the largest matrix in a llama - 2 - 7b transformer block : the up proj matrix with input dimension din = 4096. with n = 128 samples and sequence length l = 4096 ( so b = n · l = 524, 288 ), caching all products wijxjk for that row requires 524, 288 × 4096 ≈2. 15 billion float32 values ( about 8. 6gb ) ; across all 11, 008 rows this totals about 94. 6tb. a straightforward way to circumvent this issue is to consider a single row and derive a compact formulation of the per - row loss through the gram matrix g def = [UNK]. for notational convenience, we drop the row index i throughout the remainder of this section and write w ∈rdin for the row ’ s weight vector and m ∈ { 0, 1 } din for its mask. the per - row loss from equation 2 is l def = [UNK] − ( m [UNK] ) [UNK] 2 f = ( w −m [UNK] ) [UNK] 2 f = ( w −m [UNK] ) [UNK] ( w −m [UNK] ). hence, the loss depends on x only through the gram matrix g, which can be accumulated on - the - fly as calibration samples pass through the layer : g = pb b = 1 x :, [UNK] :, b. unlike the per - row formulation in the introduction, which would require caching all b · din intermediate products wjxjk, we only need to maintain the din × din matrix g, which is a reduction from o ( b · din ) to o ( d2 in ) since din is typically much smaller than b. remark 1. a different ( but in practice slightly less efficient ) perspective on this reduction is through the unitary invariance of the frobenius norm used in our pruning objective : for any matrix a and unitary matrix u ( i. e., u −1 = u [UNK] ), we have [UNK] = [UNK]. this property enables significant computational savings through singular value decomposition ( svd ) compression. precisely, let x = uσv [UNK] the svd of calibration data x ∈rdin", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 4, "frag_id": 1, "text": "din intermediate products wjxjk, we only need to maintain the din × din matrix g, which is a reduction from o ( b · din ) to o ( d2 in ) since din is typically much smaller than b. remark 1. a different ( but in practice slightly less efficient ) perspective on this reduction is through the unitary invariance of the frobenius norm used in our pruning objective : for any matrix a and unitary matrix u ( i. e., u −1 = u [UNK] ), we have [UNK] = [UNK]. this property enables significant computational savings through singular value decomposition ( svd ) compression. precisely, let x = uσv [UNK] the svd of calibration data x ∈rdin×b. since b > din, we can write σ = [ σ ′ | 0 ] with σ ′ ∈rdin×din containing the singular values on its diagonal. the compressed representation is simply x ′ = uσ ′ ∈rdin×din. letting wp = w −m [UNK] for brevity, the key insight is that pruning decisions remain equivalent under this compression : [UNK] f = wpuσv [UNK] 2 f = [UNK] f = [UNK] [ σ ′ | 0 ] [UNK] f = [UNK] ′ [UNK] f = [UNK] ′ [UNK] f, where we used unitary invariance w. r. t. v and that the zero columns do not contribute to the frobenius norm. equivalently, we have x ′ x ′ [UNK] = uσ ′ σ ′ [UNK] [UNK] = [UNK] [UNK] = [UNK] = g, since [UNK] = σ ′ σ ′ [UNK] ( the zero columns of σ do not contribute ). since all subsequent operations depend solely on g, we accumulate g directly during calibration and avoid the svd entirely. 2. 1. 3 efficient 1 - swap evaluation through efficient cost lookups and updates while the global mask selection problem is np - hard, we can still make efficient progress via local search. starting from any feasible mask m ∈ { 0, 1 } din, the idea is to iteratively perform 1 - swaps that exchange one kept and one pruned weight to reduce l while preserving the sparsity level. the key observation is that each candidate swap can be evaluated in o ( 1 ) time using g and an auxiliary correlation vector c. to that end, let p def = { j : mj = 0 } denote the set of currently pruned weight indices and", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 4, "frag_id": 2, "text": "directly during calibration and avoid the svd entirely. 2. 1. 3 efficient 1 - swap evaluation through efficient cost lookups and updates while the global mask selection problem is np - hard, we can still make efficient progress via local search. starting from any feasible mask m ∈ { 0, 1 } din, the idea is to iteratively perform 1 - swaps that exchange one kept and one pruned weight to reduce l while preserving the sparsity level. the key observation is that each candidate swap can be evaluated in o ( 1 ) time using g and an auxiliary correlation vector c. to that end, let p def = { j : mj = 0 } denote the set of currently pruned weight indices and analogously u def = { j : mj = 1 } denote the set of unpruned ( kept ) weight indices. letting further [UNK] def = [UNK] j, : ∈rb denote the j - th row ( or feature vector ) of x, we can write ( w −m [UNK] ) [UNK] = din x j = 1 ( 1 −mj ) wjxj, : = x j∈p [UNK] j = [UNK], where we define the reconstruction residual r def = p j∈p [UNK] ∈rb, the total contribution of all pruned weights to the layer output. hence, clearly, the loss is l = [UNK] 2 = [UNK]. we define the correlation vector c = ( c1,..., cdin ) [UNK] with entries ci def = ⟨ [UNK], r ⟩ = ⟨ [UNK], x j∈p [UNK] ⟩ = x j∈p wj ⟨ [UNK], [UNK] ⟩ = x j∈p wjgij, which measures how each feature [UNK] correlates with the current residual. in vector form, c = g · ( ( 1 −m ) [UNK] ). 4", "token_count": 377}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 5, "frag_id": 0, "text": "swap cost formula. a 1 - swap removes index u ∈u from the unpruned set ( making it pruned ) and adds index p ∈p to the unpruned set ( making it unpruned ). the new residual is r ′ = r + [UNK] [UNK], and the change in loss is ∆lu, p = [UNK] ′ [UNK] 2 [UNK] 2 = [UNK] + [UNK] [UNK] 2 [UNK] 2 = 2wu ⟨ [UNK], r ⟩ + w2 [UNK] 2 −2wp ⟨ [UNK], r ⟩ + w2 [UNK] 2 −2wuwp ⟨ [UNK], [UNK] ⟩. using ci = ⟨ [UNK], r ⟩ and gij = ⟨ [UNK], [UNK] ⟩, this simplifies to ∆lu, p = 2wucu + w2 uguu −2wpcp + w2 pgpp −2wuwpgup. ( 3 ) given the precomputed gram matrix g and correlation vector c, each swap evaluation requires only scalar lookups. evaluating all possible swaps therefore costs o ( | u | · | p | ) total. by systematically testing all ( ( din − | p | ) · | p | ) possible 1 - swap operations ( adding one of | u | = din − | p | unpruned weights to p, removing one of | p | pruned weights from p ) evaluating the improvement using the above expression, we iteratively pick a best swap and update the mask until we have reached a satisfactory solution or one optimal w. r. t. 1 - swap operations. the only issue that remains is to update the correlation vector after each swap. correlation vector update. after accepting a swap ( u∗, p∗ ), the residual changes to r ′ = r + [UNK]. the correlation vector updates as ci ←ci + wu∗gi, u∗−wp∗gi, p∗, ( 4 ) or in vector form, c ←c + wu∗g :, u∗−wp∗g :, p∗. this only requires accessing two columns of g and costs o ( din ). why picking p and u separately is suboptimal. the interaction term −2wuwpgup in equation 3 shows that the best u depends on the chosen p ( and vice versa ). consequently, selecting p and u based on their individual effects can yield a detrimental swap, as the following example for the scalar case with b =", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 5, "frag_id": 1, "text": ", the residual changes to r ′ = r + [UNK]. the correlation vector updates as ci ←ci + wu∗gi, u∗−wp∗gi, p∗, ( 4 ) or in vector form, c ←c + wu∗g :, u∗−wp∗g :, p∗. this only requires accessing two columns of g and costs o ( din ). why picking p and u separately is suboptimal. the interaction term −2wuwpgup in equation 3 shows that the best u depends on the chosen p ( and vice versa ). consequently, selecting p and u based on their individual effects can yield a detrimental swap, as the following example for the scalar case with b = 1 and din = 4 shows. let the current pruned weight contributions be { + 10, −1 }, so r = 9 and l = 81, and let the unpruned weight contributions be { + 9, −9 }. the best 1 - swap is to unprune the −1 contribution and prune the −9 contribution, giving r ′ = 10 + ( −9 ) = 1 and l ′ = 1. however, if we instead greedily remove the best p in isolation, we unprune + 10 since ( 9 −10 ) 2 = 1 is minimal. we must then add one index ; the best addition in isolation to the original pruned - weight - contributions { + 10, −1 } is −9. in combination, the greedily chosen swap leads to r ′ = −1 + ( −9 ) = −10 and l ′ = 100, worse than the starting point. the error stems precisely from ignoring the interaction term when selecting ( p, u ). 2. 2 the sparseswaps algorithm building upon the preceding observations, we present our complete algorithm. the method takes as input a weight matrix w ∈rdout×din, the gram matrix g = [UNK] ( accumulated during calibration ), and a warmstart pruning mask m init ∈ { 0, 1 } dout×din that already satisfies the desired sparsity constraints, e. g., obtained from wanda ( sun et al., 2024 ) or ria ( zhang et al., 2024 ). the algorithm enforces any sparsity pattern that operates per - row, including per - row sparsity ( fixed number of zeros", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 5, "frag_id": 2, "text": "when selecting ( p, u ). 2. 2 the sparseswaps algorithm building upon the preceding observations, we present our complete algorithm. the method takes as input a weight matrix w ∈rdout×din, the gram matrix g = [UNK] ( accumulated during calibration ), and a warmstart pruning mask m init ∈ { 0, 1 } dout×din that already satisfies the desired sparsity constraints, e. g., obtained from wanda ( sun et al., 2024 ) or ria ( zhang et al., 2024 ). the algorithm enforces any sparsity pattern that operates per - row, including per - row sparsity ( fixed number of zeros per row, cf. sun et al. ( 2023 ) ) and structured n : m sparsity patterns ( e. g., 2 : 4 or 4 : 8, mishra et al. ( 2021 ) ). all swap operations maintain the sparsity constraints throughout optimization ; for n : m sparsity, swaps are restricted to occur only within the same n : m blocks, while for perrow sparsity, the total number of pruned weights per row remains constant. even though each swap only changes two mask entries, the cumulative effect of multiple swaps can dramatically reduce reconstruction error compared to the initial solution. we explain the main phases of the algorithm : preparation : we initialize with the warmstart mask m init. the gram matrix g is precomputed once per layer by accumulating g = p b x :, [UNK] :, b during the calibration forward pass. row processing ( lines 2 - 5 ) : for each row i, we extract weights w and current mask m, define pruned and unpruned index sets p and u, and compute the initial correlation vector c = g · ( ( 1 − m ) [UNK] ). 5", "token_count": 397}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 6, "frag_id": 0, "text": "algorithm 1 sparseswaps : 1 - swap pruning optimization require : w ∈rdout×din, gram matrix g = [UNK], warmstart mask m init, tmax ensure : improved pruning mask m 1 : m ←m init [UNK] with warmstart solution 2 : for i = 1 to dout do [UNK] each row independently 3 : w ←wi, :, m ←mi, : [UNK] row weights and mask 4 : p ← { j : mj = 0 }, u ← { j : mj = 1 } [UNK] and unpruned sets 5 : c ←g · ( ( 1 −m ) [UNK] ) [UNK] correlation vector 6 : for t = 1 to tmax do 7 : ( p∗, u∗ ) ←arg min ( p, u ) ∆lu, p [UNK] swap via equation 3 8 : if ∆lu∗, p∗ < 0 then [UNK] improves objective 9 : mp∗←1, mu∗←0 [UNK] swap 10 : p ← ( p \\ { p∗ } ) ∪ { u∗ }, u ← ( u \\ { u∗ } ) ∪ { p∗ } 11 : c ←c + wu∗g :, u∗−wp∗g :, p∗ [UNK] correlation vector 12 : else 13 : break [UNK] optimum reached 14 : end if 15 : end for 16 : mi, : ←m [UNK] optimized row 17 : end for 1 - swap optimization ( lines 6 - 15 ) : we iteratively find the swap ( p∗, u∗ ) minimizing ∆lu, p ( cf. equation 3 ) among feasible pairs, evaluating each candidate in o ( 1 ) time. if ∆lu∗, p∗ < 0, we accept the swap and update the correlation vector via equation 4 ; otherwise we terminate. at all times, the swaps are appropriately constrained : per - row sparsity allows any swap maintaining | p | constant, while n : m sparsity restricts swaps to within the same n : m blocks. the algorithm has complexity o ( dout · tmax · ( | p | · | u | + din ) ) per layer, where tmax is the maximum number of swap iterations per row. the | p | · | u | term comes from evaluating all candidate swaps ( each in o ( 1 ) time via equation 3 ), and the din term from the correlation vector update", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 6, "frag_id": 1, "text": "##lu∗, p∗ < 0, we accept the swap and update the correlation vector via equation 4 ; otherwise we terminate. at all times, the swaps are appropriately constrained : per - row sparsity allows any swap maintaining | p | constant, while n : m sparsity restricts swaps to within the same n : m blocks. the algorithm has complexity o ( dout · tmax · ( | p | · | u | + din ) ) per layer, where tmax is the maximum number of swap iterations per row. the | p | · | u | term comes from evaluating all candidate swaps ( each in o ( 1 ) time via equation 3 ), and the din term from the correlation vector update ( equation 4 ). in practice, several factors further reduce runtime. first, we find that even tmax = 1 or tmax = 2 can drastically reduce the local pruning error ; values around tmax = 25 often suffice to significantly lower model perplexity, with diminishing returns beyond tmax = 100. second, row - wise processing can be batched and vectorized, enabling parallel swap cost computations and mask updates, and rows can be distributed across gpus if needed. third, the gram matrix g is computed once per layer and shared across all rows, and several summands of equation 3 can be similarly precomputed once per layer. 3 experimental results we outline our general experimental approach, detailing datasets, architectures, and metrics. our code is publicly available at github. com / zib - iol / sparseswaps. our study focuses on language modeling within natural language processing ( nlp ). we use pretrained models from huggingface ( wolf et al., 2020 ), specifically llama - 3. 1 - 8b ( grattafiori et al., 2024 ), gemma - 2 - 9b ( riviere et al., 2024 ), yi - 1. 5 - 9b ( young et al., 2025 ), deepseek - 7b - base ( bi et al., 2024 ), and qwen2. 5 - 7b ( yang et al., 2025 ). for calibration, we randomly draw sequences of 2048 tokens from the c4 dataset ( raffel et al., 2020 ). for validation, we similarly pick", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 6, "frag_id": 2, "text": "use pretrained models from huggingface ( wolf et al., 2020 ), specifically llama - 3. 1 - 8b ( grattafiori et al., 2024 ), gemma - 2 - 9b ( riviere et al., 2024 ), yi - 1. 5 - 9b ( young et al., 2025 ), deepseek - 7b - base ( bi et al., 2024 ), and qwen2. 5 - 7b ( yang et al., 2025 ). for calibration, we randomly draw sequences of 2048 tokens from the c4 dataset ( raffel et al., 2020 ). for validation, we similarly pick 100 sequences from the validation split. the model performance is assessed via perplexity on the wikitext dataset ( merity et al., 2016 ) and zero - shot accuracy on the eleutherai evaluation set ( gao et al., 2023 ). following sun et al. ( 2023 ), we prune all linear layers, excluding the embedding and final linear head, with uniform sparsity allocation across layers. we provide experiments for unstructured and semi - structured sparsity patterns ( mishra et al., 2021 ). we use multiple random seeds throughout our experiments. 3. 1 mask refinement at scale we begin by verifying the effectiveness of sparseswaps. we make the following observations : 6", "token_count": 305}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 7, "frag_id": 0, "text": "table 1 : llama - 3. 1 - 8b : perplexity ( ↓ ) and mean relative reduction in pruning error ( ↑ ) versus number of 1 - swap iterations for 50 % and 60 % unstructured sparsity using wanda warmstart. number of 1 - swap iterations sparsity metric 0 1 2 5 10 25 50 100 200 50 % avg. rel. error reduction ( % ) 0. 00 6. 34 8. 77 12. 51 16. 38 23. 52 30. 04 36. 48 38. 95 perplexity 10. 13 10. 31 10. 40 10. 41 10. 39 10. 38 10. 27 10. 30 10. 34 60 % avg. rel. error reduction ( % ) 0. 00 8. 04 11. 04 15. 34 19. 64 26. 92 33. 58 39. 99 43. 74 perplexity 21. 52 21. 26 21. 51 21. 17 21. 01 20. 38 19. 74 18. 96 19. 17 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 layer 0 10 20 30 40 50 60 70 80 relative reduction in pruning error ( % ) per - layer reconstruction improvement over wanda attn. q proj attn. k proj attn. v proj attn. o proj mlp. gate proj mlp. up proj figure 1 : per - layer relative reduction in local pruning error compared to wanda. the plot shows result for llama - 3. 1 - 8b, 60 % unstructured sparsity and 100 1 - swap iterations. sparseswaps consistently improves state - of - the - art methods. table 2 summarizes the main results and reports perplexity ( upper half, lower is better ) and zero - shot accuracy ( lower half, higher is better ) for warmstart masks ( wanda, ria ) as well as their refinements using dsnot and sparseswaps. for both 60 % unstructured and 2 : 4 semi - structured sparsity, sparseswaps ( with 100 1 - swap iterations ) consistently reduces perplexity and improves zero - shot accuracy over wanda and ria warm start masks. while dsnot similarly yields improvements, it", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 7, "frag_id": 1, "text": "% unstructured sparsity and 100 1 - swap iterations. sparseswaps consistently improves state - of - the - art methods. table 2 summarizes the main results and reports perplexity ( upper half, lower is better ) and zero - shot accuracy ( lower half, higher is better ) for warmstart masks ( wanda, ria ) as well as their refinements using dsnot and sparseswaps. for both 60 % unstructured and 2 : 4 semi - structured sparsity, sparseswaps ( with 100 1 - swap iterations ) consistently reduces perplexity and improves zero - shot accuracy over wanda and ria warm start masks. while dsnot similarly yields improvements, it falls short of sparseswaps. note that we left the pruning criterion of dsnot, which partially uses the wanda saliency, unchanged, even when using ria warmstart. for unstructured ria, we report results when enforcing a perrow sparsity constraint ; while ria yields good ( and slightly better ) results when enforcing truely unstructured sparsity, we decided to include the results for the per - row setting as this allows direct refinement of the mask with sparseswaps and dsnot. sparseswaps successfully optimizes the per - layer pruning loss. figure 1 shows the per - layer reductions in local pruning error relative to a wanda warmstart, grouping layers by their corresponding transformer block of llama - 3. 1 - 8b. we observe drastic improvements of close to 70 % compared to wanda, demonstrating that sparseswaps is able to successfully optimize the local loss. the attn. o proj seems to consistently benefit the most across blocks, with reductions of the objective in equation 1 ranging between 40 % - 60 %. large local error reductions do not always imply reduced perplexity. from table 2 we observe substantial perplexity gains, especially when sparsity more strongly degrades model quality ( cf. table 4 in the appendix, which shows more drastic improvements when using magnitude pruning, which more strongly degrades model quality ). in contrast, when quality is less affected ( e. g., at 50 % sparsity where wanda performs well ), sparseswaps yields limited perplexity gains despite significant local error reductions : table 1 reports perplexity", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 7, "frag_id": 2, "text": "loss. the attn. o proj seems to consistently benefit the most across blocks, with reductions of the objective in equation 1 ranging between 40 % - 60 %. large local error reductions do not always imply reduced perplexity. from table 2 we observe substantial perplexity gains, especially when sparsity more strongly degrades model quality ( cf. table 4 in the appendix, which shows more drastic improvements when using magnitude pruning, which more strongly degrades model quality ). in contrast, when quality is less affected ( e. g., at 50 % sparsity where wanda performs well ), sparseswaps yields limited perplexity gains despite significant local error reductions : table 1 reports perplexity and average relative error reduction ( % ) versus the number of 1 - swap iterations. zero iterations correspond to the wanda warm start ; one or more iterations correspond to sparseswaps from wanda. at 50 % sparsity, a single 1 - swap iteration lowers relative error by 6. 34 %, and 200 iterations by nearly 40 %, yet perplexity does not improve, but rather slightly increases. this suggests further reducing local error can overfit the calibration data and may not translate to better perplexity, although we note that the perplexity increase is relatively small. these results emphasize that while the reduction of local error is a useful proxy for perplexity reduction when pruning has a higher negative impact on the model, the local error of equation 1 remains an approximation to the reconstruction error of the entire model. 7", "token_count": 325}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 8, "frag_id": 0, "text": "table 2 : perplexity ( ↓, lower is better ) and zero - shot accuracy ( ↑, higher is better ) comparison on wikitext and eleutherai evaluation set. we report dsnot and sparseswaps refinement with wanda and ria warmstart for unstructured 60 % sparsity and semi - structured 2 : 4 sparsity. best values are highlighted in bold. we omit standard deviations for legibility. perplexity ↓ llama - 3. 1 gemma - 2 yi - 1. 5 deepseek qwen2. 5 method sparsity 8b 9b 9b 7b 7b wanda 60 % 21. 94 16. 74 11. 40 11. 41 13. 75 + dsnot 60 % 21. 94 16. 69 11. 38 11. 40 13. 75 + sparseswaps 60 % 19. 75 16. 01 10. 07 10. 93 13. 16 ria 60 % 19. 73 16. 19 10. 73 11. 80 12. 63 + dsnot 60 % 19. 73 16. 22 10. 73 11. 80 12. 63 + sparseswaps 60 % 18. 47 15. 44 9. 98 10. 79 12. 47 wanda 2 : 4 24. 82 17. 45 11. 76 11. 77 14. 53 + dsnot 2 : 4 22. 79 16. 79 10. 84 11. 70 14. 40 + sparseswaps 2 : 4 20. 17 16. 30 10. 73 11. 70 13. 95 ria 2 : 4 23. 96 16. 88 11. 29 12. 03 13. 58 + dsnot 2 : 4 24. 26 16. 82 10. 57 12. 03 13. 85 + sparseswaps 2 : 4 20. 90 16. 33 10. 50 11. 80 13. 28 accuracy ↑ llama - 3. 1 gemma - 2 yi - 1. 5 deepseek qwen2. 5 method sparsity 8b 9b 9b 7b 7b wanda 60 % 48. 18 % 63. 39 % 53. 59 % 50. 74 % 59. 26 % + dsnot 60 % 48. 18 % 63. 49 % 53. 79 % 50. 75 % 59. 26 % + sparseswaps 60 % 50. 78 % 63. 84 % 54. 84 % 51. 02 % 60. 15 % ria 60", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 8, "frag_id": 1, "text": ". 82 10. 57 12. 03 13. 85 + sparseswaps 2 : 4 20. 90 16. 33 10. 50 11. 80 13. 28 accuracy ↑ llama - 3. 1 gemma - 2 yi - 1. 5 deepseek qwen2. 5 method sparsity 8b 9b 9b 7b 7b wanda 60 % 48. 18 % 63. 39 % 53. 59 % 50. 74 % 59. 26 % + dsnot 60 % 48. 18 % 63. 49 % 53. 79 % 50. 75 % 59. 26 % + sparseswaps 60 % 50. 78 % 63. 84 % 54. 84 % 51. 02 % 60. 15 % ria 60 % 49. 56 % 64. 37 % 52. 81 % 50. 92 % 59. 84 % + dsnot 60 % 49. 56 % 64. 43 % 52. 96 % 50. 83 % 59. 81 % + sparseswaps 60 % 51. 02 % 64. 32 % 54. 45 % 51. 47 % 61. 22 % wanda 2 : 4 46. 80 % 63. 73 % 52. 58 % 51. 02 % 59. 52 % + dsnot 2 : 4 47. 01 % 63. 66 % 52. 16 % 50. 78 % 59. 09 % + sparseswaps 2 : 4 48. 83 % 64. 70 % 52. 43 % 50. 36 % 59. 92 % ria 2 : 4 47. 87 % 63. 87 % 52. 68 % 51. 22 % 58. 66 % + dsnot 2 : 4 47. 13 % 64. 17 % 51. 36 % 49. 86 % 59. 72 % + sparseswaps 2 : 4 49. 90 % 64. 60 % 52. 30 % 51. 46 % 60. 31 % 3. 2 efficiency and hyperparameter ablations resource requirements. sparseswaps is more resource - intensive than dsnot and, as a drop - in refinement, requires at least the resources of the chosen warm - start method. beyond that, sparseswaps needs memory to store the gram matrix g ∈rdin×din ( once per layer ) and the correlation vector c ∈rdin ( per row ), and compute to perform the 1 - swaps ; see the preceding section for the theoretical complexity. while we have argued in the introduction that the additional compute can be", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 8, "frag_id": 2, "text": "72 % + sparseswaps 2 : 4 49. 90 % 64. 60 % 52. 30 % 51. 46 % 60. 31 % 3. 2 efficiency and hyperparameter ablations resource requirements. sparseswaps is more resource - intensive than dsnot and, as a drop - in refinement, requires at least the resources of the chosen warm - start method. beyond that, sparseswaps needs memory to store the gram matrix g ∈rdin×din ( once per layer ) and the correlation vector c ∈rdin ( per row ), and compute to perform the 1 - swaps ; see the preceding section for the theoretical complexity. while we have argued in the introduction that the additional compute can be justified when amortized over many llm inference requests, we note that the overhead grows only linearly with the number of 1 - swap iterations tmax. table 1 shows that few iterations already yield substantial gains in both perplexity and local error reduction, especially at higher sparsity. table 3 reports wall - clock times for pruning llama - 3. 1 - 8b to 60 % sparsity on a single h100 gpu. the tmax = 0 baseline includes calibration data sampling, wanda pruning, gram matrix computation, and evaluation ; each additional iteration of sparseswaps adds a relatively small overhead. for comparison, wanda and sparsegpt take approximately 4 and 10 minutes, respectively. we note that our implementation can be further optimized and that the algorithm is fully parallelizable across rows. effect of the number of reconstruction samples. figure 2 in the appendix shows the perplexity versus the number of reconstruction samples for 50 % and 60 % unstructured sparsity when using wanda as well as sparseswaps with a wanda warmstart. we observe that the perplexity decreases 8", "token_count": 385}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 9, "frag_id": 0, "text": "table 3 : wall - clock time for applying sparseswaps to llama - 3. 1 - 8b at 60 % sparsity on a single h100 gpu. tmax 0 1 2 5 10 25 wall - clock time 8m15s 10m17s 12m7s 17m20s 26m13s 52m29s drastically when using more samples, which leads to sparseswaps slightly outperforming wanda for 50 % sparsity, despite its advantage typically being larger at higher sparsity. we emphasize that the number of reconstruction samples does not affect sparseswaps ’ s swap evaluation efficiency : the gram matrix g = [UNK] fixed size din × din regardless of b. 4 conclusion we revisited the mask selection problem for post - training pruning and showed that it can be made substantially more tractable, even at llm scale. we observed that row decoupling via equal perrow sparsity yields independent subproblems, and that individual 1 - swaps can be evaluated in o ( 1 ) time using the gram matrix g = [UNK]. this enables tractable optimization of the true rowwise quadratic loss on gpus. the resulting method, sparseswaps, is warm - start agnostic, nearly hyperparameter - free, and scalable. it consistently reduces per - layer pruning error and improves perplexity and zero - shot accuracy across modern gpt architectures. our work is not without limitations. while per - row sparsity is not necessarily detrimental for llms, our approach is restricted to that setting and only partially adapts to truly unstructured sparsity ; in its current form, the algorithm can handle unstructured sparsity but cannot reallocate sparsity levels across rows. furthermore, runtime and memory remain non - trivial for large architectures. acknowledgments this research was partially supported by the dfg cluster of excellence math + ( exc - 2046 / 1, project id 390685689 ) funded by the deutsche forschungsgemeinschaft ( dfg ) as well as by the german federal ministry of research, technology and space ( fund number 16is23025b ). 9", "token_count": 462}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 10, "frag_id": 0, "text": "references xiao bi, deli chen, guanting chen, shanhuang chen, damai dai, chengqi deng, honghui ding, kai dong, qiushi du, zhe fu, huazuo gao, kaige gao, wenjun gao, ruiqi ge, kang guan, daya guo, jianzhong guo, guangbo hao, zhewen hao, ying he, wenjie hu, panpan huang, erhang li, guowei li, jiashi li, yao li, y. k. li, wenfeng liang, fangyun lin, a. x. liu, bo liu, wen liu, xiaodong liu, xin liu, yiyuan liu, haoyu lu, shanghao lu, fuli luo, shirong ma, xiaotao nie, tian pei, yishi piao, junjie qiu, hui qu, tongzheng ren, zehui ren, chong ruan, zhangli sha, zhihong shao, junxiao song, xuecheng su, jingxiang sun, yaofeng sun, minghui tang, bingxuan wang, peiyi wang, shiyu wang, yaohui wang, yongji wang, tong wu, y. wu, xin xie, zhenda xie, ziwei xie, yiliang xiong, hanwei xu, r. x. xu, yanhong xu, dejian yang, yuxiang you, shuiping yu, xingkai yu, b. zhang, haowei zhang, lecong zhang, liyue zhang, mingchuan zhang, minghua zhang, wentao zhang, yichao zhang, chenggang zhao, yao zhao, shangyan zhou, shunfeng zhou, qihao zhu, and yuheng zou. deepseek llm : scaling open - source language models with longtermism, january 2024. url http : / / arxiv. org / abs / 2401. 02954. suresh bolusani, mathieu [UNK], ksenia bestuzheva, antonia chmiela, [UNK] dion´ısio, tim donkiewicz, jasper van doornmalen, leon eifler, mohammed ghannam, ambros gleixner, christoph graczyk, katrin halbig, ivo he", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 10, "frag_id": 1, "text": "wentao zhang, yichao zhang, chenggang zhao, yao zhao, shangyan zhou, shunfeng zhou, qihao zhu, and yuheng zou. deepseek llm : scaling open - source language models with longtermism, january 2024. url http : / / arxiv. org / abs / 2401. 02954. suresh bolusani, mathieu [UNK], ksenia bestuzheva, antonia chmiela, [UNK] dion´ısio, tim donkiewicz, jasper van doornmalen, leon eifler, mohammed ghannam, ambros gleixner, christoph graczyk, katrin halbig, ivo hedtke, alexander hoen, christopher hojny, rolf van der hulst, dominik kamp, thorsten koch, kevin kofler, jurgen lentz, julian manns, gioni mexi, erik m¨uhmer, marc e. pfetsch, franziska schl¨osser, felipe serrano, yuji shinano, mark turner, stefan vigerske, dieter weninger, and lixing xu. the scip optimization suite 9. 0. technical report, optimization online, february 2024. url https : / / optimization - online. org / 2024 / 02 / the - scip - optimization - suite - 9 - 0 /. pierre bonami, lorenz t biegler, andrew r conn, g´erard cornu´ejols, ignacio e grossmann, carl d laird, jon lee, andrea lodi, [UNK] margot, nicolas sawaya, et al. an algorithmic framework for convex mixed integer nonlinear programs. discrete optimization, 5 ( 2 ) : 186 – 204, 2008. tim dettmers, mike lewis, younes belkada, and luke zettlemoyer. llm. int8 ( ) : 8 - bit matrix multiplication for transformers at scale. august 2022. utku evci, trevor gale, jacob menick, pablo samuel castro, and erich elsen. rigging the lottery : making all tickets winners. in hal daum´e iii and aarti singh ( eds. ), proceedings of the 37th international conference on machine learning, volume 119 of proceedings of machine learning research, pp. 2943 – 2952. pmlr, 13 – 18 jul 2020. ur", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 10, "frag_id": 2, "text": "##ic framework for convex mixed integer nonlinear programs. discrete optimization, 5 ( 2 ) : 186 – 204, 2008. tim dettmers, mike lewis, younes belkada, and luke zettlemoyer. llm. int8 ( ) : 8 - bit matrix multiplication for transformers at scale. august 2022. utku evci, trevor gale, jacob menick, pablo samuel castro, and erich elsen. rigging the lottery : making all tickets winners. in hal daum´e iii and aarti singh ( eds. ), proceedings of the 37th international conference on machine learning, volume 119 of proceedings of machine learning research, pp. 2943 – 2952. pmlr, 13 – 18 jul 2020. url https : / / proceedings. mlr. press / v119 / evci20a. html. elias frantar and dan alistarh. sparsegpt : massive language models can be accurately pruned in one - shot. in international conference on machine learning, pp. 10323 – 10337. pmlr, 2023. trevor gale, erich elsen, and sara hooker. the state of sparsity in deep neural networks. arxiv preprint arxiv : 1902. 09574, 2019. leo gao, jonathan tow, baber abbasi, stella biderman, sid black, anthony dipofi, charles foster, laurence golding, jeffrey hsu, alain le noac ’ h, haonan li, kyle mcdonell, niklas muennighoff, chris ociepa, jason phang, laria reynolds, hailey schoelkopf, aviya skowron, lintang sutawika, eric tang, anish thite, ben wang, kevin wang, and andy zou. a framework for few - shot language model evaluation, 12 2023. url https : / / zenodo. org / records / 10256836. aaron grattafiori, abhimanyu dubey, abhinav jauhri, abhinav pandey, abhishek kadian, ahmad al - dahle, aiesha letman, akhil mathur, alan schelten, alex vaughan, amy yang, angela fan, anirudh goyal, anthony hartshorn, aobo yang, archi mitra, archie sravankumar, artem", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 10, "frag_id": 3, "text": "##ka, eric tang, anish thite, ben wang, kevin wang, and andy zou. a framework for few - shot language model evaluation, 12 2023. url https : / / zenodo. org / records / 10256836. aaron grattafiori, abhimanyu dubey, abhinav jauhri, abhinav pandey, abhishek kadian, ahmad al - dahle, aiesha letman, akhil mathur, alan schelten, alex vaughan, amy yang, angela fan, anirudh goyal, anthony hartshorn, aobo yang, archi mitra, archie sravankumar, artem korenev, arthur hinsvark, arun rao, aston zhang, aurelien rodriguez, austen gregerson, ava spataru, baptiste roziere, bethany biron, binh tang, bobbie chern, charlotte caucheteux, chaya nayak, chloe bi, chris marra, chris mcconnell, christian keller, christophe touret, chunyang wu, corinne wong, cristian canton ferrer, cyrus nikolaidis, damien allonsius, daniel song, danielle pintz, danny livshits, danny wyatt, david esiobu, dhruv choudhary, dhruv mahajan, diego garcia - olano, diego perino, dieuwke hupkes, egor lakomkin, ehab albadawy, elina lobanova, emily dinan, eric michael smith, filip radenovic, francisco 10", "token_count": 332}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 11, "frag_id": 0, "text": "guzm´an, frank zhang, gabriel synnaeve, gabrielle lee, georgia lewis anderson, govind thattai, graeme nail, gregoire mialon, guan pang, guillem cucurell, hailey nguyen, hannah korevaar, hu xu, hugo touvron, iliyan zarov, imanol arrieta ibarra, isabel kloumann, ishan misra, ivan evtimov, jack zhang, jade copet, jaewon lee, jan geffert, jana vranes, jason park, jay mahadeokar, jeet shah, jelmer van der linde, jennifer billock, jenny hong, jenya lee, jeremy fu, jianfeng chi, jianyu huang, jiawen liu, jie wang, jiecao yu, joanna bitton, joe spisak, jongsoo park, joseph rocca, joshua johnstun, joshua saxe, junteng jia, kalyan vasuden alwala, karthik prasad, kartikeya upasani, kate plawiak, ke li, kenneth heafield, kevin stone, khalid el - arini, krithika iyer, kshitiz malik, kuenley chiu, kunal bhalla, kushal lakhotia, lauren rantala - yeary, laurens van der maaten, lawrence chen, liang tan, liz jenkins, louis martin, lovish madaan, lubo malo, lukas blecher, lukas landzaat, luke de oliveira, madeline muzzi, mahesh pasupuleti, mannat singh, manohar paluri, marcin kardas, maria tsimpoukelli, mathew oldham, mathieu rita, maya pavlova, melanie kambadur, mike lewis, min si, mitesh kumar singh, mona hassan, naman goyal, narjes torabi, nikolay bashlykov, nikolay bogoychev, niladri chatterji, ning zhang, olivier duchenne, onur [UNK] elebi, patrick alrassy, pengchuan zhang, pengwei li, petar vasic, peter weng, prajjwal bhargava, pratik dubal, praveen krishnan, punit singh koura, puxin xu, qing he, qingxiao dong", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 11, "frag_id": 1, "text": ", marcin kardas, maria tsimpoukelli, mathew oldham, mathieu rita, maya pavlova, melanie kambadur, mike lewis, min si, mitesh kumar singh, mona hassan, naman goyal, narjes torabi, nikolay bashlykov, nikolay bogoychev, niladri chatterji, ning zhang, olivier duchenne, onur [UNK] elebi, patrick alrassy, pengchuan zhang, pengwei li, petar vasic, peter weng, prajjwal bhargava, pratik dubal, praveen krishnan, punit singh koura, puxin xu, qing he, qingxiao dong, ragavan srinivasan, raj ganapathy, ramon calderer, ricardo silveira cabral, robert stojnic, roberta raileanu, rohan maheswari, rohit girdhar, rohit patel, romain sauvestre, ronnie polidoro, roshan sumbaly, ross taylor, ruan silva, rui hou, rui wang, saghar hosseini, sahana chennabasappa, sanjay singh, sean bell, seohyun sonia kim, sergey edunov, shaoliang nie, sharan narang, sharath raparthy, sheng shen, shengye wan, shruti bhosale, shun zhang, simon vandenhende, soumya batra, spencer whitman, sten sootla, stephane collot, suchin gururangan, sydney borodinsky, tamar herman, tara fowler, tarek sheasha, thomas georgiou, thomas scialom, tobias speckbacher, todor mihaylov, tong xiao, ujjwal karn, vedanuj goswami, vibhor gupta, vignesh ramanathan, viktor kerkez, vincent gonguet, virginie do, vish vogeti, v´ıtor albiero, vladan petrovic, weiwei chu, wenhan xiong, wenyin fu, whitney meers, xavier martinet, xiaodong wang, xiaofang wang, xiaoqing ellen tan, xide xia, xinfeng xie, xuchao jia, xuewei wang, yaelle goldschlag, yashesh ga", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 11, "frag_id": 2, "text": "thomas georgiou, thomas scialom, tobias speckbacher, todor mihaylov, tong xiao, ujjwal karn, vedanuj goswami, vibhor gupta, vignesh ramanathan, viktor kerkez, vincent gonguet, virginie do, vish vogeti, v´ıtor albiero, vladan petrovic, weiwei chu, wenhan xiong, wenyin fu, whitney meers, xavier martinet, xiaodong wang, xiaofang wang, xiaoqing ellen tan, xide xia, xinfeng xie, xuchao jia, xuewei wang, yaelle goldschlag, yashesh gaur, yasmine babaei, yi wen, yiwen song, yuchen zhang, yue li, yuning mao, zacharie delpierre coudert, zheng yan, zhengxing chen, zoe papakipos, aaditya singh, aayushi srivastava, abha jain, adam kelsey, adam shajnfeld, adithya gangidi, adolfo victoria, ahuva goldstand, ajay menon, ajay sharma, alex boesenberg, alexei baevski, allie feinstein, amanda kallet, amit sangani, amos teo, anam yunus, andrei lupu, andres alvarado, andrew caples, andrew gu, andrew ho, andrew poulton, andrew ryan, ankit ramchandani, annie dong, annie franco, anuj goyal, aparajita saraf, arkabandhu chowdhury, ashley gabriel, ashwin bharambe, assaf eisenman, azadeh yazdan, beau james, ben maurer, benjamin leonhardi, bernie huang, beth loyd, beto de paola, bhargavi paranjape, bing liu, bo wu, boyu ni, braden hancock, bram wasti, brandon spence, brani stojkovic, brian gamido, britt montalvo, carl parker, carly burton, catalina mejia, ce liu, changhan wang, changkyu kim, chao zhou, chester hu, ching - hsiang chu, chris cai, chris tindal, christoph feichtenhofer, cynthia gao, damon civin, dana beaty, daniel kreymer, daniel li, david", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 11, "frag_id": 3, "text": "eisenman, azadeh yazdan, beau james, ben maurer, benjamin leonhardi, bernie huang, beth loyd, beto de paola, bhargavi paranjape, bing liu, bo wu, boyu ni, braden hancock, bram wasti, brandon spence, brani stojkovic, brian gamido, britt montalvo, carl parker, carly burton, catalina mejia, ce liu, changhan wang, changkyu kim, chao zhou, chester hu, ching - hsiang chu, chris cai, chris tindal, christoph feichtenhofer, cynthia gao, damon civin, dana beaty, daniel kreymer, daniel li, david adkins, david xu, davide testuggine, delia david, devi parikh, diana liskovich, didem foss, dingkang wang, duc le, dustin holland, edward dowling, eissa jamil, elaine montgomery, eleonora presani, emily hahn, emily wood, eric - tuan le, erik brinkman, esteban arcaute, evan dunbar, evan smothers, fei sun, felix kreuk, feng tian, filippos kokkinos, firat ozgenel, francesco caggioni, frank kanayet, frank seide, gabriela medina florez, gabriella schwarz, gada badeer, georgia swee, gil halpern, grant herman, grigory sizov, guangyi, zhang, guna lakshminarayanan, hakan inan, hamid shojanazeri, han zou, hannah wang, hanwen zha, haroun habeeb, harrison rudolph, helen suk, henry aspegren, hunter goldman, hongyuan zhan, ibrahim damlaj, igor molybog, igor tufanov, ilias leontiadis, irina - elena veliche, itai gat, jake weissman, james geboski, james kohli, janice lam, japhet asher, jean - baptiste gaya, jeff marcus, jeff tang, jennifer chan, jenny zhen, jeremy reizenstein, jeremy teboul, jessica zhong, jian jin, jingyi yang, joe cummings, jon carvill, jon shepard, jonathan mcphie, jonathan torres, josh ginsburg, junjie wang, kai wu,", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 11, "frag_id": 4, "text": "##roun habeeb, harrison rudolph, helen suk, henry aspegren, hunter goldman, hongyuan zhan, ibrahim damlaj, igor molybog, igor tufanov, ilias leontiadis, irina - elena veliche, itai gat, jake weissman, james geboski, james kohli, janice lam, japhet asher, jean - baptiste gaya, jeff marcus, jeff tang, jennifer chan, jenny zhen, jeremy reizenstein, jeremy teboul, jessica zhong, jian jin, jingyi yang, joe cummings, jon carvill, jon shepard, jonathan mcphie, jonathan torres, josh ginsburg, junjie wang, kai wu, kam hou u, karan saxena, kartikay khandelwal, katayoun zand, kathy matosich, kaushik veeraraghavan, kelly michelena, keqian li, kiran jagadeesh, kun huang, kunal chawla, kyle huang, lailin chen, lakshya garg, lavender a, leandro silva, lee bell, lei zhang, liangpeng guo, licheng yu, liron moshkovich, luca wehrstedt, madian khabsa, manav avalani, manish bhatt, martynas mankus, matan hasson, matthew lennie, matthias reso, maxim groshev, maxim naumov, maya lathi, meghan keneally, miao liu, michael l. 11", "token_count": 321}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 12, "frag_id": 0, "text": "seltzer, michal valko, michelle restrepo, mihir patel, mik vyatskov, mikayel samvelyan, mike clark, mike macey, mike wang, miquel jubert hermoso, mo metanat, mohammad rastegari, munish bansal, nandhini santhanam, natascha parks, natasha white, navyata bawa, nayan singhal, nick egebo, nicolas usunier, nikhil mehta, nikolay pavlovich laptev, ning dong, norman cheng, oleg chernoguz, olivia hart, omkar salpekar, ozlem kalinli, parkin kent, parth parekh, paul saab, pavan balaji, pedro rittner, philip bontrager, pierre roux, piotr dollar, polina zvyagina, prashant ratanchandani, pritish yuvraj, qian liang, rachad alao, rachel rodriguez, rafi ayub, raghotham murthy, raghu nayani, rahul mitra, rangaprabhu parthasarathy, raymond li, rebekkah hogan, robin battey, rocky wang, russ howes, ruty rinott, sachin mehta, sachin siby, sai jayesh bondu, samyak datta, sara chugh, sara hunt, sargun dhillon, sasha sidorov, satadru pan, saurabh mahajan, saurabh verma, seiji yamamoto, sharadh ramaswamy, shaun lindsay, shaun lindsay, sheng feng, shenghao lin, shengxin cindy zha, shishir patil, shiva shankar, shuqiang zhang, shuqiang zhang, sinong wang, sneha agarwal, soji sajuyigbe, soumith chintala, stephanie max, stephen chen, steve kehoe, steve satterfield, sudarshan govindaprasad, sumit gupta, summer deng, sungmin cho, sunny virk, suraj subramanian, sy choudhury, sydney goldman, tal remez, tamar glaser, tamara best, thilo koehler, thomas robinson, tianhe li, tianjun zhang, tim matthews, timothy chou, t", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 12, "frag_id": 1, "text": ", shenghao lin, shengxin cindy zha, shishir patil, shiva shankar, shuqiang zhang, shuqiang zhang, sinong wang, sneha agarwal, soji sajuyigbe, soumith chintala, stephanie max, stephen chen, steve kehoe, steve satterfield, sudarshan govindaprasad, sumit gupta, summer deng, sungmin cho, sunny virk, suraj subramanian, sy choudhury, sydney goldman, tal remez, tamar glaser, tamara best, thilo koehler, thomas robinson, tianhe li, tianjun zhang, tim matthews, timothy chou, tzook shaked, varun vontimitta, victoria ajayi, victoria montanez, vijai mohan, vinay satish kumar, vishal mangla, vlad ionescu, vlad poenaru, vlad tiberiu mihailescu, vladimir ivanov, wei li, wenchen wang, wenwen jiang, wes bouaziz, will constable, xiaocheng tang, xiaojian wu, xiaolan wang, xilun wu, xinbo gao, yaniv kleinman, yanjun chen, ye hu, ye jia, ye qi, yenda li, yilin zhang, ying zhang, yossi adi, youngjin nam, yu, wang, yu zhao, yuchen hao, yundi qian, yunlu li, yuzi he, zach rait, zachary devito, zef rosnbrick, zhaoduo wen, zhenyu yang, zhiwei zhao, and zhiyu ma. the llama 3 herd of models, november 2024. url http : / / arxiv. org / abs / 2407. 21783. song han, jeff pool, john tran, and william dally. learning both weights and connections for efficient neural networks. in c. cortes, n. lawrence, d. lee, m. sugiyama, and r. garnett ( eds. ), advances in neural information processing systems, volume 28. curran associates, inc., 2015. url https : / / proceedings. neurips. cc / paper / 2015 / file / ae0eb3eed39d2bcef4622b2499a05fe6 - paper. pdf. baba", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 12, "frag_id": 2, "text": "##ama 3 herd of models, november 2024. url http : / / arxiv. org / abs / 2407. 21783. song han, jeff pool, john tran, and william dally. learning both weights and connections for efficient neural networks. in c. cortes, n. lawrence, d. lee, m. sugiyama, and r. garnett ( eds. ), advances in neural information processing systems, volume 28. curran associates, inc., 2015. url https : / / proceedings. neurips. cc / paper / 2015 / file / ae0eb3eed39d2bcef4622b2499a05fe6 - paper. pdf. babak hassibi and david stork. second order derivatives for network pruning : optimal brain surgeon. in s. hanson, j. cowan, and c. giles ( eds. ), advances in neural information processing systems, volume 5. morgan - kaufmann, 1993. url https : / / proceedings. neurips. cc / paper / 1992 / file / 303ed4c69846ab36c2904d3ba8573050 - paper. pdf. deborah hendrych, hannah troppens, mathieu [UNK], and sebastian pokutta. convex integer optimization with frank - wolfe methods. mathematical programming computation, 2025. doi : 10. 1007 / s12532 - 025 - 00288 - w. url https : / / link. springer. com / article / 10. 1007 / s12532 - 025 - 00288 - w. torsten hoefler, dan alistarh, tal ben - nun, nikoli dryden, and alexandra peste. sparsity in deep learning : pruning and growth for efficient inference and training in neural networks. arxiv preprint arxiv : 2102. 00554, january 2021. steven a. janowsky. pruning versus clipping in neural networks. phys. rev. a, 39 : 6600 – 6603, jun 1989. doi : 10. 1103 / physreva. 39. 6600. woosuk kwon, sehoon kim, michael w. mahoney, joseph hassoun, kurt keutzer, and amir gholami. a fast", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 12, "frag_id": 3, "text": ", dan alistarh, tal ben - nun, nikoli dryden, and alexandra peste. sparsity in deep learning : pruning and growth for efficient inference and training in neural networks. arxiv preprint arxiv : 2102. 00554, january 2021. steven a. janowsky. pruning versus clipping in neural networks. phys. rev. a, 39 : 6600 – 6603, jun 1989. doi : 10. 1103 / physreva. 39. 6600. woosuk kwon, sehoon kim, michael w. mahoney, joseph hassoun, kurt keutzer, and amir gholami. a fast post - training pruning framework for transformers. march 2022. yann lecun, john s. denker, and sara a. solla. optimal brain damage. in david s. touretzky ( ed. ), advances in neural information processing systems 2, [ nips conference, denver, colorado, usa, november 27 - 30, 1989 ], pp. 598 – 605. morgan kaufmann, 1989. url http : / / papers. nips. cc / paper / 250 - optimal - brain - damage. vladislav lialin, vijeta deshpande, and anna rumshisky. scaling down to scale up : a guide to parameter - efficient fine - tuning. march 2023. tao lin, sebastian u. stich, luis barba, daniil dmitriev, and martin jaggi. dynamic model pruning with feedback. in international conference on learning representations, 2020. 12", "token_count": 344}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 13, "frag_id": 0, "text": "andreas lundell, jan kronqvist, and tapio westerlund. the supporting hyperplane optimization toolkit for convex minlp. journal of global optimization, 84 ( 1 ) : 1 – 41, 2022. stephen merity, caiming xiong, james bradbury, and richard socher. pointer sentinel mixture models. september 2016. asit mishra, jorge albericio latorre, jeff pool, darko stosic, dusan stosic, ganesh venkatesh, chong yu, and paulius micikevicius. accelerating sparse deep neural networks. april 2021. pavlo molchanov, stephen tyree, tero karras, timo aila, and jan kautz. pruning convolutional neural networks for resource efficient inference. november 2016. colin raffel, noam shazeer, adam roberts, katherine lee, sharan narang, michael matena, yanqi zhou, wei li, and peter j liu. exploring the limits of transfer learning with a unified text - to - text transformer. the journal of machine learning research, 21 ( 1 ) : 5485 – 5551, 2020. morgane riviere, shreya pathak, pier giuseppe sessa, cassidy hardin, surya bhupatiraju, l´eonard hussenot, thomas mesnard, bobak shahriari, alexandre ram´e, johan ferret, peter liu, pouya tafti, abe friesen, michelle casbon, sabela ramos, ravin kumar, charline le lan, sammy jerome, anton tsitsulin, nino vieillard, piotr stanczyk, sertan girgin, nikola momchev, matt hoffman, shantanu thakoor, jean - bastien grill, behnam neyshabur, olivier bachem, alanna walton, aliaksei severyn, alicia parrish, aliya ahmad, allen hutchison, alvin abdagic, amanda carl, amy shen, andy brock, andy coenen, anthony laforge, antonia paterson, ben bastian, bilal piot, bo wu, brandon royal, charlie chen, chintu kumar, chris perry, chris welty, christopher a. choquette - choo, danila sinopalnikov, david weinberger, dimple vijaykumar, dominika rogozi´", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 13, "frag_id": 1, "text": "##yk, sertan girgin, nikola momchev, matt hoffman, shantanu thakoor, jean - bastien grill, behnam neyshabur, olivier bachem, alanna walton, aliaksei severyn, alicia parrish, aliya ahmad, allen hutchison, alvin abdagic, amanda carl, amy shen, andy brock, andy coenen, anthony laforge, antonia paterson, ben bastian, bilal piot, bo wu, brandon royal, charlie chen, chintu kumar, chris perry, chris welty, christopher a. choquette - choo, danila sinopalnikov, david weinberger, dimple vijaykumar, dominika rogozi´nska, dustin herbison, elisa bandy, emma wang, eric noland, erica moreira, evan senter, evgenii eltyshev, francesco visin, gabriel rasskin, gary wei, glenn cameron, gus martins, hadi hashemi, hanna klimczak - pluci´nska, harleen batra, harsh dhand, ivan nardini, jacinda mein, jack zhou, james svensson, jeff stanway, jetha chan, jin peng zhou, joana carrasqueira, joana iljazi, jocelyn becker, joe fernandez, joost van amersfoort, josh gordon, josh lipschultz, josh newlan, ju - yeong ji, kareem mohamed, kartikeya badola, kat black, katie millican, keelin mcdonell, kelvin nguyen, kiranbir sodhia, kish greene, lars lowe sjoesund, lauren usui, laurent sifre, lena heuermann, leticia lago, lilly mcnealus, livio baldini soares, logan kilpatrick, lucas dixon, luciano martins, machel reid, manvinder singh, mark iverson, martin g¨orner, mat velloso, mateo wirth, matt davidow, matt miller, matthew rahtz, matthew watson, meg risdal, mehran kazemi, michael moynihan, ming zhang, minsuk kahng, minwoo park, mofi rahman, mohit khatwani, natalie dao, nenshad bardoliwalla, nesh devanathan, neta duma", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 13, "frag_id": 2, "text": "lauren usui, laurent sifre, lena heuermann, leticia lago, lilly mcnealus, livio baldini soares, logan kilpatrick, lucas dixon, luciano martins, machel reid, manvinder singh, mark iverson, martin g¨orner, mat velloso, mateo wirth, matt davidow, matt miller, matthew rahtz, matthew watson, meg risdal, mehran kazemi, michael moynihan, ming zhang, minsuk kahng, minwoo park, mofi rahman, mohit khatwani, natalie dao, nenshad bardoliwalla, nesh devanathan, neta dumai, nilay chauhan, oscar wahltinez, pankil botarda, parker barnes, paul barham, paul michel, pengchong jin, petko georgiev, phil culliton, pradeep kuppala, ramona comanescu, ramona merhej, reena jana, reza ardeshir rokni, rishabh agarwal, ryan mullins, samaneh saadat, sara mc carthy, sarah cogan, sarah perrin, s´ebastien m. r. arnold, sebastian krause, shengyang dai, shruti garg, shruti sheth, sue ronstrom, susan chan, timothy jordan, ting yu, tom eccles, tom hennigan, tomas kocisky, tulsee doshi, vihan jain, vikas yadav, vilobh meshram, vishal dharmadhikari, warren barkley, wei wei, wenming ye, woohyun han, woosuk kwon, xiang xu, zhe shen, zhitao gong, zichuan wei, victor cotruta, phoebe kirk, anand rao, minh giang, ludovic peran, tris warkentin, eli collins, joelle barral, zoubin ghahramani, raia hadsell, d. sculley, jeanine banks, anca dragan, slav petrov, oriol vinyals, jeff dean, demis hassabis, koray kavukcuoglu, clement farabet, elena buchatskaya, sebastian borgeaud, noah fiedel, armand joulin, kathleen", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 13, "frag_id": 3, "text": "wenming ye, woohyun han, woosuk kwon, xiang xu, zhe shen, zhitao gong, zichuan wei, victor cotruta, phoebe kirk, anand rao, minh giang, ludovic peran, tris warkentin, eli collins, joelle barral, zoubin ghahramani, raia hadsell, d. sculley, jeanine banks, anca dragan, slav petrov, oriol vinyals, jeff dean, demis hassabis, koray kavukcuoglu, clement farabet, elena buchatskaya, sebastian borgeaud, noah fiedel, armand joulin, kathleen kenealy, robert dadashi, and alek andreev. gemma 2 : improving open language models at a practical size, october 2024. url http : / / arxiv. org / abs / 2408. 00118. mingjie sun, zhuang liu, anna bair, and j. zico kolter. a simple and effective pruning approach for large language models. june 2023. mingjie sun, zhuang liu, anna bair, and j. zico kolter. a simple and effective pruning approach for large language models, may 2024. url http : / / arxiv. org / abs / 2306. 11695. ashish vaswani, noam shazeer, niki parmar, jakob uszkoreit, llion jones, aidan n gomez, łukasz kaiser, and illia polosukhin. attention is all you need. advances in neural information processing systems, 30, 2017. 13", "token_count": 359}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 14, "frag_id": 0, "text": "thomas wolf, lysandre debut, victor sanh, julien chaumond, clement delangue, anthony moi, pierric cistac, tim rault, remi louf, morgan funtowicz, joe davison, sam shleifer, patrick von platen, clara ma, yacine jernite, julien plu, canwen xu, teven le scao, sylvain gugger, mariama drame, quentin lhoest, and alexander rush. transformers : state - of - the - art natural language processing. in proceedings of the 2020 conference on empirical methods in natural language processing : system demonstrations, pp. 38 – 45, online, october 2020. association for computational linguistics. doi : 10. 18653 / v1 / 2020. emnlp - demos. 6. url https : / / aclanthology. org / 2020. emnlp - demos. 6. an yang, baosong yang, beichen zhang, binyuan hui, bo zheng, bowen yu, chengyuan li, dayiheng liu, fei huang, haoran wei, huan lin, jian yang, jianhong tu, jianwei zhang, jianxin yang, jiaxi yang, jingren zhou, junyang lin, kai dang, keming lu, keqin bao, kexin yang, le yu, mei li, mingfeng xue, pei zhang, qin zhu, rui men, runji lin, tianhao li, tianyi tang, tingyu xia, xingzhang ren, xuancheng ren, yang fan, yang su, yichang zhang, yu wan, yuqiong liu, zeyu cui, zhenru zhang, and zihan qiu. qwen2. 5 technical report, january 2025. url http : / / arxiv. org / abs / 2412. 15115. seul - ki yeom, philipp seegerer, sebastian lapuschkin, alexander binder, simon wiedemann, klaus - robert m¨uller, and wojciech samek. pruning by explaining : a novel criterion for deep neural network pruning. december 2019. lu yin, you wu, zhenyu zhang, cheng - yu hsieh, yaqing wang, yiling jia, mykola pechenizkiy, yi liang, zhangyang wang,", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 14, "frag_id": 1, "text": ", zhenru zhang, and zihan qiu. qwen2. 5 technical report, january 2025. url http : / / arxiv. org / abs / 2412. 15115. seul - ki yeom, philipp seegerer, sebastian lapuschkin, alexander binder, simon wiedemann, klaus - robert m¨uller, and wojciech samek. pruning by explaining : a novel criterion for deep neural network pruning. december 2019. lu yin, you wu, zhenyu zhang, cheng - yu hsieh, yaqing wang, yiling jia, mykola pechenizkiy, yi liang, zhangyang wang, and shiwei liu. outlier weighed layerwise sparsity ( owl ) : a missing secret sauce for pruning llms to high sparsity. october 2023. alex young, bei chen, chao li, chengen huang, ge zhang, guanwei zhang, guoyin wang, heng li, jiangcheng zhu, jianqun chen, jing chang, kaidong yu, peng liu, qiang liu, shawn yue, senbin yang, shiming yang, wen xie, wenhao huang, xiaohui hu, xiaoyi ren, xinyao niu, pengcheng nie, yanpeng li, yuchi xu, yudong liu, yue wang, yuxuan cai, zhenyu gu, zhiyuan liu, and zonghong dai. yi : open foundation models by 01. ai, january 2025. url http : / / arxiv. org / abs / 2403. 04652. mengxia yu, de wang, qi shan, colorado j. reed, and alvin wan. the super weight in large language models, july 2025. url http : / / arxiv. org / abs / 2411. 07191. yingtao zhang, haoli bai, haokun lin, jialin zhao, lu hou, and carlo vittorio cannistraci. plugand - play : an efficient post - training pruning method for large language models. in the twelfth international conference on learning representations, 2024. url https : / / openreview. net / forum? id = tr0lpx9wof. yuxin zhang, lirui zhao, mingbao lin,", "token_count": 500}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 14, "frag_id": 2, "text": "mengxia yu, de wang, qi shan, colorado j. reed, and alvin wan. the super weight in large language models, july 2025. url http : / / arxiv. org / abs / 2411. 07191. yingtao zhang, haoli bai, haokun lin, jialin zhao, lu hou, and carlo vittorio cannistraci. plugand - play : an efficient post - training pruning method for large language models. in the twelfth international conference on learning representations, 2024. url https : / / openreview. net / forum? id = tr0lpx9wof. yuxin zhang, lirui zhao, mingbao lin, yunyun sun, yiwu yao, xingjia han, jared tanner, shiwei liu, and rongrong ji. dynamic sparse no training : training - free fine - tuning for sparse llms. october 2023. max zimmer, megi andoni, christoph spiegel, and sebastian pokutta. perp : rethinking the pruneretrain paradigm in the era of llms. arxiv preprint arxiv : 2312. 15230, december 2023a. url https : / / arxiv. org / abs / 2312. 15230. max zimmer, christoph spiegel, and sebastian pokutta. how i learned to stop worrying and love retraining. in international conference on learning representations, 2023b. url https : / / openreview. net / forum? id = _ nf5imfkqi. max zimmer, christoph spiegel, and sebastian pokutta. compression - aware training of neural networks using frank – wolfe, pp. 137 – 168. de gruyter, berlin, boston, 2025. isbn 9783111376776. doi : doi : 10. 1515 / 9783111376776 - 010. url https : / / doi. org / 10. 1515 / 9783111376776 - 010. 14", "token_count": 447}
{"doc_id": "arxiv_251210922_sparseswaps", "page": 15, "frag_id": 0, "text": "a appendix a. 1 further results table 4 : perplexity ( ↓, lower is better ) comparison on wikitext. we report sparseswaps refinement with magnitude warmstart for 50 % and 60 % sparsity. best values are highlighted in bold. we omit standard deviations for legibility. perplexity ↓ llama - 3. 1 gemma - 2 deepseek method sparsity 8b 9b 7b magnitude 50 % 68. 89 31. 87 25. 05 + sparseswaps 50 % 52. 26 19. 11 16. 23 magnitude 60 % 3486. 26 184. 52 330. 07 + sparseswaps 60 % 264. 92 60. 04 80. 24 0 100 200 300 400 500 number of samples 10. 2 10. 4 10. 6 perplexity llama - 3. 1 - 8b ( 50 % sparsity ) wanda sparseswaps ( a ) 50 % unstructured sparsity 0 100 200 300 400 500 number of samples 19 20 21 22 23 perplexity llama - 3. 1 - 8b ( 60 % sparsity ) wanda sparseswaps ( b ) 60 % unstructured sparsity figure 2 : perplexity versus the number of reconstruction samples for unstructured sparsity using wanda warmstart. 15", "token_count": 279}
