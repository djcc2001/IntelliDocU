{"doc_id": "arxiv_251210935_any4d", "page": 1, "frag_id": 0, "text": "any4d : unified feed - forward metric 4d reconstruction any - 4d. github. io jay karhade nikhil keetha yuchen zhang tanisha gupta akash sharma sebastian scherer deva ramanan carnegie mellon university figure 1. any4d is a flexible feed - forward model capable of producing dense metric 4d reconstructions using n frames as input. any4d is up to 15× faster and 3× better than prior state - of - the - art, where performance can be further boosted by using diverse sensors as input. note that any4d produces dense 3d tracking vectors, but here we visualize the sparse 3d motion tracks for simplicity. abstract we present any4d, a scalable multi - view transformer for metric - scale, dense feed - forward 4d reconstruction. any4d directly generates per - pixel motion and geometry predictions for n frames, in contrast to prior work that typically focuses on either 2 - view dense scene flow or sparse 3d point tracking. moreover, unlike other recent methods for 4d reconstruction from monocular rgb videos, any4d can process additional modalities and sensors such as rgb - d frames, imu - based egomotion, and radar doppler measurements, when available. one of the key innovations that allows for such a flexible framework is a modular representation of a 4d scene ; specifically, per - view 4d predictions are encoded using a variety of egocentric factors ( depthmaps and camera intrinsics ) represented in local camera coordinates, and allocentric factors ( camera extrinsics and scene flow ) represented in global world coordinates. we achieve superior performance across diverse setups - both in terms of accuracy ( 2 −3× lower error ) and compute efficiency ( 15× faster ) - opening avenues for multiple downstream applications. 1. introduction reconstructing the 4d ( 3d + t ) world from sensor observations is a long - standing goal of computer vision. such a technology can unlock a wide range of downstream tasks. in generative ai, 4d reconstruction can improve dynamic video synthesis [ 8, 41, 72, 84 ], video understanding [ 24, 96 ], and the creation of interactive dynamic assets such as vr avatars. in robotics, 4d scene reconstruction can significantly improve predictive control ( mpc ) for an agent 1 arxiv : 2512. 10935v1 [ cs. cv ] 11 dec 2025", "token_count": 498}
{"doc_id": "arxiv_251210935_any4d", "page": 2, "frag_id": 0, "text": "navigating and manipulating in a physical world [ 44, 52 ]. although there has been significant recent progress on 4d reconstruction [ 16, 27, 37, 41, 60, 78, 92 ], dynamic reconstruction of in the wild videos remains challenging for many reasons. first, 4d reconstruction is severely under - constrained, requiring simplifying assumptions such as rigid motion, smoothness priors, or a mostly - static world assumption. second, there is a lack of large - scale 4d datasets. unlike million - scale video [ 10 ] and 3d datasets [ 2, 3 ], reliable high - quality 4d reconstruction datasets are still limited to a few thousand scenes, primarily obtained via simulation [ 18, 95 ]. third, because 4d reconstruction and tracking is such a challenging problem, progress has been largely achieved by treating dynamic attribute prediction as independent sub tasks ( i. e., 3d tracking, video - consistent depth estimation, scene flow estimation, camera pose estimation in dynamic scenes ). this focus on sub tasks has led to fragmented datasets and benchmarks that lack consistent 4d definitions and annotations. this is unsatisfying because all sub tasks observe the same underlying 4d world! to create a universal system that can reliably work on in the wild videos, we seek to address the following desiderata : a ) efficiency : much prior work often makes use of iterative optimization - based methods as a post - processing step that maybe too slow for real - time deployment. b ) multimodality : many robotic platforms use additional sensors beyond cameras, but most prior work fails to exploit such diverse configurations. c ) metric scale outputs : while existing 4d reconstruction methods produce outputs in a normalized coordinate frame, physical agents undeniably operate in the metric - scale physical world. taking a step in this direction, any4d is a unified and scalable model with the following 3 core contributions : • dense metric - scale 4d reconstruction : any4d predicts the dense geometry and motion of the scene in metric coordinates, unlike existing methods that can reconstruct only up - to - scale or sparse tracks. we propose a factored 4d representation consisting of per - view allocentric factors ( for scene flow and poses ) and egocentric factors ( for intrinsics and depth ). this factored 4d representation allows us to train on diverse datasets with partial annotations,", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 2, "frag_id": 1, "text": "methods produce outputs in a normalized coordinate frame, physical agents undeniably operate in the metric - scale physical world. taking a step in this direction, any4d is a unified and scalable model with the following 3 core contributions : • dense metric - scale 4d reconstruction : any4d predicts the dense geometry and motion of the scene in metric coordinates, unlike existing methods that can reconstruct only up - to - scale or sparse tracks. we propose a factored 4d representation consisting of per - view allocentric factors ( for scene flow and poses ) and egocentric factors ( for intrinsics and depth ). this factored 4d representation allows us to train on diverse datasets with partial annotations, including metric - scale 3d reconstruction datasets without motion annotations, and non - metric datasets with motion annotations. • flexible multi - modal inputs : when available, any4d can further improve its 4d reconstruction by exploiting additional input modalities like depth from rgbd sensors, camera poses from imus or doppler velocity from radars compared to image - only 4d reconstruction. • efficient inference : any4d infers both geometry and motion from n video frames in a single feed - forward pass, bypassing existing work that only predict motion for 2 frame inputs or require computationally - expensive optimization, making any4d up to 15× faster than the next best performing method. 2. related work reconstruction of dynamic scenes : reconstruction and camera pose estimation for static scenes has a rich history. it has been studied as simultaneous location and mapping ( slam ) [ 13, 15, 31, 33, 50, 65 ] when visual observations occur in a temporal sequence, and as structurefrom - motion ( sfm ) [ 1, 62, 64, 71 ] otherwise. since traditional optimization - based reconstruction is at odds with dynamics reconstruction, many approaches relied on adhoc semantic and motion masks to discard dynamic regions of a scene [ 6, 21, 36, 57 ]. subsequently, advances in data - driven monocular depth [ 14, 58, 59, 88 ] and optical flow [ 67, 94 ] estimation have not only enabled datadriven static reconstruction methods [ 68 ], but have also sparked research [ 34, 37, 40, 45, 63, 84 ] in dynamic scene reconstruction. although methods such as megasam [ 40 ] are promising, they rely on per -", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 2, "frag_id": 2, "text": "a temporal sequence, and as structurefrom - motion ( sfm ) [ 1, 62, 64, 71 ] otherwise. since traditional optimization - based reconstruction is at odds with dynamics reconstruction, many approaches relied on adhoc semantic and motion masks to discard dynamic regions of a scene [ 6, 21, 36, 57 ]. subsequently, advances in data - driven monocular depth [ 14, 58, 59, 88 ] and optical flow [ 67, 94 ] estimation have not only enabled datadriven static reconstruction methods [ 68 ], but have also sparked research [ 34, 37, 40, 45, 63, 84 ] in dynamic scene reconstruction. although methods such as megasam [ 40 ] are promising, they rely on per - scene optimization, making them ill - suited for real - time use. more recently, following the success of end - to - end methods [ 26, 80 ], methods such as monst3r [ 92 ] handle dynamic scenes by making independent per - frame predictions. however, they still require post - hoc optimization to establish explicit correspondences. to alleviate this, [ 32, 77, 83 ] also show the potential of feedforward multi - view inference from a set of images. following this line of work, any4d is a feed - forward model that predicts camera poses, dense 3d motion ( as scene flow ) and geometry ( as pointmaps ), fully describing a dynamic scene captured by a set of n frames in its entirety. scene flow : scene flow was introduced in [ 75 ] as the problem of recovering the 3d motion vector field for every point on every surface observed in a scene. any optical flow then is the perspective projection of scene flow onto the camera plane. subsequently, it has been studied through a wide range of approaches, ranging from variational methods [ 5, 25, 55 ] to learning - based supervised methods [ 42, 81 ] and self - supervised methods [ 49, 56, 85 ]. despite these advances, solutions to scene flow estimation have largely been tailored to specific downstream use cases, exploiting access to privileged information. in autonomous vehicles ( avs ), scene flow approaches [ 11, 74 ] typically access sensor pose through inertial and proprioceptive sensors. similarly, raft - 3d [ 69 ] assumes access to depth. recently, [ 41 ] proposed to build upon [ 77 ] for scene flow and view synthesis. however, in the spectrum of dynamism in a scene,", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 2, "frag_id": 3, "text": "subsequently, it has been studied through a wide range of approaches, ranging from variational methods [ 5, 25, 55 ] to learning - based supervised methods [ 42, 81 ] and self - supervised methods [ 49, 56, 85 ]. despite these advances, solutions to scene flow estimation have largely been tailored to specific downstream use cases, exploiting access to privileged information. in autonomous vehicles ( avs ), scene flow approaches [ 11, 74 ] typically access sensor pose through inertial and proprioceptive sensors. similarly, raft - 3d [ 69 ] assumes access to depth. recently, [ 41 ] proposed to build upon [ 77 ] for scene flow and view synthesis. however, in the spectrum of dynamism in a scene, we observe that all the above scene flow methods are limited to simplistic scenes like [ 7, 47, 48 ] with minimal dynamic motion. our model is instead capable of directly predicting scene flow in the allocentric coordinate frame. 2", "token_count": 199}
{"doc_id": "arxiv_251210935_any4d", "page": 3, "frag_id": 0, "text": "existing 4d reconstruction models any4d images - only inference flexible multi - modal inference sparse 3d motion prediction dense 3d motion prediction metric scale non - metric figure 2. any4d ’ s unified capabilities overcome major limitations of existing 4d reconstruction models. 3d tracking : while scene flow has been defined for short - range motion typically for a pair of image frames, point tracking [ 61 ] is the task of tracking a pixel trajectory over a long time horizon. following methods such as [ 20, 29, 30 ] that show the success of 2d point - tracking, tapvid - 3d [ 35 ] introduced a benchmark to address the problem of 3d point tracking. subsequently, [ 51, 86, 91 ] proposed methods for obtaining 3d point tracks and improving this benchmark. however, [ 51, 86 ] focus on ego - centric 3d point - tracking, unlike any4d which regresses allocentric 3d point - tracks. [ 87 ] recently proposed a method for allocentric 3d point tracking, by jointly optimizing camera motion, 2d and 3d point tracks. however, it is important to note that these methods can only track sparse points and require knowledge of poses and depth, either from ground truth or from running off the shelf models, limiting real - time deployment. in contrast, any4d natively supports dense 3d point tracking and can take flexible inputs, allowing adoption on a range of platforms. concurrent and recent work : we acknowledge concurrent works [ 16, 19, 27, 41, 43, 66, 93 ] that focus on predicting geometry and motion, with [ 27, 41 ] being limited to extremely small camera and scene motion. any4d differs from all concurrent methods in 3 ways ( see fig. 2 ). first, all concurrent methods require multiple feedforward passes to infer the motion, whereas any4d adopts a scalable architecture inspired by [ 32 ] and performs a single feedforward pass for all image frames at once. second, these methods only accept image inputs, while any4d which can exploit diverse multi - modal inputs. third, unlike the concurrent works, we are the only method to produce metric scale 4d reconstructions. we believe that the open - source release of any4d will set a strong foundation for the community. 3. any4d any4d is a transformer that takes flexible multi - modal inputs and outputs a dense metric - scale 4d reconstruction in a single feed -", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 3, "frag_id": 1, "text": "2 ). first, all concurrent methods require multiple feedforward passes to infer the motion, whereas any4d adopts a scalable architecture inspired by [ 32 ] and performs a single feedforward pass for all image frames at once. second, these methods only accept image inputs, while any4d which can exploit diverse multi - modal inputs. third, unlike the concurrent works, we are the only method to produce metric scale 4d reconstructions. we believe that the open - source release of any4d will set a strong foundation for the community. 3. any4d any4d is a transformer that takes flexible multi - modal inputs and outputs a dense metric - scale 4d reconstruction in a single feed - forward pass. in addition to a set rgb images i [UNK] { ii } n i = 1, any4d can use auxiliary multi - modal sensor inputs which we denote as o [UNK] ( oi ) n i = 1. then, our model can be represented as a function that maps these inputs to a factored output representation as follows : ( [UNK], { [UNK], [UNK], [UNK], [UNK] } n i = 1 ) = any4d i, o, ( 1 ) where the optional inputs o can contain information such as depth maps, camera intrinsics, camera poses from external systems or imu and doppler velocity from radar. model predictions are denoted with [UNK] order to differentiate them from ground - truth targets or auxiliary inputs. predictions include a metric scaling factor [UNK] ∈r for the entire scene, egocentric quantities predicted in the local camera coordinate frame, namely • ray directions for each view, i. e., [UNK] ∈r3×h×w • scale - normalized depth along the rays for each view, i. e., [UNK] ∈r1×h×w. and allocentric quantities predicted in a consistent world coordinate frame, namely • scale - normalized forward scene flow from the first view to all other views, i. e., [UNK] ∈r3×h×w. • camera pose of each view in the coordinate system of the first view, i. e., [UNK] [UNK] [ pi, qi ] ∈r7 represented using a scale - normalized translation vector and quaternion. now, given these output factors from any4d, one can recover the predicted metric - scale geometry [UNK] in the form of pointmaps [ 80 ] by composing the individual", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 3, "frag_id": 2, "text": "normalized depth along the rays for each view, i. e., [UNK] ∈r1×h×w. and allocentric quantities predicted in a consistent world coordinate frame, namely • scale - normalized forward scene flow from the first view to all other views, i. e., [UNK] ∈r3×h×w. • camera pose of each view in the coordinate system of the first view, i. e., [UNK] [UNK] [ pi, qi ] ∈r7 represented using a scale - normalized translation vector and quaternion. now, given these output factors from any4d, one can recover the predicted metric - scale geometry [UNK] in the form of pointmaps [ 80 ] by composing the individual quantities as [UNK] = [UNK] · [UNK] · [UNK] · [UNK] ∈r3×h×w. ( 2 ) similarly, allocentric scene flow [UNK] mi and pointmaps after motion [UNK] g ′ i can be recovered as [UNK] mi = [UNK] · [UNK] ∈r3×h×w ( 3 ) [UNK] g ′ i = [UNK] + [UNK] mi ∈r3×h×w ( 4 ) we show in sec. 4, that this parameterization of motion and geometry is optimal for model performance compared to other parameterizations. 3", "token_count": 258}
{"doc_id": "arxiv_251210935_any4d", "page": 4, "frag_id": 0, "text": "doppler dpt ( scene ﬂow ) scene flow predicted depth camera poses ray directions pose head scale mlp alternating - attention transformer shared weights scale token reference view token view 1 patch tokens view - n patch tokens view encoders rgb encoder ( dinov2 ) ray directions ray depth pose translation pose rotation doppler encoder view encoders rgb encoder ( dinov2 ) ray directions ray depth pose translation pose rotation doppler encoder view n pose depth rays rgb view 1 optional inputs optional inputs pose depth doppler rays rgb dpt ( geometry ) metric scale factor figure 3. any4d predicts a factorized dense metric 4d reconstruction represented as a global metric scale, per - view egocentric factors ( depth maps and ray directions ) and per - view allocentric factors ( forward scene flow and camera poses ) as explained in sec. 3. any4d is a n - view transformer, consisting of modality - specific encoders, followed by an alternating - attention transformer to produce contextual patch embeddings. the output tokens from the transformer are then decoded using individual decoders specific to each factor. 3. 1. architecture any4d largely follows a multi - view transformer architecture, similar to [ 32 ] ( see fig. 3 ). conceptually, it can be separated into three sections : a ) modality specific input encoders, b ) a multi - view transformer backbone that attends to the tokens from all views, and c ) output representation heads which decode the tokens into the factorized output variables for each view. multi - modal input encoders : rgb inputs i and auxiliary multi - modal sensor inputs o are mapped to viewspecific patch tokens through multi - modal view encoders with shared weights for input views which map to a r1024×h / 14×w / 14 feature space. we follow the design choices in [ 32 ] for rgb, depth, camera poses and intrinsics encoders, and additionally, add a cnn encoder to encode doppler velocity. we summarize these below : • rgb images : dinov2 [ 53 ] for encoding images, to extract the layer - normalized patch - level features from the final layer of dinov2 vit - large, fi ∈r10", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 4, "frag_id": 1, "text": "##rs : rgb inputs i and auxiliary multi - modal sensor inputs o are mapped to viewspecific patch tokens through multi - modal view encoders with shared weights for input views which map to a r1024×h / 14×w / 14 feature space. we follow the design choices in [ 32 ] for rgb, depth, camera poses and intrinsics encoders, and additionally, add a cnn encoder to encode doppler velocity. we summarize these below : • rgb images : dinov2 [ 53 ] for encoding images, to extract the layer - normalized patch - level features from the final layer of dinov2 vit - large, fi ∈r1024×h / 14×w / 14. • depth images : a shallow cnn encoder is used to encode depth images, where we normalize the input depth before passing it to the depth encoder. the normalization factor is computed independently for each local view. • doppler velocity : doppler velocity is also encoded using a cnn - based encoder. however, here the normalization factor for encoding the doppler velocity is computed from the first - view pointmap and shared globally. • camera intrinsics : camera intrinsics are encoded as rays, and also use a cnn that maps the 3 - channel raydirections into the same 1024 - dimensional latent space. • camera poses : two 4 - layer mlp encoders are used for camera rotation and translation that map normalized input poses to latent vectors, frot ∈r1024 and ftrans ∈r1024. the normalization factor for pose translation is computed globally across all views, and a positional encoding is used to indicate the reference view pref ∈r1024. • metric scale token : for metric - scale data, the depth scale and pose scale obtained from normalizing depth and pose are first transformed to log - scale and then encoded using a 4 - layer mlp, yielding two r1024 latent features. all multimodal encodings thus obtained are aggregated via summation into a per - view embedding fview ∈ r1024×h / 14×w / 14, which are flattened into tokens, along with an added learnable token to learn the metric - scale. transformer backbone : we use an alternating - attention transformer [ 77 ] across", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 4, "frag_id": 2, "text": "computed globally across all views, and a positional encoding is used to indicate the reference view pref ∈r1024. • metric scale token : for metric - scale data, the depth scale and pose scale obtained from normalizing depth and pose are first transformed to log - scale and then encoded using a 4 - layer mlp, yielding two r1024 latent features. all multimodal encodings thus obtained are aggregated via summation into a per - view embedding fview ∈ r1024×h / 14×w / 14, which are flattened into tokens, along with an added learnable token to learn the metric - scale. transformer backbone : we use an alternating - attention transformer [ 77 ] across the views, consisting of 12 blocks of 12 multi - head attention and mlps. each transformer block processes tokens with a latent dimension of 768 and contains mlps with a ratio of 4, similar to the vit - base archi4", "token_count": 202}
{"doc_id": "arxiv_251210935_any4d", "page": 5, "frag_id": 0, "text": "any4d spatialtrackerv2 st4rtrack figure 4. any4d provides dense and precise motion estimation, where on the other hand, state - of - the - art baselines either produce reliable but sparse motion ( spatialtrackerv2 [ 87 ] ) or dense per - pixel motion that is not accurate ( st4rtrack [ 16 ] ). for spatialtrackerv2, we are only able to uniformly query a maximum of 2500 points with a h100 gpu using 80 gigabytes of gpu memory. note that we don ’ t use any pre - computed segmentation mask but purely threshold our scene flow output to get a binary motion mask. st4rtrack cannot produce good binary motion masks due to incorrect scene flow predictions on object boundaries and the background. tecture. furthermore, consistent with [ 32 ] we choose to not use 2 - d rotary positional encoding ( rope ) for the inputs, and also employ flash attention [ 12 ] for efficiency. output representation heads : we decode the multiview tokens from the transformer backbone into a factored output representation as follows : • geometry dpt head : we use a dense prediction transformer ( dpt ) [ 58 ] head to predict per - view ray directions [UNK], up - to - scale ray depths [UNK], and confidence masks. • motion dpt head : a second dpt head is tasked to predict per - view forward allocentric scene flow [UNK]. the scene flow represents motion of points in the reference view - 0 to all other views. • pose decoder : the pose decoder is an average - poolingbased cnn decoder that predicts per - view, up - to - scale translations and quaternions [UNK] [UNK] [ pi, qi ]. • metric scale decoder : we use a lightweight mlp decoder to predict the log scale metric scaling factor, which is subsequently exponentiated. 3. 2. training details datasets : despite recent efforts [ 27 ], there is a lack of large - scale datasets that contain dynamic scene motion annotations. in fact, reliable, high - quality scene flow annotations are sparse and only available from simulation engines [ 18, 28 ]. we address this challenge in this work by a ) finetuning large - scale pretrained geometry models and b ) training with partial supervision. owing to our factored representation, we are able to train", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 5, "frag_id": 1, "text": "scale translations and quaternions [UNK] [UNK] [ pi, qi ]. • metric scale decoder : we use a lightweight mlp decoder to predict the log scale metric scaling factor, which is subsequently exponentiated. 3. 2. training details datasets : despite recent efforts [ 27 ], there is a lack of large - scale datasets that contain dynamic scene motion annotations. in fact, reliable, high - quality scene flow annotations are sparse and only available from simulation engines [ 18, 28 ]. we address this challenge in this work by a ) finetuning large - scale pretrained geometry models and b ) training with partial supervision. owing to our factored representation, we are able to train on a mixture of both geometry - only and dynamic datasets, where they can be synthetic or realworld with varying sparsity of labels : blendedmvs [ 89 ], megadepth [ 39 ], scannet + + [ 90 ], vkitti2 [ 7 ], paralleldomain4d [ 73 ], waymo - drivetrack [ 4 ], sail - vos3d [ 23 ] pointodyssey [ 95 ], dynamic replica [ 28 ] and kubric [ 18 ] data generated by cotracker3 [ 29 ] and gcd [ 73 ]. detailed information of all datasets used for training is available in the appendix. training with multi - modal conditioning : we preprocess the datasets and generate multi - modal inputs offline for faster training. geometric inputs consisting of poses, depths and intrinsics are directly taken from the dataset annotations. to simulate doppler velocity, we take the radial component of egocentric scene flow between data pairs. during training, multi - modal conditioning is applied with a probability of 0. 7, i. e., 70 % of training iterations include multimodal inputs alongside images. additionally, we ensure that individual modalities ( depth, rays, poses, and doppler ) are independently removed with a probability of 0. 5 to promote effective learning in flexible input configurations. fi5", "token_count": 442}
{"doc_id": "arxiv_251210935_any4d", "page": 6, "frag_id": 0, "text": "nally, we initialize our network with mapanything weights [ 32 ]. for each training batch, we sample up to 4 views from the datasets and train on 1 h100 node for 100 epochs. losses : any4d is trained using a combination of geometric and motion losses based on the type of annotation available. ray directions representing the camera intrinsics and quaternions are scale - agnostic, and therefore can be supervised via simple regression losses : lrays [UNK] n x i = 1 [UNK] [UNK] ( 5 ) lrotation [UNK] n x i = 1 min ( [UNK] [UNK], [UNK] + [UNK] ). ( 6 ) on the other hand, geometric quantities such as camera translations ti, ray depths di and scene flow fi are predicted in a scale - normalized coordinate frame. following prior work [ 32, 38, 80 ], we use the ground - truth validity masks vi and pointmaps xi and compute the ground - truth scale as the average euclidean distance of valid points with respect to the world origin ( given by the first view camera frame ) : z = [UNK] { xi [ vi ] } n i [UNK] / pn i vi. to compute scaleinvariant losses, we also compute a scale factor derived from our predictions [UNK] = [UNK] { [UNK] [ vi ] } n i [UNK] / pn i vi : ltrans [UNK] n x i ti zi − [UNK] [UNK], ( 7 ) ldepth [UNK] n x i flog di zi −flog [UNK] [UNK]! ( 8 ) where flog ( x ) [UNK] ( x / [UNK] ) log ( 1 + [UNK] ) converts quantities to log - space for numerical stability. a pointmap loss is also applied to the composed geometric predictions as follows : lpm [UNK] n x i flog xi zi −flog [UNK] [UNK]! ( 9 ) similarly, scene flow is also supervised in a scaleinvariant manner. we find that scene flow loss is dominated by static points since most of the scene is static. therefore, we find it is crucial to calculate a static - dynamic motion mask m from the ground truth scene flow, and upweight the scene flow loss in the dynamic regions by 10x more compared to static regions : lsf [UNK] n x i m · flog fi zi −flog [UNK] [UNK]! ( 10 ) finally, the predicted metric scale factor [UNK] is also supervised in the log space as follows : lscale [UNK]", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 6, "frag_id": 1, "text": "to the composed geometric predictions as follows : lpm [UNK] n x i flog xi zi −flog [UNK] [UNK]! ( 9 ) similarly, scene flow is also supervised in a scaleinvariant manner. we find that scene flow loss is dominated by static points since most of the scene is static. therefore, we find it is crucial to calculate a static - dynamic motion mask m from the ground truth scene flow, and upweight the scene flow loss in the dynamic regions by 10x more compared to static regions : lsf [UNK] n x i m · flog fi zi −flog [UNK] [UNK]! ( 10 ) finally, the predicted metric scale factor [UNK] is also supervised in the log space as follows : lscale [UNK] ( z ) − flog ( [UNK] · sg ( [UNK] ) [UNK], where sg denotes the stop - gradient operation and prevents the scale supervision from affecting other predicted quantities. the final loss is expressed as : l = ltrans + lrot + lrays + ldepth + lsf + lmask ( 11 ) 4. results & analysis we evaluate any4d on diverse benchmarking setups specifically designed for allocentric 4d reconstruction, and compare against state - of - the - art ( sota ) methods. 3d tracking : there is a lack of standard and unified benchmarks for evaluating 4d reconstruction in the existing literature. to create allocentric 3d tracking benchmarks, we follow [ 16 ] and repurpose existing 3d tracking benchmark, particularly tapvid - 3d [ 35 ]. however, tapvid - 3d have their own limitations : the aria digital twin ( adt ) sequences are largely static, parallel - studio sequences contain fixed - camera viewpoints, while the drivetrack sequences are extremely sparse. hence, we choose to drop adt, and keep parallel - studio and drive - track benchmarking sequences. we also add unseen held - out sequences from dynamic replica [ 28 ] and a zero - shot dataset lsfodyssey [ 76 ], both of which contain camera motion along with 3d tracking labels. the final benchmark contains [UNK] sequences across 4 datasets of up to 64 frames in length. we evaluate any4d against sota 3d trackers spatialtrackerv2 [ 87 ] and st4rtrack [ 16 ]. we also compose 3d reconstruction models [ 32, 38, 77, 92 ] with 2d tracks from cotracker", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 6, "frag_id": 2, "text": "camera viewpoints, while the drivetrack sequences are extremely sparse. hence, we choose to drop adt, and keep parallel - studio and drive - track benchmarking sequences. we also add unseen held - out sequences from dynamic replica [ 28 ] and a zero - shot dataset lsfodyssey [ 76 ], both of which contain camera motion along with 3d tracking labels. the final benchmark contains [UNK] sequences across 4 datasets of up to 64 frames in length. we evaluate any4d against sota 3d trackers spatialtrackerv2 [ 87 ] and st4rtrack [ 16 ]. we also compose 3d reconstruction models [ 32, 38, 77, 92 ] with 2d tracks from cotracker3 [ 29 ] for comparison. we use standard benchmarking protocols [ 16, 35, 69, 87 ] to evaluate the quality of our 4d reconstruction. following [ 16 ], we first perform median - scaling to align to metric space. we report average percent of points within delta for 3d points after motion ( apd ) and inlier percentage τ for scene flow. we also report end point error ( epe ) for 3d points after motion ( dynamic points ) and 3d scene flow vectors. apd and τ are defined as : apd = x i, t 1 · pi, t [UNK], t < δ3d ( 12 ) τ = x i, t 1 · fi, t [UNK], t < 0. 1m ( 13 ) where [UNK] represents the predicted 3d point after motion and [UNK] is the corresponding scene flow vector at time t. for apd, we use thresholds δ3d ∈ { 0. 1, 0. 3, 0. 5, 1. 0 } m. as evident in tab. 1, any4d shows state - of - the - art performance across all datasets. furthermore, it is 15× faster than the closest performing method, spatialtrackerv2. this is further reinforced qualitatively in fig. 4. dense scene flow : we construct allocentric scene flow benchmarks by repurposing 2 egocentric scene flow benchmarking datasets : vkitti - 2 [ 7 ] and kubric - 4d [ 72 ]. while scene flow in vkitti - 2 is limited to small consecutive frame motion, we can simulate scene flow across 60 frames and 16 camera viewpoints from kubric4d ( gcd )", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 6, "frag_id": 3, "text": "} m. as evident in tab. 1, any4d shows state - of - the - art performance across all datasets. furthermore, it is 15× faster than the closest performing method, spatialtrackerv2. this is further reinforced qualitatively in fig. 4. dense scene flow : we construct allocentric scene flow benchmarks by repurposing 2 egocentric scene flow benchmarking datasets : vkitti - 2 [ 7 ] and kubric - 4d [ 72 ]. while scene flow in vkitti - 2 is limited to small consecutive frame motion, we can simulate scene flow across 60 frames and 16 camera viewpoints from kubric4d ( gcd ). hence we create 2 variants for kubric4d ( gcd ) : ( a ) scene flow from static camera movement and ( b ) scene flow from wide - baseline dynamic camera movement. importantly, all 6", "token_count": 194}
{"doc_id": "arxiv_251210935_any4d", "page": 7, "frag_id": 0, "text": "table 1. any4d showcases state - of - the - art sparse 3d point tracking, while providing dense motion predictions and being an order of magnitude faster than the closest performing baseline. we report end - point error ( epe ), average points within delta ( apd ) and inlier ratio at 0. 1m ( τ ) for dynamic points in the benchmark. the runtime is computed on a h100 using 50 frames as input. best results are bold. drive track [ 4 ] dynamic replica [ 28 ] lsfodyssey [ 76 ] pstudio [ 35 ] dynamic points scene flow dynamic points scene flow dynamic points scene flow dynamic points scene flow method runtime ( s ) epe ↓ apd ↑ epe ↓ τ ↑ epe ↓ apd ↑ epe ↓ τ ↑ epe ↓ apd ↑ epe ↓ τ ↑ epe ↓ apd ↑ epe ↓ τ ↑ monst3r + cotracker3 146. 40 16. 81 0. 44 21. 87 0. 06 0. 81 43. 34 0. 18 25. 99 0. 61 50. 96 0. 41 43. 64 0. 51 51. 87 0. 52 21. 06 mast3r + cotracker3 13. 82 17. 16 1. 22 20. 01 0. 20 0. 40 57. 72 0. 23 53. 98 0. 83 45. 95 0. 62 41. 10 0. 43 54. 11 0. 43 14. 69 vggt + cotracker3 2. 31 8. 30 4. 80 11. 69 0. 77 0. 26 69. 12 0. 06 89. 37 0. 47 59. 21 0. 22 74. 11 0. 26 69. 34 0. 17 45. 77 mapanything + cotracker3 0. 73 9. 42 2. 45 12. 88 0. 43 0. 25 70. 51 0. 06 89. 59 0. 63 35. 51 0. 51 58. 00 0. 63 50. 85 0. 35 58. 01 st4rtrack 1. 12 11. 82 1. 03 14. 63 0. 10 0. 17 80. 87 0. 07 77. 90 0. 56 48. 11 0. 25 38. 31 0. 41 53. 12 0. 21 28. 46 spatialtrackerv2 11. 56 5. 45 4. 48 10. 63 0. 10 0", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 7, "frag_id": 1, "text": ". 11 0. 26 69. 34 0. 17 45. 77 mapanything + cotracker3 0. 73 9. 42 2. 45 12. 88 0. 43 0. 25 70. 51 0. 06 89. 59 0. 63 35. 51 0. 51 58. 00 0. 63 50. 85 0. 35 58. 01 st4rtrack 1. 12 11. 82 1. 03 14. 63 0. 10 0. 17 80. 87 0. 07 77. 90 0. 56 48. 11 0. 25 38. 31 0. 41 53. 12 0. 21 28. 46 spatialtrackerv2 11. 56 5. 45 4. 48 10. 63 0. 10 0. 69 62. 34 0. 06 83. 66 0. 34 68. 37 0. 09 78. 75 0. 21 74. 46 0. 14 50. 70 any4d 0. 50 3. 89 7. 81 3. 14 1. 83 0. 07 93. 44 0. 05 86. 99 0. 27 71. 70 0. 10 71. 41 0. 27 67. 43 0. 19 33. 57 table 2. any4d achieves state - of - the - art dense scene flow estimation performance. we report end - point error ( epe ), average points within delta ( apd ) and inlier ratio at 0. 1m ( τ ) for dynamic points and scene flow across three datasets, where best results are bold. kubric - 4d dynamic camera kubric - 4d static camera vkitti - 2 dynamic points scene flow dynamic points scene flow dynamic points scene flow method epe ↓ apd ↑ epe ↓ τ ↑ epe ↓ apd ↑ epe ↓ τ ↑ epe ↓ apd ↑ epe ↓ τ ↑ monst3r + sea - raft 5. 23 2. 20 3. 73 14. 69 2. 26 6. 80 1. 16 61. 79 12. 31 0. 44 1. 21 12. 93 mast3r + sea - raft 6. 35 1. 92 1. 45 13. 95 2. 85 7. 58 1. 26 53. 62 12. 25 2. 50 13. 05 10. 20 vggt + sea - raft 11. 80 3. 60 11. 76 14. 53 1. 92 15. 01 0. 78 86. 54 6. 57 2. 61", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 7, "frag_id": 2, "text": "##e ↓ τ ↑ epe ↓ apd ↑ epe ↓ τ ↑ epe ↓ apd ↑ epe ↓ τ ↑ monst3r + sea - raft 5. 23 2. 20 3. 73 14. 69 2. 26 6. 80 1. 16 61. 79 12. 31 0. 44 1. 21 12. 93 mast3r + sea - raft 6. 35 1. 92 1. 45 13. 95 2. 85 7. 58 1. 26 53. 62 12. 25 2. 50 13. 05 10. 20 vggt + sea - raft 11. 80 3. 60 11. 76 14. 53 1. 92 15. 01 0. 78 86. 54 6. 57 2. 61 0. 70 37. 63 mapanything + sea - raft 17. 65 2. 67 17. 70 9. 16 2. 82 19. 99 1. 75 73. 33 8. 46 2. 42 1. 32 13. 78 st4rtrack 2. 44 5. 79 1. 70 11. 83 2. 61 6. 53 0. 72 20. 51 14. 71 0. 00 0. 97 3. 37 any4d 1. 13 18. 14 0. 17 83. 38 1. 23 19. 53 0. 10 87. 51 4. 97 11. 70 0. 04 93. 08 via allocentric flow via egocentric flow via 3d points after motion allocentric scene flow extracted via different output representations input images figure 5. scene motion parametrized as allocentric scene flow provides the cleanest 4d reconstructions. we find that other parameterizations such as 3d points after motion ( proposed in st4rtrack [ 16 ] ) provide extreme noise on object boundaries and background. pairs from both datasets are from held - out scenes to ensure there is no data leak from the training datasets. we evaluate any4d against st4rtrack which can predict dense scene flow, and 3d reconstruction method outputs composed with optical flow from sea - raft [ 82 ], to calculate covisible scene flow. we are unable to run spatialtrackerv2 or cotracker3 as they do not support per - pixel point queries and run out - of - memory ( oom ). from tab. 2, we see that any4d outperforms baselines by 2 −3× on average on apd, and by", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 7, "frag_id": 3, "text": "##k [ 16 ] ) provide extreme noise on object boundaries and background. pairs from both datasets are from held - out scenes to ensure there is no data leak from the training datasets. we evaluate any4d against st4rtrack which can predict dense scene flow, and 3d reconstruction method outputs composed with optical flow from sea - raft [ 82 ], to calculate covisible scene flow. we are unable to run spatialtrackerv2 or cotracker3 as they do not support per - pixel point queries and run out - of - memory ( oom ). from tab. 2, we see that any4d outperforms baselines by 2 −3× on average on apd, and by even more on scene - flow metrics. video depth : we also evaluate any4d on standard video depth benchmarks [ 17, 46, 54 ] in tab. 3, against specialized video depth baselines [ 9, 22 ], feed - forward + iterative optimization baselines [ 40, 80, 87, 92 ], and single - step feed - forward baselines [ 32, 77, 79 ]. any4d shows state of the art video depth estimation over other single - shot feedforward inference baselines while being competitive with optimization based and task - specific methods. support for multi - modal inputs : since any4d can utilize flexible inputs for inference to enhance performance, 7", "token_count": 291}
{"doc_id": "arxiv_251210935_any4d", "page": 8, "frag_id": 0, "text": "table 3. any4d shows state - of - the - art video depth estimation over other single - step feed - forward baselines. it is also competitive to iterative / optimization - based methods or ones trained specifically for this task. we report the absolute relative error ( rel ) and the inlier ratio at 1. 25 % ( δ1. 25 ), where the best is bold. average bonn kitti sintel method rel ↓ δ1. 25 ↑ rel ↓ δ1. 25 ↑ rel ↓ δ1. 25 ↑ rel ↓ δ1. 25 ↑ a ) video depth : depthcrafter 0. 15 85. 23 0. 07 97. 90 0. 11 88. 50 0. 27 69. 30 vda 0. 17 86. 90 0. 05 98. 20 0. 08 95. 10 0. 37 67. 40 b ) feed - forward + iterative optimization : dust3r 0. 26 75. 83 0. 17 83. 50 0. 12 84. 90 0. 48 59. 10 monst3r 0. 16 82. 73 0. 06 95. 40 0. 08 93. 40 0. 34 59. 40 megasam 0. 10 87. 97 0. 04 97. 70 0. 07 91. 60 0. 18 74. 60 spatialtrackerv2 0. 09 88. 80 0. 03 98. 80 0. 05 97. 30 0. 20 70. 30 c ) single - step feed - forward : cut3r 0. 21 80. 30 0. 07 95. 00 0. 10 89. 90 0. 47 56. 00 vggt 0. 13 85. 85 0. 07 97. 27 0. 09 94. 37 0. 24 65. 90 mapanything 0. 14 84. 97 0. 09 94. 77 0. 09 94. 26 0. 25 65. 87 any4d 0. 13 86. 28 0. 07 97. 27 0. 09 93. 97 0. 24 67. 59 table 4. auxiliary inputs improve the 4d motion estimation performance of any4d. we compare different inputs on both dense scene flow ( kubric ) and sparse 3d point tracking ( lsfodyssey ) benchmarks using end - point error ( epe ), average points within delta ( apd ) and inlier ratio at 0. 1m ( τ ), where best is bold. “ geometry ” indicates", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 8, "frag_id": 1, "text": ". 09 94. 37 0. 24 65. 90 mapanything 0. 14 84. 97 0. 09 94. 77 0. 09 94. 26 0. 25 65. 87 any4d 0. 13 86. 28 0. 07 97. 27 0. 09 93. 97 0. 24 67. 59 table 4. auxiliary inputs improve the 4d motion estimation performance of any4d. we compare different inputs on both dense scene flow ( kubric ) and sparse 3d point tracking ( lsfodyssey ) benchmarks using end - point error ( epe ), average points within delta ( apd ) and inlier ratio at 0. 1m ( τ ), where best is bold. “ geometry ” indicates use of depth, intrinsics and poses. kubric - 4d static camera lsfodyssey dynamic points scene flow dynamic points scene flow any4d inputs epe ↓ apd ↑ epe ↓ τ ↑ epe ↓ apd ↑ epe ↓ τ ↑ images only 1. 17 21. 33 0. 11 86. 25 0. 28 71. 47 0. 12 68. 03 images + geometry 0. 23 80. 18 0. 09 86. 26 0. 19 80. 80 0. 12 68. 71 images + doppler 1. 17 21. 70 0. 12 86. 90 0. 29 71. 26 0. 11 70. 32 images + geometry + doppler 0. 23 81. 72 0. 09 87. 27 0. 19 81. 10 0. 11 71. 37 we study improvements to scene flow on the dense kubric4d static benchmark and 3d tracking on lsfodyssey benchmark by incorporating different input modalities. from tab. 4, we observe that adding geometry significantly improves apd and epe for 3d points. adding doppler further improves scene - flow, with the best performance achieved when all modalities are provided. choice of motion representation : while allocentric motion fallo is arguably the useful quantity for downstream applications, it is possible to represent the predicted scene flow output in 4 ways : • allocentric scene flow : directly predicting [UNK]. • egocentric scene flow : predicting egocentric scene flow [UNK], and using estimated geometry to recover allocentric motion as : [UNK] = tt→t + 1 p v 0 + [UNK] −p • 3d points after motion : predicting view - aligned pointmaps", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 8, "frag_id": 2, "text": "input modalities. from tab. 4, we observe that adding geometry significantly improves apd and epe for 3d points. adding doppler further improves scene - flow, with the best performance achieved when all modalities are provided. choice of motion representation : while allocentric motion fallo is arguably the useful quantity for downstream applications, it is possible to represent the predicted scene flow output in 4 ways : • allocentric scene flow : directly predicting [UNK]. • egocentric scene flow : predicting egocentric scene flow [UNK], and using estimated geometry to recover allocentric motion as : [UNK] = tt→t + 1 p v 0 + [UNK] −p • 3d points after motion : predicting view - aligned pointmaps at time 0 and t - p v 0 and p v t, and recovering the allocentric motion : [UNK] = p v t −p v 0 • backprojected 2d flow : unprojecting optical flow to obtain covisible scene flow between pointmaps. we systematically investigate these choices in tab. 5 and fig. 5. we find that directly predicting allocentric motion table 5. allocentric scene flow is the optimal output representation for 4d motion. we compare different representation types on dense scene flow ( kubric ) and sparse 3d point tracking ( lsfodyssey ) using end - point error ( epe ), average points within delta ( apd ) and inlier ratio at 0. 1m ( τ ). best results are bold. kubric - 4d static camera lsfodyssey dynamic points scene flow dynamic points scene flow representation type epe ↓ apd ↑ epe ↓ τ ↑ epe ↓ apd ↑ epe ↓ τ ↑ backprojected 2d flow 2. 14 19. 44 1. 16 75. 69 0. 49 57. 21 0. 27 70. 11 3d points after motion 1. 24 17. 33 0. 58 21. 84 0. 24 69. 30 0. 38 21. 87 egocentric scene flow 1. 26 19. 43 0. 12 85. 37 0. 24 71. 80 0. 14 65. 13 allocentric scene flow 1. 23 19. 53 0. 10 87. 51 0. 24 73. 95 0. 10 71. 46 leads to optimal performance not only on scene flow, but surprisingly, also on dynamic pointmaps after motion, compared to directly predicting points after", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 8, "frag_id": 3, "text": "##d ↑ epe ↓ τ ↑ backprojected 2d flow 2. 14 19. 44 1. 16 75. 69 0. 49 57. 21 0. 27 70. 11 3d points after motion 1. 24 17. 33 0. 58 21. 84 0. 24 69. 30 0. 38 21. 87 egocentric scene flow 1. 26 19. 43 0. 12 85. 37 0. 24 71. 80 0. 14 65. 13 allocentric scene flow 1. 23 19. 53 0. 10 87. 51 0. 24 73. 95 0. 10 71. 46 leads to optimal performance not only on scene flow, but surprisingly, also on dynamic pointmaps after motion, compared to directly predicting points after motion as adopted otherwise in [ 16 ]. limitations : although any4d takes a step forward towards achieving 4d reconstruction models, we identify important limitations. firstly, we always calculate scene - flow from the reference ( first ) view to all other frames in the sequence, necessitating that the object of interest should be present at the start of the video. one possible way to alleviate this is by training any4d in a permutation invariant manner as in [ 83 ]. secondly, we assume perfectly simulated multi - modal input and do not account for sensor noise - which is hardly true for real - world deployment. finally, as with all data - driven architectures, generalization is a function of the diversity and size of the training set. we believe that any4d ’ s performance on highly dynamic scenes and wide baselines ( or low frame - rate videos ) can be improved with the availability of richer dynamic 3d datasets [ 70 ]. 5. conclusion we presented any4d, a unified model that enables dense 4d reconstruction of dynamic scenes from both monocular and multi - modal setups. in any4d, we chose a factorized output representation of 4d scenes, which allowed the use of diverse data for training at scale with partial supervision for auxiliary sub - tasks, in addition to the target task of dense scene flow estimation. any4d is flexible, and supports optional multi - modal inputs. importantly, we showed through our experiments that our joint training scheme produces generalizable view embeddings that improve performance whenever inputs such as depth and egocentric radial velocity ( doppler ) may be available to support the output prediction quantities. finally, due to the feed - forward nature of", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 8, "frag_id": 4, "text": "##4d, a unified model that enables dense 4d reconstruction of dynamic scenes from both monocular and multi - modal setups. in any4d, we chose a factorized output representation of 4d scenes, which allowed the use of diverse data for training at scale with partial supervision for auxiliary sub - tasks, in addition to the target task of dense scene flow estimation. any4d is flexible, and supports optional multi - modal inputs. importantly, we showed through our experiments that our joint training scheme produces generalizable view embeddings that improve performance whenever inputs such as depth and egocentric radial velocity ( doppler ) may be available to support the output prediction quantities. finally, due to the feed - forward nature of any4d, we saw that one can obtain dynamic scene estimates an order of magnitude faster than existing methods such as spatialtrackerv2 [ 86 ], by exploiting n - view inference. we believe any4d will ultimately enable real - time 4d scene reconstruction for applications such as generative ai, ar / vr and robotics, and serve as a foundational 4d reconstruction model. 8", "token_count": 231}
{"doc_id": "arxiv_251210935_any4d", "page": 9, "frag_id": 0, "text": "any4d : unified feed - forward metric 4d reconstruction supplementary material table s. 1. list of datasets used to train any4d dataset dynamic scene flow domain # scenes blendedmvs [UNK] [UNK] outdoor & object centric 500 megadepth [UNK] [UNK] outdoor 275 scannet + + [UNK] [UNK] indoor 295 vkitti2 [UNK] [UNK] av 40 waymo - drivetrack [UNK] [UNK] av 1500 gcd - kubric [UNK] [UNK] synthetic random objects 5000 cotracker3 - kubric [UNK] [UNK] synthetic random objects 5000 dynamic replica [UNK] [UNK] synthetic humans & animals 500 point odyssey [UNK] [UNK] diverse synthetic assets 159 a. training datasets : we train on a combination of static and dynamic datasets with varying levels of supervision. for supervision geometric quantities - depth, intrinsics, and camra poses, all the datasets in s. 1 are used. for scene flow supervision, we only rely on kubric ( from cotracker3 ), pointodyssey and dynamic replica, as they contain both diverse camera and scene motion crucial for learning good scene flow. we find that vkitti - 2 sequences span minimal scene motion while data from gcd lacks good camera diversity, and thus, only use them for geometry supervision. implementation details : we initialize any4d ’ s weights with the public mapanything checkpoint. the doppler scene - flow encoder, and the scene - flow dpt decoder are initialized and learnt from scratch. we train the entire network with a learning rate of 1e - 5, 5e - 7 and 1e - 4 for the entire network, the dinov2 image encoder and the sceneflow dpt decoder respectively. we use a warmup of 10 epochs, and finetune the network for a total of 100 epochs, covering approximately 120k gradient steps in total on 8 h100 gpus. the images and respective quantities in each batch cropped and resized to 518 image width, with a randomized height - width aspect ratio between 0. 5 and 3. during each gradient step, we sample upto 4 views from each dataset, with a variable batch size of upto 24 views per gpu. as illustrated in fig. s. 1, we find that 4 - view training is critical for generalizing with multi - view inference. b. benchmarking setup details for the tapvid - 3d pstudio dataset and", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 9, "frag_id": 1, "text": "we use a warmup of 10 epochs, and finetune the network for a total of 100 epochs, covering approximately 120k gradient steps in total on 8 h100 gpus. the images and respective quantities in each batch cropped and resized to 518 image width, with a randomized height - width aspect ratio between 0. 5 and 3. during each gradient step, we sample upto 4 views from each dataset, with a variable batch size of upto 24 views per gpu. as illustrated in fig. s. 1, we find that 4 - view training is critical for generalizing with multi - view inference. b. benchmarking setup details for the tapvid - 3d pstudio dataset and drivetrack datasets, we evaluated on a uniform subset of 50 sequences from all available datasets and use the first 64 frames for evaluation. since the dataset is extremely sparse and each sequence only contains at most a few hundred point queries, we use all points for benchmarking. for drive - track, we filter 50 sequences that contain non - zero allocentric motion. for the dynamic - replica and lsf - odyssey datasets, we fil2 4 8 16 32 64 number of views at inference 0. 18 0. 20 0. 22 0. 24 0. 26 scene - flow epe multi - view finetuning ablation for multi - view generalization any4d : 2 - view training any4d : 4 - view training figure s. 1. 4 - view training is key to enabling multi - frame generalization during inference. any4d trained with 2 views results in higher epe at higher number of input views. in contrast, the 4view model exhibits stable behaviour even at 64 views. ter out static points ( i. e., points with zero allocentric motion ) and use dynamic points as queries for our benchmarking, to maintain homogeneity with the 2 other datasets and emphasize benchmarking of dynamic elements of a scene. we acknowledge that our evaluation is similar to [ 16 ]. figure s. 2. doppler scene flow is simulated as radial component of ego - centric scene flow. c. multi - modal conditioning simulating doppler velocity : as shown in fig. s. 2, we simulate the doppler velocity from egocentric scene flow labels. more specifically, given a 3d point", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 9, "frag_id": 2, "text": "contrast, the 4view model exhibits stable behaviour even at 64 views. ter out static points ( i. e., points with zero allocentric motion ) and use dynamic points as queries for our benchmarking, to maintain homogeneity with the 2 other datasets and emphasize benchmarking of dynamic elements of a scene. we acknowledge that our evaluation is similar to [ 16 ]. figure s. 2. doppler scene flow is simulated as radial component of ego - centric scene flow. c. multi - modal conditioning simulating doppler velocity : as shown in fig. s. 2, we simulate the doppler velocity from egocentric scene flow labels. more specifically, given a 3d point p = [ x, y, z ] and its corresponding ego scene flow vector v = [ ∆x, ∆y, ∆z ], the simulated doppler velocity vr is defined as the projection of the motion vector into the radial direction of each ray. this is simply the normalized vector from the origin of the radar to the point p. the doppler ( radial ) velocity is computed as : vr = p · v [UNK] = x · ∆x + y · ∆y + z · ∆z p x2 + y2 + z2 9", "token_count": 264}
{"doc_id": "arxiv_251210935_any4d", "page": 10, "frag_id": 0, "text": "figure s. 3. qualitative visualizations of any4d estimating 3d geometry and point tracking on tapvid - 3d waymo drive - track sequences. as visible, the image - only variant ( column 1 ) sometimes produces an offset to the scene flow at the edges. however, the predictions improve whenever sparse geometry ( column 2 ) and doppler annotations are available ( column 3 ). figure s. 4. qualitative visualizations of any4d limitations. videos with large camera motion inducing no visual overlap of background or scene motion dominating the image space are common failure modes for any4d. we believe that the availability of large - scale dense scene flow and 3d tracking datasets and integrating real - time optimization is key to overcoming these limitations. acknowledgments we thank tarasha khurana and neehar peri for their initial discussions in the project. we appreciate the help from jeff tan with setting up stereo4d ( which we ended up not using due to poor dataset quality ). lastly, we thank bardienus duisterhof and members of the airlab & deva ’ s lab at cmu for insightful discussions and feedback on the paper. this work was supported by defense science and technology agency contract # dst000ec124000205, bosch research, and the iarpa via department of interior / interior business center ( doi / ibc ) contract 140d0423c0074. the u. s. government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation thereon. disclaimer : the views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of iarpa, doi / ibc, or the u. s. government. lastly, this work was supported by a hardware grant from nvidia and used psc bridges - 2 through allocation cis220039p from the advanced cyberinfrastructure coordination ecosystem : services & support ( access ) program. references [ 1 ] sameer agarwal, noah snavely, ian simon, steven m. seitz, and richard szeliski. building rome in a day. in 2009 ieee 12th international conference on computer vision, pages 72 – 79, 2009. 2 [ 2 ] manuel l´opez antequera, pau", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 10, "frag_id": 1, "text": "necessarily representing the official policies or endorsements, either expressed or implied, of iarpa, doi / ibc, or the u. s. government. lastly, this work was supported by a hardware grant from nvidia and used psc bridges - 2 through allocation cis220039p from the advanced cyberinfrastructure coordination ecosystem : services & support ( access ) program. references [ 1 ] sameer agarwal, noah snavely, ian simon, steven m. seitz, and richard szeliski. building rome in a day. in 2009 ieee 12th international conference on computer vision, pages 72 – 79, 2009. 2 [ 2 ] manuel l´opez antequera, pau gargallo, markus hofinger, samuel rota bulo, yubin kuang, and peter kontschieder. mapillary planet - scale depth dataset. in european conference on computer vision, pages 589 – 604. springer, 2020. 2 [ 3 ] armen avetisyan, christopher xie, henry howard - jenkins, tsun - yi yang, samir aroudj, suvam patra, fuyang zhang, duncan frost, luke holland, campbell orme, et al. scenescript : reconstructing scenes with an autoregressive structured language model. in european conference on computer vision, pages 247 – 263. springer, 2024. 2 [ 4 ] arjun balasingam, joseph chandler, chenning li, zhoutong zhang, and hari balakrishnan. drivetrack : a benchmark for long - range point tracking in real - world videos. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 22488 – 22497, 2024. 5, 7 [ 5 ] tali basha, yael moses, and nahum kiryati. multi - view scene flow estimation : a view centered variational approach. international journal of computer vision, 101 : 6 – 21, 2013. 2 10", "token_count": 418}
{"doc_id": "arxiv_251210935_any4d", "page": 11, "frag_id": 0, "text": "[ 6 ] berta bescos, jos´e m f´acil, javier civera, and jos´e neira. dynaslam : tracking, mapping, and inpainting in dynamic scenes. ieee robotics and automation letters, 3 ( 4 ) : 4076 – 4083, 2018. 2 [ 7 ] yohann cabon, naila murray, and martin humenberger. virtual kitti 2. arxiv preprint arxiv : 2001. 10773, 2020. 2, 5, 6 [ 8 ] kaihua chen, tarasha khurana, and deva ramanan. reconstruct, inpaint, finetune : dynamic novel - view synthesis from monocular videos. arxiv preprint arxiv : 2507. 12646, 2025. 1 [ 9 ] sili chen, hengkai guo, shengnan zhu, feihu zhang, zilong huang, jiashi feng, and bingyi kang. video depth anything : consistent depth estimation for super - long videos. in proceedings of the computer vision and pattern recognition conference, pages 22831 – 22840, 2025. 7 [ 10 ] tsai - shien chen, aliaksandr siarohin, willi menapace, ekaterina deyneka, hsiang - wei chao, byung eun jeon, yuwei fang, hsin - ying lee, jian ren, ming - hsuan yang, et al. panda - 70m : captioning 70m videos with multiple cross - modality teachers. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 13320 – 13331, 2024. 2 [ 11 ] nathaniel chodosh, deva ramanan, and simon lucey. reevaluating lidar scene flow. in proceedings of the ieee / cvf winter conference on applications of computer vision ( wacv ), pages 6005 – 6015, 2024. 2 [ 12 ] tri dao. flashattention - 2 : faster attention with better parallelism and work partitioning. arxiv preprint arxiv : 2307. 08691, 2023. 5 [ 13 ] frank dellaert, michael kaess, et al. factor graphs for robot perception. foundations and trends® in robotics, 6 ( 1 - 2 ) : 1 –", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 11, "frag_id": 1, "text": "– 13331, 2024. 2 [ 11 ] nathaniel chodosh, deva ramanan, and simon lucey. reevaluating lidar scene flow. in proceedings of the ieee / cvf winter conference on applications of computer vision ( wacv ), pages 6005 – 6015, 2024. 2 [ 12 ] tri dao. flashattention - 2 : faster attention with better parallelism and work partitioning. arxiv preprint arxiv : 2307. 08691, 2023. 5 [ 13 ] frank dellaert, michael kaess, et al. factor graphs for robot perception. foundations and trends® in robotics, 6 ( 1 - 2 ) : 1 – 139, 2017. 2 [ 14 ] bardienus p duisterhof, jan oberst, bowen wen, stan birchfield, deva ramanan, and jeffrey ichnowski. rayst3r : predicting novel depth maps for zero - shot object completion. arxiv preprint arxiv : 2506. 05285, 2025. 2 [ 15 ] jakob engel, thomas sch¨ops, and daniel cremers. lsdslam : large - scale direct monocular slam. in european conference on computer vision, pages 834 – 849. springer, 2014. 2 [ 16 ] haiwen feng, junyi zhang, qianqian wang, yufei ye, pengcheng yu, michael j black, trevor darrell, and angjoo kanazawa. st4rtrack : simultaneous 4d reconstruction and tracking in the world. arxiv preprint arxiv : 2504. 13152, 2025. 2, 3, 5, 6, 7, 8, 9 [ 17 ] andreas geiger, philip lenz, christoph stiller, and raquel urtasun. vision meets robotics : the kitti dataset. the international journal of robotics research, 32 ( 11 ) : 1231 – 1237, 2013. 7 [ 18 ] klaus greff, francois belletti, lucas beyer, carl doersch, yilun du, daniel duckworth, david j fleet, dan gnanapragasam, florian golemo, charles herrmann, thomas kipf, abhijit kundu, dmitry lagun, issam laradji, hsuehti ( derek ) liu, henning meyer", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 11, "frag_id": 2, "text": "##5. 2, 3, 5, 6, 7, 8, 9 [ 17 ] andreas geiger, philip lenz, christoph stiller, and raquel urtasun. vision meets robotics : the kitti dataset. the international journal of robotics research, 32 ( 11 ) : 1231 – 1237, 2013. 7 [ 18 ] klaus greff, francois belletti, lucas beyer, carl doersch, yilun du, daniel duckworth, david j fleet, dan gnanapragasam, florian golemo, charles herrmann, thomas kipf, abhijit kundu, dmitry lagun, issam laradji, hsuehti ( derek ) liu, henning meyer, yishu miao, derek nowrouzezahrai, cengiz oztireli, etienne pot, noha radwan, daniel rebain, sara sabour, mehdi s. m. sajjadi, matan sela, vincent sitzmann, austin stone, deqing sun, suhani vora, ziyu wang, tianhao wu, kwang moo yi, fangcheng zhong, and andrea tagliasacchi. kubric : a scalable dataset generator. in proceedings of the ieee conference on computer vision and pattern recognition ( cvpr ), 2022. 2, 5 [ 19 ] jisang han, honggyu an, jaewoo jung, takuya narihira, junyoung seo, kazumi fukuda, chaehyun kim, sunghwan hong, yuki mitsufuji, and seungryong kim. [UNK] 2ust3r : enhancing 3d reconstruction with 4d pointmaps for dynamic scenes. arxiv preprint arxiv : 2504. 06264, 2025. 3 [ 20 ] adam w harley, zhaoyuan fang, and katerina fragkiadaki. particle video revisited : tracking through occlusions using point trajectories. in european conference on computer vision, pages 59 – 75. springer, 2022. 3 [ 21 ] mina henein, jun zhang, robert mahony, and viorela ila. dynamic slam : the need for speed. in 2020 ieee international conference on robotics and automation ( icra ), pages 2123 – 2129. ieee, 2020. 2 [ 22 ] wenbo", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 11, "frag_id": 3, "text": ": enhancing 3d reconstruction with 4d pointmaps for dynamic scenes. arxiv preprint arxiv : 2504. 06264, 2025. 3 [ 20 ] adam w harley, zhaoyuan fang, and katerina fragkiadaki. particle video revisited : tracking through occlusions using point trajectories. in european conference on computer vision, pages 59 – 75. springer, 2022. 3 [ 21 ] mina henein, jun zhang, robert mahony, and viorela ila. dynamic slam : the need for speed. in 2020 ieee international conference on robotics and automation ( icra ), pages 2123 – 2129. ieee, 2020. 2 [ 22 ] wenbo hu, xiangjun gao, xiaoyu li, sijie zhao, xiaodong cun, yong zhang, long quan, and ying shan. depthcrafter : generating consistent long depth sequences for open - world videos. in proceedings of the computer vision and pattern recognition conference, pages 2005 – 2015, 2025. 7 [ 23 ] yuan - ting hu, jiahong wang, raymond a yeh, and alexander g schwing. sail - vos 3d : a synthetic dataset and baselines for object detection and 3d mesh reconstruction from video data. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 1418 – 1428, 2021. 5 [ 24 ] junsheng huang, shengyu hao, bocheng hu, and gaoang wang. understanding dynamic scenes in ego centric 4d point clouds. arxiv preprint arxiv : 2508. 07251, 2025. 1 [ 25 ] fr´ed´eric huguet and fr´ed´eric devernay. a variational method for scene flow estimation from stereo sequences. in 2007 ieee 11th international conference on computer vision, pages 1 – 7. ieee, 2007. 2 [ 26 ] wonbong jang, philippe weinzaepfel, vincent leroy, lourdes agapito, and jerome revaud. pow3r : empowering unconstrained 3d reconstruction with camera and scene priors. arxiv preprint arxiv : 2503. 17316, 2025. 2 [ 27 ] linyi jin, richard tucker, zhengqi li, david fouhey, noah snavely, and aleksander holynski. stereo4d", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 11, "frag_id": 4, "text": "##´eric huguet and fr´ed´eric devernay. a variational method for scene flow estimation from stereo sequences. in 2007 ieee 11th international conference on computer vision, pages 1 – 7. ieee, 2007. 2 [ 26 ] wonbong jang, philippe weinzaepfel, vincent leroy, lourdes agapito, and jerome revaud. pow3r : empowering unconstrained 3d reconstruction with camera and scene priors. arxiv preprint arxiv : 2503. 17316, 2025. 2 [ 27 ] linyi jin, richard tucker, zhengqi li, david fouhey, noah snavely, and aleksander holynski. stereo4d : learning how things move in 3d from internet stereo videos. arxiv preprint, 2024. 2, 3, 5 [ 28 ] nikita karaev, ignacio rocco, benjamin graham, natalia neverova, andrea vedaldi, and christian rupprecht. dynamicstereo : consistent dynamic depth from stereo videos. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 13229 – 13239, 2023. 5, 6, 7 [ 29 ] nikita karaev, iurii makarov, jianyuan wang, natalia neverova, andrea vedaldi, and christian rupprecht. cotracker3 : simpler and better point tracking by pseudolabelling real videos. in arxiv, 2024. 3, 5, 6 [ 30 ] nikita karaev, ignacio rocco, benjamin graham, natalia neverova, andrea vedaldi, and christian rupprecht. cotracker : it is better to track together. in proc. eccv, 2024. 3 11", "token_count": 362}
{"doc_id": "arxiv_251210935_any4d", "page": 12, "frag_id": 0, "text": "[ 31 ] nikhil keetha, jay karhade, krishna murthy jatavallabhula, gengshan yang, sebastian scherer, deva ramanan, and jonathon luiten. splatam : splat track & map 3d gaussians for dense rgb - d slam. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 21357 – 21366, 2024. 2 [ 32 ] nikhil keetha, norman m¨uller, johannes sch¨onberger, lorenzo porzi, yuchen zhang, tobias fischer, arno knapitsch, duncan zauss, ethan weber, nelson antunes, et al. mapanything : universal feed - forward metric 3d reconstruction. in 2026 international conference on 3d vision ( 3dv ). ieee, 2026. 2, 3, 4, 5, 6, 7 [ 33 ] georg klein and david murray. parallel tracking and mapping for small ar workspaces. in 2007 6th ieee and acm international symposium on mixed and augmented reality, pages 225 – 234. ieee, 2007. 2 [ 34 ] johannes kopf, xuejian rong, and jia - bin huang. robust consistent video depth estimation. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 1611 – 1621, 2021. 2 [ 35 ] skanda koppula, ignacio rocco, yi yang, joe heyward, [UNK] carreira, andrew zisserman, gabriel brostow, and carl doersch. tapvid - 3d : a benchmark for tracking any point in 3d, 2024. 3, 6, 7 [ 36 ] suryansh kumar, yuchao dai, and hongdong li. monocular dense 3d reconstruction of a complex dynamic scene from two perspective frames. in proceedings of the ieee international conference on computer vision, pages 4649 – 4657, 2017. 2 [ 37 ] jiahui lei, yijia weng, adam w harley, leonidas guibas, and kostas daniilidis. mosca : dynamic gaussian fusion from casual videos via 4d motion scaffolds. in proceedings of the computer vision and pattern recognition conference, pages 6165 – 6177, 2025. 2 [ 38 ] vincent leroy, yohann cabon, and [UNK] revaud. grounding image matching in 3d with mast3r.", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 12, "frag_id": 1, "text": "##yansh kumar, yuchao dai, and hongdong li. monocular dense 3d reconstruction of a complex dynamic scene from two perspective frames. in proceedings of the ieee international conference on computer vision, pages 4649 – 4657, 2017. 2 [ 37 ] jiahui lei, yijia weng, adam w harley, leonidas guibas, and kostas daniilidis. mosca : dynamic gaussian fusion from casual videos via 4d motion scaffolds. in proceedings of the computer vision and pattern recognition conference, pages 6165 – 6177, 2025. 2 [ 38 ] vincent leroy, yohann cabon, and [UNK] revaud. grounding image matching in 3d with mast3r. in european conference on computer vision, pages 71 – 91. springer, 2024. 6 [ 39 ] zhengqi li and noah snavely. megadepth : learning singleview depth prediction from internet photos. in proceedings of the ieee conference on computer vision and pattern recognition, pages 2041 – 2050, 2018. 5 [ 40 ] zhengqi li, richard tucker, forrester cole, qianqian wang, linyi jin, vickie ye, angjoo kanazawa, aleksander holynski, and noah snavely. megasam : accurate, fast, and robust structure and motion from casual dynamic videos. arxiv preprint arxiv : 2412. 04463, 2024. 2, 7 [ 41 ] chenguo lin, yuchen lin, panwang pan, yifan yu, honglei yan, katerina fragkiadaki, and yadong mu. movies : motion - aware 4d dynamic view synthesis in one second. arxiv preprint arxiv : 2507. 10065, 2025. 1, 2, 3 [ 42 ] xingyu liu, charles r qi, and leonidas j guibas. flownet3d : learning scene flow in 3d point clouds. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 529 – 537, 2019. 2 [ 43 ] xinhang liu, yuxi xiao, donny y chen, jiashi feng, yuwing tai, chi - keung tang, and bingyi kang. trace anything : representing any video in 4d via trajectory fields. arxiv preprint arxiv : 2510.", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 12, "frag_id": 2, "text": "dynamic view synthesis in one second. arxiv preprint arxiv : 2507. 10065, 2025. 1, 2, 3 [ 42 ] xingyu liu, charles r qi, and leonidas j guibas. flownet3d : learning scene flow in 3d point clouds. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 529 – 537, 2019. 2 [ 43 ] xinhang liu, yuxi xiao, donny y chen, jiashi feng, yuwing tai, chi - keung tang, and bingyi kang. trace anything : representing any video in 4d via trajectory fields. arxiv preprint arxiv : 2510. 13802, 2025. 3 [ 44 ] zeyi liu, shuang li, eric cousineau, siyuan feng, benjamin burchfiel, and shuran song. geometry - aware 4d video generation for robot manipulation. arxiv preprint arxiv : 2507. 01099, 2025. 2 [ 45 ] hidenobu matsuki, gwangbin bae, and andrew j davison. 4dtam : non - rigid tracking and mapping via dynamic surface gaussians. in proceedings of the computer vision and pattern recognition conference, pages 26921 – 26932, 2025. 2 [ 46 ] nikolaus mayer, eddy ilg, philip hausser, philipp fischer, daniel cremers, alexey dosovitskiy, and thomas brox. a large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. in proceedings of the ieee conference on computer vision and pattern recognition, pages 4040 – 4048, 2016. 7 [ 47 ] lukas mehl, jenny schmalfuss, azin jahedi, yaroslava nalivayko, and andr´es bruhn. spring : a high - resolution highdetail dataset and benchmark for scene flow, optical flow and stereo. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 4981 – 4991, 2023. 2 [ 48 ] moritz menze and andreas geiger. object scene flow for autonomous vehicles. in conference on computer vision and pattern recognition ( cvpr ), 2015. 2 [ 49 ] himangi mittal, brian ok", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 12, "frag_id": 3, "text": "vision and pattern recognition, pages 4040 – 4048, 2016. 7 [ 47 ] lukas mehl, jenny schmalfuss, azin jahedi, yaroslava nalivayko, and andr´es bruhn. spring : a high - resolution highdetail dataset and benchmark for scene flow, optical flow and stereo. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 4981 – 4991, 2023. 2 [ 48 ] moritz menze and andreas geiger. object scene flow for autonomous vehicles. in conference on computer vision and pattern recognition ( cvpr ), 2015. 2 [ 49 ] himangi mittal, brian okorn, and david held. just go with the flow : self - supervised scene flow estimation. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 11177 – 11185, 2020. 2 [ 50 ] raul mur - artal and juan d tard´os. orb - slam2 : an opensource slam system for monocular, stereo, and rgb - d cameras. ieee transactions on robotics, 33 ( 5 ) : 1255 – 1262, 2017. 2 [ 51 ] tuan duc ngo, peiye zhuang, chuang gan, evangelos kalogerakis, sergey tulyakov, hsin - ying lee, and chaoyang wang. delta : dense efficient long - range 3d tracking for any video. arxiv preprint arxiv : 2410. 24211, 2024. 3 [ 52 ] dantong niu, yuvan sharma, haoru xue, giscard biamby, junyi zhang, ziteng ji, trevor darrell, and roei herzig. pretraining auto - regressive robotic models with 4d representations. arxiv preprint arxiv : 2502. 13142, 2025. 2 [ 53 ] maxime oquab, timoth´ee darcet, th´eo moutakanni, huy vo, marc szafraniec, vasil khalidov, pierre fernandez, daniel haziza, francisco massa, alaaeldin el - nouby, et al. dinov2 : learning robust visual features without supervision. arxiv preprint arxiv : 2304.", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 12, "frag_id": 4, "text": "giscard biamby, junyi zhang, ziteng ji, trevor darrell, and roei herzig. pretraining auto - regressive robotic models with 4d representations. arxiv preprint arxiv : 2502. 13142, 2025. 2 [ 53 ] maxime oquab, timoth´ee darcet, th´eo moutakanni, huy vo, marc szafraniec, vasil khalidov, pierre fernandez, daniel haziza, francisco massa, alaaeldin el - nouby, et al. dinov2 : learning robust visual features without supervision. arxiv preprint arxiv : 2304. 07193, 2023. 4 [ 54 ] emanuele palazzolo, jens behley, philipp lottes, philippe giguere, and cyrill stachniss. refusion : 3d reconstruction in dynamic environments for rgb - d cameras exploiting residuals. in 2019 ieee / rsj international conference on intelligent robots and systems ( iros ), pages 7855 – 7862. ieee, 2019. 7 [ 55 ] jean - philippe pons, renaud keriven, and olivier faugeras. multi - view stereo reconstruction and scene flow estimation with a global image - based matching score. international journal of computer vision, 72 : 179 – 193, 2007. 2 [ 56 ] gilles puy, alexandre boulch, and renaud marlet. flot : scene flow on point clouds guided by optimal transport. in european conference on computer vision, 2020. 2 12", "token_count": 334}
{"doc_id": "arxiv_251210935_any4d", "page": 13, "frag_id": 0, "text": "[ 57 ] yuheng qiu, chen wang, wenshan wang, mina henein, and sebastian scherer. airdos : dynamic slam benefits from articulated objects. in 2022 international conference on robotics and automation ( icra ), pages 8047 – 8053. ieee, 2022. 2 [ 58 ] ren´e ranftl, alexey bochkovskiy, and vladlen koltun. vision transformers for dense prediction. in proceedings of the ieee / cvf international conference on computer vision, pages 12179 – 12188, 2021. 2, 5 [ 59 ] ren´e ranftl, katrin lasinger, david hafner, konrad schindler, and vladlen koltun. towards robust monocular depth estimation : mixing datasets for zero - shot cross - dataset transfer. ieee transactions on pattern analysis and machine intelligence, 44 ( 3 ), 2022. 2 [ 60 ] jiawei ren, kevin xie, ashkan mirzaei, hanxue liang, xiaohui zeng, karsten kreis, ziwei liu, antonio torralba, sanja fidler, seung wook kim, and huan ling. l4gm : large 4d gaussian reconstruction model. in advances in neural information processing systems, 2024. 2 [ 61 ] p. sand and s. teller. particle video : long - range motion estimation using point trajectories. in 2006 ieee computer society conference on computer vision and pattern recognition ( cvpr ’ 06 ), pages 2195 – 2202, 2006. 3 [ 62 ] johannes l schonberger and jan - michael frahm. structurefrom - motion revisited. in proceedings of the ieee conference on computer vision and pattern recognition, pages 4104 – 4113, 2016. 2 [ 63 ] jenny seidenschwarz, qunjie zhou, bardienus p duisterhof, deva ramanan, and laura leal - taix´e. dynomo : online point tracking by dynamic online monocular gaussian reconstruction. in 2025 international conference on 3d vision ( 3dv ), pages 1012 – 1021. ieee, 2025. 2 [ 64 ] steven m seitz, brian curless, james diebel, daniel scharstein, and richard szeliski. a comparison and evaluation of multi - view stereo reconstruction algorithms. in", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 13, "frag_id": 1, "text": "motion revisited. in proceedings of the ieee conference on computer vision and pattern recognition, pages 4104 – 4113, 2016. 2 [ 63 ] jenny seidenschwarz, qunjie zhou, bardienus p duisterhof, deva ramanan, and laura leal - taix´e. dynomo : online point tracking by dynamic online monocular gaussian reconstruction. in 2025 international conference on 3d vision ( 3dv ), pages 1012 – 1021. ieee, 2025. 2 [ 64 ] steven m seitz, brian curless, james diebel, daniel scharstein, and richard szeliski. a comparison and evaluation of multi - view stereo reconstruction algorithms. in 2006 ieee computer society conference on computer vision and pattern recognition ( cvpr ’ 06 ), pages 519 – 528. ieee, 2006. 2 [ 65 ] akash sharma, wei dong, and michael kaess. compositional and scalable object slam. in 2021 ieee international conference on robotics and automation ( icra ), pages 11626 – 11632. ieee, 2021. 2 [ 66 ] edgar sucar, zihang lai, eldar insafutdinov, and andrea vedaldi. dynamic point maps : a versatile representation for dynamic 3d reconstruction. arxiv preprint arxiv : 2503. 16318, 2025. 3 [ 67 ] zachary teed and jia deng. raft : recurrent all - pairs field transforms for optical flow. in computer vision – eccv 2020 : 16th european conference, glasgow, uk, august 23 – 28, 2020, proceedings, part ii 16, pages 402 – 419. springer, 2020. 2 [ 68 ] zachary teed and jia deng. droid - slam : deep visual slam for monocular, stereo, and rgb - d cameras. advances in neural information processing systems, 34 : 16558 – 16569, 2021. 2 [ 69 ] zachary teed and jia deng. raft - 3d : scene flow using rigid - motion embeddings. in proceedings of the ieee / cvf conference on computer vision and pattern recognition ( cvpr ), 2021. 2, 6 [ 70 ] joachim tesch, giorgio becherini, prerana achar, anastasios yiannakidis, muhammed kocabas, priyanka patel, and michael j. black. bedlam2. 0 :", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 13, "frag_id": 2, "text": "2 [ 68 ] zachary teed and jia deng. droid - slam : deep visual slam for monocular, stereo, and rgb - d cameras. advances in neural information processing systems, 34 : 16558 – 16569, 2021. 2 [ 69 ] zachary teed and jia deng. raft - 3d : scene flow using rigid - motion embeddings. in proceedings of the ieee / cvf conference on computer vision and pattern recognition ( cvpr ), 2021. 2, 6 [ 70 ] joachim tesch, giorgio becherini, prerana achar, anastasios yiannakidis, muhammed kocabas, priyanka patel, and michael j. black. bedlam2. 0 : synthetic humans and cameras in motion. in the thirty - ninth annual conference on neural information processing systems datasets and benchmarks track, 2025. 8 [ 71 ] bill triggs, philip f mclauchlan, richard i hartley, and andrew w fitzgibbon. bundle adjustment — a modern synthesis. in international workshop on vision algorithms, pages 298 – 372. springer, 1999. 2 [ 72 ] basile van hoorick, rundi wu, ege ozguroglu, kyle sargent, ruoshi liu, pavel tokmakov, achal dave, changxi zheng, and carl vondrick. generative camera dolly : extreme monocular dynamic novel view synthesis. in european conference on computer vision, pages 313 – 331. springer, 2024. 1, 6 [ 73 ] basile van hoorick, rundi wu, ege ozguroglu, kyle sargent, ruoshi liu, pavel tokmakov, achal dave, changxi zheng, and carl vondrick. generative camera dolly : extreme monocular dynamic novel view synthesis. in european conference on computer vision ( eccv ), 2024. 5 [ 74 ] kyle vedder, neehar peri, ishan khatri, siyi li, eric eaton, mehmet kocamaz, yue wang, zhiding yu, deva ramanan, and joachim pehserl. neural eulerian scene flow fields. arxiv preprint arxiv : 2410. 02031, 2024. 2 [ 75 ] sundar vedula, simon baker, peter rander, robert collins, and takeo kanade. three - dimensional scene flow. in", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 13, "frag_id": 3, "text": "##xi zheng, and carl vondrick. generative camera dolly : extreme monocular dynamic novel view synthesis. in european conference on computer vision ( eccv ), 2024. 5 [ 74 ] kyle vedder, neehar peri, ishan khatri, siyi li, eric eaton, mehmet kocamaz, yue wang, zhiding yu, deva ramanan, and joachim pehserl. neural eulerian scene flow fields. arxiv preprint arxiv : 2410. 02031, 2024. 2 [ 75 ] sundar vedula, simon baker, peter rander, robert collins, and takeo kanade. three - dimensional scene flow. in proceedings of the seventh ieee international conference on computer vision, pages 722 – 729. ieee, 1999. 2 [ 76 ] bo wang, jian li, yang yu, li liu, zhenping sun, and dewen hu. scenetracker : long - term scene flow estimation network. ieee transactions on pattern analysis and machine intelligence, 2025. 6, 7 [ 77 ] jianyuan wang, minghao chen, nikita karaev, andrea vedaldi, christian rupprecht, and david novotny. vggt : visual geometry grounded transformer. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, 2025. 2, 4, 6, 7 [ 78 ] qianqian wang, vickie ye, hang gao, weijia zeng, jake austin, zhengqi li, and angjoo kanazawa. shape of motion : 4d reconstruction from a single video. in international conference on computer vision ( iccv ), 2025. 2 [ 79 ] qianqian wang, yifei zhang, aleksander holynski, alexei a efros, and angjoo kanazawa. continuous 3d perception model with persistent state. in proceedings of the computer vision and pattern recognition conference, pages 10510 – 10522, 2025. 7 [ 80 ] shuzhe wang, vincent leroy, yohann cabon, boris chidlovskii, and jerome revaud. dust3r : geometric 3d vision made easy. in proceedings of the ieee / cvf conference on computer vision and pattern recognition ( cvpr ), pages 20697 – 20709, 2024. 2, 3, 6, 7 [", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 13, "frag_id": 4, "text": "), 2025. 2 [ 79 ] qianqian wang, yifei zhang, aleksander holynski, alexei a efros, and angjoo kanazawa. continuous 3d perception model with persistent state. in proceedings of the computer vision and pattern recognition conference, pages 10510 – 10522, 2025. 7 [ 80 ] shuzhe wang, vincent leroy, yohann cabon, boris chidlovskii, and jerome revaud. dust3r : geometric 3d vision made easy. in proceedings of the ieee / cvf conference on computer vision and pattern recognition ( cvpr ), pages 20697 – 20709, 2024. 2, 3, 6, 7 [ 81 ] yue wang, yongbin sun, ziwei liu, sanjay e sarma, michael m bronstein, and justin m solomon. dynamic graph cnn for learning on point clouds. acm transactions on graphics ( tog ), 38 ( 5 ) : 1 – 12, 2019. 2 13", "token_count": 211}
{"doc_id": "arxiv_251210935_any4d", "page": 14, "frag_id": 0, "text": "[ 82 ] yihan wang, lahav lipson, and jia deng. sea - raft : simple, efficient, accurate raft for optical flow. in european conference on computer vision, pages 36 – 54. springer, 2024. 7 [ 83 ] yifan wang, jianjun zhou, haoyi zhu, wenzheng chang, yang zhou, zizun li, junyi chen, jiangmiao pang, chunhua shen, and tong he. π3 : scalable permutation - equivariant visual geometry learning. arxiv preprint arxiv : 2507. 13347, 2025. 2, 8 [ 84 ] rundi wu, ruiqi gao, ben poole, alex trevithick, changxi zheng, jonathan t barron, and aleksander holynski. cat4d : create anything in 4d with multi - view video diffusion models. arxiv preprint arxiv : 2411. 18613, 2024. 1, 2 [ 85 ] wenxuan wu, zhi yuan wang, zhuwen li, wei liu, and li fuxin. pointpwc - net : cost volume on point clouds for ( self - ) supervised scene flow estimation. in european conference on computer vision, pages 88 – 107. springer, 2020. 2 [ 86 ] yuxi xiao, qianqian wang, shangzhan zhang, nan xue, sida peng, yujun shen, and xiaowei zhou. spatialtracker : tracking any 2d pixels in 3d space. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 20406 – 20417, 2024. 3, 8 [ 87 ] yuxi xiao, jianyuan wang, nan xue, nikita karaev, iurii makarov, bingyi kang, xin zhu, hujun bao, yujun shen, and xiaowei zhou. spatialtrackerv2 : 3d point tracking made easy. in iccv, 2025. 3, 5, 6, 7 [ 88 ] lihe yang, bingyi kang, zilong huang, xiaogang xu, jiashi feng, and hengshuang zhao. depth anything : unleashing the power of large - scale unlabeled data. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 10371 – 10381, 2024", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 14, "frag_id": 1, "text": "8 [ 87 ] yuxi xiao, jianyuan wang, nan xue, nikita karaev, iurii makarov, bingyi kang, xin zhu, hujun bao, yujun shen, and xiaowei zhou. spatialtrackerv2 : 3d point tracking made easy. in iccv, 2025. 3, 5, 6, 7 [ 88 ] lihe yang, bingyi kang, zilong huang, xiaogang xu, jiashi feng, and hengshuang zhao. depth anything : unleashing the power of large - scale unlabeled data. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 10371 – 10381, 2024. 2 [ 89 ] yao yao, zixin luo, shiwei li, jingyang zhang, yufan ren, lei zhou, tian fang, and long quan. blendedmvs : a largescale dataset for generalized multi - view stereo networks. in proceedings of the ieee / cvf conference on computer vision and pattern recognition, pages 1790 – 1799, 2020. 5 [ 90 ] chandan yeshwanth, yueh - cheng liu, matthias nießner, and angela dai. scannet + + : a high - fidelity dataset of 3d indoor scenes. in proceedings of the ieee / cvf international conference on computer vision, pages 12 – 22, 2023. 5 [ 91 ] bowei zhang, lei ke, adam w harley, and katerina fragkiadaki. tapip3d : tracking any point in persistent 3d geometry. arxiv preprint arxiv : 2504. 14717, 2025. 3 [ 92 ] junyi zhang, charles herrmann, junhwa hur, varun jampani, trevor darrell, forrester cole, deqing sun, and minghsuan yang. monst3r : a simple approach for estimating geometry in the presence of motion. arxiv preprint arxiv : 2410. 03825, 2024. 2, 6, 7 [ 93 ] songyan zhang, yongtao ge, jinyuan tian, guangkai xu, hao chen, chen lv, and chunhua shen. pomato : marrying pointmap matching with temporal motion for dynamic 3d reconstruction. arxiv preprint arxiv : 2504. 05692, 2025.", "token_count": 500}
{"doc_id": "arxiv_251210935_any4d", "page": 14, "frag_id": 2, "text": "] junyi zhang, charles herrmann, junhwa hur, varun jampani, trevor darrell, forrester cole, deqing sun, and minghsuan yang. monst3r : a simple approach for estimating geometry in the presence of motion. arxiv preprint arxiv : 2410. 03825, 2024. 2, 6, 7 [ 93 ] songyan zhang, yongtao ge, jinyuan tian, guangkai xu, hao chen, chen lv, and chunhua shen. pomato : marrying pointmap matching with temporal motion for dynamic 3d reconstruction. arxiv preprint arxiv : 2504. 05692, 2025. 3 [ 94 ] yuchen zhang, nikhil keetha, chenwei lyu, bhuvan jhamb, yutian chen, yuheng qiu, jay karhade, shreyas jha, yaoyu hu, deva ramanan, et al. ufm : a simple path towards unified dense correspondence with flow. advances in neural information processing systems, 2025. 2 [ 95 ] yang zheng, adam w. harley, bokui shen, gordon wetzstein, and leonidas j. guibas. pointodyssey : a large - scale synthetic dataset for long - term point tracking. in iccv, 2023. 2, 5 [ 96 ] hanyu zhou and gim hee lee. llava - 4d : embedding spatiotemporal prompt into lmms for 4d scene understanding. arxiv preprint arxiv : 2505. 12253, 2025. 1 14", "token_count": 349}
