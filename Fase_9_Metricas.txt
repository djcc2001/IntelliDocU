==================================================
Fase 9 – Evaluación y Métricas
==================================================

Módulo:
    src/common/evaluation/

Archivos incluidos:
    - metrics.py
    - evaluate.py
    - evaluate_all.py

==================================================
1. Archivo: metrics.py
==================================================

Propósito:
    Define las métricas utilizadas para evaluar el desempeño de las distintas
    versiones experimentales del sistema IntelliDocU sobre un conjunto de
    preguntas y respuestas de referencia.

--------------------------------------------------
Funciones implementadas
--------------------------------------------------

1. exact_match(prediction, reference)

    - Retorna 1 si la respuesta generada coincide exactamente con la
      respuesta de referencia.
    - La comparación ignora diferencias de mayúsculas y espacios extra.
    - Es una métrica estricta que solo contabiliza coincidencias exactas.

    Ejemplo:
        exact_match("SparseSwaps", "sparseSwaps") → 1


--------------------------------------------------
2. f1_score(prediction, reference)

    - Calcula la métrica F1 a nivel de tokens entre la predicción y la referencia.
    - Permite medir similitud parcial cuando la respuesta no coincide
      exactamente.
    - Es útil para evaluar respuestas parcialmente correctas.

    Ejemplo:
        f1_score("SparseSwaps algorithm", "SparseSwaps") → 0.6667


--------------------------------------------------
3. abstention_accuracy(predictions, references,
   abstention_token="It is not mentioned in the document.")

    - Mide la precisión de abstención del sistema.
    - Una abstención se considera correcta cuando:
        • La respuesta de referencia es exactamente el token de abstención.
        • La respuesta generada también es exactamente el token de abstención.
    - Esta métrica es especialmente relevante para versiones RAG que
      incorporan mecanismos explícitos de abstención ante preguntas imposibles.

==================================================
2. Archivo: evaluate.py
==================================================

Propósito:
    Evalúa un único archivo de predicciones frente a un archivo de respuestas
    correctas, calculando las métricas:
        - Exact Match
        - F1 Score
        - Abstention Accuracy

Uso:
    python src/common/evaluation/evaluate.py \
        --pred_file results/model_answers.json \
        --reference_file data/questions/answers.json

Salida:
    - Exact Match promedio
    - F1 Score promedio
    - Abstention Accuracy


==================================================
3. Archivo: evaluate_all.py
==================================================

Propósito:
    Permite evaluar múltiples modelos o configuraciones en una sola ejecución,
    mostrando una tabla comparativa de métricas.

Uso:
    python src/common/evaluation/evaluate_all.py --refs data/questions/answers.json --preds results/v1_baseline/baseline_answers.json results/v2_rag_basic/rag_basic_answers.json results/v3_rag_advanced/rag_advanced_answers.json

Salida:
    Tabla comparativa con:
        - Nombre del modelo (derivado del nombre del archivo)
        - Exact Match
        - F1 Score
        - Abstention Accuracy

==================================================
4. Formato de los archivos JSON
==================================================

Cada archivo de predicciones o referencias contiene una lista ordenada de
diccionarios. La posición de cada elemento corresponde a la misma pregunta
en el dataset.

Estructura de cada elemento:
    {
        "question_id": str,
        "doc_id": str,
        "question": str,
        "type": str,
        "answer": str
    }

Ejemplo:
[
  {
    "question_id": "q1",
    "doc_id": "arxiv_251210922_sparseswaps",
    "question": "What is SparseSwaps?",
    "type": "fact",
    "answer": "SparseSwaps is a graph algorithm."
  },
  {
    "question_id": "q2",
    "doc_id": "arxiv_251210922_sparseswaps",
    "question": "Does the paper mention quantum computing?",
    "type": "impossible",
    "answer": "It is not mentioned in the document."
  }
]

==================================================
5. Métricas reportadas
==================================================

- Exact Match (EM):
    Proporción de respuestas generadas que coinciden exactamente con la
    referencia.

- F1 Score:
    Promedio de la métrica F1 calculada a nivel de tokens entre predicción
    y referencia.

- Abstention Accuracy:
    Proporción de preguntas imposibles en las que el sistema se abstuvo
    correctamente utilizando el token de abstención definido.

==================================================
6. Consideraciones importantes
==================================================

- El modelo de lenguaje no es entrenado con este dataset.
- Las métricas evalúan exclusivamente la calidad de recuperación de contexto,
  grounding y generación de respuestas.
- Los scripts de evaluación son independientes del modelo utilizado y
  permiten comparaciones directas entre distintas versiones del sistema.
- La incorporación de abstención mejora la robustez del sistema ante
  preguntas sin respuesta en los documentos.
